<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-07T09:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Improvements to Distributional Sentence Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>18-21 October 2013. 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Interactive Computing</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
							<email>jacobe@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Interactive Computing</orgName>
								<orgName type="department" key="dep2">School of Interactive Computing Georgia Institute of Technology</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Improvements to Distributional Sentence Similarity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Seattle, Washington, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="891" to="896"/>
							<date type="published">18-21 October 2013. 2013</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Matrix and tensor factorization have been applied to a number of semantic relatedness tasks, including paraphrase identification. The key idea is that similarity in the latent space implies semantic relatedness. We describe three ways in which labeled data can improve the accuracy of these approaches on paraphrase classification. First, we design a new discriminative term-weighting metric called TF-KLD, which outperforms TF-IDF. Next, we show that using the latent representation from matrix factorization as features in a classification algorithm substantially improves accuracy. Finally, we combine latent features with fine-grained n-gram overlap features , yielding performance that is 3% more accurate than the prior state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Measuring the semantic similarity of short units of text is fundamental to many natural language processing tasks, from evaluating machine translation ( <ref type="bibr" target="#b6">Kauchak and Barzilay, 2006</ref>) to grouping redundant event mentions in social media <ref type="bibr" target="#b13">(Petrovi´cPetrovi´c et al., 2010</ref>). The task is challenging because of the infinitely diverse set of possible linguistic realizations for any idea <ref type="bibr">(Bhagat and Hovy, 2013)</ref>, and because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse.</p><p>Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. This can be accomplished by factoring a matrix or tensor of term-context counts <ref type="bibr">(Turney and Pan- tel, 2010)</ref>; proximity in the induced latent space has been shown to correlate with semantic similarity ( <ref type="bibr" target="#b10">Mihalcea et al., 2006</ref>). However, factoring the term-context matrix means throwing away a considerable amount of information, as the original matrix of size M × N (number of instances by number of features) is factored into two smaller matrices of size M × K and N × K, with K M, N . If the factorization does not take into account labeled data about semantic similarity, important information can be lost.</p><p>In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called TF-KLD, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task ( <ref type="bibr" target="#b1">Dolan et al., 2004</ref>), this method improves on both traditional TF-IDF and Weighted <ref type="bibr">Textual Matrix Factorization (WTMF;</ref><ref type="bibr" target="#b5">Guo and Diab, 2012</ref>). Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification. We then add "finegrained" features about the lexical similarity of the sentence pair. The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score ( <ref type="bibr" target="#b17">Wan et al., 2006;</ref><ref type="bibr" target="#b9">Madnani et al., 2012)</ref>, as well as string kernels ( <ref type="bibr">Bu et al., 2012)</ref>; (2) syntactic operations on the parse structure ( <ref type="bibr" target="#b18">Wu, 2005;</ref><ref type="bibr">Das and Smith, 2009)</ref>; and (3) distributional methods, such as latent semantic analysis (LSA; <ref type="bibr" target="#b7">Landauer et al., 1998)</ref>, which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives ( <ref type="bibr" target="#b6">Kauchak and Barzilay, 2006</ref>). Alternatively, <ref type="bibr">Blacoe and Lapata (2012)</ref> show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. <ref type="bibr" target="#b14">Socher et al. (2011)</ref> propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree.</p><p>We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by <ref type="bibr" target="#b10">Mihalcea et al. (2006)</ref> and <ref type="bibr" target="#b5">Guo and Diab (2012)</ref>, who treat sentences as pseudo-documents in an LSA framework, and identify paraphrases using similarity in the latent space. We show that the performance of such techniques can be improved dramatically by using supervised information to (1) reweight the individual distributional features and (2) learn the importance of each latent dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discriminative feature weighting</head><p>Distributional representations <ref type="bibr" target="#b16">(Turney and Pantel, 2010)</ref> can be induced from a co-occurrence matrix W ∈ R M ×N , where M is the number of instances and N is the number of distributional features. For paraphrase identification, each instance is a sentence; features may be unigrams, or may include higher-order n-grams or dependency pairs. By decomposing the matrix W, we hope to obtain a latent representation in which semantically-related sentences are similar. Singular value decomposition (SVD) is traditionally used to perform this factorization. However, recent work has demonstrated the robustness of nonnegative matrix factorization (NMF; <ref type="bibr" target="#b8">Lee and Seung, 2001</ref>) for text mining tasks ( <ref type="bibr" target="#b19">Xu et al., 2003;</ref><ref type="bibr">Arora et al., 2012)</ref>; the difference from SVD is the addition of a non-negativity constraint in the latent representation based on non-orthogonal basis.</p><p>While W may simply contain counts of distributional features, prior work has demonstrated the utility of reweighting these counts <ref type="bibr" target="#b16">(Turney and Pantel, 2010)</ref>. TF-IDF is a standard approach, as the inverse document frequency (IDF) term increases the importance of rare words, which may be more discriminative. <ref type="bibr" target="#b5">Guo and Diab (2012)</ref> show that applying a special weight to unseen words can further improvement performance on paraphrase identification.</p><p>We present a new weighting scheme, TF-KLD, based on supervised information. The key idea is to increase the weights of distributional features that are discriminative, and to decrease the weights of features that are not. Conceptually, this is similar to Linear Discriminant Analysis, a supervised feature weighting scheme for continuous data <ref type="bibr" target="#b11">(Murphy, 2012)</ref>.</p><p>More formally, we assume labeled sentence pairs of the form w</p><formula xml:id="formula_0">i , w<label>(1)</label></formula><p>i , r i , where w</p><formula xml:id="formula_2">(1) i</formula><p>is the binarized vector of distributional features for the first sentence, w</p><formula xml:id="formula_3">(2) i</formula><p>is the binarized vector of distributional features for the second sentence, and r i ∈ {0, 1} indicates whether they are labeled as a paraphrase pair. Assuming the order of the sentences within the pair is irrelevant, then for k-th distributional feature, we define two Bernoulli distributions:</p><formula xml:id="formula_4">• p k = P (w (1) ik |w (2) ik = 1, r i = 1)</formula><p>. This is the probability that sentence w</p><p>(1) i contains feature k, given that k appears in w <ref type="bibr">(2)</ref> i and the two sentences are labeled as paraphrases, r i = 1.</p><p>• q k = P (w</p><formula xml:id="formula_5">(1) ik |w (2) ik = 1, r i = 0)</formula><p>. This is the probability that sentence w</p><p>(1) i contains feature k, given that k appears in w <ref type="bibr">(2)</ref> i and the two sentences are labeled as not paraphrases, r i = 0.</p><formula xml:id="formula_6">The Kullback-Leibler divergence KL(p k ||q k ) = x p k (x) log p k (x) q k (x)</formula><p>is then a measure of the discriminability of feature k, and is guaranteed to be non- negative. <ref type="bibr">1</ref> We use this divergence to reweight the features in W before performing the matrix factorization. This has the effect of increasing the weights of features whose likelihood of appearing in a pair of sentences is strongly influenced by the paraphrase relationship between the two sentences. On the other hand, if p k = q k , then the KL-divergence will be zero, and the feature will be ignored in the matrix factorization. We name this weighting scheme TF-KLD, since it includes the term frequency and the KL-divergence.  <ref type="figure" target="#fig_0">Figure 1</ref> shows the distributions of these and other unigram features with respect to p k and 1 − q k . The diagonal line running through the middle of the plot indicates zero KL-divergence, so features on this line will be ignored. <ref type="table">Table 1</ref>: Fine-grained features for paraphrase classification, selected from prior work ( <ref type="bibr" target="#b17">Wan et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">unigram recall 2 unigram precision 3 bigram recall 4 bigram precision 5 dependency relation recall 6 dependency relation precision 7 BLEU recall 8 BLEU precision 9 Difference of sentence length 10 Tree-editing distance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised classification</head><p>While previous work has performed paraphrase classification using distance or similarity in the latent space <ref type="bibr" target="#b5">(Guo and Diab, 2012;</ref><ref type="bibr" target="#b14">Socher et al., 2011</ref>), more direct supervision can be applied. Specifically, we convert the latent representations of a pair of sentences v 1 and v 2 into a sample vector,</p><formula xml:id="formula_7">s( v 1 , v 2 ) = v 1 + v 2 , | v 1 − v 2 | ,<label>(1)</label></formula><p>concatenating the element-wise sum v 1 + v 2 and absolute difference | v 1 − v 2 |. Note that s(·, ·) is symmetric, since s( v 1 , v 2 ) = s( v 2 , v 1 ). Given this representation, we can use any supervised classification algorithm. A further advantage of treating paraphrase as a supervised classification problem is that we can apply additional features besides the latent representation. We consider a subset of features identified by <ref type="bibr" target="#b17">Wan et al. (2006)</ref>, listed in <ref type="table">Table 1</ref>. These features mainly capture fine-grained similarity between sentences, for example by counting specific unigram and bigram overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our experiments test the utility of the TF-KLD weighting towards paraphrase classification, using the Microsoft Research Paraphrase Corpus ( <ref type="bibr" target="#b1">Dolan et al., 2004</ref>). The training set contains 2753 true paraphrase pairs and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairs, respectively.</p><p>The TF-KLD weights are constructed from only the training set, while matrix factorizations are per-formed on the entire corpus. Matrix factorization on both training and (unlabeled) test data can be viewed as a form of transductive learning ( <ref type="bibr" target="#b3">Gammerman et al., 1998</ref>), where we assume access to unlabeled test set instances. <ref type="bibr">2</ref> We also consider an inductive setting, where we construct the basis of the latent space from only the training set, and then project the test set onto this basis to find the corresponding latent representation. The performance differences between the transductive and inductive settings were generally between 0.5% and 1%, as noted in detail below. We reiterate that the TF-KLD weights are never computed from test set data.</p><p>Prior work on this dataset is described in section 2. To our knowledge, the current state-of-theart is a supervised system that combines several machine translation metrics ( <ref type="bibr" target="#b9">Madnani et al., 2012</ref>), but we also compare with state-of-the-art unsupervised matrix factorization work <ref type="bibr" target="#b5">(Guo and Diab, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Similarity-based classification</head><p>In the first experiment, we predict whether a pair of sentences is a paraphrase by measuring their cosine similarity in latent space, using a threshold for the classification boundary. As in prior work <ref type="bibr" target="#b5">(Guo and Diab, 2012)</ref>, the threshold is tuned on held-out training data. We consider two distributional feature sets: FEAT 1 , which includes unigrams; and FEAT 2 , which also includes bigrams and unlabeled dependency pairs obtained from MaltParser ( <ref type="bibr" target="#b12">Nivre et al., 2007)</ref>. To compare with <ref type="bibr" target="#b5">Guo and Diab (2012)</ref>, we set the latent dimensionality to K = 100, which was the same in their paper. Both SVD and NMF factorization are evaluated; in both cases, we minimize the Frobenius norm of the reconstruction error. <ref type="table" target="#tab_1">Table 2</ref> compares the accuracy of a number of different configurations. The transductive TF-KLD weighting yields the best overall accuracy, achieving 72.75% when combined with nonnegative matrix factorization. While NMF performs slightly better than SVD in both comparisons, the major difference is the performance of discriminative TF-KLD weighting, which outperforms TF-IDF regardless of the factorization technique. When we <ref type="bibr">2</ref> Another example of transductive learning in NLP is when <ref type="bibr" target="#b15">Turian et al. (2010)</ref> induced word representations from a corpus that included both training and test data for their downstream named entity recognition task. perform the matrix factorization on only the training data, the accuracy on the test set is 73.58%, with F1 score 80.55%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supervised classification</head><p>Next, we apply supervised classification, constructing sample vectors from the latent representation as shown in Equation 1. For classification, we choose a Support Vector Machine with a linear kernel <ref type="bibr" target="#b2">(Fan et al., 2008)</ref>, leaving a thorough comparison of classifiers for future work. The classifier parameter C is tuned on a development set comprising 20% of the original training set. <ref type="figure" target="#fig_2">Figure 2</ref> presents results for a range of latent dimensionalities. Supervised learning identifies the important dimensions in the latent space, yielding significantly better performance that the similaritybased classification from the previous experiment. In <ref type="table" target="#tab_2">Table 3</ref>, we compare against prior published work, using the held-out development set to select the best value of K (again, K = 400). The best result is from TF-KLD, with distributional features FEAT 2 , achieving 79.76% accuracy and 85.87% F1. This is well beyond all known prior results on this task. When we induce the latent basis from only the training data, we get 78.55% on accuracy and 84.59% F1, also better than the previous state-of-art.</p><p>Finally, we augment the distributional representation, concatenating the ten "fine-grained" features in <ref type="table">Table 1</ref>    racy now improves to 80.41%, with an F1 score of 85.96%. When the latent representation is induced from only the training data, the corresponding results are 79.94% on accuracy and 85.36% F1, again better than the previous state-of-the-art. These results show that the information captured by the distributional representation can still be augmented by more fine-grained traditional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented three ways in which labeled data can improve distributional measures of semantic similarity at the sentence level. The main innovation is TF-KLD, which discriminatively reweights the distributional features before factorization, so that discriminability impacts the induction of the latent representation. We then transform the latent representation into a sample vector for supervised learning, obtaining results that strongly outperform the prior state-of-the-art; adding fine-grained lexical features further increases performance. These ideas may have applicability in other semantic similarity tasks, and we are also eager to apply them to new, large-scale automatically-induced paraphrase corpora ( <ref type="bibr" target="#b4">Ganitkevitch et al., 2013</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conditional probabilities for a few handselected unigram features, with lines showing contours with identical KL-divergence. The probabilities are estimated based on the MSRPC training set (Dolan et al., 2004).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Taking the unigram feature not as an example, we have p k = [0.66, 0.34] and q k = [0.31, 0.69], for a KL-divergence of 0.25: the likelihood of this word being shared between two sentence is strongly de- pendent on whether the sentences are paraphrases. In contrast, the feature then has p k = [0.33, 0.67] and q k = [0.32, 0.68], for a KL-divergence of 3.9 × 10 −4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of feature and weighting combinations in the classification framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Similarity-based paraphrase identification accuracy. Results for WTMF are reprinted from the paper by Guo 
and Diab (2012). 

Acc. 
F1 
Most common class 
66.5 
79.9 
(Wan et al., 2006) 
75.6 
83.0 
(Das and Smith, 2009) 
73.9 
82.3 
(Das and Smith, 2009) with 18 features 
76.1 
82.7 
(Bu et al., 2012) 
76.3 not reported 
(Socher et al., 2011) 
76.8 
83.6 
(Madnani et al., 2012) 
77.4 
84.1 
FEAT 2 , TF-KLD, SVM 
79.76 
85.87 
FEAT 2 , TF-KLD, SVM, Fine-grained features 80.41 
85.96 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Supervised classification. Results from prior work are reprinted.</figDesc><table></table></figure>

			<note place="foot" n="1"> We obtain very similar results with the opposite divergence KL(q k ||p k ). However, the symmetric Jensen-Shannon divergence performs poorly.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful feedback, and Weiwei Guo for quickly answering questions about his implementation. This research was supported by a Google Faculty Research Award to the second author. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<editor>COL-ING</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning by transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodya</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fourteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling Sentences in the Latent Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Paraphrasing for automatic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introduction to Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrel</forename><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discource Processes</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for Non-Negative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Re-examining Machine Translation Metrics for Paraphrase Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Corpus-based and knowledge-based measures of text semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning: A Probabilistic Perspective</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MaltParser: A languageindependent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Streaming first story detection with application to twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Petrovi´cpetrovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic Pooling And Unfolding Recursive Autoencoders For Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word Representation: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using Dependency-based Features to Take the &quot;Para-farce&quot; out of Paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Workshop</title>
		<meeting>the Australasian Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing paraphrases and textual entailment using inversion transduction grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Document Clustering based on Non-Negative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
