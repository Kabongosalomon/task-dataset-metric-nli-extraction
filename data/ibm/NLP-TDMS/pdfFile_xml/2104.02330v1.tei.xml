<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Alignment Constraint for Continuous Sign Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuecong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Hao</surname></persName>
							<email>aiming.hao@vipl.ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
							<email>chaixiujuan@caas.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Agricultural Information Institute</orgName>
								<orgName type="institution">Chinese Academy of Agricultural Sciences</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Alignment Constraint for Continuous Sign Language Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented gestures from image sequences. To better train CSLR models, the iterative training scheme is widely adopted to alleviate the overfitting of the alignment model. Although the iterative training scheme can improve performance, it will also increase the training time. In this work, we revisit the overfitting problem in recent CTC-based CSLR works and attribute it to the insufficient training of the feature extractor. To solve this problem, we propose a Visual Alignment Constraint (VAC) to enhance the feature extractor with more alignment supervision. Specifically, the proposed VAC is composed of two auxiliary losses: one makes predictions based on visual features only, and the other aligns short-term visual and longterm contextual features. Moreover, we further propose two metrics to evaluate the contributions of the feature extractor and the alignment model, which provide evidence for the overfitting problem. The proposed VAC achieves competitive performance on two challenging CSLR datasets and experimental results show its effectiveness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sign Language is a complete and natural language that conveys information through both manual components (hand/arm gestures) and non-manual components (facial expressions, head movements, and body postures) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> with its own grammar and lexicon <ref type="bibr" target="#b41">[42]</ref>. Vision-based CSLR aims to automatically recognize signs from image streams, which can bridge the communication gap between the Deaf and hearing people. It can also provide more non-intrusive communication technologies for deaf sign language users. Different from speech recognition, the data collection and annotation of sign language are costly and not scalable, which poses a significant problem for recognition <ref type="bibr" target="#b1">[2]</ref>. Therefore, most recent CSLR works solve this problem in a weakly supervised manner and adopt network architectures composed of the feature extractor and the alignment model. The feature extractor abstracts visual information from image sequences, and the alignment model searches the possible alignments between visual features and the corresponding labeling. Previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> adopt HMMs to update frame-wise state labels for the training of the feature extractor. Graves et al. <ref type="bibr" target="#b14">[15]</ref> provide a more elegant solution named Connectionist Temporal Classification (CTC) to align the prediction and labeling by maximizing the sum of probability of all feasible alignments, which is adopted by many works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47]</ref>. Although CTC-based CSLR methods provide convenience in training, some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref> show that end-to-end training limits the discriminative power of the feature extractor. They leverage the iterative training scheme to enhance the feature extractor, which significantly improves the performance. Nevertheless, it requires an additional fine-tuning process besides the end-to-end training and increases the training time. Several recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> try to accelerate this training scheme by adopting fully convolutional networks and fine-grained labels.</p><p>In this work, we revisit CTC-based CSLR models at different iteration steps and observe that key frames play a vital role in training. The feature extractor abstracts visual information and provides initial localization of key frames for the alignment model. The alignment model further refines the recognition results from the feature extractor and learns long-term relationships with its powerful temporal modeling ability. Due to the spiky nature of CTC <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>, the alignment model converges much faster than the feature extractor on limited SLR datasets and cannot provide enough feedback to the feature extractor. The overfitting of the alignment model leads to insufficient training of the feature extractor and deteriorates the generalization ability of the trained model. The iterative training scheme tries to solve this problem by enhancing the feature extractor with iteratively refined pseudo labels.</p><p>Based on these observations, we conclude that constraining the feature space is critical to efficiently train CSLR models. Therefore, we propose a Visual Alignment Constraint (VAC) to make CSLR networks end-to-end trainable. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the proposed VAC is composed of two auxiliary losses which provide extra supervision for the feature extractor. The visual enhancement loss forces the feature extractor to make predictions based on visual features only and the visual alignment loss aligns the short-term visual and long-term contextual features. The combined use of both losses achieves competitive performance to the latest methods on PHOENIX14 <ref type="bibr" target="#b27">[28]</ref> and CSL <ref type="bibr" target="#b22">[23]</ref> datasets.</p><p>To better understand the performance gains, we present two metrics named Word Deterioration Rate (WDR) and Word Amelioration Rate (WAR) to evaluate the contributions of the feature extractor and the alignment model, which can be used as the indicators of overfitting. Experimental results show that, different from the iterative training scheme, the proposed VAC can make better use of visual features while obtaining a more powerful feature extractor.</p><p>The major contributions are summarized as follows:</p><p>• We revisit the overfitting problem in CSLR and attribute it to the insufficient training of the feature extractor with limited training data.</p><p>• We propose a visual alignment constrain to make the network end-to-end trainable by enhancing the feature extractor and aligning visual and contextual features.</p><p>• We present two metrics to evaluate the contributions of the feature extractor and the alignment model, which verifies the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Continuous Sign Language Recognition</head><p>Sign Language Recognition (SLR) methods can be roughly categorized into isolated SLR <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> and con-tinuous SLR <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. Unlike isolated SLR, most CSLR approaches model sequence recognition in a weakly supervised manner: only sentence-level labeling is provided. Some early CSLR methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref> adopt a divide-andconquer paradigm that splits sign video into several subunits with HMM-based recognition systems to work with limited data. Hand-crafted features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref> are carefully selected to provide better visual information.</p><p>The recent successes of CNNs in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref> provide a powerful tool for visual features representation. However, CNNs often need frame-wise annotations contrary to the weakly supervised nature of CSLR. To solve this problem, Koller et al. <ref type="bibr" target="#b28">[29]</ref> propose an iterative expectation-maximization approach that introduces a hand shape classifier to the GMM-HMM model as an intermediate task to provide frame-level supervision. A few works extend this work by proposing CNN+LSTM+HMM framework <ref type="bibr" target="#b29">[30]</ref>, incorporating more clues <ref type="bibr" target="#b26">[27]</ref> and improving the iterative alignment approach <ref type="bibr" target="#b30">[31]</ref>. This iterative CNN-LSTM-HMM setup provides robust visual features that are adopted by many recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Although the CNN-LSTM-HMM hybrid approaches achieve great results, they still need HMMs to provide frame-wise labels. Graves et al. <ref type="bibr" target="#b14">[15]</ref> propose the CTC Loss to maximize the probabilities of all feasible alignments, and this approach is widely used in many sequence problems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>. Several recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> use CTC Loss to achieve the end-to-end CSLR alignment and recognition. However, some works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40]</ref> find that such an end-toend approach cannot train feature extractor well and bring the iterative training back in use. Until very recently, some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> try to accelerate the training process. Cheng et al. <ref type="bibr" target="#b5">[6]</ref> propose a gloss feature enhancement module to learn better visual features. Niu and Mak <ref type="bibr" target="#b35">[36]</ref> propose a multiple states approach and several operations to alleviate the overfitting problem. In this work, we try to figure out what iterative training does and propose a more efficient way to train SLR models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Auxiliary Learning</head><p>Different from the conventional Multi-Task Learning <ref type="bibr" target="#b4">[5]</ref>, which aims to improve the generalization of all tasks, auxiliary learning chooses proper auxiliary tasks to assist in the generalization of the primary task. One straightforward way is to combine multiple tasks at the output stage. In this direction, Kim et al. <ref type="bibr" target="#b25">[26]</ref> use CTC to speed up the training process and provide a monotonic alignment constraint. Pu et al. <ref type="bibr" target="#b39">[40]</ref> propose an iteratively alignment network that jointly optimizes the CTC decoder and the LSTM decoder, additionally with a soft-DTW alignment constraint. Goyal et al. <ref type="bibr" target="#b12">[13]</ref> propose an auxiliary loss to alleviate the the posterior collapsing phenomenon in autoregressive decoder <ref type="bibr" target="#b0">[1]</ref>. Another idea is to use different supervision at  different stages. Sanabria et al. <ref type="bibr" target="#b40">[41]</ref> use several lower-level tasks, such as phoneme recognition, to constrain intermediate representations for speech recognition. In this work, we adopt the auxiliary learning strategy to provide the visual alignment constraint for the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting the Iterative Training in CSLR</head><p>The CSLR aims to predict the corresponding class label sequence l = (l 1 , · · · , l N ) based on a sequence of T frames X = (x 1 , · · · , x T ). Feature extractor plays an important role in CSLR, which extracts visual features from image sequences. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, we choose 2D-CNN to extract frame-wise features and 1D-CNN to extract local posture and motion information V = (v 1 , · · · , v T ) from neighboring frames as previous works do <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref>. The glosswise features are fed into a two-layer BiLSTM to combine long-term relationships and provide the predicted logits Z = (z 1 , · · · , z T ). CTC Loss is adopted to provide supervision by aligning the predictions and sequence labelings. This scheme is widely adopted in recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The iterative training is based on the above scheme and performed by training end-to-end and fine-tuning feature extractor iteratively. End-to-end training provides pseudo labels for the feature extractor, and fine-tuning feature extractor provides a more robust initialization for end-to-end training. Some works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref> replace 2D-CNN and 1D-CNN combination with 3D-CNN, but the iterative training scheme is similar. Considering experimental efficiency and flexibility, we adopt the former architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Spiky Nature of CTC</head><p>The Connectionist Temporal Classification <ref type="bibr" target="#b14">[15]</ref> is designed for end-to-end temporal classification tasks with unsegmented data. To provide more effective supervision, CTC introduces a 'blank' to represent unlabeled data (movement epenthesis and non-gesture segments in SLR) and solves the alignment problem in a dynamic programming manner. All labels are drawn from an extended gloss set G = G ∪ {blank}.</p><p>CTC defines a many-to-one function B : G T → G ≤T to align label sequence referred to as path π ∈ G T and labeling l ∈ G ≤T by sequentially removing the repeated labels and the blanks from the path. </p><formula xml:id="formula_0">L CT C = − log p(l|x; θ) = − log( π∈B −1 (l) p(π|x; θ))<label>(1)</label></formula><p>The conditional probability p(π|x) can be calculated according to the conditional independence assumption:</p><formula xml:id="formula_1">p(π|x) = T t=1 p(π t |x; θ)<label>(2)</label></formula><p>where the probabailities are calculated by applying softmax fuction to the the network output logits: P θ = softmax(z). As mentioned above, CTC aligns the path and the labeling by introducing a blank class and removing the repeat labels. When adopting CTC as the optimization objective, the network outputs tend to form a series of spike activation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>. The main reason for this phenomenon is that predicting a blank label is a much safer choice for CTC when the network cannot confidently distinguish gloss boundaries. For example, both B(aaab) and B(a − −b) are corresponding to the same labeling, but B(abab) will bring larger loss even if there is only one mistake. Therefore, the CTC loss mainly focuses on key frames, and the final predictions are composed of a few non-blank key frames and many high-confidence blank frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visualizing the Gate Values of LSTM</head><p>Long Short-Term Memory <ref type="bibr" target="#b21">[22]</ref> is widely used in sequence modeling, which has an excellent ability to model long-term dependencies. The core component of LSTM is its memory design: the input and forget gates control information from current inputs and the past memory to the current memory. The output gate controls what is expected to output from the current memory. The total update mechanism is as follows ( denotes the Hadamard product):</p><formula xml:id="formula_2">    i t f t o t c t     =     σ σ σ tanh     W v t h t−1 c t = f t c t−1 + i t ĉ t h t = o t tanh(c t )<label>(3)</label></formula><p>Here the i t , f t and o t are corresponding to input, forget and output gates, respectively, the vector h t and c t are hidden and cell states. Element-wise sigmoid and tanh functions are reprensented by σ and tanh.</p><p>To explore how LSTM makes predictions in CSLR, we begin by visualizing the averaged gate values of the last BiLSTM layer and the network predictions at different iteration steps in <ref type="figure" target="#fig_4">Fig. 3</ref>. For the predictions, we only visualize non-blank classes that occur in the labeling. We can make some observations from the comparison of line charts:</p><p>1) The gate values and the predictions have positive correlations on the training set, and they reach the local extreme maximum on similar frame subsets.</p><p>2) The correlations appear to be weakened as the iteration progresses, especially for the input and output gates, which become larger and smoother.</p><p>The above two observations are quite puzzling, as three gates are expected to play different roles in information flow. As shown in Equ. 3, three gates take the same inputs and have independent parameters. Therefore, we pinpoint the problem to the magnitude of input features and further visualize the l 2 norms of the activations before the first and the second BiLSTM layers, which are referred to as the gloss and sequence norms in <ref type="figure" target="#fig_4">Fig. 3.</ref>  <ref type="figure" target="#fig_4">Fig. 3</ref> presents an interesting observation that the l 2 norms of gloss and sequence features have similar tendencies with gates values and final predictions. Besides, the magnitudes variances of both gloss and sequence become smaller as the iteration progresses. Several recent papers found that well-separated features tend to have larger magnitudes <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46]</ref>, and we hypothesize the magnitudes variances are relevant to the importance of frames:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Magnitude Hypothesis</head><p>The l 2 norms of the features are effect indicators that reflect the importance of frames: the stochastic gradient descent algorithm will decrease the magnitudes of activations when suppressing the non-key frames.</p><p>With the above hypothesis, we can observe in <ref type="figure" target="#fig_4">Fig. 3</ref> that some frames have larger magnitudes than their neighbors and refer them to as key frames. The magnitudes of gloss key frames have greater variance than the corresponding sequence magnitudes. Based on this observation, we interpret the learning process in two stages: 1) the feature extractor provides visual and initial localization information for the alignment model, and 2) the BiLSTM layers refine the localization and learn long-term relationships among key frames. Such a learning strategy can make efficient use of the data and accelerate the training process.</p><p>However, current CSLR datasets contain less data than other sequence learning tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, which means the BiLSTM layers can easily overfit the whole training set with partial visual information and other frames are decreasingly involved in the training progress. Although the network can achieve stable convergence, the feature extractor is not sufficiently trained. Therefore, the feature extractor cannot provide robust features of key frames during inference and deteriorate the generalization performance.</p><p>Based on these analyses, we can attribute the success of iterative training to the reduction of the overfitting problem. With pseudo labels generated by the alignment model, the fine-tuning stage can enhance the feature extractor and provide robust visual features. However, experimental results in Sect. 5.2 suggest that finetuning with pseudo labels cannot make full use of visual features. Therefore, we propose the visual alignment constraint to constrain the feature space, which is more compatible with the spiky activations and achieves competitive results in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Visual Alignment Constraint</head><p>As mentioned above, the BiLSTM layers can easily overfit the training set with partial visual information. We can roughly divide recent methods into two categories from this overfitting perspective: enhancing the feature extractor <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> and weakening the alignment model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. In this paper, we propose the Visual Alignment Constraint (VAC) to enhance the feature extractor with more alignment supervision. The proposed VAC is implemented by two simple auxiliary losses: the Visual Enhancement (VE) loss and the Visual Alignment (VA) loss. Besides, we further propose two new evaluation metrics named Word Deterioration Rate (WDR) and Word Amelioration Rate (WAR) to evaluate the contributions of the feature extractor and the alignment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Loss Design of VAC</head><p>VE Loss. To enhance the feature extractor, we introduce an auxiliary classifier f a on visual features V to get the auxiliary logitsZ = (z 1 , · · · ,z T ) = f a (V ) and propose the VE loss that directly provides supervision for the feature extractor. This auxiliary loss enforces the feature extractor to make predictions based on local visual information only. Compared to previous gloss-wise supervision that needs to generate pseudo labels, we adopt an additional CTC loss on the auxiliary classifier, which is compatible with the primary CTC loss and flexible to network designs. The proposed VE loss only provides supervision for parameters θ v of the feature extractor and the auxiliary classifier:  Alignment results of the proposed metrics. We highlight the alignment results of the auxiliary classifier, the primary classifier , and wrong recognition results.</p><formula xml:id="formula_3">L V E = L v CT C = − log p(l|x; θ v )<label>(4)</label></formula><p>VA Loss. The visual enhancing loss is independent of the primary loss and lacks contextual supervision. To align the short-term visual and long-term contextual features, we further introduce the VA loss. The proposed VA loss is implemented as a knowledge distillation loss <ref type="bibr" target="#b20">[21]</ref>, which regards the entire network and the visual feature extractor as the teacher and student models, respectively. We adopt a high temperature τ to "soften" probability distribution from spiky activations. The distillation process is formulated as:</p><formula xml:id="formula_4">L V A = KL softmax( Z τ ), softmax(Z τ )<label>(5)</label></formula><p>In summary, to achieve the visual alignment goal, the VE loss enforces the feature extractor to provide more robust visual features for the alignment model, while the VA loss aligns the predictions of two classifiers by providing longterm supervision for the visual extractor. With the help of both losses, the feature extractor obtains more supervision which is compatible with the alignment model. The final objective function is composed of the primary CTC loss, the visual enhancement loss, and the visual alignment loss:</p><formula xml:id="formula_5">L = L CT C + L V E + αL V A (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inconsistent Prediction Discovery</head><p>Word Error Rate (WER) is a widely-used metric to evaluate the performance of recognition algorithms in CSLR <ref type="bibr" target="#b27">[28]</ref>. It is also referred to as the length normalized edit distance, which first aligns the recognized sequence with the reference sentence and then counts the number of operations, including substitution (sub), deletion (del), and insertion (ins), to transfer from the reference to the recognized sequence: WER = (#sub + #del + #ins) / #reference.</p><p>As shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, both of the auxiliary and the primary recognized sentences (HYP a and HYP p ) have the same WER 22.22% (HYP a has two deletion errors, and HYP p has two insertion errors). The primary classifier corrects the misrecognized results of the auxiliary classifier but makes new mistakes, which can not be measured by WER. Therefore, we firstly align sentence triplet (REF * , HYP * a , HYP * p ) and then calculate WDR and WAR: WDR measures the ratio that is correctly recognized by the auxiliary classifier but misrecognized by the primary classifier (two 'SUED' in HYP * p ), and WAR does in the opposite direction ('MEHR' and 'KALT' in HYP * p ). With the proposed metrics, we can connect the WER * 1 performances of two classifiers by:</p><formula xml:id="formula_6">WER * p = WER * a + WDR − WAR<label>(7)</label></formula><p>Based on Equ. 7, we can analyze the final results WER * p from three aspects: how well the visual extractor performs (WER * a ), how much visual information is not utilized (WDR) and how many predictions are made by contextual information only (WAR). More details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Dataset. We evaluate the proposed method on two widely used datasets: RWTH-PHOENIX-Weather-2014 (PHOENIX14) dataset <ref type="bibr" target="#b27">[28]</ref> and Chinese Sign Language (CSL) dataset <ref type="bibr" target="#b22">[23]</ref>.</p><p>The PHOENIX14 dataset is a widely used CSLR dataset recorded from the German TV weather forecasts and performed by nine hearing SL interpreters. It contains 6841 sentences with 1295 different glosses. The dataset is split into 5672 training sentences, 540 dev sentences, and 629 test sentences for the multi-signer setup.</p><p>The CSL dataset is collected under laboratory conditions with 100 sign language sentences with a vocabulary size of 178. Fifty signers perform each sentence five times (in 25000 videos with 100+ hours). We follow the previous setting <ref type="bibr" target="#b5">[6]</ref> and split the dataset into training and test sets according to the ratio of 8:2. Implementation Details. We select ResNet18 <ref type="bibr" target="#b19">[20]</ref> as the frame-wise feature extraction considering its efficiency on the PHOENIX14 dataset. For the CSL dataset, we adopt VGG11 <ref type="bibr" target="#b42">[43]</ref> as the backbone to reduce side effects of inconsistent statistics under the signer-independent setting. The gloss-wise temporal layer and two BiLSTM layers with 2×512 dimensional hidden states are adopted as the default setting. The weight α for L V A is set to 10 and its temperature τ is set to 8 by default. We train all the models for 80 epochs for PHOENIX14 and 20 epochs for CSL with a mini-batch size of 2. Adam optimizer is used with an initial learning rate of 10 −4 , divided by five after 40 and 60 epochs for PHOENIX14 and 10 and 15 for CSL. For iterative training, we reduce the learning rate by a factor of five after each iteration. All frames are resized to 256x256, and the training set is augmented with random crop (224x224), horizontal flip (50%), and random temporal scaling (±20%).  <ref type="table">Table 3</ref>. Ablations (WER,%) on the VAC design on PHOENIX14. </p><formula xml:id="formula_7">L CT C L V E L V A Dev</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Results</head><p>Comparison with iterative training and BN. Batch Normalization (BN) <ref type="bibr" target="#b23">[24]</ref> is a widely-used tool to accelerate the training of deep networks by normalizing the activations. Although we adopt a small batch size, BN significantly improves the performance. As shown in <ref type="table" target="#tab_0">Table 1</ref>, adding a BN layer after each temporal convolution layer brings 5.5%, 3.4%, and 3.6% performance gains at each iteration on the dev set, which indicates the existence of insufficient training of the feature extractor. We can also observe that adopting the iterative training strategy can lead to noticeable performance gains compared to non-iterative training.</p><p>Adjusting learning paces can improve performance. A natural idea to solve the insufficient training problem is adjusting the learning paces of the feature extractor and the alignment model. In <ref type="table">Table 2</ref>, we compare different learning rate ratios. Adopting a smaller learning rate for the feature extractor leads to comparable results with iterative training, which suggests the existence of insufficient training. However, it is hard to find an optimal learning setting to solve the overfitting problem. We adopt a non-iterative model with BN layers and a 1:1 learning rate ratio as our baseline. The Effectiveness of the VAC. Ablations on VAC have been presented in <ref type="table">Table 3</ref>. Constraining visual features with L V E and L V A improves the recognition results (2.1% and 0.9% on dev set), which verifies the need to strengthen supervision on the feature extractor. It is also worth noting that although adopting the L V A only leads to smaller gains than the L V E only, adopting both losses can achieve further improvement. It suggests that aligning two spiky activations provides more effective supervision than adopting independent supervision or distillation only.  Obeservations about the overfitting problem. <ref type="figure" target="#fig_8">Fig. 6</ref> visualizes performance comparison with different evaluation metrics and we can draw some interesting observations about overfitting. First, the primary classifier can reach much lower WER on the training set than the auxiliary classifier in <ref type="figure" target="#fig_8">Fig. 6(a)</ref>, which reflects its powerful temporal modeling ability. Second, there exists a significant performance gap between the training and dev sets for WDR, which indicates the BiLSTM layers do not fully incorporate the visual information although it successfully overfits the training set. Third, the actual performance gap is much larger than the WER shows (∆WER * ). For example, the performance gap between two classifiers of Baseline on dev set in <ref type="figure" target="#fig_8">Fig. 6(b)</ref> is only 4.9% (=30.4%-25.5%), however, the primary classifier makes 11.3% correct predictions based on contextual information only (WAR) and ignores 6.5% correct visual information (WDR). The proposed inconsistent prediction metrics provide a helpful tool to understand and evaluate the overfitting problem.</p><p>The VAC can narrow the performance gap. Another interesting observation from <ref type="figure" target="#fig_8">Fig. 6(b)</ref> is that while the itera- tive training strategy strengthens the visual extractor, it also increases the WDR. We assume that the pseudo-label-based approach is not well compatible with the primary CTC loss (previous work <ref type="bibr" target="#b5">[6]</ref> adopts a balanced ratio to reduce the effects of "blank" labels). Therefore, we adopt an additional CTC loss as our L V E and it significantly improves both WAR and WDR. The proposed L V A has a limited effect on the visual extractor but it can narrow the performance gap between two classifiers. The combined use of both auxiliary losses achieves better performance with a smaller actual performance gap (WDR and WAR), which verifies the effectiveness of the proposed visual alignment constraint.</p><p>The VAC is flexible with temporal network design. Previous pseudo-label-based methods need to carefully design the temporal receptive field, which is set to approximate the average length of the isolated sign <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. <ref type="table" target="#tab_2">Table 4</ref> presents the performance comparison with different temporal receptive fields ∆t to evaluate the effectiveness and flexibility of the proposed VAC. To our surprise, the frame-wise feature extractor still achieves competitive results to other settings, and there is a small performance difference in the temporal layer designs with different receptive fields. These results are different from previous experimental results <ref type="bibr" target="#b8">[9]</ref> that the network with small ∆t fails to optimize the CTC. We conclude this is because the VAC provides more flexible supervision for the feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results</head><p>Results Visualization. To better understand the learning process, we visualize some sequences in <ref type="figure" target="#fig_7">Fig. 5</ref>. The upper sample from the training set shows that the auxiliary classifier of the baseline does not correctly recognize some glosses (NACHT, loc-SUEDWEST, ORT-PLUSPLUS ), but the primary classifier can still deliver the correct result.</p><p>Although it is reasonable for the primary classifier to make predictions based on contextual information only, the lack of constraint on the feature space increases the risk of overfitting, which may lead to unpredictable predictions when context changes during inference. With the help of the VAC, both auxiliary and primary classifiers are sufficiently trained and make better predictions on the training set. The lower sample from the dev set shows a failure case of the alignment model. The auxiliary classifier makes the correct predictions (HEUTE, OST and SCHON) based on visual features only. Nevertheless, the primary classifier neglects this information and gives a worse result, which is not mentioned in the WER metric but can be identified by the proposed metrics. More qualitative results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with the State-of-the-art.</head><p>We present the comparison results with several state-ofthe-art approaches in <ref type="table" target="#tab_3">Table 5 and Table 6</ref>. From <ref type="table" target="#tab_3">Table 5</ref> we can see that the proposed method with gloss-wise temporal layer and VAC achieves competitive results with previous iteration-based methods. We can also illustrate the success of STMC <ref type="bibr" target="#b49">[50]</ref> and CMA <ref type="bibr" target="#b37">[38]</ref> from the overfitting perspective: the former enforces the feature extractor to extract visual information from extra supervision and the latter weakens the contextual information with the data augmentation.</p><p>To examine the generalization of the proposed method, we also evaluate it on the CSL dataset. As no official split is given, the performance comparison among methods in <ref type="table">Table 6</ref> has limited practical value. The proposed method shows improvement than baseline and achieves better performance than recent work <ref type="bibr" target="#b5">[6]</ref> under the same setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Discussion</head><p>The proposed VAC is an attempt to make better use of visual information, which may not be the optimal solution but provides a new perspective to solve this problem. How to better use visual features with a more powerful temporal model, which will be easier to overfit but can further improve WAR, is a challenging problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, analytical experiments show that original end-to-end training leads to insufficient training for the feature extractor due to the overfitting problem. To solve this problem, we propose the visual alignment constraint to make CSLR networks end-to-end trainable by enforcing the feature extractor to make predictions with more alignment supervision. Two new metrics are presented to evaluate the different behaviors of the feature extractor and the alignment model. Experimental results verify the proposed VAC successfully narrows the gap between two classifiers. The proposed metrics and relevant analysis provide a new perspective on the relationship between visual and alignment models and we hope they can inspire future studies on CSLR and other sequence classification tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overview of the proposed non-iterative CLSR network with the visual alignment constraint. The overfitting of the alignment model leads to insufficient training of the feature extractor. The proposed VAC enforces the visual extractor to provide general visual features by constraining the feature space with the alignment supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An overview of the proposed framework. It consists of three components: a feature extractor, an alignment model, and an auxiliary visual classifier fa. The feature extractor first takes image sequence to abstract frame-wise features, and then 1D-CNN is applied to extract the local visual information with the temporal receptive field of ∆t. The outputs of 1D-CNN noted as visual features are sent to the alignment model and the auxiliary classifier. Two auxiliary losses are adopted during training. The visual enhancement loss (LV E ) aligns visual features and the target sequence. The visual alignment loss (LV A) aligns short-term visual and long-term context features through knowledge distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, B(−aaa − −aabbb−) = B(−a − ab−) = aab. With the help of this function, CTC can provide supervision for parameters θ of the feature extractor and the alignment model by summing the probabilities of all feasible paths:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of the gate values, the l2 norm of features and the final prediction of a training sample among different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>REF:</head><label></label><figDesc>__ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT REF : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUEDOST ABER KALT HYP : __ON__ HEUTE NACHT **** SCHNEE NORD SUEDOST ABER **** REF * : __ON__ HEUTE NACHT MEHR SCHNEE NORD **** SUEDOST **** ABER KALT HYP * : __ON__ HEUTE NACHT **** SCHNEE NORD **** SUEDOST **** ABER **** HYP * : __ON__ HEUTE NACHT MEHR SCHNEE NORD SUED SUEDOST SUED ABER KALT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Alignment results of the proposed metrics. We highlight the alignment results of the auxiliary classifier, the primary classifier , and wrong recognition results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison among different settings with samples from training (the upper) and dev (the lower) sets on PHOENIX14. Wrong recognition results (except del) are marked in red. The primary classifier and auxiliary classifier outputs are marked as (P) and (A). ΔWER * (a) Evaluation results on PHOENIX14 training set. (b) Evaluation results on PHOENIX14 dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of performance comparison with different metrics and settings (∆WER * = WER * a −WER * p = WAR−WDR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Recognition Dev/Test WERs comparison (%) with iterative training and BN on PHOENIX14.</figDesc><table><row><cell>Iterations</cell><cell></cell><cell>w/o BN</cell><cell></cell><cell cols="2">w/ BN</cell></row><row><cell>1</cell><cell></cell><cell>32.7 / 33.0</cell><cell></cell><cell cols="2">27.2 / 28.0</cell></row><row><cell>2</cell><cell></cell><cell>28.9 / 29.8</cell><cell></cell><cell cols="2">25.5 / 26.3</cell></row><row><cell>3</cell><cell></cell><cell>28.3 / 28.9</cell><cell></cell><cell cols="2">24.7 / 26.2</cell></row><row><cell>None</cell><cell></cell><cell>30.4 / 32.1</cell><cell></cell><cell cols="2">25.4 / 26.6</cell></row><row><cell cols="6">Table 2. Performance comparison (%) with different learning rate</cell></row><row><cell cols="6">(lr) ratios (lr of the feature extractor: lr of the alignment model) on</cell></row><row><cell>PHOENIX14.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR Ratio</cell><cell>0.1:1</cell><cell>0.5:1</cell><cell>1:1</cell><cell>2:1</cell><cell>10:1</cell></row><row><cell>Dev</cell><cell>25.0</cell><cell>25.6</cell><cell>25.4</cell><cell>26.9</cell><cell>34.8</cell></row><row><cell>Test</cell><cell>25.6</cell><cell>26.5</cell><cell>26.6</cell><cell>27.5</cell><cell>35.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">Performance comparison (%) with different temporal</cell></row><row><cell cols="4">layer designs on PHOENIX14. Cβ and Pβ correspond to 1D con-</cell></row><row><cell cols="4">volutional layer and max pooling layer with a kernel size of β.</cell></row><row><cell></cell><cell cols="3">Temporal Layers ∆t Dev / Test</cell></row><row><cell>Frame-wise</cell><cell>C1 C3</cell><cell>1 3</cell><cell>25.2 / 26.5 24.4 / 25.4</cell></row><row><cell cols="2">Subgloss-wise C5-P2</cell><cell>6</cell><cell>24.0 / 24.3</cell></row><row><cell>Gloss-wise</cell><cell>C5-P2-C5-P2</cell><cell cols="2">16 22.1 / 23.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison (%) on PHOENIX14 dataset. We show the results of the proposed method based on ResNet18 with Gloss-wise temporal layer. The entries denoted by "*" used extra clues (such as keypoints and tracked face regions).</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Iteration</cell><cell>Dev(%) del/ins</cell><cell>WER</cell><cell>Test(%) del/ins</cell><cell>WER</cell></row><row><cell>SubUNet [3]</cell><cell>CaffeNet</cell><cell></cell><cell>14.6/4.0</cell><cell>40.8</cell><cell>14.3/4.0</cell><cell>40.7</cell></row><row><cell>Staged-Opt [8]</cell><cell>VGG-S/GoogLeNet</cell><cell></cell><cell>13.7/7.3</cell><cell>39.4</cell><cell>12.2/7.5</cell><cell>38.7</cell></row><row><cell>Align-iOpt [40]</cell><cell>3D-ResNet</cell><cell></cell><cell>12.6/2.6</cell><cell>37.1</cell><cell>13.0/2.5</cell><cell>36.7</cell></row><row><cell>Re-Sign [31]</cell><cell>GoogLeNet</cell><cell></cell><cell>-</cell><cell>27.1</cell><cell>-</cell><cell>26.8</cell></row><row><cell>SFL [36]</cell><cell>ResNet18</cell><cell></cell><cell>7.9/6.5</cell><cell>26.2</cell><cell>7.5/6.3</cell><cell>26.8</cell></row><row><cell>STMC [50]</cell><cell>VGG11</cell><cell></cell><cell>-</cell><cell>25.0</cell><cell>-</cell><cell>-</cell></row><row><cell>DNF [9]</cell><cell>GoogLeNet</cell><cell></cell><cell>7.8/3.5</cell><cell>23.8</cell><cell>7.8/3.4</cell><cell>24.4</cell></row><row><cell>FCN [6]</cell><cell>Custom</cell><cell></cell><cell>-</cell><cell>23.7</cell><cell>-</cell><cell>23.9</cell></row><row><cell>CMA [38]</cell><cell>GoogLeNet</cell><cell></cell><cell>7.3/2.7</cell><cell>21.3</cell><cell>7.3/2.4</cell><cell>21.9</cell></row><row><cell>CNN+LSTM+HMM [27]*</cell><cell>GoogLeNet</cell><cell></cell><cell>-</cell><cell>26.0</cell><cell>-</cell><cell>26.0</cell></row><row><cell>DNF [9]*</cell><cell>GoogLeNet</cell><cell></cell><cell>7.3/3.3</cell><cell>23.1</cell><cell>6.7/3.3</cell><cell>22.9</cell></row><row><cell>STMC [50]*</cell><cell>VGG11</cell><cell></cell><cell>7.7/3.4</cell><cell>21.1</cell><cell>7.4/2.6</cell><cell>20.7</cell></row><row><cell>Baseline</cell><cell>ResNet18</cell><cell></cell><cell>8.3/3.1</cell><cell>25.4</cell><cell>8.8/3.2</cell><cell>26.6</cell></row><row><cell>Baseline+VAC</cell><cell>ResNet18</cell><cell></cell><cell>8.1/2.5</cell><cell>22.1</cell><cell>8.2/2.4</cell><cell>23.0</cell></row><row><cell cols="2">Table 6. Performance comparison (%) on CSL dataset. The entries</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">denoted by "*" used extra clues (such as keypoints).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>WER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LS-HAN [23]</cell><cell>17.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SubUNet [3]</cell><cell>11.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SF-Net [48]</cell><cell>3.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FCN [6]</cell><cell>3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STMC [50]*</cell><cell>2.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline+VAC</cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The adopted alignment strategy leads to a little performance degradation than the general WER due to different weights of operations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sign language recognition, generation, and translation: An interdisciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Bellard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larwan</forename><surname>Berke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Boudreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annelies</forename><surname>Braffort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><surname>Verhoef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 21st International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subunets: End-to-end hand shape and continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sign language transformers: Joint end-toend sign language recognition and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10023" to="10033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><surname>Leong Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="697" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7784" to="7793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for continuous sign language recognition by staged optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7361" to="7369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep neural framework for continuous sign language recognition by iterative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1880" to="1891" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speech recognition techniques for a sign language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Zahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orientation histograms for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Automatic Face and Gesture recognition</title>
		<meeting>the International Workshop on Automatic Face and Gesture recognition</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A chinese sign language recognition system based on sofm/srn/hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaolin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2389" to="2402" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised sequence labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised sequence labelling with recurrent neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="52" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine learning</title>
		<meeting>the 23rd International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modelling and segmenting subunits for sign language recognition based on hand motion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="623" to="633" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-based sign language recognition without temporal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ms-asl: A largescale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaezi</forename><surname>Hamid Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint ctcattention based end-to-end speech recognition using multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised learning with multi-stream cnnlstm-hmms to discover sequential parallelism in sign language videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihan</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="108" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3793" to="3802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep sign: hybrid cnn-hmm for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent cnn-hmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4297" to="4305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6205" to="6214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinterpreting ctc-based training as iterative fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107392</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic fine-grained labeling of multi-state sign glosses for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic sign language analysis: A survey and the future beyond lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Sylvie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surendra</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="873" to="891" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting continuous sign language recognition via cross modality augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1497" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dilated convolutional network with iterative optimization for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Iterative alignment network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4165" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical multitask learning with ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Spoken Language Technology Workshop</title>
		<meeting>the 2018 IEEE Spoken Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Sign language and linguistic universals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Lillo-Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative exemplar coding for sign language recognition with kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Kun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1418" to="1428" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Connectionist temporal fusion for sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Gang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1483" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01341</idno>
		<title level="m">Sf-net: Structured feature network for continuous sign language recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic pseudo label decoding for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1282" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial-temporal multi-cue network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13009" to="13016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
