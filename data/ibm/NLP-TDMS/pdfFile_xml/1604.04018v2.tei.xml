<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Oriented Text Detection with Fully Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
							<email>zchengquan@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<email>wei.shen@t.shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Oriented Text Detection with Fully Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel approach for text detection in natural images. Both local and global cues are taken into account for localizing text lines in a coarse-to-fine procedure. First, a Fully Convolutional Network (FCN) model is trained to predict the salient map of text regions in a holistic manner. Then, text line hypotheses are estimated by combining the salient map and character components. Finally, another FCN classifier is used to predict the centroid of each character, in order to remove the false hypotheses. The framework is general for handling text in multiple orientations, languages and fonts. The proposed method consistently achieves the state-of-the-art performance on three text detection benchmarks: MSRA-TD500, ICDAR2015 and ICDAR2013.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Driven by the increasing demands for many computer vision tasks, reading text in the wild (from scene images) has become an active direction in this community. Though extensively studied in recent years, text spotting under uncontrolled environments is still quite challenging. Especially, detecting text lines with arbitrary orientations is an extremely difficult task, as it takes much more hypotheses into account, which drastically enlarges the searching space. Most existing approaches are successfully designed for detecting horizontal or near-horizontal text <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. However, there is still a large gap when applying them to multi-oriented text, which has been verified by the low accuracies reported in the recent ICDAR2015 competition for text detection <ref type="bibr" target="#b9">[10]</ref>.</p><p>Text, which can be treated as sequence-like objects with unconstrained lengths, possesses very distinctive appearance and shape compared to generic objects. Consequently, the detection methods in scene images based on * Authors contributed equally sliding windows <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b16">17]</ref> and connected component <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref> have become mainstream in this specific domain. In particular, the component-based methods utilizing Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b14">[15]</ref> as the basic representations achieved the state-of-theart performance on ICDAR2013 and ICDAR2015 competitions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. Recently, <ref type="bibr" target="#b5">[6]</ref> utilized a convolution neural network to learn highly robust representations of character components. Usually, the component grouping algorithms including clustering algorithms or some heuristic rules are essential for localizing text at a word or line level.</p><p>As an unconventional approach, <ref type="bibr" target="#b34">[35]</ref> directly hits text lines from cluttered images, benefiting from symmetry and selfsimilarity properties of them. Therefore, it seems that both local (character components) and global (text regions) information are very helpful for text detection. In this paper, an unconventional detection framework for multi-oriented text is proposed. The basic idea is to integrate local and global cues of text blocks with a coarse-tofine strategy. At the coarse level, a pixel-wise text/non-text salient map is efficiently generated by utilizing a Fully Convolutional Network (FCN) <ref type="bibr" target="#b11">[12]</ref>. We show that the salient map provides a powerful guidance for estimating orientations and generating candidate bounding boxes of text lines, <ref type="figure">Figure 2</ref>.  while combining it with local character components. More specifically, the pipeline of the proposed detection framework is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. First, a salient map is generated and segmented into several candidate text blocks. Second, character components are extracted from the text blocks. Third, projections of the character components are used for estimating the orientation. Then, based on the estimated orientation, all candidate bounding boxes of text lines are constructed by integrating cues from the components and the text blocks. Finally, detection results are obtained by removing false candidates through a filtering algorithm.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g)</formula><p>Our contributions are in three folds: First, we present a novel way for computing text salient map, through learning a strong text labeling model with FCN. The text labeling model is trained and tested in a holistic manner, highly stable to the large scale and orientation variations of scene text, and quite efficient for localizing text blocks at the coarse level. In addition, it is also applicable to multi-script text. Second, an efficient method for extracting bounding boxes of text line candidates in multiple orientations is presented. We show that the local (character components) and the global (text blocks from the salient map) cues are both helpful and complementary to each other. Our third contribution is to propose a novel method for filtering false candidates. We train an efficient model (another FCN) to predict character centroids within the text line candidates. We show that the predicted character centroids provide accurate positions of each character, which are effective features for removing the false candidates. The proposed detection framework achieves the state-of-the-art performance on both horizontal and multi-oriented scene text detection benchmarks.</p><p>The remainder of this paper is organized as follows: In Sec. 2, we briefly review the previously related work. In Sec. 3, we describe the proposed method in detail, including text block detection, strategies for multi-oriented text line candidate generation, and false alarm removal. Experimental results are presented in Sec. 4. Finally, conclusion remarks and future work are given in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Text detection in natural images has received much attention from the communities of computer vision and document analysis. However, most text detection methods focus on detecting horizontal or near-horizontal text mainly in two ways: 1) localizing the bounding boxes of words <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, 2) combining detection and recognition procedures into an end-to-end text recognition method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>. Comprehensive surveys for scene text detection and recognition can be referred to <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>In this section, we focus on the most relevant works that are presented for multi-oriented text detection. Multioriented text detection in the wild is first studied by <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29]</ref>. Their detection pipelines are similar to the traditional methods based on connected component extraction, integrating orientation estimation of each character and text line. <ref type="bibr" target="#b8">[9]</ref> treated each MSER component as a vertex in a graph, then text detection is transferred into a graph partitioning problem. <ref type="bibr" target="#b31">[32]</ref> proposed a multi-stage clustering algorithm for grouping MSER components to detect multi-oriented text. <ref type="bibr" target="#b27">[28]</ref> proposed an end-to-end system based on SWT <ref type="bibr" target="#b3">[4]</ref> for multi-oriented text. Recently, a challenging benchmark for multi-oriented text detection has been released for the IC-DAR2015 text detection competition, and many researchers have reported their results on it.</p><p>In addition, it is worth mentioning that both of the recent approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref> and our method, which used the deep convolutional neural network, have achieved superior performance over conventional approaches in several aspects: 1) learn a more robust component representation by pixel labeling with CNN <ref type="bibr" target="#b7">[8]</ref>; 2) leverage the powerful discrimination ability of CNN for better eliminating false positives <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>; 3) learn a strong character/word recognizer with CNN for end-to-end text detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref>. However, these methods only focus on horizontal text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methodology</head><p>In this section, we describe the proposed method in detail. First, text blocks are detected via a fully convolutional network (named Text-Block FCN). Then, multi-oriented text line candidates are extracted from these text blocks by taking the local information (MSER components) into account. Finally, false text line candidates are eliminated by the character centroid information. The character centroid information is provided by a smaller fully convolutional network (named Character-Centroid FCN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text Block Detection</head><p>In the past few years, most of the leading methods in scene text detection are based on detecting characters. In early practice <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>, a large number of manually designed features are used to identify characters with strong classifiers. Recently, some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> have achieved great performance, adopting CNN as a character detector. However, even the state-of-the-art character detector <ref type="bibr" target="#b7">[8]</ref> still performs poorly at complicated background ( <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>). The performance of the character detector is limited due to three aspects: firstly, characters are susceptible to several conditions, such as blur, non-uniform illumination, low resolution, disconnected stroke, etc.; secondly, a great quantity of elements in the background are similar in appearance to characters, making them extremely hard to distinguish; thirdly, the variation of the character itself, such as fonts, colors, languages, etc., increases the learning difficulty for classifiers. By comparison, text blocks possess more distinguishable and stable properties. Both local and global appearances of text block are useful cues for distinguishing between text and non-text regions ( <ref type="figure" target="#fig_2">Fig. 4 (c)</ref>).</p><p>Fully convolutional network (FCN), a deep convolutional neural network proposed recently, has achieved great performance on pixel level recognition tasks, such as object segmentation <ref type="bibr" target="#b11">[12]</ref> and edge detection <ref type="bibr">[26]</ref>. This kind of network is very suitable for detecting text blocks, owing to several advantages: 1) It considers both local and global context information at the same time.; 2) It is trained in an end-to-end manner; 3) Benefiting from the removal of fully connected layers, FCN is efficient in pixel labeling. In this section, we learn a FCN model, named Text-Block FCN, to label salient regions of text blocks in a holistic way. Text-Block FCN We convert the VGG 16-layer net <ref type="bibr" target="#b21">[22]</ref> into our text block detection model that is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. The first 5 convolutional stages are derived from the VGG 16-layer net. The receptive field sizes of the convolutional stages are variable, contributing to that different stages can capture context information with different sizes. Each convolutional stage is followed by a deconvolutinal layer (equals to a 1 × 1 convolutional layer and a upsampling layer) to generate feature maps of the same size. The discriminative and hierarchical fusion maps are then the concatenation in depth of these upsampled maps. Finally, the fully-connected layers are replaced with a 1 × 1 convolutional layer and a sigmoid layer to efficiently make the pixel-level prediction.</p><p>In the training phase, pixels within the bounding box of each text line or word are considered as the positive region for the following reasons: firstly, the regions between adjacent characters are distinct in contrast to other non-text regions; secondly, the global structure of text can be incorporated into the model; thirdly, bounding boxes of text lines or words are easy to be annotated and obtained. An example of the ground truth map is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The cross-entropy loss function and stochastic gradient descent are used to train this model.</p><p>In the testing phase, the salient map of text regions, leveraging all context information from different stages, is com-puted by the trained Text-Block FCN model at first. As shown in <ref type="figure">Fig. 2</ref>, the feature map of stage-1 captures more local structures like gradient ( <ref type="figure">Fig. 2 (b)</ref>), while the higher level stages capture more global information ( <ref type="figure">Fig. 2</ref> (e) (f)). Then, the pixels whose probability is larger than 0.2 are reserved, and the connected pixels are grouped together into several text blocks. An example of the text block detection result is shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Oriented Text Line Candidate Generation</head><p>In this section, we introduce how to form multi-oriented text line candidates based on text blocks. Although the text blocks detected by the Text-Block FCN provide coarse localizations of text lines, they are still far from satisfactory. To further extract accurate bounding boxes of text lines, taking the information about the orientation and scale of text into account is required. Character components within a text line or word reveal the scale of text. Besides, the orientation of text can be estimated by analyzing the layout of the character components. At first, we extract the character components within the text blocks by MSER <ref type="bibr" target="#b15">[16]</ref>. Then, similar to many skew correction approaches in document analysis <ref type="bibr" target="#b18">[19]</ref>, the orientation of the text lines within a text block is estimated by component projection. Finally, text line candidates are extracted by a novel method that effectively combines block-level (global) cue and componentlevel (local) cue.</p><p>Character Components Extraction. Our approach uses MSER <ref type="bibr" target="#b15">[16]</ref> to extract the character components ( <ref type="figure" target="#fig_0">Fig. 1(d)</ref>) since MSER is insensitive to variations in scales, orientations, positions, languages and fonts. Two constraints are adopted to remove the most of false components: area and aspect ratio. Specifically, the minimal area ratio of a character candidate needs to be more than the threshold T 1 , and the aspect ratio of them must be limited to [ 1 T2 , T 2 ]. Under these two constraints, the most of the false components are excluded.</p><p>Orientation Estimation. In this paper, we assume that text lines from the same text block have a roughly uniform spatial layout, and characters from one text line are in the arrangement of straight or near-straight line. Inspired by projection profile based skew estimation algorithms in documents analysis <ref type="bibr" target="#b18">[19]</ref>, we propose a projection method according to counting components, in order to estimate the possible orientation of text lines. Suppose the orientation of text lines within a text block is θ, and the vertical-coordinate offset is h, we can draw a line across the text block (as the green or red line is shown in <ref type="figure" target="#fig_4">Fig. 6(b)</ref>). And the value of counting components Φ(θ, h) equals the number of the character components that are passed through by the line. Since the component number in the right direction often has the maximum value, the possible orientation θ r can be easily found if we have statistics on the peak value of counting component in all directions <ref type="figure" target="#fig_4">(Fig. 6(a)</ref>). By this means, θ r can be easily calculated as the following formulation:</p><formula xml:id="formula_1">θ r = arg max θ max h Φ(θ, h)<label>(1)</label></formula><p>where Φ(θ, h) represents the number of components when the orientation is θ and the vertical-coordinate offset is h. Text Line Candidate Generation. Different from component based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>, the process of generating text line candidates in our approach does not require to catch all the characters within a text line, under the guidance of a text block. First, we divide the components into groups. A pair of the components <ref type="figure">(A and B)</ref> within the text block α are grouped together if they satisfy following conditions:</p><formula xml:id="formula_2">2 3 &lt; H(A) H(B) &lt; 3 2 ,<label>(2)</label></formula><formula xml:id="formula_3">− π 12 &lt; O(A, B) − θ r (α) &lt; π 12 ,<label>(3)</label></formula><p>where H(A) and H(B) represent the heights of A and B, O(A, B) represents the orientation of the pair, and θ r (α) is the estimated orientation of α.</p><p>Then, for one group β = {c i }, c i is i-th component, we draw a line l along the orientation θ r (α) passing the center of β. The point set P is defined as:</p><formula xml:id="formula_4">P = {p i }, p i ∈ l ∩ B(α),<label>(4)</label></formula><p>where B(α) represents the boundary points of α. Finally, the minimum bounding box bb of β is computed as a text line candidate: <ref type="bibr" target="#b4">(5)</ref> where denotes the minimum bounding box that contains all points and components.  <ref type="figure" target="#fig_5">Fig. 7</ref> illustrates this procedure. We repeat this procedure for each text block to obtain all the text line candidates within an image. By considering both of the two level cues at the same time, our approach has two advantages compared to component based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. First, under the guidance of text blocks, MSER components are not required to catch all characters accurately. Even though some characters are missed or partially detected by MSER, the generation of text line candidates will not be affected (such as the three candidates found at <ref type="figure" target="#fig_5">Fig. 7)</ref>. Second, the previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref> of multi-oriented text detection in natural scenes usually estimate the orientation of text based on the character level with some fragile clustering/grouping algorithms. This kind of methods is sensitive to missing characters and non-text noise. Our method estimates the orientation from the holistic profile by using a projection method, which is more efficient and robust than character clustering/grouping based methods.</p><formula xml:id="formula_5">bb = {p 1 , p 2 , ...p i , c 1 , c 2 , ..., c j }, p i ∈ P, c j ∈ β,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text Line Candidates Classification</head><p>A fraction of the candidates generated in the last stage (Sec. 3.2) are non-text or redundancy. In order to remove false candidates, we propose two criteria based on the character centroids of text line candidates. To predict the character centroids, we employ another FCN model, named Character-Centroid FCN.</p><p>Character-Centroid FCN The Character-Centroid FCN is inherited from the Text-Block FCN (Sec. 3.1), but only the first 3 convolutional stages are used. Same as the Text-Block FCN, each stage is followed by a 1 × 1 convolutional layer and a upsampling layer. The fully-connected layers are also replaced with a 1 × 1 convolutional layer and a sigmoid layer. This network is trained with the cross-entropy loss function as well. In general, the Character-Centroid FCN is a small version of the Text-Block FCN.</p><p>Several examples along with ground truth maps are shown in <ref type="figure">Fig. 8</ref>. The positive region of the ground truth map consists of the pixels whose distance to the character centroids is less than 15% of the height of the corresponding character. In the testing phase, we can obtain the centroid probability map of a text line candidate at first. Then, extreme points E = {(e i , s i )} on the map are collected as the centroids, where e i represents i-th extreme point, and s i represents the score defined as the value of the probability map on e i . Several examples are shown in <ref type="figure">Fig. 9</ref>.</p><p>In order to remove false candidates, two intuitive yet effective criteria based on intensity and geometric properties are adopted, after the centroids are obtained:</p><p>Intensity criterion. For a text line candidate, if the number of the character centroids n c &lt; 2, or the average score of the centroids s avg &lt; 0.6, we regard it as a false text line candidate. The average score of the centroids is defined as:</p><formula xml:id="formula_6">s avg = 1 n c nc i=1 s i ,<label>(6)</label></formula><p>Geometric criterion. The arrangement of the characters within a text line candidate is always approximated to a straight line. We adopt the mean of orientation angles µ and the standard deviation σ of orientation angles between the centroids to characterize these properties. µ and σ are defined as:</p><formula xml:id="formula_7">µ = 1 n c nc i=1 nc j=1 O(e i , e j ),<label>(7)</label></formula><formula xml:id="formula_8">σ = 1 n c nc i=1 nc j=1 (O(e i , e j ) − µ) 2 ,<label>(8)</label></formula><p>where O(e i , e j ) denotes the orientation angle between e i and e j . In practice, we only reserve the candidates whose µ &lt; π 32 and σ &lt; π 16 . Through the above two constraints, the false text line candidates are excluded, but there are still some redundant candidates. To further remove the redundant candidates, a standard non-maximum suppression is applied to remaining candidates, and the score that used in non-maximum suppression is defined as the sum of the score of all the centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To fully compare the proposed method with competing methods, we evaluate our method on several recent stan- dard benchmarks: ICDAR2013, ICDAR2015 and MSRA-TD500.</p><formula xml:id="formula_9">(a) (b) (c) (d) (e) (f) (g) (h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our method on three datasets, where the first two are multi-oriented text datasets, and the third one is a horizontal text dataset. MSRA-TD500. The MSRA-TD500 dataset introduced in <ref type="bibr" target="#b28">[29]</ref>, is a multi-orientation text dataset including 300 training images and 200 testing images. The dataset contains text in two languages, namely Chinese and English. This dataset is very challenging due to the large variation in fonts, scales, colors and orientations. Here, we followed the evaluation protocol employed by <ref type="bibr" target="#b28">[29]</ref>, which considers both of the area overlap ratios and the orientation differences between predictions and the ground truth. ICDAR2015 -Incidental Scene Text dataset. The IC-DAR2015 -Incidental Scene Text dataset is the benchmark of ICDAR2015 Incidental Scene Text Competition. This dataset includes 1000 training images and 500 testing images. Different from the previous ICDAR competition, in which the text are well-captured, horizontal, and typically centered in images, these datasets focus on the incidental scene where text may appear in any orientation and any location with small size or low resolution. The evaluation protocol of this dataset inherits from <ref type="bibr" target="#b13">[14]</ref>. Note that this competition provides an online evaluation system and our method is evaluated in the same way. Unlike MSRA-TD500, in which the ground truth is marked at the sentence level, the annotations of IC-DAR2015 are word level. To satisfy the requirement of ICDAR 2015 measurement, we perform the word partition on the text lines generated by our method according to the blanks between words. ICDAR2013. The ICDAR 2013 dataset is a horizontal text database which is used in previous ICDAR competitions. This dataset consists of 229 images for training and 233 images for testing. The evaluation algorithm is introduced by <ref type="bibr" target="#b10">[11]</ref> and we evaluate our method on the ICDAR2013 online evaluation system. Since this dataset also provides wordlevel annotations, we adopt the same word partition procedure as we did on ICDAR 2015 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In the proposed method, two models are used: the Text-Block FCN is used to generate text salient maps and the Character-Centroid FCN is used to predict the centroids of characters. Both of the two models are trained under the same network configuration. Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">26]</ref>, we also use fine-tuning with the pre-trained VGG-16 network. The two models both are trained 20×10 5 iterations in all. Learning rates start from 10 −6 , and are multiplied by 1 10 after 10 × 10 5 and 15 × 10 5 iterations. Weight decays are 0.0001, and momentums are 0.9. No dropout or batch normalization is used in our model.</p><p>All training images are harvested from the training set of ICDAR2013, ICDAR2015 and MSRA-TD500 with data augmentation. In the training phase of the Text-Block FCN, we randomly crop 30K 500 × 500 patches from the images as training examples. To compute the salient map in the testing phase, each image is proportionally resized to three scales, where the heights are 200, 500 and 1000 pixels respectively. For the Character-Centroid FCN, the patches (32 × 256 pixels) around the word level ground truth are collected as training examples. We randomly collect 100K patches in the training phase. In the testing phase, we rotate </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Results</head><p>MSRA-TD500. As shown in Tab. 1, our method outperforms other methods in both precision and recall on MSRA-TD500. The proposed method achieves precision 0.83, recall 0.67 and f-measure 0.74. Compared to <ref type="bibr" target="#b31">[32]</ref>, our method obtains significant improvements on precision (0.02), recall (0.04) and f-measure (0.03). In addition, the time cost of our method is reported in Tab. 1. Benefiting from GPU acceleration, our method takes 2.1s for each image in average on MSRA-TD500. ICADR2015 -Incidental Scene Text. As this dataset has been released recently for the competition in ICDAR2015, there is no literature to report the experimental result on it. Therefore, we collect competition results <ref type="bibr" target="#b9">[10]</ref> as listed in Tab. 2 for comprehensive comparisons. Our method achieves the best F-measure over all methods. ICDAR 2013. We also test our method on the ICDAR2013 dataset, which is the most popular for horizontal text detection. As shown in Tab. 3, the proposed method achieves 0.88, 0.78, 0.83 in precision, recall and F-measure, re- spectively, outperforming all other recent methods only designed for horizontal text.</p><p>The consistent top performance achieved on the three datasets demonstrates the effectiveness and generality of the proposed method. Besides the quantitative experimental results, several detection examples under various challenging cases of the proposed method on the MSRA-TD500 and IC-DAR2013 datasets are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>. As can be seen, our method successfully detects the text with inner texture in <ref type="figure" target="#fig_0">Fig. 10 (a)</ref>, non-uniform illumination in <ref type="figure" target="#fig_0">Fig. 10</ref> (b) (f), dot fonts ( <ref type="figure" target="#fig_0">Fig. 10 (e)</ref>), broken strokes ( <ref type="figure" target="#fig_0">Fig. 10 (h)</ref>), multiple orientations <ref type="figure" target="#fig_0">(Fig. 10 (c)</ref>, and (d)), perspective distortion ( <ref type="figure" target="#fig_0">Fig. 10 (h)</ref>) and mixture of multi-language ( <ref type="figure" target="#fig_0">Fig. 10 (g)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of Parameters</head><p>In this section, we investigate the effect of parameters T 1 and T 2 , which are used to extract MSER components for computing text line candidates. The performance of different parameters is computed on MSRA-TD500. <ref type="figure" target="#fig_0">Fig. 12</ref> (a) and <ref type="figure" target="#fig_0">Fig. 12 (b)</ref> show how the recall of text line candidates changes under the different settings of T 1 and T 2 . As we can see, the recall of text line candidates is insensitive to the change of T 1 and T 2 in a large range. This proves our method does not depend on the quality of character candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitations of the Proposed Algorithm</head><p>The proposed method achieves excellent performance and is able to deal with several challenging cases. However, our method still has a great gap to achieve a perfect performance. Several failure cases are illustrated in <ref type="figure" target="#fig_0">Fig. 11</ref>. As can be seen, false positives and missing characters may appear in certain situations, such as extremely low contrast ( <ref type="figure" target="#fig_0">Fig. 11 (a)</ref>), curvature ( <ref type="figure" target="#fig_0">Fig. 11 (e)</ref>), strong reflect light ( <ref type="figure" target="#fig_0">Fig. 11 (b) (f)</ref>), too closed text lines ( <ref type="figure" target="#fig_0">Fig. 11 (c)</ref>), or tremendous gap between characters ( <ref type="figure" target="#fig_0">Fig. 11 (d)</ref>). Another limitation is the speed of the proposed method, which is still far from the requirement of real-time systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented a novel framework for multioriented scene text detection. The main idea that integrates semantic labeling by FCN and MSER provides a natural solution for handling multi-oriented text. The superior performance over other competing methods in the literature on both horizontal and multi-oriented text detection benchmarks verifies that combining local and global cues for text line localization is an interesting direction that is worthy of being studied. In the future, we could extend the proposed method to an end-to-end text recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The procedure of the proposed method. (a) An input image; (b) The salient map of the text regions predicted by the Text-Block FCN; (c) Text block generation; (d) Candidate character component extraction; (e) Orientation estimation by component projection; (f) Text line candidates extraction; (g) The detection results of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The network architecture of the Text-Block FCN whose 5 convolutional stages are inherited from VGG 16-layer model. For each stage, a deconvolutional layer (equals to a 1 × 1 convolutional layer and a upsampling layer) is connected. All the feature maps are concatenated with a 1 × 1 convolutional layer and a sigmoid layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The results of a character detector and our method. (a) An input image; (b) The character response map, which is generated by the state-of-the-art method<ref type="bibr" target="#b7">[8]</ref>; (c) The salient map of text regions, which is generated by the Text-Block FCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The illustration of the ground truth map used in the training phase of the Text-Block FCN. (a) An input image. The text lines within the image are labeled with red bounding boxes; (b) The ground truth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>(a) The line chart about the counting number of components in different orientations. The right direction has the maximum value (red circle), and the wrong direction has smaller value (green circle); (b) The red line and green line correspond to circles on the line chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The illustration of text line candidate generation. (a) The character components within a text block are divided into groups; (b) The middle red point is the center of a group and the other two red points belong to P; (c) The minimum bounding box is computed as a text line candidate; (d) All the text line candidates of this text block are extracted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .Figure 8 .</head><label>108</label><figDesc>Detection examples of the proposed method on MSRA-TD500 and ICDAR2013. The illustration of the ground truth map used for training the Character-Centroid FCN. (a) Input images; (b) The ground truth maps. The white circles in (b) indicate the centroid of characters within the input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .Figure 11 .</head><label>911</label><figDesc>Examples of probability maps predicted by the Character-Centroid FCN. (a) Input images; (b) The probability maps of character centroids. Several failure cases of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>The recall of text line candidates with different T1 and T2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparisons on the MSRA-TD500 dataset.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Precision</cell><cell cols="3">Recall F-measure Time cost</cell></row><row><cell>Proposed</cell><cell>0.83</cell><cell></cell><cell>0.67</cell><cell>0.74</cell><cell>2.1s</cell></row><row><cell>Yin et al. [32]</cell><cell>0.81</cell><cell></cell><cell>0.63</cell><cell>0.71</cell><cell>1.4s</cell></row><row><cell>Kang et al. [9]</cell><cell>0.71</cell><cell></cell><cell>0.62</cell><cell>0.66</cell><cell>-</cell></row><row><cell>Yin et al. [33]</cell><cell>0.71</cell><cell></cell><cell>0.61</cell><cell>0.65</cell><cell>0.8s</cell></row><row><cell>Yao et al. [29]</cell><cell>0.63</cell><cell></cell><cell>0.63</cell><cell>0.60</cell><cell>7.2s</cell></row><row><cell cols="6">Table 2. Performance of different algorithms evaluated on the IC-</cell></row><row><cell cols="6">DAR2015 dataset. The comparison results are collected from IC-</cell></row><row><cell cols="6">DAR 2015 Competition on Robust Reading [10].</cell></row><row><cell cols="2">Algorithm</cell><cell cols="3">Precision Recall</cell><cell>F-measure</cell></row><row><cell cols="2">Proposed</cell><cell></cell><cell>0.71</cell><cell>0.43</cell><cell>0.54</cell></row><row><cell cols="2">StradVision-2</cell><cell></cell><cell>0.77</cell><cell>0.37</cell><cell>0.50</cell></row><row><cell cols="2">StradVision-1</cell><cell></cell><cell>0.53</cell><cell>0.46</cell><cell>0.50</cell></row><row><cell cols="2">NJU Text</cell><cell></cell><cell>0.70</cell><cell>0.36</cell><cell>0.47</cell></row><row><cell>AJOU</cell><cell></cell><cell></cell><cell>0.47</cell><cell>0.47</cell><cell>0.47</cell></row><row><cell cols="2">HUST MCLAB</cell><cell></cell><cell>0.44</cell><cell>0.38</cell><cell>0.41</cell></row><row><cell cols="2">Deep2Text-MO</cell><cell></cell><cell>0.50</cell><cell>0.32</cell><cell>0.39</cell></row><row><cell cols="2">CNN Proposal</cell><cell></cell><cell>0.35</cell><cell>0.34</cell><cell>0.35</cell></row><row><cell cols="2">TextCatcher-2</cell><cell></cell><cell>0.25</cell><cell>0.34</cell><cell>0.29</cell></row><row><cell cols="6">text line candidates to horizontal orientation and proportion-</cell></row><row><cell cols="6">ally resize them to 32 pixels height. For all experiments,</cell></row><row><cell cols="5">threshold values are: T 1 = 0.5%, T 2 = 3.</cell></row><row><cell cols="6">The proposed method is implemented with Torch7 and</cell></row><row><cell cols="6">Matlab (with C/C++ mex functions) and runs on a work-</cell></row><row><cell cols="6">station(2.0GHz 8-core CPU, 64G RAM, GTX TitanX and</cell></row><row><cell cols="6">Windows 64-bit OS) for all the experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of different algorithms evaluated on the IC-DAR 2013 dataset.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">Precision Recall F-measure</cell></row><row><cell>Proposed</cell><cell>0.88</cell><cell>0.78</cell><cell>0.83</cell></row><row><cell>Zhang et al. [35]</cell><cell>0.88</cell><cell>0.74</cell><cell>0.80</cell></row><row><cell>Tian et al. [23]</cell><cell>0.85</cell><cell>0.76</cell><cell>0.80</cell></row><row><cell>Lu et al. [13]</cell><cell>0.89</cell><cell>0.70</cell><cell>0.78</cell></row><row><cell>iwrr2014 [34]</cell><cell>0.86</cell><cell>0.70</cell><cell>0.77</cell></row><row><cell>USTB TexStar [33]</cell><cell>0.88</cell><cell>0.66</cell><cell>0.76</cell></row><row><cell>Text Spotter [16]</cell><cell>0.88</cell><cell>0.65</cell><cell>0.74</cell></row><row><cell>Yin et al. [32]</cell><cell>0.84</cell><cell>0.65</cell><cell>0.73</cell></row><row><cell>CASIA NLPR [1]</cell><cell>0.79</cell><cell>0.68</cell><cell>0.73</cell></row><row><cell>Text Detector CASIA [21]</cell><cell>0.85</cell><cell>0.63</cell><cell>0.72</cell></row><row><cell>I2R NUS FAR [1]</cell><cell>0.75</cell><cell>0.69</cell><cell>0.72</cell></row><row><cell>I2R NUS [1]</cell><cell>0.73</cell><cell>0.66</cell><cell>0.69</cell></row><row><cell>TH-TextLoc [1]</cell><cell>0.70</cell><cell>0.65</cell><cell>0.67</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://dag.cvc.uab.es/icdar2013competition" />
		<title level="m">ICDAR 2013 robust reading competition challenge 2 results</title>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pho-toOCR: Reading text in uncontrolled conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Text localization in natural images using stroke feature transform and text covariance descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust scene text detection with convolution neural network induced mser trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reading text in the wild with convolutional neural networks. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Orientation robust text line detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scene text extraction based on edges and support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ICDAR 2005 text locating competition results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDAR</title>
		<meeting>of ICDAR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scene text localization and recognition with oriented stroke detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detection of linear oblique structures and skew scan in digitized documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR</title>
		<meeting>of ICPR</meeting>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast and robust text spotter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WACV</title>
		<meeting>of WACV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene text detection using graph model built upon maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text flow: A unified text detection system in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR</title>
		<meeting>of ICPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text detection in stores using a repetition prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WACV</title>
		<meeting>of WACV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Text string detection from natural scenes by structure-based partition and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2594" to="2605" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiorientation scene text detection with adaptive clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>. 1, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="970" to="983" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Text localization based on fast feature pyramids and multi-resolution maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Noce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV workshop</title>
		<meeting>of ACCV workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
