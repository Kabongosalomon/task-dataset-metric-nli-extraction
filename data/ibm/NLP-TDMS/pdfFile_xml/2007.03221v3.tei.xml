<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Xu</surname></persName>
							<email>zixuanxu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banghuai</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Geng</surname></persName>
							<email>gengm@buaa.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<email>yuanye@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AnchorFace: An Anchor-based Facial Landmark Detector Across Large Poses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial landmark localization aims to detect the predefined points of human faces, and the topic has been rapidly improved with the recent development of neural network based methods. However, it remains a challenging task when dealing with faces in unconstrained scenarios, especially with large pose variations. In this paper, we target the problem of facial landmark localization across large poses and address this task based on a split-and-aggregate strategy. To split the search space, we propose a set of anchor templates as references for regression, which well addresses the large variations of face poses. Based on the prediction of each anchor template, we propose to aggregate the results, which can reduce the landmark uncertainty due to the large poses. Overall, our proposed approach, named AnchorFace, obtains stateof-the-art results with extremely efficient inference speed on four challenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be available at https://github.com/ nothingelse92/AnchorFace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Facial landmark localization, or face alignment, refers to detect a set of predefined landmarks on the human face. It is a fundamental step for many facial related applications, e.g. face verification/recognition, expression recognition, and facial attribute analysis.</p><p>With the recent development of convolutional neural network based methods <ref type="bibr" target="#b21">(Ma et al. 2018;</ref><ref type="bibr" target="#b37">Xu et al. 2020)</ref>, the performance for facial landmark localization in constrained scenarios has been greatly improved <ref type="bibr" target="#b44">Zhu et al. 2019;</ref><ref type="bibr" target="#b32">Wu et al. 2018</ref>). However, unconstrained scenarios, for example, faces with large pose, still limit the wide application of the existing landmark algorithms. In this paper, we target to address the problem of facial landmark localization across large poses.</p><p>There are two challenges for facial landmark detection across large poses. On one hand, faces with large poses will significantly increase the difficulty for landmark localization due to the large variations among different poses. As shown in <ref type="figure">Fig. 1</ref>, directly regressing the point coordinates may not be able to localize every landmark point precisely. On the <ref type="figure">Figure 1</ref>: A comparison between direct regression and anchor-based regression (AnchorFace). Our AnchorFace includes two steps. The first step is to introduce the anchor templates and regress the offsets based on each anchor template (Second Column). The second step is to aggregate the prediction results from multiple anchor templates <ref type="table">(Third  Column)</ref> other hand, there usually exists a large probability of uncertainty due to the self-occlusion and noisy annotations. For example, occlusion will usually lead to invisible landmarks, which will increase the uncertainty for the landmark prediction. Besides, the faces with a large pose will also cause difficulty during the data annotation process.</p><p>To address the above two challenges, we propose a novel pipeline for facial landmark localization based on an anchorbased design. The new pipeline includes two steps: split and aggregate. An overview of our pipeline can be found in <ref type="figure">Fig. 2</ref>. To deal with the first challenge with large pose variations, we adopt the divide-and-conquer way following an anchor-based design. We propose to use the anchor templates to split the search space, and each anchor will serve as a reference for regression. This can significantly reduce the pose variations for each anchor. To address the second issue with pose uncertainty, we propose to aggregate each anchor result weighted by the predicted confidence.</p><p>In summary, we propose AnchorFace to implement the split-and-aggregate strategy. There are three contributions in our paper.</p><p>• We propose a novel pipeline with a split-and-aggregate strategy which can well address the challenges for face alignment across large poses.</p><p>• To implement the split-and-aggregate strategy, we introduce the anchor design into the facial landmark problem, which can simplify the search space for each anchor template and meanwhile improve the robustness for landmark uncertainty.</p><p>• Our proposed AnchorFace can achieve promising results on four challenging benchmarks with a realtime inference speed of ∼45 FPS 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Facial Landmark Localization. In the literature of facial landmark localization, a number of achievements have been developed including the classic ASMs <ref type="bibr" target="#b22">(Milborrow and Nicolls 2008)</ref>, AAMs <ref type="bibr" target="#b11">(Ikeuchi, Hebert, and Delingette 1995;</ref><ref type="bibr" target="#b13">Kahraman et al. 2007</ref>), CLMs <ref type="bibr">Cootes 2008, 2006)</ref>, and Cascaded Regression Models <ref type="bibr" target="#b2">(Cao et al. 2014;</ref><ref type="bibr" target="#b45">Zhu et al. 2015;</ref><ref type="bibr" target="#b46">Zhu et al. 2016)</ref>. Nowadays, more and more deep learning-based methods have been applied in this area. These deep learning based methods could be divided into two categories, i.e. coordinate regression methods and heatmap regression methods.</p><p>Coordinate regression methods directly map the discriminative features to the target landmark coordinates. The earliest work can be dated to <ref type="bibr" target="#b29">(Sun, Wang, and Tang 2013)</ref>. <ref type="bibr" target="#b29">Sun et al. (Sun, Wang, and Tang 2013)</ref> used a three-level cascade CNN to do facial landmark localization in a coarse-tofine manner, and achieved promising localization accuracy. MDM <ref type="bibr" target="#b30">(Trigeorgis et al. 2016</ref>) was the first to apply a recurrent convolutional network model for facial landmark localization in an end-to-end manner. <ref type="bibr" target="#b43">Zhang et al. (Zhang et al. 2014</ref>) utilized a multi-task learning framework to optimize facial landmark localization and correlated facial attributes analysis simultaneously. Recently, Wingloss ) was proposed as a new loss function for landmark localization, which can obtain robust performance against widely used L2 loss.</p><p>Heatmap regression methods generate a probability heatmap for each landmark, respectively. Benefit from FCN <ref type="bibr" target="#b18">(Long, Shelhamer, and Darrell 2015)</ref> and Hourglass <ref type="bibr" target="#b23">(Newell, Yang, and Deng 2016)</ref>, heatmap regression methods have been successfully applied to landmark localization problems and have achieved state-of-the-art performance. JMFA ) achieved high localization accuracy with a stacked hourglass network <ref type="bibr" target="#b23">(Newell, Yang, and Deng 2016)</ref> for multi-view facial landmark localization in the Menpo  competition. <ref type="bibr" target="#b39">Yang et al. (Yang, Liu, and Zhang 2017)</ref> adopted a supervised face transformation to normalize the faces, then employed an Hourglass network to regress it. Recently, LAB <ref type="bibr" target="#b32">(Wu et al. 2018)</ref> proposed to use additional boundary lines as the geometric structure of a face image to help facial landmark localization.</p><p>Faces with Large Pose. Large pose is a challenging task for facial landmark localization, and different strategies have been proposed to address the difficulty. Multi-view framework and 3D model are two popular ways. Multi-view framework uses different landmark configurations for different views. For example, TSPM <ref type="bibr" target="#b49">(Zhu and Ramanan 2012)</ref> and CDM <ref type="bibr" target="#b40">(Yu et al. 2013</ref>) employ DPM-like <ref type="bibr" target="#b9">(Felzenszwalb et al. 2010</ref>) method to align faces with different shape models, and choose the highest possibility model as the final result. However, multi-view methods have to cover each view, making it impractical in the wild. 3D face models have been widely used in recent years, which fit a 3D morphable model (3DMM) <ref type="bibr" target="#b0">(Blanz and Vetter 2003)</ref> by minimizing the difference between face image and model appearance. Lost face information can be recovered to localize the invisible facial landmarks <ref type="bibr" target="#b16">Liu et al. 2017;</ref><ref type="bibr" target="#b48">Zhu et al. 2016b;</ref><ref type="bibr" target="#b1">Bulat and Tzimiropoulos 2017)</ref>. However, 3D face models are limited by their own database and the iterative label generation method. Besides, researchers have applied multi-task learning to address the difficulties resulting from pose variations. Other facial analysis tasks, such as pose estimation or facial attributes analysis, can be jointly trained with facial landmark localization <ref type="bibr" target="#b36">(Xu and Kakadiaris 2017;</ref><ref type="bibr" target="#b42">Zhang et al. 2016)</ref>. With joint training, multi-task learning can boost the performance of each subtask. The facial landmark localization task can achieve robust performance. But the multi-task framework is not specially designed for landmark localization, it contains much redundant information and contributes to large models.</p><p>In this paper, we propose an anchor-based model for facial landmark localization. Different from <ref type="bibr" target="#b35">(Xiong et al. 2019)</ref>, which utilized anchor points to predict the positions of a human 3D pose, our approach introduces a split-and-aggregate pipeline for the facial landmark localization. Anchor is utilized as a reference for regression in our approach. Overall, our model requires neither cascaded networks nor large backbones, leading to a great reduction in model parameters and computation complexity, while still achieving comparable or even better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>In this paper, we propose a new split-and-aggregate strategy for facial landmark detector across large poses. An overview of our pipeline can be found in <ref type="figure">Fig. 2</ref>. To implement the split-and-aggregate strategy, we introduce the anchor-based design, and our approach is named Anchor-Face. In the following section, we will discuss the split and aggregate steps separately, followed by the details on the network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Split Step</head><p>Due to the large pose variations among different poses, it is a challenging problem to directly regress the facial landmarks while maintaining high localization precision. In this paper, we propose to utilize the divide-and-conquer way to address the issue from large pose variations. More specifically, we propose to employ the anchor templates as regression references to split the search space. Different from the Backbone Regression Branch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Branch</head><p>Split</p><formula xml:id="formula_0">C i C j</formula><p>Anchor configuration</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicted landmarks</head><p>Anchor template <ref type="figure">Figure 2</ref>: The pipeline of our proposed AnchorFace landmark detector. AnchorFace is based on a split-and-aggregate strategy, which consists of the backbone and two functional branches: the offset regression branch and the confidence branch. In the split step, we predict the landmark position based on each anchor template. During aggregate step, the predictions of multiple anchor templates are averaged by weighted confidence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anchor templates on one sample anchor point</head><p>Anchor grid Anchor area <ref type="figure">Figure 3</ref>: An illustration of our anchor configuration. Anchor area is a region centered at the image center with a spatial neighborhood. Based on the anchor area, we setup a grid of anchor points, where each anchor point contains a set of anchor templates to model various pose variations traditional methods which regress the landmarks with a uniform facial landmark detector, we propose to regress the offsets base on a set of anchor templates.</p><p>Anchor Configuration. As shown in <ref type="figure">Fig. 3</ref>, there are three hyper-parameters for designing the anchor configuration: anchor area, anchor grid, and anchor templates.</p><p>Anchor area is denoted as the region to set the anchors. It is usually centered at the image center with a spatial neighborhood. The reason to define the anchor area is that the input image is cropped to put the face near the image center. Thus, we select a region near the image center, which is called anchor area, to set up the anchors. Based on the anchor area, we sample a set of anchor points in a grid, e.g. a 7 × 7 grid, as shown in <ref type="figure">Fig. 3</ref>. Each anchor point can be considered as the center of a set of anchor templates. The anchor templates are designed to address the challenges from large pose variations. Intuitively, these anchor templates are used to split the search space for regression and can serve as references for offsets prediction. Therefore, the sampling of anchor templates should be able to cover different variations of large poses and reduce the redundancy for the anchor sets.</p><p>To implement the anchor templates, we present two potential ways. The first one is to hand-design the anchors based on prior knowledge. The second one integrates the proposals generated by the data distribution.</p><p>An overview of our hand-designed anchors can be found in <ref type="figure">Fig. 3</ref>. For each anchor point, we explore the 3D pose spaces (yaw, roll, pitch) and design the pose-level anchor set as follows. As unconstrained large-pose faces have large variations on the yaw direction, we first select N yaw base anchors (N yaw = 3 in our paper representing the anchors for the left, frontal, right faces). To generate the N yaw base anchors, we utilize a heuristic approach to divide the training faces into three buckets and compute the average face landmarks for each bucket to obtain the anchor proposal. More specifically, we use the ratio of two eyes' width for bucket assignment. We define an indicator to estimate the yaw angle of each training face:</p><formula xml:id="formula_1">r = |p l1 − p l2 | 2 |p r1 − p r2 | 2 − |p r1 − p r2 | 2 |p l1 − p l2 | 2 ,<label>(1)</label></formula><p>where p l1 , p l2 , p r1 , p r2 are the coordinates of left eye inner corner, left eye outer corner, right eye inner corner, and right eye outer corner respectively. With a threshold γ, we put the faces into the left or right bucket, when r &gt; γ or r &lt; −γ.</p><p>The other faces will be kept into the frontal bucket. We set γ = 6 in our experiments, as shown in <ref type="figure">Fig. 4</ref>. <ref type="figure">Figure 4</ref>: An illustration of the metric r for classifying the faces into three buckets along the yaw direction Based on the N yaw base anchors, to cover the roll variations, we rotate each anchor on the roll dimension. For example, we can get twenty-four templates by rotating the basic three anchors each 45 • from 0 • to 360 • . Optionally, we can involve the pitch variations by directly projecting (rotating) along the pitch dimension. However, based on our experimental results, the anchors designed along the pitch view cannot further improve the performance but compromise the computational speed. Thus, in our final design, only anchors along the yaw and roll dimensions are utilized as shown in <ref type="figure">Fig. 3</ref>.</p><formula xml:id="formula_2">0 r = 6 r  6 r  −</formula><p>An alternative solution for the anchor design is based on the data distribution among the training faces. We first perform KMeans clustering, and we can generate a set of base anchors. One example is shown in <ref type="figure" target="#fig_0">Fig. 5</ref>. We can see that the clustered anchors among all the training faces obtain similar anchors along the yaw direction, as discussed in handdesigned anchors. Following the similar steps for the handdesigned anchors, we can rotate the generated prototypes along the roll and pitch direction to generate more anchors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample anchor templates by KMeans</head><p>Sample anchor templates by hand-design Regression and Confidence Branch. Based on the anchor proposals, we design a new head structure which involves two branches: regression branch and confidence branch. Regression branch aims to regress the landmark coordinate offsets based on each anchor. Confidence branch assigns each anchor with a confidence score. Among all the anchor templates, those anchors which are close to the pose of the ground-truth face should be given higher confidence.</p><p>As shown in <ref type="figure">Fig. 2</ref>, both the confidence branch and the regression branch are built upon the output feature map of the backbone network. While we set h · w anchors in the image, the output of the confidence and regression branch are C con · h · w and C reg · h · w respectively, where C con and C reg are denoted as the output channel number of the confidence branch and the regression branch respectively. Here C con = K and C reg = K · 2L, where K, L refer to the number of anchor templates on each anchor point and the number of facial landmarks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol Definition</head><p>A A set of Anchor points on the spatial anchor grid a</p><p>One anchor point a ∈ A T</p><p>A set of anchor templates as in <ref type="figure">Fig. 3  T (a, t)</ref> Anchor template t ∈ T centering at anchor point a Tj (a, t)</p><p>Landmark j on anchor template T (a, t) O(a, t)</p><p>Output from the regression branch based on T (a, t) O(a, t)</p><p>Ground-truth (GT) offsets based on T (a, t) C(a, t)</p><p>Output from the confidence branch based on T (a, t) C(a, t)</p><p>Confidence GT label based on T (a, t) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate Step</head><p>Large-pose faces will increase the uncertainty for the landmark prediction. To address this problem, we propose to aggregate the predictions from different anchor templates. More specifically, we first set a threshold C th to pick up the reliable anchor predictions. The anchor predictions with low confidence scores are regarded as outliers and will be discarded. The remaining anchor predictions will be averaged by the weighted confidence for each prediction. As a result, the position of landmark j can be obtained as the weighted average of the outputs of all anchor faces as:</p><formula xml:id="formula_3">S j = a∈A,t∈T C(a, t) · (O j (a, t) + T j (a, t)) a∈A,t∈T C(a, t) ,<label>(2)</label></formula><p>where C(a, t) = 0, C(a, t) &lt; C th C(a, t), others</p><p>The definition of the symbols can be found from <ref type="table" target="#tab_0">Table 1</ref>, and the threshold C th is set to 0.6 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Training</head><p>In this subsection, we will discuss the ground-truth setting for the regression and confidence branch as well as the related losses. For the regression branch, the target is to regress the offsets against each of the predefined anchor. The regression loss L reg can be defined as:</p><formula xml:id="formula_5">L reg = a∈A,t∈T C(a, t) j |O j (a, t) − O j (a, t)|,<label>(4)</label></formula><p>where O j (a, t) and O j (a, t) refer to the prediction offsets and the ground-truth offsets for jth landmark. C(a, t) is denoted as the confidence weight for the anchor template T (a, t). The detailed symbol definitions can be found in <ref type="table" target="#tab_0">Table 1</ref>. For the confidence branch, we set the targeted confidence output C(a, t) as the inverse L2 distance between the anchor pose v 1 and the ground-truth pose v 2 as ||v 1 − v 2 || 2 , where v 1 , v 2 refer to flatten landmark coordinates. To normalize the pose difference, we perform a tanh operation as:</p><formula xml:id="formula_6">C = tanh(( ||v 1 − v 2 || 2 β · 2L ) −1 ),<label>(5)</label></formula><p>where β is a hyperparameter and L refers to the count of facial landmarks. The confidence loss is then defined as: <ref type="figure">C(a, t))</ref>).</p><formula xml:id="formula_7">L con = a∈A,t∈T (−C(a, t) · log C(a, t) − (1 − C(a, t)) · log(1 −</formula><p>The network is jointly supervised by the two loss functions above with end-to-end training. The final training loss is then defined as:</p><p>L total = L reg + λ · L con (7) where λ is a hyperparameter in our method, and it is insensitive to the localization accuracy in our experiments.  <ref type="bibr" target="#b32">(Wu et al. 2018)</ref>. They are all widely used benchmarks in the facial landmark research area. More details about these datasets can be found in our supplementary materials.</p><p>Evaluation metric. We adopt the normalized mean error (NME) for evaluation. The normalized mean error is defined as the average Euclidean distance between the predicted facial landmark locations O i,j and their corresponding ground-truth facial landmark annotations O i,j :</p><formula xml:id="formula_9">N M E = 1 N N i=1 1 L L j=1 |O i,j − O i,j | 2 d<label>(8)</label></formula><p>where N is the number of images in the testing set, L is the number of landmarks, and d is the normalization factor. On AFLW dataset, we follow <ref type="bibr" target="#b45">(Zhu et al. 2015)</ref> to use face size as the normalization factor. On Menpo dataset, we use the distance between left-top corner and right-bottom corner as the normalization factor. On 300W and WFLW dataset, we follow MDM <ref type="bibr" target="#b30">(Trigeorgis et al. 2016)</ref> and  to use the "inter-ocular" normalization factor, i.e. the distance between the outer eye corners. In addition, on WFLW dataset, two further statistics i.e. the area-under-the-curve (AUC) <ref type="bibr" target="#b38">(Yang et al. 2015)</ref> and the failure rate (which is defined as the proportion of failed detected faces) are measured for furthter analysis. Especially, any normalized error above 0.1 is considered as a failure <ref type="bibr" target="#b32">(Wu et al. 2018)</ref>. Implementation details. In our method, the original images are cropped and resized to a fixed resolution, i.e. 224 × 224, according to the provided bounding boxes. Anchor templates are generated based on KMeans clustering following , while anchor area and anchor grid are set as 56 × 56 and 7 × 7 respectively. Random rotation and translation are applied for data augmentation. We apply the Adam optimizer with the weight decay of 1 × 10 −5 and train the network for 50 epochs in total. The learning rate is set to 1 × 10 −3 and divided by ten at 20-th, 30-th, 40-th epoch. β = 0.05 and λ = 0.5 are applied to all models across four benchmarks. If there are no special instructions, ShuffleNet-V2 <ref type="bibr" target="#b21">(Ma et al. 2018</ref>) is utilized as the backbone for our algorithm in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the state-of-the-art methods</head><p>For a fair comparison, we only compare the methods following the standard settings, as discussed in Section . Therefore, those methods which are trained from external datasets or combined multiple datasets are not compared. Due to the space limit, comparisons about Menpo are reported in our supplementary materials.</p><p>AFLW dataset: We first evaluate our algorithm on the AFLW dataset. The performance comparisons are given in <ref type="table" target="#tab_1">Table 2</ref>. It can be observed that, on this large dataset, our network outperforms the other approaches. As mentioned in Section , AFLW contains lots of faces with large poses. Note that our method has a significant improvement on AFLW-Full set against AFLW-Frontal, which means that we achieve more robust localization performance on faces in unconstrained scenarios, including large pose. This essentially validates the superiority of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFLW-Full AFLW-Frontal</head><p>LBF <ref type="bibr" target="#b25">(Ren et al. 2016)</ref> 4.24 2.74 CFSS <ref type="bibr" target="#b45">(Zhu et al. 2015)</ref> 3.92 2.69 CCL <ref type="bibr" target="#b47">(Zhu et al. 2016a)</ref> 2.72 2.17 TSR  2.17 -SAN  1.91 1.85 Wing  1.65 -SA  1.62 -ODN  1.63 1.38 AnchorFace 1.56 1.38 300W dataset: We compare our approach against several state-of-the-art methods on 300W Fullset. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Since there are fewer large pose variations across the whole dataset and the cropped faces normally center near the image center point, 300W dataset is not very challenging compared with the other three benchmarks. However, our algorithm still can achieve promising localization performance with an efficient speed at ∼45 fps with batch size 1. Compared with LAB <ref type="bibr" target="#b32">(Wu et al. 2018)</ref>, which is slightly better than our method, our approach is much faster (45 fps vs 17 fps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common</head><p>Challenge Full MDM <ref type="bibr" target="#b30">(Trigeorgis et al. 2016)</ref> 4.83 10.14 5.88 Two-Stage  4.36 7.42 4.96 RDR <ref type="bibr" target="#b34">(Xiao et al. 2017)</ref> 5.03 8.95 5.80 Pose-Invariant  5.43 9.88 6.30 SBR  3.28 7.58 4.10 PCD-CNN <ref type="bibr" target="#b14">(Kumar and Chellappa 2018)</ref> 3.67 7.62 4.44 LAB <ref type="bibr" target="#b32">(Wu et al. 2018)</ref> 2.98 5.19 3.49 SAN  3.34 6.60 3.98 ODN  3.56 6.67 4.17</p><p>AnchorFace 3.12 6.19 3.72  WFLW dataset: A comparison of the performance from our proposed approach as well as state-of-the-art methods on WFLW dataset is shown in <ref type="table" target="#tab_3">Table 4</ref>. Considering the difficulty of WFLW, we attempt to adopt HRNet-18  as the backbone to implement our AnchorFace. As indicated in <ref type="table" target="#tab_3">Table 4</ref>, AnchorFace equipped with HRNet-18 ) achieves the best performance on NME &amp; AUC metrics and achieves the second best performance on Failure Rate metric. Furthermore, the efficent Anchor-Face with ShuffleNet-V2 <ref type="bibr" target="#b21">(Ma et al. 2018</ref>) also achieves impressive results considering the huge computation cost of other methods. This also directly verifies the versatility of our method. Detailed results about each subset of WFLW are reported in our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model analysis</head><p>Our proposed AnchorFace introduces a novel split-andaggregate strategy based on anchor design to address the face alignment across large poses. In this section, we perform further analysis of its mechanism.</p><p>Anchor design. Anchor templates serve as regression references to split the search space in our proposed approach. In comparison with directly regressing target landmark coordinates in whole 2D space, regress offsets based on anchor templates can simplify the search space and boost the robustness of localization accuracy. We conduct several experiments on AFLW dataset and make statistics across yaw dimension which is shown in <ref type="figure">Fig. 6</ref>. It is quite clear that An-chorFace significantly outperforms the baseline with lower NME and smaller variances in each subinterval especially for large pose, which can well verify our assumptions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: A comparison of baseline and AnchorFace across yaw dimension</head><p>Split-and-aggregate strategy. In our proposed algorithm, we follow the divide-and-conquer way to address the challenges for face alignment across large poses. To verify its effectiveness, we adopt Pearson correlation coefficient to measure the correlation between confidence scores C(a, t) and prediction errors |O(a, t) − O(a, t)| 2 :</p><formula xml:id="formula_10">P = 1 N N i=1 [r i a,t (|O(a, t) − O(a, t)| 2 , C(a, t))] (9)</formula><p>Where r represents the calculation function of Pearson correlation coefficient. We conduct experiments on AFLW dataset and get P = -0.82, which means a strong negative correlation between them. In other words, anchor template with larger confidence score can achieve more accurate predicted landmarks. It can help filter prediction outliers and aggregate remaining predictions to mitigate the uncertainty of the localization result on a single anchor face. Due to the confidence score is defined as mathematical modeling of the distance between the anchor pose and the groundtruth pose, we can come to another conclusion that closer anchors tend to achieve more accurate localization, which also directly proves our search space split strategy based on anchors. Comparison details can be found in our ablation studies and we show several intuitive samples in our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>In this section, we perform the ablation study for our proposed algorithm on the AFLW dataset, which is a challenging benchmark with large pose variations. More specifically, we divide the test set into four subsets according to the yaw dimension, i.e. Light (0 • ∼ 30 • ), Medium (30 • ∼ 60 • ), Large (60 • ∼ 90 • ), and Heavy (90 • ∼). Normalized mean error is utilized to evaluate the performance of our algorithm. Without explicitly specified, we use anchor templates as KMeans-24 (KMeans clustering to generate 24 anchor templates), anchor area as 56 × 56, anchor grid as 7 × 7, and the aggregating strategy is weighted average for ablation.</p><p>Comparison with the regression baseline. <ref type="table" target="#tab_6">Table 5</ref> compares the performance of our proposed approach with the baseline of direct regression on AFLW dataset. "Baseline" directly maps the discriminative features to the target landmark coordinates with ShuffleNet-V2 backbone. A fully connected layer with length 2L is used as the output of the baseline network. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our proposed anchor-based method significantly outperforms the baseline by a large margin across yaw variations.The improvements are attributed to two reasons. First, the anchor design can significantly reduce the search space and simplify the regression problem. Second, the aggregating of different anchors can further improve model robustness.</p><p>Comparison of various split configurations. Due to the challenges from the large-pose faces, we propose a set of anchor templates as references for regression to split search space. Split strategy consists of three hyper-parameters: anchor templates, anchor area, and anchor grid, as shown in <ref type="figure">Fig. 3</ref>   Anchor template plays a voting role in our method as a reference for regression. As mentioned in Section , we get three basic template faces from the training dataset by hand design or KMeans clustering. Then we do some transformations to get more templates, corresponding to the pose variations in yaw, roll, and pitch dimension. By comparing KMeans-24 against HandDesign-24 in <ref type="table" target="#tab_7">Table 6</ref>, KMeans is better than hand-design approach based on the same anchor number (24). The potential reason is that KMeans utilizes more data features to generate the base anchors, which should be more general compared with hand-designed based anchors. Besides, as shown in <ref type="table" target="#tab_7">Table 6</ref>, 24 may be a good option for the number of anchor templates compared with 3 or 48 in our algorithm.</p><p>Anchor area is the area where we set anchors in the image for the spatial domain. As the input image is cropped and resized to 224 × 224 and the face is around the image center, we set anchor points at a center area with size 14×14, 28 × 28, 56 × 56, 128 × 128, respectively. As shown in Table 7, 56 × 56 around the image center would be a good choice for putting anchors in the spatial domain.</p><p>Anchor grid defines how many anchors we set in the anchor area. For example, 7 × 7 means we sample 49 spatial  points in a 7 × 7 grid from the anchor area to generate the anchor templates. As shown in <ref type="table" target="#tab_11">Table 8</ref>, it is a good choice to set at 7 × 7.  Comparison of various aggregate strategies. To mitigate the uncertainty of the localization result on a single anchor face, we aggregate the predictions from different anchor templates. We introduce three aggregate strategies: Mean, Argmax, confidence weighted voting (Weighted). As shown in <ref type="table">Table 9</ref>, aggregating the predictions with weighted confidences can obtain superior results compared with the argmax choice without aggregating. Besides, the confidence generated by the confidence branch is important if we compare the strategy of "Weighted" and "Mean".  <ref type="table">Table 9</ref>: A comparison of different aggregation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, a novel split-and-aggregate strategy is proposed for large-pose faces. By introducing an anchor-based design, our proposed approach can simplify the regression problem by splitting the search space. Moreover, aggregating the prediction results contributes to reducing uncertainty and improving the localization performance. As validated on four challenging benchmarks, our proposed AnchorFace obtains state-of-the-art results with extremely fast inference speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>A comparison of three base anchors generated by hand-design approach and KMeans clustering based on AFLW(Köstinger et al. 2011) dataset    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The experiments are evaluated on four challenging datasets, i.e. AFLW (Köstinger et al. 2011), 300W (Sagonas et al. 2013), Menpo (Deng et al. 2019; Zafeiriou et al. 2017), and WFLW</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The definition of symbols</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Normalized mean error (%) on AFLW-Full and</cell></row><row><cell>AFLW-Frontal set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Normalized mean error (%) on 300W Common sub-</cell></row><row><cell cols="3">set, Challenging subset, and Full set.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>NME(%)</cell><cell>Failure Rate(%)</cell><cell>AUC</cell><cell>Flops(G)</cell></row><row><cell>CFSS (Zhu et al. 2015)</cell><cell>9.07</cell><cell>29.40</cell><cell>0.3659</cell><cell>-</cell></row><row><cell>DVLN (Wu and Yang 2017)</cell><cell>6.08</cell><cell>10.84</cell><cell>0.4551</cell><cell>-</cell></row><row><cell>LAB (Wu et al. 2018)</cell><cell>5.27</cell><cell>7.56</cell><cell>0.5323</cell><cell>18.85</cell></row><row><cell>SAN (Dong et al. 2018)</cell><cell>5.22</cell><cell>6.32</cell><cell>0.5355</cell><cell>-</cell></row><row><cell>Wing (Feng et al. 2017)</cell><cell>5.11</cell><cell>6.00</cell><cell>0.5504</cell><cell>5.40</cell></row><row><cell>AVS (Qian et al. 2019)</cell><cell>5.25</cell><cell>7.44</cell><cell>0.5034</cell><cell>-</cell></row><row><cell>AWing (Wang, Bo, and Fuxin 2019)</cell><cell>4.36</cell><cell>2.84</cell><cell>0.5719</cell><cell>26.79</cell></row><row><cell>HRNET (Sun et al. 2019)</cell><cell>4.60</cell><cell>4.64</cell><cell>0.5237</cell><cell>-</cell></row><row><cell>AnchorFace</cell><cell>4.62</cell><cell>4.20</cell><cell>0.5516</cell><cell>1.71</cell></row><row><cell>AnchorFace*</cell><cell>4.32</cell><cell>2.96</cell><cell>0.5769</cell><cell>5.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Evaluation results about WFLW test set for</cell></row><row><cell>NME(%), Failure Rate@0.1(%) and AUC@0.1. * means we</cell></row><row><cell>adopt HRNet-18 (Sun et al. 2019) as the backbone network</cell></row><row><cell>to implement AnchorFace.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Method</cell><cell>Full</cell><cell>Light</cell><cell>Medium</cell><cell>Large</cell><cell>Heavy</cell></row><row><cell>Baseline</cell><cell>1.67</cell><cell>1.46</cell><cell>1.92</cell><cell>1.99</cell><cell>2.13</cell></row><row><cell>AnchorFace</cell><cell>1.56</cell><cell>1.40</cell><cell>1.74</cell><cell>1.80</cell><cell>1.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>A comparison of direct regression and anchor-based regression.</figDesc><table><row><cell>Template</cell><cell>Full</cell><cell>Light</cell><cell>Medium</cell><cell>Large</cell><cell>Heavy</cell></row><row><cell>Kmeans-3</cell><cell>1.60</cell><cell>1.43</cell><cell>1.80</cell><cell>1.83</cell><cell>2.10</cell></row><row><cell>Kmeans-24</cell><cell>1.56</cell><cell>1.40</cell><cell>1.74</cell><cell>1.80</cell><cell>1.96</cell></row><row><cell>Kmeans-48</cell><cell>1.58</cell><cell>1.41</cell><cell>1.79</cell><cell>1.82</cell><cell>2.06</cell></row><row><cell>HandDesign-24</cell><cell>1.58</cell><cell>1.42</cell><cell>1.76</cell><cell>1.84</cell><cell>2.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>A comparison of different template settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>A comparison of different anchor area settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>A comparison of different anchor grid settings.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The computational speed of ∼45 FPS is calculated on one Nvidia Titan Xp GPU with batchsize 1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2003.1227983</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How Far are We from Solving the 2D &amp; 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<idno>15505499. doi:10.1109/ ICCV.2017.116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face Alignment by Explicit Shape Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-013-0667-3</idno>
		<ptr target="https://doi.org/10.1007/s11263-013-0667-3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<idno type="DOI">0031-3203.doi:https:/doi.org/10.1016/j.patcog.2008.01.024</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0031320308000630" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature Detection and Tracking with Constrained Local Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-018-1134-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-018-1134-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="599" to="624" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint Multi-View Face Alignment in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3636" to="3648" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Style Aggregated Network for Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00047</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part-Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.167</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Spherical Representation for Recognition of Free-Form Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<idno type="DOI">10.1109/34</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page" from="681" to="690" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pose-Invariant Face Alignment With a Single CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Active Illumination and Appearance (AIA) Model for Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383399</idno>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotated Facial Landmarks in the Wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno>doi:10.1109/ ICCVW.2011.6130513</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semantic Alignment: Finding Semantically Consistent Ground-truth for Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv abs/1903.10661</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Deep Regression Architecture with Two-Stage Reinitialization for High Performance Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.393</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3691" to="3700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shuf-fleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Locating Facial Features with an Extended Active Shape Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
		<idno>978-3-540-88693-8</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2008</title>
		<editor>Forsyth, D.</editor>
		<editor>Torr, P.</editor>
		<editor>and Zisserman, A.</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Networks for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno>978-3-319-46484-8</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B.</editor>
		<editor>Matas, J.</editor>
		<editor>Sebe, N.</editor>
		<editor>and Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.01025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision 2019-Octob</title>
		<meeting>the IEEE International Conference on Computer Vision 2019-Octob</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10152" to="10162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face Alignment via Regressing Local Binary Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2016.2518867</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1233" to="1245" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2013.59</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep highresolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00584</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5686" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Convolutional Network Cascade for Facial Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mnemonic Descent Method: A Recurrent Process Applied for End-to-End Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4177" to="4187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression URL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.07399" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Leveraging Intra and Inter-Dataset Variations for Robust Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent 3D-2D Dual Learning for Large-Pose Facial Landmark Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou Tianyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE Conference on International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint Head Pose Estimation and Face Alignment Framework Using Global and Local CNN Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2017.81</idno>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beta R-CNN: Looking into Pedestrian Detection from Another Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An Empirical Study of Recent Face Alignment Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<idno>abs/1511.05049</idno>
		<ptr target="http://arxiv.org/abs/1511.05049" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stacked Hourglass Network for Robust Facial Landmark Localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pose-Free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.244</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Menpo Facial Landmark Localisation Challenge: A Step Towards the Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2116" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2016.2603342</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning Deep Representation for Face Alignment with Auxiliary Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3967</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust Facial Landmark Detection via Occlusion-Adaptive Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face Alignment by Coarse-to-Fine Shape Searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4998" to="5006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unconstrained Face Alignment via Cascaded Compositional Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.371</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3409" to="3417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unconstrained Face Alignment via Cascaded Compositional Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face Alignment Across Large Poses: A 3D Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6248014</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
