<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Gaifman Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
							<email>mathias.niepert@neclabs.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">NEC Labs Europe Heidelberg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Gaifman Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present discriminative Gaifman models, a novel family of relational machine learning models. Gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (KBs). Considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of overfitting, and facilitates weight sharing. Gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world KBs. We present the core ideas of Gaifman models and apply them to large-scale relational learning problems. We also discuss the ways in which Gaifman models relate to some existing relational machine learning approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases are attracting considerable interest both from industry and academia <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref>. Instances of knowledge bases are the web graph, social and citation networks, and multi-relational knowledge graphs such as Freebase <ref type="bibr" target="#b1">[2]</ref> and YAGO <ref type="bibr" target="#b10">[11]</ref>. Large knowledge bases motivate the development of scalable machine learning models that can reason about objects as well as their properties and relationships. Research in statistical relational learning (SRL) has focused on particular formalisms such as Markov logic <ref type="bibr" target="#b21">[22]</ref> and PROBLOG <ref type="bibr" target="#b7">[8]</ref> and is often concerned with improving the efficiency of inference and learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>. The scalability problems of these statistical relational languages, however, remain an obstacle and have prevented a wider adoption. Another line of work focuses on efficient relational machine learning models that perform well on a particular task such as knowledge base completion and relation extraction. Examples are knowledge base factorization and embedding approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> and random-walk based ML models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b9">10]</ref>. We aim to advance the state of the art in relational machine learning by developing efficient models that learn knowledge base embeddings that are effective for probabilistic query answering on the one hand, and interpretable and widely applicable on the other.</p><p>Gaifman's locality theorem <ref type="bibr" target="#b8">[9]</ref> is a result in the area of finite model theory <ref type="bibr" target="#b15">[16]</ref>. The Gaifman graph of a knowledge base is the undirected graph whose nodes correspond to objects and in which two nodes are connected if the corresponding objects co-occur as arguments of some relation. Gaifman's locality theorem states that every first-order sentence is equivalent to a Boolean combination of sentences whose quantifiers range over local neighborhoods of the Gaifman graph. With this paper, we aim to explore Gaifman locality from a machine learning perspective. If every first-order sentence is equivalent to a Boolean combination of sentences whose quantifiers range over local neighborhoods only, we ought to be able to develop models that learn effective representations from these local neighborhoods. There is increasing evidence that learning representations that are built up from local structures can be highly successful. Convolutional neural networks, for instance, learn features over locally connected regions of images. The aim of this work is to investigate the effectiveness and efficiency of machine learning models that perform learning and inference within and across 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. arXiv:1610.09369v1 [cs.LG] 28 Oct 2016 locally connected regions of knowledge bases. This is achieved by combining relational features that are often used in statistical relatinal learning with novel ideas from the area of deep learning. The following problem motivates Gaifman models. Problem 1. Given a knowledge base (relational structure, mega-example, knowledge graph) or a collection of knowledge bases, learn a relational machine learning model that supports complex relational queries. The model learns a probability for each tuple in the query answer.</p><p>Note that this is a more general problem than knowledge base completion since it includes the learning of a probability distribution for a complex relational query. The query corresponding to knowledge base completion is r(x, y) for logical variables x and y, and relation r. The problem also touches on the problem of open-world probabilistic KBs <ref type="bibr" target="#b6">[7]</ref> since tuples whose prior probability is zero will often have a non-zero probability in the query answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We first review some important concepts and notation in first-order logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relational First-order Logic</head><p>An atom r(t 1 , ..., t n ) consists of predicate r of arity n followed by n arguments, which are either elements from a finite domain D = {a, b, ...} or logical variables {x, y, ...}. We us the terms domain element and object synonymously. A ground atom is an atom without logical variables. Formulas are built from atoms using the usual Boolean connectives and existential and universal quantification. A free variable in a first-order formula is a variable x not in the scope of a quantifier. We write ϕ(x, y) to denote that x, y are free in ϕ, and free(ϕ) to refer to the free variables of ϕ. A substitution replaces all occurrences of logical variable x by t in some formula ϕ and is denoted by ϕ[x/t].</p><p>A vocabulary consists of a finite set of predicates R and a domain D. Every predicate r is associated with a positive integer called the arity of r. A R-structure (or knowledge base) D consists of the domain D, a set of predicates R, and an interpretation. The Herbrand base of D is the set of all ground atoms that can be constructed from R and D. The interpretation assigns a truth value to every atom in the Herbrand base by specifying r D ⊆ D n for each n-ary predicate r ∈ R. For a formula ϕ(x 1 , ..., x n ) and a structure D, we write D |= ϕ(d 1 , ..., d n ) to say that D satisfies ϕ if the variables x 1 , ..., x n are substituted with the domain elements d 1 , ...., d n . We define ϕ(D) := {(d 1 , ..., d n ) ∈ D n | D |= ϕ(d 1 , ..., d n )}. For the R-structure D and C ⊆ D, C D denotes the substructure induced by C on D, that is, the R-structure C with domain C and r C := r D ∩ C n for every n-ary r ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gaifman's Locality Theorem</head><p>The Gaifman graph of a R-structure D is the graph G D with vertex set D and an edge between two vertices d, d ∈ D if and only if there exists an r ∈ R and a tuple (d 1 , ..., d k ) ∈ r D such that d, d ∈ {d 1 , ..., d k }. <ref type="figure" target="#fig_0">Figure 1</ref> depicts a fragment of a knowledge base and the corresponding Gaifman graph. The distance d D (d 1 , d 2 ) between two elements d 1 , d 2 ∈ D of a structure D is the length of the shortest path in G D connecting d 1 and d 2 . For r ≥ 1 and d ∈ D, we define the r-neighborhood</p><formula xml:id="formula_0">of d to be N r (d) := {x ∈ D | d D (d, x) ≤ r}.</formula><p>We refer to r also as the depth of the neighborhood. Let d = (d 1 , ..., d n ) ∈ D n . The r-neighborhood of d is defined as</p><formula xml:id="formula_1">N r (d) = n i=1 N r (d i ).</formula><p>For the Gaifman graph in <ref type="figure" target="#fig_0">Figure 1</ref>, we have that N 1 (d 4 ) = {d 1 , d 2 , d 5 } and N 1 ((d 1 , d 2 )) = {d 1 , ..., d 6 }. ϕ Nr (x) is the formula obtained from ϕ(x) by relativizing all quantifiers to N r (x), that is, by replacing every subformula of the form ∃yψ(x, y, z) by ∃y(d D (x, y) ≤ r ∧ ψ(x, y, z)) and every subformula of the form ∀yψ(x, y, z) by ∀y(d D (x, y) ≤ r → ψ(x, y, z)). A formula ψ(x) of the form ϕ Nr (x), for some ϕ(x), is called r-local. Whether an r-local formula ψ(x) holds depends only on the r-neighborhood of x, that is, for every structure D and every d ∈ D we have D |= ψ(d)  </p><formula xml:id="formula_2">∃x 1 · · · ∃x k   1≤i&lt;j≤k d D (x i , x j ) &gt; 2r ∧ 1≤i≤k ψ(x i )   .</formula><p>We can now state Gaifman's locality theorem. Theorem 1.</p><p>[9] Every first-order sentence is equivalent to a Boolean combination of local sentences.</p><p>Gaifman's locality theorem states that any first-order sentence can be expressed as a Boolean combination of r-local sentences defined for neighborhoods of objects that are mutually far apart (have distance at least 2r + 1). Now, a novel approach to (statistical) relational learning would be to consider a large set of objects (or tuples of objects) and learn models from their local neighborhoods in the Gaifman graphs. It is this observation that motivates Gaifman models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Gaifman Models</head><p>Instead of taking the costly approach of applying relational learning and inference directly to entire knowledge bases, the representations of Gaifman models are learned bottom up, by performing inference and learning within bounded-size, locally connected regions of Gaifman graphs. Each Gaifman model specifies the data generating process from a given knowledge base (or collection of knowledge bases), a set of relational features, and a ML model class used for learning. Definition 1. Given a R-structure D, a discriminative Gaifman model for D is a tuple (q, r, k, Φ, M) as follows:</p><p>• q is a first-order formula called the target query with at least one free variable;</p><p>• r is the depth of the Gaifman neighborhoods;</p><p>• k is the size-bound of the Gaifman neighborhoods;</p><p>• Φ is a set of first-order formulas (the relational features);</p><p>• M is the base model class (loss, hyper-parameters, etc.).</p><p>Throughout the rest of the paper, we will provide detailed explanations of the different parameters of Gaifman models and their interaction with data generation, learning, and inference.</p><p>During the training of Gaifman models, neighborhoods are generated for tuples of objects d ∈ D n based on the parameters r and k. We first describe the procedure for arbitrary tuples d of objects and will later explain where these tuples come from. For a given tuple d the r-neighborhood of d within the Gaifman graph is computed. This results in the set of objects N r (d). Now, from this neighborhood we sample w neighborhoods consisting of at most k objects. Sampling bounded-size sub-neighborhoods from N r (d) is motivated as follows:</p><p>1. The degree distribution of Gaifman graphs is often skewed (see <ref type="figure" target="#fig_1">Figure 2</ref>), that is, the number of other objects a domain element is related to varies heavily. Generating smaller, bounded-size neighborhoods allows the transfer of learned representations between more and less connected objects. Moreover, the sampling strategy makes Gaifman models more robust to object uncertainty <ref type="bibr" target="#b18">[19]</ref>. We show empirically that larger values for k reduce the effectiveness of the learned models for some knowledge bases.</p><p>2. Relational learning and inference is performed within the generated neighborhoods. N r (d) can be very large, even for r = 1 (see <ref type="figure" target="#fig_1">Figure 2</ref>), and we want full control over the complexity of the computational problems. We can now define the set of (r, k)-neighborhoods generated from a r-neighborhood.</p><formula xml:id="formula_3">N r,k (d) := {N | N ⊆ N r (d) and |N| = k} if |N r (d)| ≥ k {N r (d)} otherwise.</formula><p>For a given tuple of objects d, Algorithm 1 returns a set of w neighborhoods drawn from N r,k (d) such that the number of objects for each d i is the same in expectation.</p><p>The formulas in the set Φ are indexed and of the form  Deciding D |= ϕ for a structure D and a first-order formula ϕ is referred to as model checking and computing ϕ(D) is called ϕ-counting. The combined complexity of model checking is PSPACEcomplete <ref type="bibr" target="#b28">[29]</ref> and there exists a ||D|| O(||ϕ||) algorithm for both problems where || · || is the size of an encoding. Clearly, for most real-world KBs this is not feasible. For Gaifman models, however, where the neighborhoods are bounded-size, typically 10 ≤ |N| = k ≤ 100, the above representation can be computed very efficiently for a large class of relational features. We can now state the following complexity result. Existing SRL approaches could be applied to the generated neighborhoods, treating each as a possible world for structure and parameter learning. However, our goal is to learn relational models that utilize embeddings computed by multi-layered neural networks.  </p><formula xml:id="formula_4">ϕ i (s 1 , ..., s n , u 1 , ..., u m ) with s j ∈ free(q) and u j ∈ free(q). For every tuple d = (d 1 , ..., d n ), generated neighborhood N ∈ N r,k (d), and ϕ i ∈ Φ, we perform the substitution [s 1 /d 1 , ..., s n /d n ] and relativize ϕ i 's quantifiers to N, resulting in ϕ N i [s 1 /d 1 , ..., s n /d n ] which we write as ϕ N i [s/d]. Let N be the substructure induced by N on D.</formula><formula xml:id="formula_5">v i :=    ϕ N i [s/d]( N ) if free(ϕ N i [s/d]) &gt; 0 1 if N |= ϕ N i [s/d] 0 otherwise. That is, if ϕ N i [s/d]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Distributions for Relational Queries</head><p>Let q be a first-order formula (the relational query) and S(q) the result set of the query, that is, all groundings that render the formula satisfied in the knowledge base. The feature representations generated for tuples of objects d ∈ S(q) serve as positive training examples. The Gaifman models' aim is to learn neighborhood embeddings that capture local structure of tuples for which we know that the target query evaluates to true. Similar to previous work, we generate negative examples by corrupting tuples that correspond to positive examples. The corruption mechanism takes a positive input tuple d = (d 1 , ..., d n ) and substitutes, for each i ∈ {1, ..., n}, the domain element d i with objects sampled from D while keeping the rest of the tuple fixed.</p><p>The discriminative Gaifman model performs the following steps. Learning the final Gaifman model depends on the base ML model class M and its loss function. We obtained state of the art results with neural networks, gradient-based learning, and categorical cross-entropy as loss function</p><formula xml:id="formula_6">L = −   N∈N log p M (v N ) + Ñ ∈Ñ log(1 − p M (vÑ))   , where p M (v N )</formula><p>is the probability the model returns on input v N . However, other loss functions are possible. The probability of a particular substitution of the target query to be true is now</p><formula xml:id="formula_7">P (q[s/d] = True) = E N∈N (r,k) (d) [p M (v N )].</formula><p>The expected probability of a representation of a neighborhood drawn uniformly at random from N (r,k) (d). It is now possible to generate several neighborhoods N and their representations v N to estimate P (q[s/d] = True), simply by averaging the neighborhoods' probabilities. We have found experimentally that a single neighborhood already leads to highly accurate results but also that more neighborhood samples further improve the accurracy.</p><p>Let us emphasize again the novel semantics of Gaifman models. Gaifman models generate a large number of small, bounded-size structures from a large structure, learn a representation for these bounded-size structures, and use the resulting representation to answer queries concerning the original structure as a whole. The advantages are model weight sharing across a large number of neighborhoods and efficiency of the computational problems. <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref> illustrate learning from bounded-size neighborhood structures and inference in Gaifman models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure Learning</head><p>Structure learning is the problem of determining the set of relational features Φ. We provide some directions and leave the problem to future work. Given a collection of bounded-size neighborhoods of the Gaifman graph, the goal is to determine suitable relational features for the problem at hand. There is a set of features which we found to be highly effective. For example, formulas of the form ∃x r(s 1 , x), ∃x r(s 1 , x) ∧ r(x, s 2 ), and ∃x, y r 1 (s 1 , x) ∧ r 2 (x, y) ∧ r 3 (y, s 2 ) for all relations. The latter formulas capture fixed-length paths between s 1 and s 2 in the neighborhoods. Hence, Path Ranking type features <ref type="bibr" target="#b14">[15]</ref> can be used in Gaifman models as a particular relational feature class. For path formulas with several different relations we cannot include all |R| 3 combinations and, hence, we have to determine a subset occurring in the training data. Fortunately, since the neighborhood size is bounded, it is computationally feasible to compute frequent paths in the neighborhoods and to use these as features. The complexity of this learning problem is in the number of elements in the neighborhood and not in the number of all objects in the knowledge base. Relation paths that do not occur in the data can be discarded. Gaifman models can also use features of the form ∀x, y r(x, y) ⇒ r(y, x), ∃x, y r(x, y), and ∀x, y, z r(x, y) ∧ r(y, z) ⇒ r(x, z), to name but a few. Moreover, features with free variables, such as r(s 1 , x) are counting features (here: the r out-degree of s 1 ). It is even computationally feasible to include specific second-order features (for instance, quantifiers ranging over R) and aggregations of feature values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prior Confidence Values, Types, and Numerical Attributes</head><p>Numerous existing knowledge bases assign confidence values (probabilities, weights, etc.) to their statements. Gaifman models can incorporate confidence values during the sampling and learning process. Instead of adding random noise to the representations, which we have found to be beneficial, noise can be added inversely proportional to the confidence values. Statements for which the prior confidence values are lower are more likely to be dropped out during training than statements with higher confidence values. Furthermore, Gaifman models can directly incorporate object types such as Actor and Action Movie as well as numerical features such as location and elevation. One simply has to specify a fixed position in the neighborhood representation v for each object position within the input tuples d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Recent work on relational machine learning for knowledge graphs is surveyed in <ref type="bibr" target="#b19">[20]</ref>. We focus on a select few methods we deem most related to Gaifman models and refer the interested reader to the above article. A large body of work exists on learning inference rules from knowledge bases. Examples include <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b0">[1]</ref> where inference rules of length one are learned; and <ref type="bibr" target="#b24">[25]</ref> where general inference rules are learned by applying a support threshold. Their method does not scale to large KBs and depends on predetermined thresholds. Lao et al. <ref type="bibr" target="#b14">[15]</ref> train a logistic regression classifier with path features to perform KB completion. The idea is to perform a random walk between objects and to exploit the discovered paths as features. SFE <ref type="bibr" target="#b9">[10]</ref> improves PRA by making the generation of random walks more efficient. More recent embedding methods have combined paths in KBs with KB embedding methods <ref type="bibr" target="#b16">[17]</ref>. Gaifman models support a much broader class of relational features subsuming path features. For instance, Gaifman models incorporate counting features that have shown to be beneficial for relational models.</p><p>Latent feature models learn features for objects and relations that are not directly observed in the data. Examples of latent feature models are tensor factorization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> and embedding models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. The majority of these models can be understood as more or less complex neural networks operating on object and relation representations. Gaifman models can also be used to learn knowledge base embeddings. Indeed, one can show that it generalizes or complements existing approaches. For instance, the universal schema <ref type="bibr" target="#b22">[23]</ref> considers pairs of objects where relation membership variables comprise the model's features. We have the following interesting relationship between universal schemas <ref type="bibr" target="#b22">[23]</ref> and Gaifman models. Given a knowledge base D. The Gaifman model for D with r = 0, k = 2, Φ = r∈R {r(s 1 , s 2 ), r(s 2 , s 1 )}, w = 1 andw = 0 is equivalent to the Universal Schema <ref type="bibr" target="#b22">[23]</ref> for D up to the base model class M. More recent methods combine embedding methods and inference-based logical approaches for relation extraction <ref type="bibr" target="#b23">[24]</ref>. Contrary to most existing multi-relational ML models <ref type="bibr" target="#b19">[20]</ref>, Gaifman models natively support higher-arity relations, functional and type constraints, numerical features, and complex target queries. The aim of the experiments is to understand the efficiency and effectiveness of Gaifman models for typical knowledge base inference problems. We evaluate the proposed class of models with two data sets derived from the knowledge bases WORDNET and FREEBASE <ref type="bibr" target="#b1">[2]</ref>. Both data sets consist of a list of statements r(d 1 , d 2 ) that are known to be true. For a detailed description of the data sets, whose statistics are listed in <ref type="table" target="#tab_1">Table 1</ref>, we refer the reader to previous work <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>After training the models, we perform entity prediction as follows. For each statement r(d 1 , d 2 ) in the test set, d 2 is replaced by each of the KB's objects in turn. The probabilities of the resulting statements are predicted and sorted in descending order. Finally, the rank of the correct statement within this ordered list is determined. The same process is repeated now with replacements of d 1 . We compare Gaifman models with q = r(x, y) to state of the art knowledge base completion approaches which are listed in <ref type="table" target="#tab_2">Table 2</ref>. We trained Gaifman models with r = 1 and different values for k, w, andw. We use a neural network architecture with two hidden layers, each having 100 units and sigmoid activations, dropout of 0.2 on the input layer, and a softmax layer. Dropout makes the model more robust to missing relations between objects. We trained one model per relation and left the hyper-parameters fixed across models. We did not perform structure learning and instead used the following set of relational features Φ := r∈R, i∈{1,2} r(s 1 , s 2 ), r(s 2 , s 1 ), ∃x r(x, s i ), ∃x r(s i , x), ∃x r(s 1 , x) ∧ r(x, s 2 ), ∃x r(s 2 , x) ∧ r(x, s 1 ) .</p><p>To compute the probabilities, we averaged the probabilities of N = 1, 2, or 3 generated (r, k)neighborhoods. We performed runtime experiments to evaluate the models' efficiency. Embedding models have the advantage that one dot product for every candidate object is sufficient to compute the score for the corresponding statement and we need to assess the performance of Gaifman models in this context. All experiments were run on commodity hardware with 64G RAM and a single 2.8 GHz CPU.  <ref type="figure" target="#fig_0">N = 1, 2, 3</ref>) the better the result. When the entire 1-neighborhood is considered (k = ∞), the performance for WN18 does not deteriorate as it does for FB15k. This is due to the fact that objects in WN18 have on average few neighbors. FB15k has more variance in the Gaifman graph's degree distribution (see <ref type="figure" target="#fig_1">Figure 2</ref>) which is reflected in the better performance for smaller k values. The experiments also show that it is beneficial to generate a large number of representations (both positive and negative ones). The performance improves with larger number of training examples.</p><p>The runtime experiments demonstrate that Gaifman models perform inference very efficiently for k ≤ 20. <ref type="figure" target="#fig_8">Figure 5</ref> depicts the number of query answers the Gaifman models are able to serve per second, averaged over relation types. A query answer returns the probability for one object pair. These numbers include neighborhood generation and network inference. The results are promising with about 5000 query answers per second (averaged across relation types) as long as k remains small. Since most object pairs of WN18 have a 1-neighborhood whose size is smaller than 20, the answers per second rates for k &gt; 20 is not reduced as drastically as for FB15k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Gaifman models are a novel family of relational machine learning models that perform learning and inference within and across locally connected regions of relational structures. Future directions of research include structure learning, more sophisticated base model classes, and application of Gaifman models to additional relational ML problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A knowledge base fragment for the pair(d 1 , d 2) and the corresponding Gaifman graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The degree distribution of the Gaifman graph for the Freebase fragment FB15K. if and only if N r (d) |= ψ(d). For r, k ≥ 1 and ψ(x) being r-local, a local sentence is of the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For every formula ϕ i (s 1 , ..., s n , u 1 , ..., u m ) and every n ∈ N m , we now have thatD |= ϕ N i [s/d, u/n] if and only if N |= ϕ N i [s/d, u/n].In other words, satisfaction is now checked locally within the neighborhoods N, by deciding whether N |= ϕ N i [s/d, u/n]. The relational semantics of Gaifman models is based on the set of formulas Φ. The feature vector v = (v 1 , ..., v |Φ| ) for tuple d, and neighborhood N ∈ N r,k (d), written as v N , is constructed as follows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>has free variables, v i is equal to the number of groundings of ϕ i [s/d] that are satisfied within the neighborhood substructure N ; if ϕ i [s/d] has no free variables, v i = 1 if and only if ϕ i [s/d] is satisfied within the neighborhod substructure N ; and v i = 0 otherwise. The neighborhood representations v capture r-local formulas and help the model learn formula combinations that are associated with negative and positive examples. For the right choices of the parameters r and k, the neighborhood representations of Gaifman models capture the relational structure associated with positive and negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 2 .</head><label>2</label><figDesc>Let D be a relational structure (knowledge base), let d be the size of the largest rneighborhood of D's Gaifman graph, and let s be the greatest encoding size of any formula in Φ. For a Gaifman model with parameters r and k, the worst-case complexity for computing the feature representations of N neighborhoods is O(N (d + |Φ|k s )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 6 :UFigure 3 :</head><label>163</label><figDesc>GENNEIGHS: Computes a list of w neighborhoods of size k for an input tuple d. 1: input: tuple d ∈ D n , parameters r, k, and w 2: S = [ ] 3: while |S| &lt; w do for all i ∈ {1, ..., n} do 7: U = min( k/n , |N r (d i )|) elements sampled uniformly from N r (d i ) = min(|S| − k, |N |) elements sampled uniformly from N Learning of a Gaifman model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>W 1 Figure 4 :</head><label>14</label><figDesc>Inference with a Gaifman model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 .</head><label>1</label><figDesc>Evaluate the target query q and compute the result set S(q) 2. For each tuple d in the result set S(q): • Compute N , a multiset of w neighborhoodsÑ ∈ N r,k (d) with Algorithm 1; each such neighborhood serves as a positive training example • ComputeÑ , a multiset ofw neighborhoods N ∈ N r,k (d) for corrupted versions of d with Algorithm 1; each such neighborhood serves as a negative training example • Perform model checking and counting within the neighborhoods to compute the feature representations v N and vÑ for each N ∈ N andÑ ∈Ñ , respectively 3. Learn a ML model with the generated positive and negative training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Query answers per second rates for different values of the parameter k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3. Even for a single object tuple d we can generate a large number of training examples if |N r (d)| &gt; k. This mitigates the risk of overfitting. The number of training examples per tuple strongly influences the models' accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the data sets.</figDesc><table><row><cell>Dataset WN18 40,943 |D| FB15k 14,951 1,345 483,142 59,071 |R| # train # test 18 141,442 5,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>lists the experimental results for different parameter settings [N, k, w,w]. The Gaifman models achieve the highest hits@10 and hits@1 values for both data sets. As expected, the more neighborhood samples are used to compute the probability estimate (</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of the entity prediction experiments.</figDesc><table><row><cell>Data Set Metric RESCAL[21] SE[5] LFM[12] TransE[4] TransR[18] DistMult[30]</cell><cell cols="6">WN18 Mean rank Hits@10 Hits@1 Mean rank Hits@10 Hits@1 FB15K 1,163 52.8 -683 44.1 -985 80.5 -162 39.8 -456 81.6 -164 33.1 -251 89.2 8.9 51 71.5 28.1 219 91.7 -78 65.5 -902 93.7 76.1 97 82.8 44.3</cell></row><row><cell>Gaifman [1, ∞, 1, 5] Gaifman [1, 20, 1, 2] Gaifman [1, 20, 5, 25] Gaifman [2, 20, 5, 25] Gaifman [3, 20, 5, 25]</cell><cell>298 357 392 378 352</cell><cell>93.9 88.1 93.6 93.9 93.9</cell><cell>75.8 66.8 76.4 76.7 76.1</cell><cell>124 114 97 84 75</cell><cell>78.1 79.2 82.1 83.4 84.2</cell><cell>59.8 60.1 65.6 68.5 69.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Many thanks to Alberto García-Durán, Mohamed Ahmed, and Kristian Kersting for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open-world probabilistic databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Principles of Knowledge Representation and Reasoning (KR)</title>
		<meeting>the 15th International Conference on Principles of Knowledge Representation and Reasoning (KR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ProbLog2: Probabilistic logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kimmig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Renkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vlasselaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9286</biblScope>
			<biblScope unit="page" from="312" to="315" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On local and non-local properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gaifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the herbrand symposium, logic colloquium</title>
		<meeting>the herbrand symposium, logic colloquium</meeting>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="105" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient and expressive knowledge base completion using subgraph feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1488" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yago2: A spatially and temporally enhanced knowledge base from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="28" to="61" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>D. Schuurmans and M. P. Wellman</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lifted probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Elements Of Finite Model Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Libkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>SpringerVerlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic Models with Unknown Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Milch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the ACL (NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning first-order horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1088" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lifted inference and learning in statistical relational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The complexity of relational query languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM symposium on Theory of computing</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised resolution of objects and relations on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
