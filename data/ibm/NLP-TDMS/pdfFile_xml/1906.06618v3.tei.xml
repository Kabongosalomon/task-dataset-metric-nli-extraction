<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How To Train Your Deep Multi-Object Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria, LJK</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Ban</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria, LJK</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Distributed Robotics Lab</orgName>
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria, LJK</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
							<email>leal.taixe@tum.de3yban@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria, LJK</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How To Train Your Deep Multi-Object Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent trend in vision-based multi-object tracking (MOT) is heading towards leveraging the representational power of deep learning to jointly learn to detect and track objects. However, existing methods train only certain submodules using loss functions that often do not correlate with established tracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and Precision (MOTP). As these measures are not differentiable, the choice of appropriate loss functions for end-to-end training of multiobject tracking methods is still an open research problem. In this paper, we bridge this gap by proposing a differentiable proxy of MOTA and MOTP, which we combine in a loss function suitable for end-to-end training of deep multiobject trackers. As a key ingredient, we propose a Deep Hungarian Net (DHN) module that approximates the Hungarian matching algorithm. DHN allows estimating the correspondence between object tracks and ground truth objects to compute differentiable proxies of MOTA and MOTP, which are in turn used to optimize deep trackers directly. We experimentally demonstrate that the proposed differentiable framework improves the performance of existing multi-object trackers, and we establish a new state of the art on the MOTChallenge benchmark. Our code is publicly available from https://github.com/yihongXU/deepMOT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-based multi-object tracking (MOT) is a longstanding research problem with applications in mobile robotics and autonomous driving. It is through tracking that we become aware of surrounding object instances and anticipate their future motion. The majority of existing methods for pedestrian tracking follow the tracking-by-detection paradigm and mainly focus on the association of detector responses over time. A significant amount of research investigated combinatorial optimization techniques for this challenging data association problem <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Recent data-driven trends in MOT leverage the representational power of deep networks for learning identity-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepMOT Deep Hungarian Net DeepMOT Loss Deep Multi-Object Tracker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Images</head><p>Bounding Boxes gradients <ref type="figure">Figure 1</ref>. We propose DeepMOT, a general framework for training deep multi-object trackers including the DeepMOT loss that directly correlates with established tracking evaluation measures <ref type="bibr" target="#b5">[6]</ref>. The key component in our method is the Deep Hungarian Net (DHN) that provides a soft approximation of the optimal prediction-to-ground-truth assignment, and allows to deliver the gradient, back-propagated from the approximated tracking performance measures, needed to update the tracker weights.</p><p>preserving embeddings for data association <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54]</ref>, learning the appearance model of individual targets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b58">59]</ref> and learning to regress the pose of the detected targets <ref type="bibr" target="#b3">[4]</ref>. However, these methods train individual parts of the MOT pipeline using proxy losses (e.g. triplet loss <ref type="bibr" target="#b46">[47]</ref> for learning identity embeddings), that are only indirectly related to the MOT evaluation measures <ref type="bibr" target="#b5">[6]</ref>. The main difficulty in defining loss functions that resemble standard tracking evaluation measures is due to the need of computing the optimal matching between the predicted object tracks and the ground-truth objects. This problem is usually solved by using the Hungarian (Munkres) algorithm (HA) <ref type="bibr" target="#b26">[27]</ref>, which contains non-differentiable operations. The significant contribution of this paper is a novel, differentiable framework for the training of multi-object trackers ( <ref type="figure">Fig. 1)</ref>: it proposes a differentiable variant of the standard CLEAR-MOT <ref type="bibr" target="#b5">[6]</ref> evaluation measures, which we combine into a novel loss function, suitable for endto-end training of MOT methods. In particular, we introduce a differentiable network module -Deep Hungarian Net (DHN) -that approximates the HA and provides a soft approximation of the optimal prediction-to-ground-truth assignment. The proposed approximation is based on a bidirectional recurrent neural network (Bi-RNN) that computes the (soft) assignment matrix based on the prediction-to-ground-truth distance matrix. We then express both the MOTA and MOTP <ref type="bibr" target="#b5">[6]</ref> as differentiable functions of the computed (soft) assignment matrix and the distance matrix. Through DHN, the gradients from the approximated tracking performance measures are back-propagated to update the tracker weights. In this way, we can train object trackers in a data-driven fashion using losses that directly correlate with standard MOT evaluation measures. In summary, this paper makes the following contributions:</p><p>(i) We propose novel loss functions that are directly inspired by standard MOT evaluation measures <ref type="bibr" target="#b5">[6]</ref> for end-to-end training of multi-object trackers. (ii) In order to back-propagate losses through the network, we propose a new network module -Deep Hungarian Net -that learns to match predicted tracks to groundtruth objects in a differentiable manner. (iii) We demonstrate the merit of the proposed loss functions and differentiable matching module by training the recently published Tracktor <ref type="bibr" target="#b3">[4]</ref> using our proposed framework. We demonstrate improvements over the baseline and establish a new state-of-the-art result on MOTChallenge benchmark datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tracking as Discrete Optimization.</p><p>With the emergence of reliable object detectors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> tracking-bydetection has become the leading tracking paradigm. These methods first perform object detection in each image and associate detections over time, which can be performed online via frame-to-frame bi-partite matching between tracks and detections <ref type="bibr" target="#b26">[27]</ref>. As early detectors were noisy and unreliable, several methods search for the optimal association in an offline or batch fashion, often posed as a network flow optimization problem <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Alternatively, tracking can be posed as a maximum-aposteriori (MAP) estimation problem by seeking an optimal set of tracks as a conditional distribution of sequential track states. Several methods perform inference using conditional random fields (CRFs) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40]</ref>, Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b38">[39]</ref> or a variational expectationmaximization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. These methods in general, use hand-crafted descriptors for the appearance model, such as color histograms <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b10">11]</ref>, optical flow based descriptors <ref type="bibr" target="#b11">[12]</ref> and/or motion models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref> as association cues. Therefore typically only a few parameters are trainable and are commonly learned using grid/random search or tree of parzen window estimators <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">40]</ref>. In the case of CRFbased methods, the weights can be trained using structured SVM <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Deep Multi-Object Tracking. Recent data-driven trends in MOT leverage representational power of deep neural networks. Xiang et al. <ref type="bibr" target="#b55">[56]</ref> learn track birth/death/association policy by modeling them as Markov Decision Processes (MDP). As the standard evaluation measures <ref type="bibr" target="#b5">[6]</ref> are not differentiable, they learn the policy by reinforcement learning.</p><p>Several existing methods train parts of their tracking methods using losses, not directly related to tracking evaluation measures <ref type="bibr" target="#b5">[6]</ref>. Kim et al. <ref type="bibr" target="#b24">[25]</ref> leverages pre-learned CNN features or a bilinear LSTM <ref type="bibr" target="#b25">[26]</ref> to learn the longterm appearance model. Both are incorporated into (Multiple Hypothesis Tracking) MHT framework <ref type="bibr" target="#b41">[42]</ref>. Other methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50]</ref> learn identity-preserving embeddings for data association using deep neural networks, trained using contrastive <ref type="bibr" target="#b19">[20]</ref>, triplet <ref type="bibr" target="#b46">[47]</ref> or quadruplet loss <ref type="bibr" target="#b49">[50]</ref>. At inference time, these are used for computing data association affinities. Approaches by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b58">59]</ref> learn the appearance model of individual targets using an ensemble of single-object trackers that share a convolutional backbone. A spatiotemporal mechanism (learned online using a cross-entropy loss) guides the online appearance adaptation and prevents drifts. All these methods are only partially trained, and sometimes in various stages. Moreover, it is unclear how to train these methods to maximize established tracking metrics.</p><p>Most similar to our objective, Wang et al. <ref type="bibr" target="#b54">[55]</ref> propose a framework for learning parameters of linear cost association functions, suitable for network flow optimization <ref type="bibr" target="#b57">[58]</ref> based multi-object trackers. They train parameters using structured SVM. Similar to our method, they devise a loss function, that resembles MOTA: the intra-frame loss penalizes false positives (FP) and missed targets while the inter-frame component of the loss penalizes false associations, ID switches, and missed associations. However, their loss is not differentiable and is only suitable for training parameters within the proposed min-cost flow framework. Chu et al. <ref type="bibr" target="#b12">[13]</ref> propose an end-to-end training framework that jointly learns feature, affinity and multi-dimensional assignment. However, their losses are not directly based on MOTA and MOTP. Schulter et al. <ref type="bibr" target="#b47">[48]</ref> parameterize (arbitrary) cost functions with neural networks and train them end-to-end by optimizing them with respect to the min-flow training objective. Different from <ref type="bibr" target="#b47">[48]</ref>, our approach goes beyond learning the association function, and can be used by any learnable tracking method.</p><p>Bergmann et al. <ref type="bibr" target="#b3">[4]</ref> propose a tracking-by-regression approach to MOT. The method is trained for the object detection task using a smooth L 1 loss for the bounding box regressor. Empirically, their method is able to regress bounding boxes in high-frame rate video sequences with no significant camera motion. Apart from the track birth and death management, this approach is fully trainable, and thus it is a perfect method for demonstrating the merit of our training framework. Training this approach on a sequence-level data using our proposed loss further improves the performance and establishes a new state of the art on the MOTChallenge benchmark <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview and Notation</head><p>The objective of any MOT method is to predict tracks in a video sequence. Each track X i is associated with an identity i, and consists of L i image bounding boxes x i t l ∈ R 4 (2D location and size), l = 1 . . . , L i . The task of a multi-object tracker is to accurately estimate the bounding boxes for all identities through time.</p><p>At evaluation time, the standard metrics operate frameby-frame. At frame t, the N t predicted bounding boxes,</p><formula xml:id="formula_0">x i1 t , . . . , x i N t t</formula><p>must be compared to the M t ground-truth objects, y j1 t , . . . , y</p><formula xml:id="formula_1">j M t t .</formula><p>We first need to compute the correspondence between predicted bounding boxes and groundtruth objects. This is a non-trivial problem as multiple ground-truth boxes may overlap and thus can fit to several track hypotheses. In the following we will omit temporal index t to ease the reading. All expressions will be evaluated with respect to time index t unless specified otherwise.</p><p>The standard metrics, proposed in <ref type="bibr" target="#b5">[6]</ref>, tackle this association problem using bi-partite matching. First, a predictionto-ground-truth distance matrix D ∈ R N ×M , 1 d nm ∈ [0, 1] is computed. For vision-based tracking, an intersectionover-union (IoU) based distance is commonly used. Then, the optimal prediction-to-ground-truth assignment binary matrix is obtained by solving the following integer program using the Hungarian algorithm (HA) <ref type="bibr" target="#b26">[27]</ref>: By solving this integer program we obtain a mutually consistent association between ground-truth objects and track predictions. The constraints ensure that all rows and columns of the assignment should sum to 1, thus avoiding multiple assignments between the two sets. After finding the optimal association, A * , we can compute the MOTA and MOTP measures using A * and D: 2</p><formula xml:id="formula_2">A * = argmin</formula><formula xml:id="formula_3">MOTA = 1 − t (FP t + FN t + IDS t ) t M t ,<label>(1)</label></formula><formula xml:id="formula_4">MOTP = t n,m d tnm a * tnm t | TP t | ,<label>(2)</label></formula><p>where a * tnm is the (n, m)-th entry of A * at time t. The true positives (TP) correspond to the number of matched predicted tracks and false positives (FP) correspond to the number of non-matched predicted tracks. False negatives (FN) denote the number of ground-truth objects without a match. Finally, to compute ID switches (IDS) we need to keep track of past-frame assignments. Whenever the track <ref type="bibr" target="#b0">1</ref> The distance matrix D is considered without those objects/tracks that are thresholded-out, i.e., too far from any possible assignment. <ref type="bibr" target="#b1">2</ref> Accounting also for the objects/tracks that were left out.</p><p>assigned to a ground truth object changes, we increase the number of IDS and update the assignment structure. As these evaluation measures are not differentiable, existing strategies only optimize the trackers' hyperparameters (using, e.g. random or grid search) that maximize MOTA or MOTP or a combination of both. In their current version, MOTA and MOTP cannot be directly used for tracker optimization with gradient descent techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DeepMOT</head><p>The first step to compute the CLEAR-MOT <ref type="bibr" target="#b5">[6]</ref> tracking evaluation measures is to perform bi-partite matching between the sets of ground-truth objects and of predicted tracks. Once the correspondence between the two sets is established, we can count the number of TP, FN, and IDS needed to express MOTA and MOTP. As the main contribution of this paper, we propose a differentiable loss inspired by these measures, following the same two-step strategy. We first propose to perform a soft matching between the two sets using a differentiable function, parameterized as a deep neural network. Once we establish the matching, we design a loss, approximating the CLEAR-MOT measures, as a combination of differentiable functions of the (soft) assignment matrix and the distance matrix. Alternative measures such as IDF1 <ref type="bibr" target="#b43">[44]</ref> focus on how long the tracker correctly identifies targets instead of how often mismatches occur. However, MOTA and IDF1 have a strong correlation. This is reflected in our results -by optimizing our loss, we also improve the IDF1 measure (see Sec. 5.3). In the following, we discuss both the differentiable matching module (Sec. 4.1) and the differentiable version of the CLEAR-MOT measures <ref type="bibr" target="#b5">[6]</ref> (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep Hungarian Net: DHN</head><p>In this section, we introduce DHN, a fundamental block in our DeepMOT framework. DHN produces a proxyÃ that is differentiable w.r.t. D. Thus DHN provides a bridge to deliver gradient from the loss (to be described later on) to the tracker. We formalize DHN with a non-linear mapping that inputs D and outputs the proxy soft assignment matrix A. DHN is modeled by a neural networkÃ = g(D, ω d ) with parameters ω d . Importantly, the DHN mapping must satisfy several properties: (i) the outputÃ must be a good approximation to the optimal assignment matrix A * , (ii) this approximation must be differentiable w.r.t. D, (iii) both input and output matrix are of equal, but varying size and (iv) g must take global decisions as the HA does.</p><p>While (i) will be achieved by setting an appropriate loss function when training the DHN (see Sec. 5.1), (ii) is ensured by designing DHN as a composite of differentiable functions. The requirements (iii) and (iv) push us to design a network that can process variable (but equal) input and output sizes, where every output neuron has a receptive field equals to the entire input. We opt for bi-directional recurrent neural networks (Bi-RNNs). Alternatively, one could consider the use of fully convolutional networks, as these would be able to process variable input/output sizes. However, large assignment problems would lead to partial receptive fields, and therefore, to local assignment decisions. We outline our proposed architecture in <ref type="figure" target="#fig_1">Fig. 2</ref>. In order to process a 2D distance matrix D using RNNs, we perform row-wise (column-wise) flattening of D. This is inspired by the original HA that performs sequentially row-wise and column-wise reductions and zero-entry verifications and fed it to Bi-RNNs (see details below), opening the possibility for g(·) to make global assignment decisions.</p><p>Precisely, we perform flattening sequentially, i.e., first row-wise followed by column-wise. The row-wise flattened D is input to a first Bi-RNN that outputs the first-stage hidden representation of size N × M × 2h, where h is the size of the Bi-RNN hidden layers. Intuitively the firststage hidden representation encodes the row-wise intermediate assignments. We then flatten the first-stage hidden representation column-wise, to input to a second Bi-RNN that produces the second-stage hidden representation of size N × M × 2h. The two Bi-RNNs have the same hidden size, but they do not share weights. Intuitively, the secondstage hidden representation encodes the final assignments. To translate these encodings into the final assignments, we feed the second-stage hidden representation through three fully-connected layers (along the 2h dimension, i.e., independently for each element of the original D). Finally, a sigmoid activation produces the optimal N × M softassignment matrixÃ. Note that in contrast to the binary output of the Hungarian algorithm, DHN outputs a (soft) assignment matrixÃ ∈ [0, 1] N ×M .</p><p>Distance Matrix Computation. The most common metric for measuring the similarity between two bounding boxes is the Intersection-over-Union (IoU). Note that, in principle, the input D can be any (differentiable) distance function. However, if two bounding boxes have no intersection, the distance 1−IoU will always be a constant value of 1. In that case, the gradient from the loss will be 0, and no information will be back-propagated. For this reason, our distance is an average of the Euclidean center-point distance and the Jaccard distance J (defined as 1 − IoU):</p><formula xml:id="formula_5">d nm = f (x n , y m ) + J (x n , y m ) 2 .<label>(3)</label></formula><p>f is the Euclidean distance normalized w.r.t. the image size:</p><formula xml:id="formula_6">f (x n , y m ) = c(x n ) − c(y m ) 2 √ H 2 + W 2 ,<label>(4)</label></formula><p>where function c(·) computes the center point of the bounding box and H and W are the height and the width of the video frame, respectively. Both the normalized Euclidean distance and Jaccard distance have values in the range of [0, 1], so do all entries d nm . Our framework admits any distance that is expressed as a composition of differentiable distance functions. In the experimental section, we demonstrate the benefits of adding a term that measures the cosine distance between two learned appearance embeddings. In the following, we explain how we compute a differentiable proxy of MOTA and MOTP as functions of D andÃ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Differentiable MOTA and MOTP</head><p>In this section, we detail the computation of two components of the proposed DeepMOT loss: differentiable MOTA (dM OT A) and MOTP (dM OT P ). As discussed in Sec. 3, to compute the classic MOTA and MOTP evaluation measures, we first find the optimal matching between predicted tracks and ground-truth objects. Based on A * , we count FN, FP and IDS. The latter is computed by comparing assignments between the current frame and previous frames. To compute the proposed dM OT A and dM OT P , we need to express all these as differentiable functions of D andÃ computed using DHN (see Sec. 4.1).</p><p>The operations described in the following are illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. First, we need to count FN and FP. Therefore, we need to obtain a count of non-matched tracks and nonmatched ground-truth objects. To this end, we first construct a matrix C r by appending a column toÃ, filled with a threshold value δ (e.g., δ = 0.5), and perform rowwise softmax <ref type="figure" target="#fig_2">(Fig. 3a</ref>). Analogously, we construct C c by appending a row toÃ and perform column-wise softmax <ref type="figure" target="#fig_2">(Fig. 3b</ref>). Then, we can express a soft approximation of the number of FP and FN as:</p><formula xml:id="formula_7">FP = n C r n,M +1 ,FN = m C c N +1,m .<label>(5)</label></formula><p>Intuitively, if all elements inÃ are smaller than the threshold δ, then entries of C r n,M +1 and C c N +1,m will be close to 1, signaling we have a FP or FN. Otherwise, the element with the largest value in each row/column of C r and C c (respectively) will be close to 1, signaling that we have a match. Therefore, the sum of the N + 1-th row of C c <ref type="figure" target="#fig_2">(Fig. 3b</ref>) and of the M + 1-th column of C r <ref type="figure" target="#fig_2">(Fig. 3a</ref>) provide an soft estimate of the number of FN and the number of FP, respectively. We will refer to these asFN andFP.</p><p>To compute the soft approximationsĨDS and dM OT P , we additionally need to construct two binary matrices B TP and B TP -1 , whose non-zero entries signal true positives at the current and previous frames respectively. Row indices of these matrices correspond to indices assigned to our tracks and column indices correspond to ground truth object identities. We need to pad B TP -1 for element-wise multiplication because the number of tracks and objects varies from frame-to-frame. We do this by filling-in rows and columns of B TP -1 to adapt the matrix size for the newly-appeared objects at the current frame by copying their corresponding rows and columns from B TP . Note that we do not need to modify B TP to compensate for newly appearing objects as these do not cause IDS. By such construction, the sum of C c</p><formula xml:id="formula_8">1:N,1:M B TP -1 (where B is the binary complement of B)</formula><p>yields the (approximated) number of IDS ( <ref type="figure" target="#fig_2">Fig. 3c)</ref>:</p><formula xml:id="formula_9">IDS = C c 1:N,1:M B TP -1 1 ,<label>(6)</label></formula><p>where · 1 is the L 1 norm of a flattened matrix. With these ingredients, we can evaluate dM OT A:</p><formula xml:id="formula_10">dM OT A = 1 −F P +FN +γĨDS M .<label>(7)</label></formula><p>γ controls the penalty we assign toĨDS. Similarly, we can express dM OT P as:</p><formula xml:id="formula_11">dM OT P = 1 − D B TP 1 B TP 0 .<label>(8)</label></formula><p>Intuitively, the L 1 norm expresses the distance between the matched tracks and ground-truth objects, and the zero-norm · 0 counts the number of matches. Since we should train the tracker to maximize MOTA and MOTP, we propose the following DeepMOT loss:</p><formula xml:id="formula_12">L DeepMOT = (1 − dM OT A) + λ(1 − dM OT P ), (9)</formula><p>where λ is a loss balancing factor. By minimizing our proposed loss function L DeepMOT , we are penalizing FP, FN and IDS -all used by the CLEAR-MOT measures <ref type="bibr" target="#b5">[6]</ref>. Same as for the standard CLEAR-MOT measures, dM OT A, dM OT P must be computed at every time frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">How To Train Your Deep Multi-Object Tracker</head><p>The overall tracker training procedure is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We randomly sample a pair of consecutive frames from the training video sequences. These two images together with their ground-truth bounding boxes constitute one training instance. For each such instance, we first initialize the tracks with ground-truth bounding boxes (at time t) and run the forward pass to obtain the track's bounding-box predictions in the following video frame (time t+1). To mimic the effect of imperfect detections, we add random perturbations to the ground-truth bounding boxes (see supplementary material for details). We then compute D and use our proposed DHN to computeÃ (Sec. 4.1). Finally, we compute our proxy loss based on D andÃ (Sec. <ref type="bibr">4.2)</ref>. This provides us with a gradient that accounts for the assignment, and that is used to update the weights of the tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>In this section, we first experimentally verify that our proposed DHN is a good approximation to HA <ref type="bibr" target="#b26">[27]</ref> for bi-partite matching, as required by MOT evaluation measures (Sec. 5.1). To show the merit of the proposed framework, we conduct several experiments on several datasets for evaluating pedestrian tracking performance (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">DHN Implementation Details</head><p>In this section, we provide insights into the performance of our differentiable matching module and outline the training and evaluation details.</p><p>DHN Training. To train the DHN, we create a data set with pairs of matrices (D and A * ), separated into 114,483 matrices for training and 17,880 for matrices testing. We generate distance matrices D using ground-truth bounding boxes and public detections, provided by the MOT challenge datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>. We generate the corresponding assignment matrices A * (as labels for training) using HA described in <ref type="bibr" target="#b5">[6]</ref>. We pose the DHN training as a 2D binary classification task using the focal loss <ref type="bibr" target="#b32">[33]</ref>. We compensate for the class imbalance (between the number of zeros n 0 and ones n 1 in A * ) by weighting the dominant zero-class using w 0 = n 1 /(n 0 + n 1 ). We weight the one-class by w 1 = 1 − w 0 . We evaluate the performance of DHN by computing the weighted accuracy (WA):</p><formula xml:id="formula_13">WA = w 1 n * 1 + w 0 n * 0 w 1 n 1 + w 0 n 0 ,<label>(10)</label></formula><p>where n * 1 and n * 0 are the number of true and false positives, respectively. Since the output of the DHN are between 0 and 1, we threshold the output at 0.5. Under these conditions, the network in <ref type="figure" target="#fig_1">Fig. 2</ref> scores a WA of 92.88%. In the supplementary material, we provide (i) an ablation study on the choice of recurrent unit, (ii) a discussion of alternative architectures, (iii) an analysis of the impact of the distance matrix size on the matching precision and (iv) we experimentally assess how well the DHN preserves the properties of assignment matrices. DHN Usage. Once the DHN is trained with the strategy described above, its weights are fixed: they are not updated in any way during the training of the deep trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Settings</head><p>We demonstrate the practical interest of the proposed framework by assessing the performance of existing (deep) multi-object trackers when trained using the proposed framework on several datasets for pedestrian tracking. We first ablate the loss terms and the tracking architectures. We also evaluate the impact of the framework with respect to other training alternatives. Finally, we establish a new stateof-the-art score on the MOTChallenge benchmark.</p><p>Datasets and Evaluation Metrics. We use the MOT15, MOT16, and MOT17 datasets, which provide crowded pedestrian video sequences captured in the real-world outdoor and indoor scenarios. For the ablation study, we divide the training sequences into training and validation. The details of the split can be found in the supplementary material. In addition to the standard MOTP and MOTA measures <ref type="bibr" target="#b5">[6]</ref>, we report the performance using the IDF1 <ref type="bibr" target="#b43">[44]</ref> measure, defined as the ratio of correctly identified detections over the average number of ground-truth objects and object tracks. We also report mostly tracked (MT) and mostly lost (ML) targets, defined as the ratio of ground-truth trajectories that are covered by a track hypothesis more than 80% and less than 20% of their life span respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tracktor.</head><p>Tracktor <ref type="bibr" target="#b3">[4]</ref> is an adaptation of the Faster RCNN <ref type="bibr" target="#b42">[43]</ref> object detector to the MOT task. It uses a region proposal network (RPN) and the classification/regression heads of the detector to (i) detect objects and (ii) to follow the detected targets in the consecutive frames using a bounding box regression head. As most parts of Tracktor are trainable, this makes this method a perfect candidate to demonstrate the benefits of our framework. Note that Tracktor was originally trained only on the MOTChallenge detection dataset and was only applied to video sequences during inference. In the following, we will refer to Tracktor trained in this setting as Vanilla Base Tracktor. Thanks to DeepMOT, we can train Tracktor directly on video sequences, optimizing for standard MOT measures. We will refer to this variant as DeepMOT Base Tracktor.</p><p>Tracktor+ReID. Vanilla Tracktor has no notion of track identity. Therefore <ref type="bibr" target="#b3">[4]</ref> proposed to use an externally trained ReID module during inference to mitigate IDS. This external ReID module is a feature extractor with a ResNet-50 backbone, trained using a triplet loss <ref type="bibr" target="#b46">[47]</ref> on the MOTChallenge video sequences. We will refer to this variant as +ReI-Dext. Note that this does not give Tracktor any notion of identity during training. This means that the DeepMOT loss which penalizes the number of IDS will have no significant effect on the final performance. For this reason, we propose to replace ReIDext with a lightweight ReID head that we can train jointly with Tracktor using DeepMOT. This in turn allows us to utilizeĨDS and to fully optimize performance to all components of CLEAR-MOT measures. We refer to this variant as +ReIDhead. It takes the form of a fully-connected layer with 128 units plugged into Tracktor. In the supplementary material we provide details on how we embed the ID information into the distance matrix D.</p><p>Even if such a network head has been previously used in <ref type="bibr" target="#b53">[54]</ref>, it was trained externally using the triplet loss <ref type="bibr" target="#b46">[47]</ref>. To the best of our knowledge, we are the first to optimize such an appearance model by directly optimizing the whole network for tracking evaluation measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOT-by-SOT.</head><p>To demonstrate the generality of our method, we propose two additional simple trainable base- lines to perform MOT by leveraging two existing offthe-shelf (trainable) single-object trackers (SOTs): GO-TURN <ref type="bibr" target="#b20">[21]</ref> and SiamRPN <ref type="bibr" target="#b31">[32]</ref>. During inference we initialize and terminate tracks based on object detections. For each object, the SOTs take a reference image at time t−1 of the person and a search region in image t as input. Based on this reference box and search region, the SOTs then regress a bounding box for each object independently.</p><formula xml:id="formula_14">Method MOTA ↑ MOTP ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDS ↓ Van.</formula><p>Track Management. In all cases, we use a simple (nontrainable) track management procedure. We (i) use detector responses to initialize object tracks in regions, not covered by existing tracks (can be either public detections or Faster RCNN detection responses in the case of Tracktor); (ii) we regress tracks from frame t − 1 to frame t using either a SOT or Tracktor and (iii) we terminate tracks that have no overlap with detections (SOT baseline) or invoke the classification head of Tracktor, that signals whether a track is covering an object or not. As an alternative to direct termination, we can set a track as invisible for K frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Discussion</head><p>Beyond Bounding Box Regression. In Tab. 1, we first establish the Vanilla Base Tracktor performance on our validation set and compare it to the DeepMOT Base Tracktor. This experiment (i) validates that our proposed training pipeline based on DHN delivers the gradient to the trackers and improves the overall performance, and (ii) confirms our intuition that training object trackers using a loss that directly correlates with the tracking evaluation measures has a positive impact. Note that the impact on IDS is minimal, which may be on the first sight surprising, as our proposed loss penalizes IDS in addition to FP, FN, and bounding box misalignment.</p><p>We study this by first evaluating the impact of applying external ReID module, i.e., +ReIDext. As can be seen in Tab. 1, ReIDext has a positive impact on the performance, as expected, in terms of MOTA (+0.23% and +0.19%) and IDS (−174 and −160) compared to Base for Vanilla and DeepMOT training respectively. To further demonstrate the interest of a ReID module, we also report the +ReIDhead architecture trained with DeepMOT. Importantly, +ReIDhead cannot be trained in the Vanilla setting due to the lack of mechanisms to penalize IDS. Remarkably, +ReIDhead trained end-to-end with Tracktor does not only improve over the Base performance (MOTA +0.23%, IDS ↓ 191),  We conclude that (i) training existing trackers using our proposed loss clearly improves the performance and (ii) we can easily extend existing trackers such as Tracktor to go beyond simple bounding box regression and incorporate the appearance module directly into the network. All modules are optimized jointly in a single training.</p><p>DeepMOT Loss Ablation. Next, we perform several experiments in which we study the impact of different components of our proposed loss (Eq. 9) to the performance of Tracktor (DeepMOT+ReIDhead). We outline our results in Tab. 2. In addition to Vanilla+ReIDext (representing the best performance trained in Vanilla settings), we also report results obtained by training the same architecture using only the Smooth L 1 loss (see <ref type="figure" target="#fig_3">Fig. 4</ref>). We train the regression head with Smooth L 1 loss using a similar training procedure as for DeepMOT (see Sec. 4.3), to regress predicted bounding boxes to the ones at current time step of their associated tracks. This approach is limited in the sense that we cannot (directly) penalize FP, FN and IDS.   MOT-by-SOT Ablation. Using DeepMOT, we can turn trainable SOT methods into trainable MOT methods by combining them with the track management mechanism (as explained in Sec. 5.2) and optimize their parameters using our loss. In Tab. 3, we outline the results of the two MOTby-SOT baselines (GOTURN <ref type="bibr" target="#b20">[21]</ref> and SiamRPN <ref type="bibr" target="#b31">[32]</ref>). For both, we show the performance when using (i) a pre-trained network, (ii) a network fine-tuned using the Smooth L 1 loss, and (iii) the one trained with DeepMOT. Based on the results outlined in Tab. 3, we conclude that training using the Smooth L 1 loss improves the MOTA for both SOTs (GOTURN: +6.29%, SiamRPN: +1.16%). Moreover, compared to models trained with Smooth L 1 loss, we further improve MOTA and reduce the number of IDS when we train them using DeepMOT. For GO-TURN (SiamRPN), we record a MOTA improvement of 1.81% (0.65%) while reducing the number of IDS by 211 <ref type="bibr" target="#b5">(6)</ref>. We also outline the improvements comparing Vanilla+ReIDext Tracktor trained with Smooth L 1 loss, and DeepMOT+ReIDhead Tracktor trained using Deep-MOT. These results further validate the merit and generality of our method for training deep multi-object trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOTChallenge Benchmark Evaluation</head><p>We evaluate the trackers trained using our framework on the MOTChallenge benchmark (test set) using the best-performing configuration, determined previously using the validation set. During training and inference, we use the camera motion compensation module, as proposed by <ref type="bibr" target="#b3">[4]</ref>, for the three trained trackers. We discuss the results obtained on MOT16-17. MOT15 results and parameters are in the supplementary.</p><p>We follow the standard evaluation practice and compare our models to methods that are officially published on the MOTChallenge benchmark and peer-reviewed. For MOT16 and MOT17, we average the results obtained using the three sets of provided public detections (DPM <ref type="bibr" target="#b17">[18]</ref>, SDP <ref type="bibr" target="#b15">[16]</ref> and Faster R-CNN <ref type="bibr" target="#b42">[43]</ref>). As in <ref type="bibr" target="#b3">[4]</ref>, we use these public detections for track initialization and termination. Importantly, in the case of Tracktor, we do not use the internal detection mechanism of the network, but only public detections.</p><p>As can be seen in Tab. 4, DeepMOT-Tracktor establishes a new state-of-the-art on both MOT17 and MOT16. We improve over Tracktor (on MOT17 and MOT16, respectively) in terms of (i) MOTA (0.2% and 0.4%), (ii) IDF1 (1.5% and 0.9%) and (iii) IDS (125 and 37). On both benchmarks, Vanilla Tracktor is the second best-performing method, and our simple SOT-by-MOT baseline DeepMOT-SiamRPN is the third. We observe large improvements over our MOTby-SOT pre-trained models and models trained using Deep-MOT. For GOTURN, we improve MOTA by 9.8% and 9.7% and we significantly reduce the number of IDS by 6536 and 2071, for MOT17 and MOT16 respectively. Similar impact on DeepMOT-SiamRPN is observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose an end-to-end MOT training framework, based on a differentiable approximation of HA and CLEAR-MOT metrics. We experimentally demonstrate that our proposed MOT framework improves the performance of existing deep MOT methods. Thanks to our method, we set a new state-of-the-art score on the MOT16 and MOT17 datasets. We believe that our method was the missing block for advancing the progress in the area of endto-end learning for deep multi-object tracking. We expect that our training module holds the potential to become a building block for training future multi-object trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. DHN</head><p>For training the DHN, we use the RMSprop optimizer <ref type="bibr" target="#b50">[51]</ref> with a learning rate of 0.0003, gradually decreasing by 5% every 20,000 iterations. We train DHN for 20 epochs (6 hours on a Titan XP GPU). For the focal loss, we weight zero-class by w 0 = n 1 /(n 0 + n 1 ) and one-class by w 1 = 1 − w 0 . Here n 0 is the number of zeros and n 1 the number of ones in A * . We also use a modulating factor of 2 in the focal loss. Once the DHN training converges, we freeze the DHN weights and keep them fixed when training trackers with DeepMOT.</p><p>Datasets. To train the DHN, we generate training pairs as follows. We first compute distance matrices D using ground-truth labels (bounding boxes) and object detections provided by the MOTChallenge datasets (MOT 15-17) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37]</ref>. We augment the data by setting all entries, higher than the randomly (with an uniform distribution ranging from 0 to 1) selected threshold, to a large value to discourage these assignments. This way, we obtain a rich set of various distance matrices. We then compute assignments using the (Hungarian algorithm) HA (variant used in <ref type="bibr" target="#b5">[6]</ref>) to get the corresponding (binary) assignment matrices A * , used as a supervisory signal. In this way, we obtain a dataset of matrix pairs (D and A * ), separated into 114,483 training and 17,880 testing instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Trackers</head><p>Datasets. For training object trackers, we use the MOT17 train set. For the ablation studies, we divide the MOT17 into train/val sets. We split each sequence into three parts: the first, one containing 50% of frames, the second one 25%, and the third 25%. We use the first 50% for training data and the last 25% for validation to make sure there is no overlap between the two. In total, we use 2,664 frames for the train set, containing 35,836 ground-truth bounding boxes and 306 identities. For the validation split, we have 1,328 frames with 200 identities. The public object detections (obtained by DPM <ref type="bibr" target="#b17">[18]</ref>, SDP <ref type="bibr" target="#b56">[57]</ref> and Faster RCNN <ref type="bibr" target="#b42">[43]</ref> detectors) from the MOTChallenge are used only during tracking.</p><p>Training. We use the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> with a learning rate of 0.0001. We train the SOTs for 15 epochs (72h), and we train Tracktor (regression head and ReID head) for 18 epochs (12h) on a Titan XP GPU.</p><p>Loss Hyperparameters. When training trackers using our DeepMOT loss, we set the base value of δ = 0.5, and the loss balancing factors of λ = 5, γ = 2, as determined on the validation set.</p><p>Training Details. To train object trackers, we randomly select one training instance from the sequence that corresponds to a pair of consecutive frames. Then, we initialize object trackers using ground-truth detections and predict track continuations in the next frame. At each time step, we use track predictions and ground-truth bounding boxes to compute D, which we pass to our DHN and, finally, compute loss and back-propagate the gradients to the tracker.</p><p>Data Augmentation. We initialize trackers using groundtruth bounding boxes. To mimic the effects of imperfect object detectors and prevent over-fitting, we perform the following data augmentations during the training:</p><p>• We randomly re-scale the bounding boxes with a scaling factor ranging from 0.8 to 1.2.</p><p>• We add random vertical and horizontal offset vectors (bounding box width and/or height scaled by a random factor ranging from 0 to 0.25).</p><p>Training with the ReIDhead. While training Tracktor with our ReIDhead, we make the following changes. Instead of selecting a pair of video frames, we randomly select ten consecutive frames. This is motivated by the implementation of external ReID mechanism in <ref type="bibr" target="#b3">[4]</ref>, where tracker averages appearance features over ten most recent frames. At each training step, we compute representative embedding by averaging embeddings of the past video frames and use it to compute the cosine distance to the ground-truth object embeddings.</p><p>Test-time Track Managment. For the MOT-by-SOT baseline, we use detections from three different detectors (DPM, SDP, and FRCNN) to refine the track predictions. When the IoU between a track prediction and detection is higher than 0.6, we output their average. We also reduce FP in the public detections based on detection scores produced by a Faster RCNN detector. For the birth and death processes, we initialize a new track only when detections appear in 3 consecutive frames, and they have a minimal consecutive IoU overlap of 0.3. Tracks that can not be verified by the detector are marked invisible and are terminated after K = 60 frames. For Tracktor, we use the same track management and suppression strategy as proposed in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional DHN Ablation</head><p>We perform DHN ablation using our test split of 17,880 DHN training instances, as explained in Sec. A.1. In addition, we evaluate the generalization of DHN by evaluating performing evaluation using distance matrices, generated during the DeepMOT training process.</p><p>Accuracy. We compute the weighted accuracy as (using the same weighting factors w 1 and w 0 as for the loss):</p><formula xml:id="formula_15">WA = w 1 n * 1 + w 0 n * 0 w 1 n 1 + w 0 n 0 .<label>(11)</label></formula><p>Here, n * 1 and n * 0 are the number of true and false positives, respectively.</p><p>Validity. The output of the matching algorithm should be a permutation matrix; i.e., there should be at most one assignment per row/column. In the case of the HA, this is explicitly enforced via constraints on the solution. To study how well the predicted (discretized) assignment matrices preserve this property, we count the number of rows and columns by the following criteria:</p><p>• Several Assignments (SA) counts the number of rows/columns that have more than one assignment (when performing column-wise maximum and rowwise maximum, respectively).</p><p>• Missing Assignments (MA) counts the number of rows/columns that are not assigned (when performing column-wise maximum and row-wise maximum, respectively) when ground-truth assignment matrix A * has an assignment or inversely, no assignment in A * whileĀ (see below) has an assignment in the corresponding rows/columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discretization.</head><p>To perform the evaluation, we first need to discretize the soft assignment matrixÃ, predicted by our DHN to obtain a discrete assignment matrixĀ. There are two possibilities.</p><p>(i) For each row ofĀ, we set the entry ofĀ corresponding to the largest value of the row to 1 (as long as it exceeds 0.5) and the remaining values are set to 0. We refer to this variant as row-wise maximum.</p><p>(ii) Analogously, we can perform column-wise maximum by processing columns instead of rows.</p><p>DHN variants. We compare three different DHN architectures:</p><p>(i) Sequential DHN (seq, see <ref type="figure">Fig. 5</ref>),</p><p>(ii) Parallel DHN (paral, see <ref type="figure">Fig. 6</ref>), (iii) 1D Convolutional DHN (1d conv, see <ref type="figure">Fig. 7</ref>).</p><p>The recurrent unit of the two recurrent architectures, seq and paral, is also ablated between long-short term memory units (lstm) <ref type="bibr" target="#b22">[23]</ref> and gated recurrent units (gru) <ref type="bibr" target="#b9">[10]</ref>. From Tab. 5, we see that the proposed sequential DHN (seq gru) obtains the highest WA (92.88% for row-wise maximum and 93.49% for column-wise maximum) compared to others. Compared to the 1D convolutional DHN   wise maximum). As for the validity, the proposed seq gru commits the least missing assignments (MA) (4.79% and 6.41% for row-wise and column-wise maximum, respectively), and commits only a few SA compared to other variants. DHN is a key component of our proposed DeepMOT training framework. To evaluate how well DHN performs during training as a proxy to deliver gradients from the DeepMOT loss to the tracker, we conduct the following experiment. We evaluate DHN using distance matrices D, collected during the DeepMOT training process. As can be seen in Tab. 6, the proposed sequential DHN (seq gru) outperforms the others variants, with a WA of 92.71% for rowwise and 92.36% for column-wise maximum. For validity, it also attains the lowest MA: 13.17% (row) and 12.21% (column). The SA is kept at a low level with 9.70% and 3.69% for row-wise and column-wise maximum discretizations, respectively. Based on these results, we conclude that (i) our proposed DHN generalizes well to matrices, used to train our trackers, and (ii) it produces outputs that closely resemble valid permutation matrices.</p><p>Matrix Size. To provide further insights into DHN, we study the impact of the distance matrix size on the assignment accuracy. We visualize the relation between WA and the input matrix size in <ref type="figure" target="#fig_5">Fig. 8</ref>. For validation, we generate square matrices with sizes ranging from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">300]</ref>. Precisely, we generate D with a uniform distribution [0, 1) and use the Hungarian algorithm implementation from <ref type="bibr" target="#b5">[6]</ref> to generate assignment matrices A * . For each size, we evaluate 10 matrices, which gives us 2,990 matrices in total. As can be seen in <ref type="figure" target="#fig_5">Fig. 8</ref>, (i) the proposed seq gru consistently outperforms the alternatives. (ii) The assignment accuracy of DHN and its variants decreases with the growth of the matrix size. Moreover, we observe a performance drop for very small matrices (i.e., M = N 6). This may be due to the imbalance with respect to the matrix size during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Gradient Visualization</head><p>The negative gradient should reflect the direction that minimizes the loss. In in <ref type="figure" target="#fig_6">Fig. 9</ref> we plot the negative gradient of different terms that constitute our DeepMOT loss w.r.t. the coordinates of each predicted bounding box to demonstrate visually the effectiveness of our DeepMOT. In this example, we manually generated the cases which contain the FP, FN or IDS. We observe that the negative gradient does encourage the tracks' bounding boxes to be close to those of their associated objects during the training.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MOT15 Results</head><p>We summarize the results we obtain on MOT15 dataset in Tab. 7. Our key observations are:</p><p>(i) For the MOT-by-SOT baseline, we significantly improve over the trainable baselines (SiamRPN and GO-TURN). DeepMOT-SiamRPN increases MOTA for +2.3%, MOTP for +0.7% and IDF1 for +2.0%. Remarkably, DeepMOT-SiamRPN suppresses 2,416 FP and 143 IDS. We observe similar performance gains for DeepMOT-GOTURN.</p><p>(ii) DeepMOT-Tracktor obtains results, comparative to the vanilla Tracktor <ref type="bibr" target="#b3">[4]</ref>. Different from MOT16 and MOT17 datasets, we observe no improvements in terms of MOTA, which we believe is due to the fact that labels in MOT15 are very noisy, and vanilla Tracktor already achieves impressive performance. Still, we increase MOTP for 0.3% and reduce FP for 392.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A∈{0,1} N ×M n,m d nm a nm , s.t. m a nm ≤ 1, ∀n; n a nm ≤ 1, ∀m; m,n a nm = min{N, M }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Structure of the proposed Deep Hungarian Net. The row-wise and column-wise flattening are inspired by the original Hungarian algorithm, while the Bi-RNN allows for all decisions to be taken globally, thus is accounting for all input entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>DeepMOT loss: dM OT P (top) is computed as the average distance of matched tracks and dM OT A (bottom) is composed withFP,ĨDS andFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(ii) Proposed Full-MOT Back-prop; the gradient goes through DHN (i) Assignment-less Back-prop Baseline gradients The proposed MOT training strategy (bottom) accounts for the track-to-object assignment problem, solved by the proposed DHN, and approximates the standard MOT losses, as opposed to the classical training strategies (top) using the nondifferentiable HA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Sequential DHN: Structure of the proposed Deep Hungarian Net. The row-wise and column-wise flattening are inspired by the original Hungarian algorithm, while the Bi-RNN allows for all decisions to be taken globally, thus is accounting for all input entries. Parallel DHN variant: (i) We perform row-wise and the column-wise flattening of D. (ii) We process the flattened vectors using two different Bi-RNNs. (iii) They then are respectively passed to an FC layer for reducing the number of channels and are concatenated along the channel dimension. (iv) After two FC layers we reshape the vector and apply the sigmoid activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Evaluation of performance of DHN and its variants on D of different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Visualization of negative gradients (direction and magnitude) from different terms in the proposed DeepMOT loss: (a) FP and FN (b) MOTP(c-d) IDS (compare (c) t − 1 with (d) t).The predicted bounding-boxes are shown in blue, the ground-truth are shown in green and the gradient direction is visualized using red arrows.Method MOTA ↑ MOTP ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDS ↓ 2D MOT 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Impact of the different ReID strategies for the two training strategies on Tracktor's performance.</figDesc><table><row><cell></cell><cell>Base</cell><cell>59.97</cell><cell>89.50</cell><cell>70.84 35.13 27.66 276 31827 326</cell></row><row><cell></cell><cell>+ReIDext</cell><cell>60.20</cell><cell>89.50</cell><cell>71.15 35.13 27.80 276 31827 152</cell></row><row><cell>DeepMOT</cell><cell cols="2">Base +ReIDext +ReIDhead 60.66 60.43 60.62</cell><cell>91.82 91.82 91.82</cell><cell>71.44 35.41 27.25 218 31545 309 71.66 35.41 27.39 218 31545 149 72.32 35.41 27.25 218 31545 118</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Training loss MOTA ↑ MOTP ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDS ↓ 35.41 27.39 254 31597 142 dM OT A+dM OT P -ĨDS 60.61 92.03 72.10 35.41 27.25 222 31579 124 dM OT A+dM OT P 60.66 91.82 72.32 35.41 27.25 218 31545 118</figDesc><table><row><cell>Vanilla</cell><cell>60.20</cell><cell>89.50</cell><cell>71.15 35.13 27.80 276 31827 152</cell></row><row><cell>Smooth L1</cell><cell>60.38</cell><cell>91.81</cell><cell>71.27 34.99 27.25 294 31649 164</cell></row><row><cell>dM OT P</cell><cell>60.51</cell><cell>91.74</cell><cell>71.75 35.41 26.83 291 31574 142</cell></row><row><cell>dM OT A</cell><cell>60.52</cell><cell>88.31</cell><cell>71.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the effect the training loss on Tracktor.but it also outperforms +ReIDext (MOTA ↑ 0.04 and IDS ↓ 31). Very importantly, the lightweight ReID head contains a significantly lower number of parameters (≈ 131 K) compared to the external ReID module (≈ 25 M).Finally, in addition to improve the performance measures for which we optimize Tracktor, DeepMOT consistently improves tracking measures such as IDF1 (↑ 1.17 improvement of DeepMOT+ReIDhead over Vanilla+ReIDext).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The Smooth L 1 training, when compared to Vanilla, has a positive impact on almost all performance measures, except for MT, FP, and IDS. However, both Vanilla and Smooth L 1 are outperformed almost systematically for all performance measures by the various variants of the DeepMOT loss. Remarkably, when using dM OT A term in our loss, we significantly reduce the number of IDS and FP. Training with dM OT P has the highest impact on MOTP, as it is the case when training with Smooth L 1 . When only optimizing for dM OT A, we have a higher impact on the MOTA and IDF1 measure. Remarkably, when training with (dM OT A+dM OT P ), we obtain a consistent improvement on all tracking evaluation measures with respect to Vanilla and Smooth L 1 .Finally, we asses the impact ofĨDS, by setting the weight γ to 0 (Eq. 7) (line dM OT A+dM OT P -ĨDS). In this settings, the trackers exhibits a higher number of IDS compared to using the full loss, confirming that the latter is the best strategy.Training MOTA ↑ MOTP ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDS ↓ GOTURN Pre-trained 45.99 85.87 49.83 22.27 36.51 2927 39271 1577 Smooth L 1 52.28 90.56 63.53 29.46 34.58 2026 36180 472 DeepMOT 54.09 90.95 66.09 28.63 35.13 927 36019 261 SiamRPN Pre-trained 55.35 87.15 66.95 33.61 31.81 1907 33925 356 Smooth L 1 56.51 90.88 68.38 33.75 32.64 925 34151 167 DeepMOT 57.16 89.32 69.49 33.47 32.78 889 33667 161</figDesc><table><row><cell>Tracktor</cell><cell>Vanilla Smooth L 1 60.38 60.20 DeepMOT 60.66</cell><cell>89.50 91.81 91.82</cell><cell>71.15 35.13 27.80 276 31827 152 71.27 34.99 27.25 294 31649 164 72.32 35.41 27.25 218 31545 118</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>DeepMOT vs. Smooth L1 using MOT-by-SOT baselines and Tracktor.</figDesc><table><row><cell></cell><cell>DeepMOT-Tracktor</cell><cell>53.7</cell><cell>77.2</cell><cell>53.8 19.4 36.6 11731 247447 1947</cell></row><row><cell></cell><cell>Tracktor [4]</cell><cell>53.5</cell><cell>78.0</cell><cell>52.3 19.5 36.6 12201 248047 2072</cell></row><row><cell></cell><cell>DeepMOT-SiamRPN</cell><cell>52.1</cell><cell>78.1</cell><cell>47.7 16.7 41.7 12132 255743 2271</cell></row><row><cell>MOT17</cell><cell>SiamRPN [32] DeepMOT-GOTURN GOTURN [21]</cell><cell>47.8 48.1 38.3</cell><cell>76.4 77.9 75.1</cell><cell>41.4 17.0 41.7 38279 251989 4325 40.0 13.6 43.5 22497 266515 3792 25.7 9.4 47.1 55381 282670 10328</cell></row><row><cell></cell><cell>eHAF [49]</cell><cell>51.8</cell><cell>77.0</cell><cell>54.7 23.4 37.9 33212 236772 1834</cell></row><row><cell></cell><cell>FWT [22]</cell><cell>51.3</cell><cell>77.0</cell><cell>47.6 21.4 35.2 24101 247921 2648</cell></row><row><cell></cell><cell>jCC [24]</cell><cell>51.2</cell><cell>75.9</cell><cell>54.5 20.9 37.0 25937 247822 1802</cell></row><row><cell></cell><cell>MOTDT17 [34]</cell><cell>50.9</cell><cell>76.6</cell><cell>52.7 17.5 35.7 24069 250768 2474</cell></row><row><cell></cell><cell>MHT DAM [25]</cell><cell>50.7</cell><cell>77.5</cell><cell>47.2 20.8 36.9 22875 252889 2314</cell></row><row><cell></cell><cell>DeepMOT-Tracktor</cell><cell>54.8</cell><cell>77.5</cell><cell>53.4 19.1 37.0 2955 78765 645</cell></row><row><cell></cell><cell>Tracktor [4]</cell><cell>54.4</cell><cell>78.2</cell><cell>52.5 19.0 36.9 3280 79149 682</cell></row><row><cell></cell><cell>DeepMOT-SiamRPN</cell><cell>51.8</cell><cell>78.1</cell><cell>45.5 16.1 45.1 3576 83699 641</cell></row><row><cell>MOT16</cell><cell>SiamRPN [32] DeepMOT-GOTURN GOTURN [21]</cell><cell>44.0 47.2 37.5</cell><cell>76.6 78.0 75.4</cell><cell>36.6 15.5 45.7 18784 82318 1047 37.2 13.7 46.1 7230 87781 1206 25.1 8.4 46.5 17746 92867 3277</cell></row><row><cell></cell><cell>HCC [36]</cell><cell>49.3</cell><cell>79.0</cell><cell>50.7 17.8 39.9 5333 86795 391</cell></row><row><cell></cell><cell>LMP [52]</cell><cell>48.8</cell><cell>79.0</cell><cell>51.3 18.2 40.1 6654 86245 481</cell></row><row><cell></cell><cell>GCRA [35]</cell><cell>48.2</cell><cell>77.5</cell><cell>48.6 12.9 41.1 5104 88586 821</cell></row><row><cell></cell><cell>FWT [22]</cell><cell>47.8</cell><cell>75.5</cell><cell>44.3 19.1 38.2 8886 85487 852</cell></row><row><cell></cell><cell>MOTDT [34]</cell><cell>47.6</cell><cell>74.8</cell><cell>50.9 15.2 38.3 9253 85431 792</cell></row></table><note>Method MOTA ↑ MOTP ↑ IDF1 ↑ MT ↑ ML ↓ FP ↓ FN ↓ IDS ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>We establish a new state-of-the-art on MOT16 and MOT17 public benchmarks by using the proposed DeepMOT.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Net<ref type="bibr" target="#b44">[45]</ref>. The encoder consists of two 1D-convolution layers of shapes<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15]</ref> and<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b14">15]</ref> ([#input channels, #output channels, kernel size]). The decoder consists of two 1D convolutional layers of shapes [96,<ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5]</ref> and [72,<ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5]</ref>. Finally, we apply an 1D convolution and a sigmoid activation to produceÃ. variant (WA of 56.43% and 56.18% for row-wise and column-wise maximum, respectively), Bi-RNN shows the advantage of its global view due to the receptive field, equal to the entire input. For the sequential DHN setting, we observe in Tab. 5 that gru units consistently outperform lstm units with WA +9.22% (row-wise maximum) and +6.42% (column-wise maximum). Finally, the proposed sequential DHN is more accurate compared to the parallel variant of DHN (+3.32% for row-wise and +2.48% for column-</figDesc><table><row><cell>1/2 Figure 7. 1D convolutional DHN: Our 1D convolutional DHN 1/ 2× M × N 48 Upsampling × 2 1/ 2× M × N 48 Conv1D (96,48,5) Concatenate M × N 48 Upsampling × 2 M × N 24 Conv1D (72,24,5) Concatenate M × N 25 Concatenate N M Soft Assignment Matrix Conv1D (25,1,1) ... M × N Sigmoid Reshape Conv1D(48, 48,15) D Ã variant is inspired by the U-Discretization Network WA % (↑) MA% (↓) SA% (↓) Row-wise maximum seq gru (proposed) 92.88 4.79 3.39 seq lstm 83.66 13.79 5.98 paral gru 89.56 8.21 4.99 paral lstm 88.93 8.67 5.38 1d conv 56.43 35.06 2.78 Column-wise maximum seq gru (proposed) 93.49 6.41 26.57 seq lstm 87.07 13.54 47.04 paral gru 91.01 7.98 46.25 paral lstm 90.50 8.60 47.43 1d conv 56.18 79.54 7.73 Table 5. Evaluation results: comparison of different network struc-tures and settings in terms of WA, MA and SA on the DHN test set. Discretization Network WA % (↑) MA% (↓) SA% (↓) Row-wise maximum seq gru (proposed) 92.71 13.17 9.70 seq lstm 91.64 14.55 10.37 paral gru 86.84 23.50 17.15 paral lstm 71.58 42.48 22.62 1d conv 83.12 32.73 5.73 Column-wise maximum seq gru (proposed) 92.36 12.21 3.69 seq lstm 91.93 13.15 4.71 paral gru 87.24 20.56 16.67 paral lstm 72.58 39.55 23.16 1d conv 82.74 32.94 1.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Evaluation results. Comparison of different network structures and settings in terms of WA, MA and SA on distance matrices during training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Results on MOTChallenge MOT15 benchmark.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We gratefully acknowledge the mobility grants from the Department for Science and Technology of the French Embassy in Berlin (SST) and the French Institute for Research in Computer Science and Automation (Inria), especially the Perception team. We are grateful to the Dynamic Vision and Learning Group, Technical University of Munich as the host institute, especially Guillem Brasó and Tim Meinhardt for the fruitful discussions. Finally, this research was partially funded by the Humboldt Foundation through the Sofja Kovalevskaja Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An on-line variational bayesian model for multi-person tracking from cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sileye</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Xompero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="64" to="76" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational bayesian inference for audiovisual tracking of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking multiple persons based on a variational bayesian model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sileye</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<title level="m">Tracking without bells and whistles. ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: The clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIVP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi object tracking as maximum weight independent set. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-target tracking by lagrangian relaxation to min-cost network flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multi-sensor fusion system for moving object detection and tracking in urban driving environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunggi</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Woo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragunathan Raj</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Near-online multi-target tracking with aggregated local flow descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent autoregressive networks for online multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised moving vehicle tracking with stereo sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to track at 100 fps with deep regression networks. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improvements to frank-wolfe optimization for multi-detector multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08314</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple hypothesis tracking revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arridhana</forename><surname>Ciptadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-object tracking with neural gating using bilinear lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>William Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryn</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning by tracking: Siamese cnn for robust target association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Motchallenge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled object detection and tracking from static cameras and moving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Bastian Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1683" to="1698" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Real-time multiple people tracking with deeply learned candidate selection and person re-identification. ICME</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><surname>Haizhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Zijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang</forename><surname>Chong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Trajectory factory: Tracklet cleaving and re-connection by deep siamese bi-gru for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqing</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICME</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Customized multi-person tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<title level="m">MOT16: A benchmark for multi-object tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo data association for multi-target tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="497" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combined image-and world-space tracking in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljoša</forename><surname>Ošep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An algorithm for tracking multiple targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="854" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation. MICCAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep network flow for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan Krishna</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Heterogeneous association graph fusion for target association in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-object tracking with quadruplet convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Maxmargin markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MOTS: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning optimal parameters for multi-target tracking with contextual interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="484" to="501" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Online multi-object tracking with dual matching attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
