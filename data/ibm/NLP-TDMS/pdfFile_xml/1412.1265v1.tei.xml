<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks.</p><p>Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts.</p><p>(3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition achieved great progress thanks to extensive research effort devoted to this area <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref>. While pursuing higher performance is a central topic, understanding the mechanisms behind it is equally important. When deep neural networks begin to approach human on challenging face benchmarks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref> such as LFW <ref type="bibr" target="#b15">[16]</ref>, people are eager to know what has been learned by these neurons and how such high performance is achieved. In cognitive science, there are a lot of studies <ref type="bibr" target="#b33">[34]</ref> on analyzing the mechanisms of face processing of neurons in visual cortex. There are 512 neurons in the top hidden layer of DeepID2+. We subsample 32 for illustration. Right: a few neurons are selected to show their activation histograms over all the LFW face images (as background), all the images belonging to Bush, all the images with attribute Male, and all the images with attribute Female. A neuron is generally activated on about half of the face images. But it may constantly have activations (or no activation) for all the images belonging to a particular person or attribute. In this sense, neurons are sparse, and selective to identities and attributes.</p><p>Inspired by those works, we analyze the behaviours of neurons in artificial neural networks in a attempt to explain face recognition process in deep nets, what information is encoded in neurons, and how robust they are to corruptions.</p><p>Our study is based on a high-performance deep convolutional neural network (deep ConvNet <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>), referred to as DeepID2+, proposed in this paper. It is improved upon the state-of-the-art DeepID2 net <ref type="bibr" target="#b27">[28]</ref> by increasing the dimension of hidden representations and adding supervision to early convolutional layers. The best single DeepID2+ net (taking both the original and horizontally flipped face images as input) achieves 98.70% verification accuracy on LFW (vs. 96.72% by DeepID2). Combining 25 DeepID2+ nets sets new state-of-the-art on multiple benchmarks: 99.47% on LFW for face verification (vs. 99.15% by DeepID2 <ref type="bibr" target="#b27">[28]</ref>), 95.0% and 80.7% on LFW for closed-and open-set face identification, respectively (vs. 82.5% and 61.9% by Web-Scale Training (WST) <ref type="bibr" target="#b31">[32]</ref>), and 93.2% on YouTubeFaces <ref type="bibr" target="#b34">[35]</ref> for face verification (vs. 91.4% by DeepFace <ref type="bibr" target="#b30">[31]</ref>).</p><p>With the state-of-the-art deep ConvNets and through extensive empirical evaluation, we investigate three properties of neural activations crucial for the high performance: sparsity, selectiveness, and robustness. They are naturally owned by deepID2+ after large scale training on face data, and we did NOT enforce any extra regularization to the model and training process to achieve them. Therefore, these results are valuable for understanding the intrinsic properties of deep networks.</p><p>It is observed that the neural activations of DeepID2+ are moderately sparse. As examples shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, for an input face image, around half of the neurons in the top hidden layer are activated. On the other hand, each neuron is activated on roughly half of the face images. Such sparsity distributions can maximize the discriminative power of the deep net as well as the distance between images. Different identities have different subsets of neurons activated. Two images of the same identity have similar activation patterns. This motivates us to binarize the neural responses in the top hidden layer and use the binary code for recognition. Its result is surprisingly good. Its verification accuracy on LFW only slightly drops by 1% or less. It has significant impact on large-scale face search since huge storage and computation time is saved. This also implies that binary activation patterns are more important than activation magnitudes in deep neural networks.</p><p>Related to sparseness, it is also observed that neurons in higher layers are highly selective to identities and identityrelated attributes. When an identity (who can be outside the training data) or attribute is presented, we can identify a subset of neurons which are constantly excited and also can find another subset of neurons which are constantly inhibited. A neuron from any of these two subsets has strong indication on the existence/non-existence of this identity or attribute, and our experiment show that the single neuron alone has high recognition accuracy for a particular identity or attribute. In other words, neural activations have sparsity on identities and attributes, as examples shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such highlevel concepts. Directly employing the face representation learned by DeepID2+ leads to much higher classification accuracy on identity-related attributes than widely used handcrafted features such as high-dimensional LBP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Our empirical study shows that neurons in higher layers are much more robust to image corruption in face recognition than handcrafted features such as high-dimensional LBP or neurons in lower layers. As an example shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, when a face image is partially occluded, its binary activation patterns remain stable, although the magnitudes could change. We conjecture the reason might be that neurons in higher layers capture global features and are less sensitive to local variations. DeepID2+ is trained by natural web face images and no artificial occlusion patterns were added to the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Only very recently, deep learning achieved great success on face recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> and significantly outperformed systems using low level features <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5]</ref>. There are two notable breakthroughs. The first is large-scale face identification with deep neural networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. By classifying face images into thousands or even millions of identities, the last hidden layer forms features highly discriminative to identities. The second is supervising deep neural networks with both face identification and verification tasks <ref type="bibr" target="#b27">[28]</ref>. The verification task minimizes the distance between features of the same identity, and decreases intra-personal variations <ref type="bibr" target="#b27">[28]</ref>. By combining features learned from many face regions, <ref type="bibr" target="#b27">[28]</ref> achieved the current state-of-the-art (99.15%) of face verification on LFW.</p><p>Attribute learning is an active topic <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40]</ref>. There have been works on first learning attribute classifiers and using attribute predictions for face recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref>. What we have tried in this paper is the inverse, by first predicting the identities, and then using the learned identityrelated features to predict attributes.</p><p>Sparse representation-based classification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref> was extensively studied for face recognition with occlusions. Tang et al. <ref type="bibr" target="#b32">[33]</ref> proposed Robust Boltzmann Machine to distinguish corrupted pixels and learn latent representations. These methods designed components explicitly handling occlusions, while we show that features learned by DeepID2+ have implicitly encoded invariance to occlusions. This is naturally achieved without adding regulation to models or artificial occlusion patterns to training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DeepID2+ nets</head><p>Our DeepID2+ nets are inherited from DeepID2 nets <ref type="bibr" target="#b27">[28]</ref>, which have four convolutional layers, with 20, 40, 60, and 80 feature maps, followed by a 160-dimensional feature layer fully-connected to both the third and fourth convolutional layers. The 160-dimensional feature layer (DeepID2 feature layer) is supervised by both face identification and verification tasks. Given a pair of training images, it obtains two DeepID2 feature vectors (f i and f j ) by forward-propagating the two images through the DeepID2 net. Then each DeepID2 feature vector is classified as one of 8192 identities in the training set, and the classification (identification) error is generated. The verification error is <ref type="bibr">1 2</ref> </p><formula xml:id="formula_0">f i − f j 2 2</formula><p>if f i and f j are from the same identity, or <ref type="bibr" target="#b0">1</ref> 2 max 0, m − f i − f j 2 2 otherwise. It was shown that combining identification and verification supervisory signals helps to learn features more effectively <ref type="bibr" target="#b27">[28]</ref>. Compared to DeepID2, DeepID2+ makes three improvements. First, it is larger with 128 feature maps in each of the four convolutional layers. The final feature representation is increased from 160 to 512 dimensions. Second, our training data is enlarged by merging the CelebFaces+ dataset <ref type="bibr" target="#b28">[29]</ref>, the WDRef dataset <ref type="bibr" target="#b5">[6]</ref>, and some newly collected identities exclusive from LFW. The larger DeepID2+ net is trained with around 290, 000 face images from 12, 000 identities compared to 160, 000 images from 8, 000 identities used to train the DeepID2 net. Third, in the DeepID2 net, supervisory signals are only added to one fully-connected layer connected to the third and fourth convolutional layers, while the lower convolutional layers can only get supervision with gradients back-propagated from higher layers. We enhance the supervision by connecting a 512-dimensional fully-connected layer to each of the four convolutional layers (after max-pooling), denoted as FC-n for n = 1, 2, 3, 4, and supervise these four fully-connected layers with the identification-verification supervisory signals <ref type="bibr" target="#b27">[28]</ref> simultaneously as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this way, supervisory signals become "closer" to the early convolutional layers and are more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">High-performance of DeepID2+ nets</head><p>To verify the improvements, we learn the Joint Bayesian model <ref type="bibr" target="#b5">[6]</ref> for face verification based on each of the four 512-dimensional feature vectors (neural activations) FC-n for n = 1, 2, 3, 4 in the DeepID2+ net. Joint Bayesian is trained on 2000 people in our training set (exclusive from people in LFW) which are not used for training DeepID2+ net, and tested on the 6, 000 given face pairs in LFW for face verification. These 2000 identities also serve as a validating set when training the DeepID2+ net to determine learning rates and training iterations. The DeepID2+ net (proposed) is compared to three nets with one of the three improvements removed, respectively, as shown in <ref type="figure" target="#fig_2">Fig.  3</ref>. For the net with no layer-wise supervision, only the gradients of FC-4 is back-propagated to the convolutional layers. For the net with less training data, only the training data from CelebFaces+ is used. For the smaller network, the numbers of feature maps in convolutional layers are the same as those in DeepID2 and 160-dimensional features are  used for FC-n for n = 1, 2, 3, 4. All the networks compared are learned on a fixed region covering the entire face. We can clearly see the improvements of the three aspects from <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>To compare with DeepID2 nets <ref type="bibr" target="#b27">[28]</ref>, we train 25 DeepID2+ nets taking the same 25 face regions selected by DeepID2 as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> in <ref type="bibr" target="#b27">[28]</ref>. Features in the FC-4 layer of DeepID2+ are extracted to train Joint Bayesian for face verification (features are extracted on either the original or the horizontally flipped face regions as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> in <ref type="bibr" target="#b27">[28]</ref>). The comparison between the 25 deep ConvNets on the LFW face verification task is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. DeepID2+ nets improve approximately 2% accuracy on average over DeepID2. When combining  High-dim LBP <ref type="bibr" target="#b6">[7]</ref> 95.17 ± 1.13 TL Joint Bayesian <ref type="bibr" target="#b4">[5]</ref> 96.33 ± 1.08 DeepFace <ref type="bibr" target="#b30">[31]</ref> 97.35 ± 0.25 DeepID <ref type="bibr" target="#b28">[29]</ref> 97.45 ± 0.26 GaussianFace <ref type="bibr" target="#b22">[23]</ref> 98.52 ± 0.66 DeepID2 <ref type="bibr" target="#b27">[28]</ref> 99. <ref type="bibr" target="#b14">15</ref>  Face identification is a more challenging task to evaluate high-performance face recognition systems <ref type="bibr" target="#b31">[32]</ref>. Therefore we further evaluate the 25 DeepID2+ nets on the closedand open-set face identification tasks on LFW, following the protocol in <ref type="bibr" target="#b2">[3]</ref>. The closed-set identification reports the Rank-1 identification accuracy while the open-set identification reports the Rank-1 Detection and Identification rate (DIR) at a 1% False Alarm Rate (FAR). The comparison results are shown in Tab. 3. Our results significantly outperform the previous best <ref type="bibr" target="#b31">[32]</ref> with 95.0% and 80.7% closed and open-set identification accuracies, respectively.  <ref type="bibr" target="#b11">[12]</ref> 81.3 ± 1.6 DDML (combined) <ref type="bibr" target="#b11">[12]</ref> 82.3 ± 1.5 EigenPEP <ref type="bibr" target="#b20">[21]</ref> 84.8 ± 1.4 DeepFace-single <ref type="bibr" target="#b30">[31]</ref> 91.4 ± 1.1 DeepID2+</p><p>93.2 ± 0.2  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Moderate sparsity of neural activations</head><p>Neural activations are moderately sparse in both the sense that for each image, there are approximately half of the neurons which are activated (with positive activation values) on it, and for each neuron, there are approximately half of the images on which it is activated. The moderate sparsity on images makes faces of different identities maximally distinguishable, while the moderate sparsity on neurons makes them to have maximum discrimination abilities. We verify this by calculating the histogram of the  <ref type="bibr" target="#b2">[3]</ref> 66.5 35 DeepFace <ref type="bibr" target="#b30">[31]</ref> 64.9 44.5 WST Fusion <ref type="bibr" target="#b31">[32]</ref> 82.5 61.9 DeepID2+ 95.0 80.7</p><p>activated neural numbers on each of the 46, 594 images in our validating dataset ( <ref type="figure" target="#fig_6">Fig. 7 left)</ref>, and the histogram of the number of images on which each neuron are activated ( <ref type="figure" target="#fig_6">Fig. 7 right)</ref>. The evaluation is based on the FC-4 layer neurons in a single DeepID2+ net taking the entire face region as input. Compared to all 512 neurons in the FC-4 layer, the mean and standard deviation of the number of activated neurons on images is 292 ± 34, while compared to all 46, 594 validating images, the mean and standard deviation of the number of images on which each neuron are activated is 26, 565 ± 5754, both of which are approximated centered at half of all neurons/images. We further verify that the activation patterns, i.e., whether neurons are activated, are more important than precise activation values. We convert neural activations to binary code by thresholding and compare its face verification ability on LFW to that of the original representation. As shown in Tab. 4, the binary representation, when coupled with Joint Bayesian, sacrifices 1% or less accuracies (97.67% and 99.12% with a single net or combining 25 nets, respectively). More interestingly, the binary code can still achieve 96.45% and 97.47% accuracy with a single net or combining 25 nets, respectively, even by directly calculating the Hamming distances. This shows that the state of excitation or inhibition of neurons already contains the majority of discriminative information. Binary code is economic for storage and fast for image search. We believe this would be an interesting direction of future work. We test DeepID2+ features for two binary classification tasks. The first is to classify the face images of one person against those of all the other people or the background. The second is to classify a face image as having an attribute or not. DeepID2+ features are taken from the FC-4 layer of a single DeepID2+ net on the entire face region and its horizontally flipped counterpart, respectively. The experiments are conducted on LFW with people unseen by the DeepID2+ net during training. LFW is randomly split into two subsets and the cross-validation accuracies are reported. The accuracies are normalized w.r.t. the image numbers in the positive and negative classes. We also compare to the high-dimensional LBP features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> with various feature dimensions. As shown in <ref type="figure" target="#fig_7">Fig. 8</ref>, DeepID2+ features significantly outperform LBP in attribute classification (it is not surprising that DeepID2+ has good identity classification result). <ref type="figure" target="#fig_8">Fig. 9</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref> show identity and attribute classification accuracies with only one best feature selected. Different best features are selected for different identities (attributes). With a single feature (neuron), DeepID2+ reaches approximately 97% for some identity and attribute. This is the evidence that DeepID2+ features are identity and attribute selective. Apparently LBP does not have it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Excitatory and inhibitory neurons</head><p>We find that the discrimination to identities and facial attributes are due to neurons' excitation and inhibition patterns on certain identities or attributes. For example, a neuron may be excited when it sees George Bush while becoming inhibitive when it sees Colin Powell, or a neuron   may be excited for western people while being inhibitive for Asian. <ref type="figure" target="#fig_0">Fig. 11a</ref> compares the mean and standard deviation of DeepID2+ neural activations over images belonging to a particular single identity (left column) and over the remaining images (middle column), as well as showing the per-neuron classification accuracies of distinguishing each given identity from the remaining images (right column). The top five identities with the most face images in LFW are evaluated (the other identities have similar results). Neural orders are sorted by the mean neural activations on the evaluated identity for figures in all the three columns. For each given identity there are neurons strongly excited (e.g., those with neural ID smaller than 200) or inhibited (e.g., those with neural ID larger than 600). For the excited neurons, their activations are distributed in higher values, while other images have significantly lower mean values on these neurons. Therefore, the excitatory neurons can easily distinguish an identity from others, which is verified by their high classification accuracies shown by the red dots with small neural IDs in figures in the right column.</p><p>For neurons ranked in the middle (e.g., those with neural ID around 400), their activation distributions on the given identity are largely overlapped with those on other identities. They have weak discrimination abilities for the given identity, verified by the low accuracies of the red and blue dots near the junction of the two colors. The excitation or inhibition state of these neurons has much uncertainty.</p><p>When mean activations further decrease (e.g., neural ID above 600), the neurons demonstrate inhibitory properties, and are seldom activated for the given identity compared to others. These inhibitory neurons also have discrimination abilities with relatively high classification accuracies.</p><p>However, similar phenomena cannot be found on LBP features as shown in <ref type="figure" target="#fig_0">Fig. 11b</ref>. The range of LBP features on given identities and the remaining images are overlapped for all features. Compared to DeepID2+ neural activations, LBP features have much lower classification accuracies, the majority of which are accumulated on the 50% random guess line <ref type="figure" target="#fig_0">Fig. 12a</ref> compares the range of neural activations on faces containing a particular attribute (left column) and the remaining images (middle column), as well as showing the per-neuron classification accuracies of distinguishing each attribute from the remaining images (right column). Similar to identities, neurons of lower and higher ranks exhibit selectiveness to attributes as shown in this figure, including Male, White, Black, Asian, Child, Senior, Bald, and Gray Hair. These attributes are discriminative to identities. The selectiveness is relatively weak to other attributes such as Indian, Youth, Middle Aged, Black Hair, Blond Hair, and Brown Hair (not shown). These attributes are either visually ambiguous or less discriminative to identities. For example, Indian people sometimes look like Asian, and often times we see the same identity photographed at both youth and middle aged, or photographed in different hair colors. <ref type="figure" target="#fig_0">Fig. 12b</ref> compares the range of LBP features and per-feature classification accuracies for the same set of attributes as in <ref type="figure" target="#fig_0">Fig. 12a</ref>. The range of LBP features on given attributes and the remaining images are overlapped for all features, and the classification accuracies are accumulated on the 50% random guess line. <ref type="figure" target="#fig_0">Fig. 13 and Fig. 14</ref> show examples of the histograms of neural activations over given identities or attributes. <ref type="figure" target="#fig_0">Fig. 13</ref> first row also shows the histograms over all images of five randomly selected neurons. For each neuron, approximately half of its activations are zero (or close to zero) and another half have larger values. In contrast, the histograms over given identities exhibit strong selectiveness. Some neurons are constantly activated for a given identity, with activation histograms distributed in positive values, as shown in the first row of histograms of each identity in <ref type="figure" target="#fig_0">Fig. 13</ref>, while some others are constantly inhibited, with activation histograms accumulated at zero or small values, as shown in the second row of histograms of each identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Neural activation distribution</head><p>For attributes, in each column of <ref type="figure" target="#fig_0">Fig. 14a, 14b, 14c</ref>, and 14d, we show histograms of a single neuron over a few related attributes, i.e., those related to sex, race, age, and hair, respectively. The neurons are selected to be excitatory (in red frames) or inhibitory (in green frames) and can best classify the attribute shown in the left of each row. As shown in these figures, neurons exhibit strong selectiveness (either activated or inhibited) to certain attributes, in which the neurons are activated (inhibited) for the given attribute while inhibited (activated) for the other attributes in the same category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Robustness of DeepID2+ features</head><p>We test the robustness of DeepID2+ features on face images with occlusions. In the first setting, faces are partially occluded by 10% to 70% areas, as shown in <ref type="figure" target="#fig_0">Fig.  15</ref> first row. In the second setting, faces are occluded by random blocks of 10 × 10 to 70 × 70 pixels in size, as shown in <ref type="figure" target="#fig_0">Fig. 15</ref> second row. In the occlusion experiments, DeepID2+ nets and Joint Bayesian models are learned on the original face images in our training set without any artificially added occlusions, while the occluded faces are only used for test. We also test the high-dimensional LBP features plus Joint Bayesian models <ref type="bibr" target="#b6">[7]</ref> for comparison. <ref type="figure" target="#fig_0">Fig. 16</ref> compares the face verification accuracies of DeepID2+ and LBP features on LFW test set with varying degrees of partial occlusion. The DeepID2+ features are taken from the FC-1 to FC-4 layers with increasing depth in a single DeepID2+ net taking the entire face region as input. We also evaluate our entire face recognition system with 25 DeepID2+ nets. The high-dimensional LBP features compared are 99, 120 dimensions extracted from 21 facial landmarks. As shown in <ref type="figure" target="#fig_0">Fig. 16</ref>, the performance of LBP drops dramatically, even with slight 10% -20% occlusions. In contrast, for the DeepID2+ features with two convolutions and above (FC-2, FC-3, and FC-4), the performance degrades slowly in a large range. Face verification accuracies of DeepID2+ are still above 90% when 40% of the faces are occluded (except FC-1 layer), while the performance of LBP features has dropped below 70%. The performance of DeepID2+ only degrades quickly with over 50% occlusions, when the critical eye regions are occluded. It also shows that features in higher     activation histograms over all face images of five randomly selected neurons, with neural ID labeled above each histogram. Second to the last row: activation histograms over the first five people with the most face images in LFW. For each person, histograms of five excitatory neurons (even rows) and five inhibitory neurons (odd rows except the first row) with the highest binary classification accuracies of distinguishing the given identity and the remaining images are shown. People names are given in the left of every two rows. Neural ID and classification accuracies are shown above each histogram.</p><p>1 are local features, sensitive to occlusions. Combining DeepID2+ nets extracted from 25 face regions achieves the most robustness with 93.9% face verification accuracy with 40% occlusions and 88.2% accuracy even only showing the forehead and hairs.</p><p>We also evaluate face verification of DeepID2+ and LBP features over face images with random block occlusions, with n × n block size for n = 10 to 70, respectively. This setting is challenging since the positions of the occluded regions in two faces to be verified are generally different. Therefore images of the same person would look much    <ref type="figure" target="#fig_0">Fig. 16</ref>. different in the sense of pixel distances. <ref type="figure" target="#fig_0">Fig. 17</ref> shows the comparison results, the accuracies of LBP features begin to drop quickly when block sizes are greater than 20 × 20, while DeepID2+ features (except FC-1) maintain the performance in a large range. With 50 × 50 block occlusions, the performance of LBP features has dropped to approximately 70%, while the FC-4 layer of a single DeepID2+ net still has 89.2% accuracy, and the combi- nation of 25 DeepID2+ nets has an even higher 92.4% accuracy. Again, the behavior of features in the shallow FC-1 layer are closer to LBP features. The above experiments show that it is the deep structure that makes the neurons more robust to image corruptions. Such robustness is inherent in deep ConvNets without explicit modelings. <ref type="figure" target="#fig_0">Fig. 18 and 19</ref> show the mean activations of FC-4 layer neurons over images of a single identity with various degrees of partial and random block occlusions, respectively. The neurons are ordered according to their mean activations on the original images of each identity. For both types of occlusions, activation patterns keep largely unchanged until a large degree of occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper designs a high-performance DeepID2+ net which sets new sate-of-the-art on LFW and YouTube Faces for both face identification and verification. Through empirical studies, it is found that the face representations learned by DeepID2+ are moderately sparse, highly selective to <ref type="figure" target="#fig_0">Figure 19</ref>: Mean neural activations over images with random block occlusions (shown in <ref type="figure" target="#fig_0">Fig. 15 second row)</ref>. <ref type="figure">Figure description</ref> is the same as <ref type="figure" target="#fig_0">Fig. 18</ref>.</p><p>identities and attributes, and robust to image corruption. In the past, many research works have been done aiming to achieve such attractive properties by explicitly adding components or regularizations to their models or systems. However, they can be naturally achieved by the deep model through large scale training. This work not only significantly advances the face recognition performance, but also provides valuable insight to help people to understand deep learning and its connection with many existing computer vision researches such as sparse representation, attribute learning and occlusion handling. Such insights may inspire more exciting research in the future. As an example, in this work, we have shown that binary neural activation patterns are highly efficient and effective for face recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: neural responses of DeepID2+ on images of Bush and Powell. The second face is partially occluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>DeepID2+ net and supervisory signals. Conv-n deneotes the n-th convolutional layer (with max-pooling). FC-n denotes the n-th fully connected layer. Id and Ve denote the identification and verification supervisory signals. Blue arrows denote forward-propagation. Yellow arrows denote supervisory signals. Nets in the left and right are the same DeepID2+ net with different input faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of DeepID2+ net and those with no layer-wise supervision, less training data, and fewer feature maps, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of face verification accuracies on LFW with ConvNets trained on 25 face regions given in DeepID2<ref type="bibr" target="#b27">[28]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>ROC of face verification on LFW. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>ROC of face verification on YouTube Faces. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Left: the histogram of the number of activated neurons for each of the validating images. Right: the histogram of the number of images on which each neuron is activated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Accuracy comparison between DeepID2+ and LBP features for attribute classification on LFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Identity classification accuracy on LFW with one single DeepID2+ or LBP feature. Initials of identity names are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Attribute classification accuracy on LFW with one single DeepID2+ or LBP feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) DeepID2+ neural activation distributions and per-neuron classification accuracies. (b) LBP feature activation distributions and per-feature classification accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of distributions of DeepID2+ neural and LBP feature activations and per-neuron (feature) classification accuracies for the first five people with the most face images in LFW. Left column: mean and standard deviations of neural (feature) activations on images belonging to a single identity. Mean is represented by a red line while standard deviations are represented by vertical segments between (mean − standard deviation) and (mean + standard deviation). Neurons (features) are sorted by their mean activations on the given identity. Middle column: mean and standard deviations of neural (feature) activations on the remaining images. Neural (feature) orders are the same as those in the left column. Right column: per-neuron (feature) classification accuracies on the given identity. Neural (feature) orders are the same as those in the left and middle columns. Neurons (features) activated and inhibited for a given identity are marked as red and blue dots, respectively. layers (which are supposed to be more globally distributed) are more robust to occlusions, while both LBP and FC-(a) Histogram of neural activations over sex-related attributes (Male and Female). (b) Histogram of neural activations over race-related attributes, i.e., White, Black, Asian, and Indian. (c) Histogram of neural activations over age-related attributes, i.e., Baby, Child, Youth, Middle Aged, and Senior. (d) Histogram of neural activations over hair-related attributes, i.e., Bald, Black Hair, Gray Hair, Blond Hair, and Brown Hair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Histogram of neural activations over attributes. Each column of each subfigure shows histograms of a single neuron over each of the attributes given in the left, respectively. Histograms of excitatory and inhibitory neurons which best distinguish each attribute from the remaining images are shown, and are framed in red and green, respectively, with neural ID and classification accuracies shown above each histogram. The other histograms are framed in black with only neural ID above.(a) DeepID2+ neural activation distributions and per-neuron classification accuracies. (b) LBP feature activation distributions and per-feature classification accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Comparison of distributions of DeepID2+ neural and LBP feature activations and per-neuron (feature) classification accuracies of face images of particular attributes in LFW. Figure description is the same as Fig. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Histogram of neural activations. First row:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>The occluded images tested in our experiments. First row: faces with 10% to 70% areas occluded, respectively. Second row: faces with 10 × 10 to 70 × 70 random block occlusions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Face verification accuracies of DeepID2+ and high-dimensional LBP on LFW with partial occlusions. The red, green, blue, and magenta curves evaluate the features of a single DeepID2+ net, extracted from various network depth (from FC-4 to FC-1 layer). We also evaluate the combination of 25 DeepID2+ net FC-4 layer features, shown by the black curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Face verification accuracies of DeepID2+ and high-dimensional LBP on LFW with random block occlusions. Curve description is the same as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Mean neural activations over partially occluded face images (shown in Fig. 15 first row). Each column shows the mean activations over face images of a single identity given in the top of each column, with various degrees of occlusions given in the left of each row. Neurons in figures in each column are sorted by their mean activations on the original images of each identity. Activation values are mapped to a color map with warm colors indicating positive values and cool colors indicating zero or small values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Face verification on LFW.</figDesc><table><row><cell>method</cell><cell>accuracy (%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>± 0.13 DeepID2+ 99.47 ± 0.12 FC-4 layer features extracted from all the 25 face regions and their horizontally flipped counterparts with the 25 DeepID2+ nets, respectively, we achieve 99.47% and 93.2% face verification accuracies on LFW and YouTube Faces datasets, respectively. Tab. 1 and Tab. 2 are accuracy comparisons with the previous best results on the two datasets. Fig. 5 and Fig. 6 are the ROC comparisons. Our DeepID2+ nets outperform all previous results on both datasets. There are a few wrongly labeled test face pairs in LFW and YouTubeFaces. After correction, our face verification accuracy increases to 99.52% on LFW and 93.8% on YouTubeFaces.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Face verification on YouTube Faces.</figDesc><table><row><cell>method</cell><cell>accuracy (%)</cell></row><row><cell>LM3L [13]</cell><cell>81.3 ± 1.2</cell></row><row><cell>DDML (LBP)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Closed-and open-set identification tasks on LFW.</figDesc><table><row><cell>method</cell><cell>Rank-1 (%)</cell><cell>DIR @ 1%</cell></row><row><cell></cell><cell></cell><cell>FAR (%)</cell></row><row><cell>COTS-s1 [3]</cell><cell>56.7</cell><cell>25</cell></row><row><cell>COTS-s1+s4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the original DeepID2+ features and its binarized representation for face verification on LFW. The first two rows of results are accuracies of the original (real-valued) FC-4 layer representation of a single net (real single) and of the 25 nets (real comb.), respectively, with Joint Bayesian as the similarity metrics. The last two rows of results are accuracies of the corresponding binary representations, with Joint Bayesian or Hamming distance as the similarity metrics, respectively.</figDesc><table><row><cell></cell><cell>Joint Bayesian</cell><cell>Hamming dis-</cell></row><row><cell></cell><cell>(%)</cell><cell>tance (%)</cell></row><row><cell>real single</cell><cell>98.70</cell><cell>N/A</cell></row><row><cell>real comb.</cell><cell>99.47</cell><cell>N/A</cell></row><row><cell>binary single</cell><cell>97.67</cell><cell>96.45</cell></row><row><cell>binary comb.</cell><cell>99.12</cell><cell>97.47</cell></row><row><cell cols="3">6. Selectiveness on identities and attributes</cell></row><row><cell cols="3">6.1. Discriminative power of neurons</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tom-vs-Pete classifiers and identity-preserving alignment for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno>TR MSU-CSE-14-1</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing people: Poselet-based attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A practical transfer learning algorithm for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep attribute networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust classification using structured sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is that you? Metric learning approaches for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large margin multimetric learning for face and kinship verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Large scale strongly supervised ensemble metric learning, with applications to face verification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno>TR115</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">NEC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled Faces in the Wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust and practical face recognition via structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Eigen-pep for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparse representation using nonnegative curds and whey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Surpassing human-level face verification performance on LFW with GaussianFace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3840</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A deep sum-product architecture for robust facial attributes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4773</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple one-shots for utilizing class label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Webscale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5266</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust boltzmann machines for recognition and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural mechanisms for face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Livingstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="411" to="438" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gabor feature based sparse representation for face recognition with gabor occlusion dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An associate-predict model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: Which helps face recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
