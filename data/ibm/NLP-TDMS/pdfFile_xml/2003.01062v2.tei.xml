<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bala</forename><forename type="middle">Murali</forename><surname>Manoghar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><forename type="middle">Sashank</forename><surname>Dorbala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProxEmo: Gait-based Emotion Learning and Multi-view Proxemic Fusion for Socially-Aware Robot Navigation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Supplemental version including Code, Video, Datasets at https://gamma.umd.edu/proxemo/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotionguided navigation taking into account social and proxemic constraints. To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent advances in AI and robotics technology are gradually enabling humans and robots to coexist and share spaces in different environments. This is especially common in places such as hospitals, airports, and shopping malls. Navigating a robot with collision-free and socially-acceptable paths in such scenarios poses several challenges <ref type="bibr" target="#b0">[1]</ref>. For example, in the case of a crowded shopping mall, the robot needs to be aware of the intentions of an oblivious shopper coming towards it for friendly navigation. Knowing the perceived emotional state of a human in such scenarios allows the robot to make more informed decisions and navigate in a socially-aware manner.</p><p>Understanding human emotion has been a well-studied subject in several areas of literature, including psychology, human-robot interaction, etc. There have been several works that try to determine the emotion of a person from verbal (speech, text, and tone of voice) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and non-verbal (facial expressions, walking styles, postures) <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> cues. There also exist multi-modal approaches that use a combination of these cues to determine the person's emotion <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>.</p><p>In our work, we focus on emotionally-aware robot navigation in crowded scenarios. Here, verbal cues for emotion classification are not easily attainable. With non-verbal cues, facial expressions that are often occluded from the egocentric view of the robot and might not be fully visible. Besides, emotion analysis from facial features is a topic of debate in several previous works: these features are inherently unreliable caused by vague expressions emerging from a variety of psychological and environmental factors <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. As such, in our work, we focus on using "walking styles" or "gaits" to extract the emotions of people in crowds.</p><p>Obtaining perceived emotions from gaits is a challenging problem that has been well documented in the past. More recently, various machine learning solutions <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> have been proposed to tackle this problem. However, these approaches suffer from the following drawbacks: We present a gait-based emotion and proxemics learning algorithm to perform socially-aware robot navigation. The red arrow indicates the path of the robot without social awareness. The green arrow indicates the new path after an angry emotion is detected. Observe the significant shift away from the pedestrian when an angry gait is detected. This form of navigation is especially useful when the robot is expected to navigate safely through crowds without causing discomfort to nearby pedestrians.</p><p>• The training datasets used are singular in direction, i.e., there is motion capture only when a person is walking in a straight line towards the camera. This is a significant disadvantage for our task of socially-aware crowd navigation, where the robot often encounters people walking from several directions towards or away from the camera. • Some approaches that are tailored towards using emotion for enhancing the task of robot navigation assume a static overhead camera that captures the trajectories of pedestrians. This is not ideal, as the overhead camera might not always be available in all scenarios. To overcome these challenges, we propose ProxEmo, a novel algorithm for realtime gait-based emotion classification for socially-guided navigation. ProxEmo is tailored towards working with commodity RGB egocentric cameras that can be retrofitted onto moving platforms or robots for navigating We first capture an RGB video from an onboard camera and extract pedestrian poses and track them at each frame. These tracked poses over a predefined time period are embedded into an image, which is then passed into our ProxEmo model for classifying emotions into four classes. The obtained emotions then undergo proxemic fusion with the LIDAR data and are finally passed into the navigation stack. among pedestrians. The major contributions of our work can be summarized as follows:</p><p>• We introduce a novel approach using group convolutions to classify pedestrian emotions from gaits, which drastically improves accuracy compared to SOTA. • Our method explicitly takes into consideration pedestrian behavior in crowds as we train our model on skeletal data of people approaching the robot from multiple directions, as opposed to approaching from a single view from the front. • We present a new navigation scheme using Proxemic Fusion that accounts for pedestrian emotions. • Finally, we introduce a Variational Comfort Space, which integrates into our navigation scheme, taking into account varying pedestrian orientations. We note that identifying the true nature of a person's emotion via only a visual medium can be difficult. Therefore in this work, we focus only on the perceived emotions from the point of an external observer as opposed to actual internal emotion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we present a brief overview of socialrobot navigation algorithms. We also review related work on emotion modeling and classification from visual cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Social Robotics and Emotionally-Guided Navigation</head><p>As robots have become more commonplace, their impact on humans' social lives has emerged as an active area of research. Studies from multiple domains <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> have tried to quantify this impact in several ways. In <ref type="bibr" target="#b0">[1]</ref>, Kruse et al. present a comprehensive survey on navigation schemes for robots in social scenarios. They describe various social norms (interpersonal distances, human comfort, sociability) that the robot must consider not to cause discomfort to people around it. Michaid et al. <ref type="bibr" target="#b16">[17]</ref> discuss about how robots can attain artificial emotions for social interactions. Several classical <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref> and deep learning <ref type="bibr" target="#b20">[21]</ref> approaches tackle the problem of navigation through highly dynamic environments. More recently, reinforcement learning methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> have been described for collision avoidance in such environments. For pedestrian handling, in particular, Randhavane et al. <ref type="bibr" target="#b23">[24]</ref> make use of a pedestrian dominance model (PDM) to identify the dominance level of humans and plan a trajectory accordingly. In <ref type="bibr" target="#b24">[25]</ref>, Rios-Martinez et al. present a detailed survey on the proxemics involved with socially aware navigation. In <ref type="bibr" target="#b25">[26]</ref>, Kitazawa et al. discuss ideas such as Information Process Space of a human. In <ref type="bibr" target="#b26">[27]</ref>, Pandey et al. discuss a strategy to plan a socially aware path using milestones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Emotion Modeling and Classification</head><p>There exists a substantial amount of research that focuses on identifying the emotions of humans based on body posture, movement, and other non-verbal cues. Ruiz-Garcia et al. <ref type="bibr" target="#b27">[28]</ref> and Tarnowski et al. <ref type="bibr" target="#b28">[29]</ref>, use deep learning to classify different categories of emotion from facial expressions. The approach by <ref type="bibr" target="#b7">[8]</ref> uses multiple modalities such as facial cues, human pose and scene understanding. Randhavane et al. <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> classify emotions into four classes based on affective features obtained from 3D skeletal poses extracted from human gait cycles. Their algorithm, however, requires a large number of 3D skeletal key-points to detect emotions and is limited to single individual cases. Bera et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> classify emotions based on facial expressions along with a pedestrian trajectory obtained from overhead cameras. Although this technique achieves good accuracy in predicting emotions from trajectories and facial expressions, it explicitly requires overhead cameras in its pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Recognition for Emotions</head><p>The task of action recognition involves identifying human actions from sequences of data (usually videos) <ref type="bibr" target="#b33">[34]</ref>. A common task in many of these models is recognizing gaitbased actions such as walking and running. Thus, the task of gait action recognition is closely related to the task of emotion recognition from gaits, as both perform classification on the same input. Bhattacharya et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref> use graph convolutions for the emotion recognition task, in a method similar to the action recognition model used in Yan et al. <ref type="bibr" target="#b35">[36]</ref>. Ji et al. <ref type="bibr" target="#b36">[37]</ref> propose a CNN-based method that gives state-of-the-art results on gait based action recognition tasks. Their model is invariant to viewpoint changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW AND METHODOLOGY</head><p>We propose a novel approach, ProxEmo, for classifying emotions from gaits that works with an egocentric camera setup. Our method uses 3D poses of human gaits obtained from an onboard robot camera to classify perceived emotions. These perceived emotions are then used to compute variable proxemic constraints in order to perform socially aware navigation through a pedestrian environment. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates how we incorporate ProxEmo into an end-to-end emotionally-guided navigation pipeline.</p><p>The following subsections will describe our approach in detail. We first discuss the dataset and the augmentation details we used for training. Then, we briefly discuss our pose estimation model, followed by a detailed discussion of our emotion classification model, ProxEmo. Finally, we describe how socially-aware navigation can be performed using the obtained emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>In our formulation, we represent the human with 16 joints as shown in <ref type="figure" target="#fig_3">figure 4</ref>. Thus, a pose P ∈ R 16×3 of a human is a set of 3D positions of each joint j i , where i ∈ {0, 1, ..., 15}. For any RGB video V , we represent the gait extracted using 3D pose estimation as G. The gait G is a set of 3D poses P 1 , P 2 , ..., P τ where τ is the number of frames in the input video V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Preparation</head><p>We make use of two labeled datasets by Randhavane et al. <ref type="bibr" target="#b37">[38]</ref> and Bhattacharya et al. <ref type="bibr" target="#b11">[12]</ref>, containing time-series 3D joints of 342 and 1835 gait cycles each (a total of 2177 gait samples). Each gait cycle has 75 timesteps with 16 joints as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Thus, each sample in this new dataset has a dimension of joints × time× dimensions = 16 * 75 * 3 = 3600. These samples are labeled into 4 emotion classes: angry, sad, happy, and neutral with 10 labelers per video (to capture the perceptual difference between different labelers). In order to train our network for prediction from multiple views, we augment the dataset as follows. First, we consider a camera placed at a particular distance from the human, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Then for different camera positions oriented towards the human, we perform augmentation by applying transformations given in equation 1.</p><formula xml:id="formula_0">j aug =   cosθ 0 −sinθ 0 1 0 sinθ 0 cosθ   ×   x y z   +   T x T y T z   (1)</formula><p>where j aug are the coordinates of the augmented joints, T x , T y , T z are the translation vectors, and θ is the rotation along Y axis. For our experiments, we attain 72 × 4 = 288 augmentations for each sample by considering θ at gradients of 5 • , with 4 translations of [1m−4m] along the Z axis (T z ). Thus, after augmentation, we have a total of 288 × 2177 = 626, 976 gaits in our dataset. By applying specific translations and rotations, we augment the data into different camera views. We divide the viewpoints into four view-groups based on the angle of approach to categorize the direction in which the person is walking. The augmentations also take into consideration varying distances of the camera from the origin point of the gait sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Human-Pose Estimation</head><p>A pose estimation strategy for humans walking in a crowded real-world scenario has to be robust to noise coming from human attire or any items they might be carrying. To account for this, we employ a robust approach described in <ref type="bibr" target="#b38">[39]</ref> for this task. Their paper describes a two-step network trained in a weakly supervised fashion. First, a Structure-Aware PoseNet (SAP-Net) trained on spatial information provides an initial estimate of joint locations of people in video frames. Later, a Temporal PoseNet (TP-Net) trained on time-series information corrects this initial estimate by adjusting illegal joint angles and joint distances. The final output is a sequence of well-aligned 3D skeletal poses P. <ref type="figure" target="#fig_3">Figure 4</ref> is a representation of the skeletal output obtained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Generating Image Embeddings</head><p>We observe that 2D convolutions are much faster and efficient as opposed to graph convolutions <ref type="bibr" target="#b36">[37]</ref>. Hence, we embed the spatial-temporal skeletal gait sequence G as an image I, using the equations described in 2.</p><formula xml:id="formula_1">I = {R (x,y) = Z (t,j) ; G (x,y) = Y (t,j) ; B (x,y) = X (t,j) }<label>(2)</label></formula><p>Here, R, B, and G are image color channels, x, y are the co-ordinates of image, and X (t,j) , Y (t,j) , Z (t,j) are the co- ordinates of skeletal joint j at time t.This image I is finally upscaled to 244 × 244 × 3 for training our ProxEmo model. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the architecture of our model for emotion classification. The image embedding I obtained from the gaits are passed through two stages of group convolutions to obtain an emotion label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ProxEmo: Classifying Emotions from Gaits</head><p>1) Group Convolutions: We take inspiration from <ref type="bibr" target="#b39">[40]</ref> and make use of group convolutional layers in designing our ProxEmo architecture. Group Convolution Layers (GC), in essence, operate just like 2D convolution layers, except that they fragment the input into n g groups and perform convolution operations individually on them before stacking the outputs together. The advantage of doing this is that the network learns from different parts of the input in isolation. This is especially useful in our case because we have a dataset that varies based on two factors, view-group and emotion labels. The variation in the view-groups is learned by the different convolution groups GC, and the emotions are learned by the convolutions taking place within each group. Group convolutions increase the number of channels in each layer by n g times. The output (h i ) of each group in the convolution layer is h i = x i * k i and h out = [h 1 |...|h ng ]. where, h out is the output of the group convolution, x i represents the input, and k i represents the kernel for convolution. The output [h 1 |...|h ng ] is a matrix concatenation of all the group outputs along channel axis. In our case, we choose n g as 4 because we have 4 view-groups.</p><p>2) ProxEmo Architecture: The network consists of seven convolution layers. The initial layer is a traditional 2D convolution layer, which performs channel up-sampling for the forthcoming group convolution operations. These operations take place in two stages -Stage 1: This consists of two GC layers, each having 128 convolution filters (32 per group × n g ). Stage 2: This consists of two convolution GC layers, however, unlike stage 1, each GC 256 convolution filters (64 per view-group × n g ). Both traditional 2D convolution and GC layers are passed through a ReLU non-linear activation and max pooling layer. The outputs from Stage 1 and Stage 2 are represented by h s where s = 1, 2. We also perform batch normalization.</p><p>The output of each both the group convolution stages, h s are given by,</p><formula xml:id="formula_2">p * s = GC(x s , k 1 s ) p s = M axP ool(ReLU (p * s )) h * s = GC(p s , k 2 s ) h s = BatchN orm(M axP ool(ReLU (h * s )))<label>(3)</label></formula><p>where, s represents the two group convolution stages as described before, x s is the input to the group convolution stage 's', k 1 s and k 2 s represent convolution kernels for first and second GC layers within a stage, p * s and h * s are the first and second GC layer outputs determined using equation above.</p><p>After performing the group convolutions, the output h 2 is passed through two 2D convolution layers. These convolution layers help in gathering the features learned by the GC layers to finally predict both the view-group and emotion of the gait sequences.</p><p>Rather than using fully-connected layers for predicting the view-group, our method utilizes convolution layers to predict the n k × n g output, where n k is the number of emotions and n g is the number of view-groups. This makes our model considerably lighter (number of model parameters) and faster (run-time performance), compared to other state-of-the-art algorithms.</p><p>The final output of the classifier consists of multi-class sof tmax prediction, E i,j , given by the equation 4. Here e i,j refers to the final hidden layer output of the network, where i = 0, 1, . . . (n k − 1) is the emotion class and j = 0, 1, . . . , (n g − 1) is the view-group class.</p><formula xml:id="formula_3">E i,j = exp (e i,j ) n k −1 i=0 ng−1 j=0 exp (e i,j )<label>(4)</label></formula><p>E i,j can be considered as a 4 × 4 matrix containing 16 values corresponding to different view-groups and emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Emotion-guided Navigation using Proxemic Fusion</head><p>We use the emotions E i,j predicted from ProxEmo to compute the comfort space (c) of a pedestrian, which is the socially comfortable distance (in cm) around a person.</p><p>We combine c along with the LIDAR data (L) to perform "proxemic fusion" (III-F.3), obtaining a set of points where it is permissible for the robot to navigate in a sociallyacceptable manner. This is illustrated in figure 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comfort Space Computation:</head><p>In order to model the predicted emotions E i,j from ProxEmo into a comfort space distance c, we use the following equation:</p><formula xml:id="formula_4">c = 4 j=1 c j · max (E j ) 4 j=1 E j · v g<label>(5)</label></formula><p>Here, E j represents a column vector of the softmax output, which corresponds to the group outcomes for each individual emotion. c j is a constant derived from psychological experiments described in <ref type="bibr" target="#b40">[41]</ref> to compute the limits on an individual's comfort spaces and is chosen from a set {90.04, 112.71, 99.75, 92.03} corresponding to the comfort spaces (radius in cm) for {happy, sad, angry, neutral} respectively. We acknowledge that these distances depend on many factors, including cultural differences, environment, or a pedestrian's personality, and restrict our claims to variations in comfort spaces due to the emotional difference. These distances are based on how comfortable pedestrians are while interacting with others. v g is a view-group constant defined in the following subsection. We consider a varying comfort space c around a person based on their position (defined by the view-group g) in front of the robot. In scenario 1, the pedestrian approaches the robot from the front. Here, as the pedestrian is aware of the robot's presence, it needs to be more respectful of the proxemic comfort space and take action V comf ort represented by the green arrow. In scenario 2, the robot is approaching the person from behind. An unaware pedestrian need not be disturbed by the robot, due to which it can be more liberal with its actions. The violet arrow representing the safe action V saf e coincides with V comf ort in this case.</p><p>2) Variational Comfort Space (v g ): We take inspiration from the Information Process Space defined by Kitazawa et al. <ref type="bibr" target="#b25">[26]</ref> to define our own Variational Comfort Space constant v g . This constant acts as a scaling factor in the comfort space based on the orientation of the pedestrian in the robot's view. This orientation is easily obtainable as ProxEmo also gives us a view-group output along with the emotion. v g is chosen from a set of {1, 0.5, 0, 0.5} based on the view group g predicted. This is chosen based on the fact that people have varying personal space with respect to their walking direction, i.e., a pedestrian will care more about his/her personal space in front as compared to the sides. Also, the pedestrian might not care about personal and comfort space behind them since it does not lie in their field of view <ref type="bibr" target="#b41">[42]</ref>. In <ref type="figure" target="#fig_5">figure 6</ref>, we look at two scenarios to illustrate how the robot handles pedestrians considering variational comfort spaces:</p><p>• Scenario 1: The robot is positioned in front of the person walking towards it. This is classified as viewgroup 1, having a v g value of 1. As the robot is visible to the person, in this case, it should be more precautious in safely maneuvering around the person. The comfort space around the pedestrian is larger in this case, and the robot takes a more skewed trajectory. • Scenario 2: The robot is approaching the pedestrian from behind. This gait is classified as view-group 3 and has a v g value of 0. As the robot is not in the person's field of vision, in this case, it can afford to safely pass around the fixed space F s of the person. At any time instant, the velocity of the robot will be directed towards the goal, and if there is an obstacle, it will lead to a collision v coll . If an obstacle avoidance algorithm is used, the navigation scheme avoids it with an action v saf e . However, for socially acceptable proximally-aware navigation, this is not sufficient, as this requires the robot to follow certain social norms. In order to adhere to these social norms, we incorporate the emotions predicted by ProxEmo to navigate in a socially acceptable manner represented by V comf ort .</p><p>3) Proxemic Fusion: We fuse the LIDAR data to include proxemic constraints by performing a Minkowski sum (M ) of the set of LIDAR points L and a set containing the points in a circle Z defined by a radius r. The Minkowski sum M provides us with a set of all the admissible points where the robot can perform emotionally-guided navigation. This is formulated using the following equations.</p><formula xml:id="formula_5">L = {a | a − a 0 = d lidar } (6) Z = {b | dist(a, b) ≤ r} M = L + Z = {a + b | a ∈ L, b ∈ Z}</formula><p>Here, a 0 is a reference point on the LIDAR, and d lidar is the distance measurement (in metres). r is the inflation radius and is defined using the comfort space c as:</p><formula xml:id="formula_6">r = c − [max(dh) − min(dh)]<label>(7)</label></formula><p>where dh ∈ L is a set of the LIDAR distances only for points where a human was detected. The maximum value of dh corresponds to the farthest distance from the person from their fixed inner space F s , while the minimum value of dh corresponds to the closest distance of the person from this space. F s is represented by the blue circle around the person in the figure 6. In terms of mathematical morphology, the outcome of proxemic fusion is similar the dilation operation of the human, modelled as a obstacle, with the comfort space. <ref type="figure">Fig. 7</ref>: Emotionally-Guided Navigation: We use the emotions detected by ProxEmo along with the LIDAR data to perform Proxemic Fusion. This gives us a comfort distance c around a pedestrian for emotionally-guided navigation. The green arrows represent the path after accounting for c while the violet arrows indicate the path without considering this distance. Observe the significant change in the path taken in the sad case. Note that the overhead image is representational, and ProxEmo works entirely from a egocentric camera on a robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Metrics</head><p>We evaluate our model using two metrics:</p><p>• Mean Accuracy (%) -1 n k ×ng n k i=0 ng j=0 T Pi,j Ni,j • Mean F1 score -2 n k ×ng n k i=0 ng j=0 P ri,j * Rci,j P ri,j +Rci,j where, n k (= 4) is the number of emotion classes, n g (= 4) is the number of view-groups, T P i,j is the number of true predictions for i th emotion class and j th view-group, N i,j is the total number of data samples for i th emotion class and j th view-group, P r i,j and Rc i,j is the precision and recall for i th emotion class and j th view-group. All the metrics mentioned are derived from a confusion matrix generated by comparing actual vs predicted emotion and view-group for the data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>For training, our dataset (III-B) has a train-validation split of 90%-10%. We generate a set of angles and translations that are different from the original dataset to formulate the test set.</p><p>We perform training using an ADAM <ref type="bibr" target="#b42">[43]</ref> optimizer, with decay parameters of (β 1 = 0.9 and β 2 = 0.999). The experiments were run with a learning rate of 0.009 and with 10% decay every 250 epochs. The models were trained with softmax multi-class cross-entropy loss, L, represented in equation 8. The training was done on 2 Nvidia RTX 2080 Ti GPUs having 11GB of GPU memory each and 64 GB of RAM.</p><formula xml:id="formula_7">L = 1 m M m=1 n k ,ng i=0,j=0 −y m,i,j log(E m,i,j )<label>(8)</label></formula><p>where, y m,i,j is the target one-hot encoded label representing emotion class i{= 0, 1, ..n k } and view-group j{= 0, 1, ...n g } for the data sample m{= 0, 1, .., M }. E m,i,j is the predicted sof tmax output probability for data sample m being emotion class i and view-group class j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing ProxEmo with other Emotion Classifiers</head><p>We evaluate the performance of our ProxEmo network, against two other emotion classification algorithms <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b37">[38]</ref>. Since the other emotion classification algorithms don't consider the arbitrary view scenario, we compare our results with just single-view data, i.e., skeletal gaits that are directly approaching the RGB-D camera. <ref type="table" target="#tab_0">Table I</ref> presents these results. The accuracy metrics reported are generated by modifying the equations in IV-A, for a single view-group (i.e., n g = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy (%) Venture et al. <ref type="bibr" target="#b43">[44]</ref> 30.83 Daoudi et al <ref type="bibr" target="#b44">[45]</ref> 42.5 Li et al. <ref type="bibr" target="#b45">[46]</ref> 53.7 Baseline (Vanilla LSTM) <ref type="bibr" target="#b37">[38]</ref> 55.47 Crenn et al <ref type="bibr" target="#b46">[47]</ref> 66.2 STEP <ref type="bibr" target="#b11">[12]</ref> 78.24 ProxEmo (ours) 82.4 We compare the accuracy (%) of our ProxEmo network with existing emotion classification algorithms on single-view (facing the camera) data samples. We observe that our network outperforms the current state-of-theart algorithm by 4%. Furthermore, our network outperforms the state-of-the-art algorithm across each emotion class. The accuracy numbers reported for <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b11">[12]</ref> and ProxEmo are evaluated on the same dataset discussed in section III-B. The other methods are evaluated on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparing ProxEmo with Action Recognition Models</head><p>As mentioned in section II-C, action recognition models and emotion recognition models that have inputs as gaits are closely related tasks. Thus, we can evaluate ProxEmo on pre-existing action recognition models by fine-tuning them on the emotion recognition task. We compare our model with two existing state-of-the-art action recognition models, (i) Spatial-Temporal Graph convolution networks (ST-GCN) <ref type="bibr" target="#b35">[36]</ref>, and (ii) VS-CNN <ref type="bibr" target="#b36">[37]</ref>. These architectures were trained using the datasets <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref> (discussed in Section III-B).</p><p>1) ST-GCN: The spatial-temporal graph convolution networks <ref type="bibr" target="#b35">[36]</ref> perform skeletal action recognition using undirected spatial-temporal graphs for hierarchical representation of skeleton gait sequences. In the original implementation, the spatial-temporal graphs are used in a graph convolution network to detect the action performed through the sequence.</p><p>We fine-tune ST-GCN to predict human emotion instead of the actions. The human emotions modeled as a class label for the implementation. Here we present the performance metrics (discussed in section IV-A) of our ProxEmo network compared to the state-of-the-art arbitrary view action recognition models. We perform a comprehensive comparison of models across multiple distances of skeletal gaits from the camera and across multiple view-groups. It can be seen that our ProxEmo network outperforms other state-of-the-art network by 50% at an average in terms of prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 9: Confusion Matrix:</head><p>We show the percentage of gaits belonging to every emotion class that were correctly classified by our algorithm, ProxEmo.</p><p>2) VS-CNN: One of the major drawbacks of ST-GCN is that it is not tuned for multi-view/arbitrary-view skeletal gait sequences. View-guided Skeleton CNN (VS-CNN) <ref type="bibr" target="#b36">[37]</ref> approaches this problem by building a dataset that multiple view-points with respect to the human reference frame. The multiple views are combined into four groups, each consisting of the one-quarter (90 degrees) of the view-points sequences. The action recognition is performed in three stages: (i) a view-group predictor network that predicts the view-group C (of 4 view-groups) of the sequence. (ii) a view-group feature network that consists of four individual networks, based on SK-CNN <ref type="bibr" target="#b47">[48]</ref>, for each view-group, and finally, (iii) a channel classifier network that combines (i) and (ii) to predict the action label for the skeletal gait sequence.</p><p>The VS-CNN also steers away from graph convolutions with an aim to increase the run-time performance of the network. 2D convolutions were observed to be much faster and efficient as opposed to graph convolutions. Hence, the spatial-temporal skeletal gait sequences are transformed into images. In our experiment, we tweak the final output of VS-CNN architecture using equation 4 to predict human emotions as opposed to actions. The network was trained with a sof tmax cross-entropy loss function, represented in equation 8.</p><p>The <ref type="table" target="#tab_0">table I and figure 8</ref> present a comparison of our model against VS-CNN and ST-GCN. We can observe that ProxEmo outperforms the state-of-the-art action recognition algorithms in both single-view and arbitrary-view skeletal gait sequences. Also, observe that in table II, ProxEmo takes up the least number of model parameters. This is because we perform group convolutions and eliminate Fully Connected layers in our network. <ref type="figure">Figure 9</ref> is a confusion matrix of the predicted vs actual emotion classes of ProxEmo. We can infer from this matrix that our model performs fairly well across all emotion classes with a high accuracy. Since, the evaluation metrics for socially acceptable is not welldefined, we don't report any evaluation on our emotionguided navigation planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>View-Groups</head><p>Model Parameters ST-GCN <ref type="bibr">[</ref>   <ref type="bibr" target="#b35">[36]</ref> and VS-CNN <ref type="bibr" target="#b36">[37]</ref>. This is due to the fact that we use Group Convolutions (GC) and eliminate Fully Connected (FC) layers in our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION, LIMITATIONS AND FUTURE WORK</head><p>We present ProxEmo, a novel group convolution-based deep learning network that takes 3D skeletal gaits of a human and predicts the perceived emotional states {happy, sad, angry, neutral} for emotionally-guided robot navigation. Our model specifically takes into consideration arbitrary orientations of pedestrians and is trained using augmented data comprising of multiple view-groups. We also present a new approach for socially-aware navigation that takes into consideration the predicted emotion and view-group of the pedestrian in the robot's field of view. In doing this, we also define a new metric for computing comfort space, that incorporates constants derived from emotion and view-group predictions. The limitation of our model during inference time is that it is reliant on real-time 3D skeletal tracking.</p><p>In the future, we plan to look at multi-modal cues for emotion recognition. We intend to dynamically compute proxemic constraints using continual feedback in a rewardbased training scheme. We also plan to add higher-level information, with regards to the environmental or cultural context that are known to influence human emotions, which can further improve our classification results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>ProxEmo:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our Pipeline:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Data Augmentation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Skeleton Representation: We represent a pedestrian by 16 joints (ji). The overall pose of the pedestrian is defined using these joint positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>ProxEmo Network Architecture: The network is trained on image embeddings of the 5D gait set G, which are scaled up to 244 × 244. The architecture consists of four group convolution (GC) layers. Each GC layer consists of four groups that have been stacked together. This represents the four group convolution outcomes for each of the four emotion labels. The group convolutions are stacked in two stages represented by Stage 1 and Stage 2. The output of the network has a dimension of 4 × 4 after passing through a sof tmax layer. The final predicted emotion is given by the maxima of this 4×4 output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Variational Comfort Space:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Comparison of ProxEmo with other arbitrary view algorithms :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of ProxEmo with other state-of-theartemotion classification algorithms:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison of model parameters: Our ProxEmo model has significantly fewer parameters compared to ST-GCN</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This research was supported in part by ARO Grants W911NF1910069, W911NF1910315, NIST and Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human-aware robot navigation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1726" to="1743" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech based emotion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foo</forename><forename type="middle">Say</forename><surname>Tin Lay Nwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TENCON 2001</title>
		<meeting>TENCON 2001</meeting>
		<imprint>
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech emotion analysis in noisy realworld environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="4605" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="5562" to="5570" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Affective body expression perception and recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of emotion recognition using facial expressions, speech and multimodal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICMI</title>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emoticon: Context-aware multimodal emotion recognition using frege&apos;s principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An intelligent and generic approach for detecting human emotions: a case study with facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Faiçal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pessin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ueyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>Soft Computing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emotional expressions reconsidered: challenges to inferring emotion from human facial movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Pollak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science in the Public Interest</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Emotion recognition through gait on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="page" from="800" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Step: Spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page">13421350</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward sociable robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>socially Interactive Robots</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey of socially interactive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="143" to="166" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>socially Interactive Robots</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glmprealtime pedestrian path prediction using global and local movement patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pratapa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5528" to="5535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cmetric: A driving behavior measure using centrality functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pirjanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Létourneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Emotion and Social Robotics</title>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generalized velocity obstacles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="5573" to="5578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal reciprocal collision avoidance for multi-agent navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Robotics and Automation</title>
		<meeting>of the IEEE International Conference on Robotics and Automation<address><addrLine>Anchorage (AK), USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reciprocal velocity obstacles for real-time multi-agent navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1928" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dronet: Learning to fly by driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Del-Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1088" to="1095" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards optimally decentralized multi-robot collision avoidance via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fanl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6252" to="6259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Crowdmove: Autonomous mapless navigation in crowded scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07870</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pedestrian dominance modeling for socially-aware robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5621" to="5628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From proxemics theory to socially-aware navigation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rios-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spalanzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">IJSR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pedestrian vision and collision avoidance behavior: Investigation of the information process space of pedestrians using an eye tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pedestrian and Evacuation Dynamics</title>
		<editor>W. W. F. Klingsch, C. Rogsch, A. Schadschneider, and M. Schreckenberg</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="95" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A framework towards a socially aware mobile robot motion in human-centered dynamic environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="5855" to="5860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition in faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruiz-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altahhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning</title>
		<editor>A. E. Villa, P. Masulli, and A. J. Pons Rivero</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Emotion recognition using facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koodziej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Rak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international Conference on Computational Science</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="12" to="14" />
		</imprint>
	</monogr>
	<note>ICCS</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Identifying emotions from walking using affective and deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The liar&apos;s walk: Detecting deception with gait and gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06874</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The emotionally intelligent robot: Improving socially-aware human prediction in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How are you feeling? multimodal emotion learning for socially-assistive robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prinja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="894" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Image and vision computing</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roncal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A large-scale varying-view rgb-d action dataset for arbitrary-view human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning perceived emotion using affective and deep features for mental health applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The effect of facial expressions on peripersonal and interpersonal spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ruggiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frassinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rapuano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Di Cola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Personal space, evasive movement and pedestrian level of service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of advanced transportation</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recognizing emotions conveyed by human gait</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kadone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grèzes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berthoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hicheur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="621" to="632" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Delevoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="550" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Identifying emotions from noncontact gaits information based on microsoft kinects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="591" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Body expression recognition from animated 3d skeleton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouakaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on 3D Imaging (IC3D)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
