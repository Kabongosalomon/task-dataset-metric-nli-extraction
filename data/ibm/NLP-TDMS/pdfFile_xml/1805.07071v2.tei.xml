<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-level Wavelet-CNN for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<email>cskaizhang@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-level Wavelet-CNN for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The tradeoff between receptive field size and efficiency is a crucial issue in low level vision. Plain convolutional networks (CNNs) generally enlarge the receptive field at the expense of computational cost. Recently, dilated filtering has been adopted to address this issue. But it suffers from gridding effect, and the resulting receptive field is only a sparse sampling of input image with checkerboard patterns. In this paper, we present a novel multi-level wavelet CNN (MWCNN) model for better tradeoff between receptive field size and computational efficiency. With the modified U-Net architecture, wavelet transform is introduced to reduce the size of feature maps in the contracting subnetwork. Furthermore, another convolutional layer is further used to decrease the channels of feature maps. In the expanding subnetwork, inverse wavelet transform is then deployed to reconstruct the high resolution feature maps. Our MWCNN can also be explained as the generalization of dilated filtering and subsampling, and can be applied to many image restoration tasks. The experimental results clearly show the effectiveness of MWCNN for image denoising, single image super-resolution, and JPEG image artifacts removal. Conv+BN+ReLU DWT IWT 160 Conv 160 160 640 256 1024 256 160 256 256 256 256 256 256 256 256 640 160 160 160 ... 256 256 256 256 256 256 Sum Connection 1024 160</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration, which aims to recover the latent clean image x from its degraded observation y, is a fundamental and long-standing problem in low level vision. For decades, varieties of methods have been proposed for image restoration from both prior modeling and discriminative learning perspectives <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52]</ref>. Recently, convolutional neural networks (CNNs) have also been extensively studied and achieved state-of-the-art performance in several representative image restoration tasks, such as single image super-resolution (SISR) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, image denoising <ref type="bibr" target="#b56">[57]</ref>, image deblurring <ref type="bibr" target="#b57">[58]</ref>, and lossy image compres- * Corresponding author.  <ref type="figure">Figure 1</ref>. The run time vs. PSNR value of representative CNN models, including SRCNN <ref type="bibr" target="#b15">[16]</ref>, FSRCNN <ref type="bibr" target="#b13">[14]</ref>, ES-PCN <ref type="bibr" target="#b44">[45]</ref>, VDSR <ref type="bibr" target="#b28">[29]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref>, RED30 <ref type="bibr" target="#b36">[37]</ref>, LapSRN <ref type="bibr" target="#b30">[31]</ref>, DRRN <ref type="bibr" target="#b46">[47]</ref>, MemNet <ref type="bibr" target="#b46">[47]</ref> and our MWCNN. The receptive field of each model are also provided. The PSNR and time are evaluated on Set5 with the scale factor ×4 running on a GTX1080 GPU.</p><p>sion <ref type="bibr" target="#b33">[34]</ref>. The popularity of CNN in image restoration can be explained from two aspects. On the one hand, existing CNN-based solutions have outperformed the other methods with a large margin for several simple tasks such as image denoising and SISR <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57]</ref>. On the other hand, recent studies have revealed that one can plug CNN-based denoisers into model-based optimization methods for solving more complex image restoration tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b57">58]</ref>, which also promotes the widespread use of CNNs. For image restoration, CNN actually represents a mapping from degraded observation to latent clean image. Due to the input and output images usually should be of the same size, one representative strategy is to use the fully convolutional network (FCN) by removing the pooling layers. In general, larger receptive field is helpful to restoration performance by taking more spatial context into account. However, for FCN without pooling, the receptive field size can be enlarged by either increasing the network depth or using filters with larger size, which unexceptionally results in higher computational cost. In <ref type="bibr" target="#b57">[58]</ref>, dilated filtering <ref type="bibr" target="#b54">[55]</ref> is adopted to enlarge receptive field without the sacrifice of computational cost. Dilated filtering, however, inherently suffers from gridding effect <ref type="bibr" target="#b49">[50]</ref>, where the receptive field only considers a sparse sampling of input image with checkerboard patterns. Thus, one should be careful to enlarge receptive field while avoiding the increase of computational burden and the potential sacrifice of performance improvement. Taking SISR as an example, <ref type="figure">Figure 1</ref> illustrates the receptive field, run times, and PSNR values of several representative CNN models. It can be seen that FS-RCNN <ref type="bibr" target="#b13">[14]</ref> has relatively larger receptive field but achieves lower PSNR value than VDSR <ref type="bibr" target="#b28">[29]</ref> and DnCNN <ref type="bibr" target="#b56">[57]</ref>.</p><p>In this paper, we present a multi-level wavelet CNN (MWCNN) model to enlarge receptive field for better tradeoff between performance and efficiency. Our MWCNN is based on the U-Net <ref type="bibr" target="#b40">[41]</ref> architecture consisting of a contracting subnetwork and an expanding subnetwork. In the contracting subnetwork, discrete wavelet transform (DWT) is introduced to replace each pooling operation. Since DWT is invertible, it is guaranteed that all the information can be kept by such downsampling scheme. Moreover, DWT can capture both frequency and location information of feature maps <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, which may be helpful in preserving detailed texture. In the expanding subnetwork, inverse wavelet transform (IWT) is utilized for upsampling low resolution feature maps to high resolution ones. To enrich feature representation and reduce computational burden, elementwise summation is adopted for combining the feature maps from the contracting and expanding subnetworks. Moreover, dilated filtering can also be explained as a special case of MWCNN, and ours is more general and effective in enlarging receptive field. Experiments on image denoising, SISR, and JPEG image artifacts removal validate the effectiveness and efficiency of our MWCNN. As shown in Figure 1, MWCNN is moderately slower than LapSRN <ref type="bibr" target="#b30">[31]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref> and VDSR <ref type="bibr" target="#b28">[29]</ref> in terms of run time, but can have a much larger receptive field and higher PSNR value. To sum up, the contributions of this work include:</p><p>• A novel MWCNN model to enlarge receptive field with better tradeoff between efficiency and restoration performance.</p><p>• Promising detail preserving ability due to the good time-frequency localization of DWT.</p><p>• State-of-the-art performance on image denoising, SISR, and JPEG image artifacts removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we present a brief review on the development of CNNs for image denoising, SISR, JPEG image artifacts removal, and other image restoration tasks. Specifically, more discussions are given to the relevant works on enlarging receptive field and incorporating DWT in CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image denoising</head><p>Since 2009, CNNs have been applied for image denoising <ref type="bibr" target="#b24">[25]</ref>. These early methods generally cannot achieve state-of-the-art denoising performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b52">53]</ref>. Recently, multi-layer perception (MLP) has been adopted to learn the mapping from noise patch to clean pixel, and achieve comparable performance with BM3D <ref type="bibr" target="#b7">[8]</ref>. By incorporating residual learning with batch normalization <ref type="bibr" target="#b23">[24]</ref>, the DnCNN model by Zhang et al. <ref type="bibr" target="#b56">[57]</ref> can outperform traditional non-CNN based methods. Mao et al. <ref type="bibr" target="#b36">[37]</ref> suggest to add symmetric skip connections to FCN for improving denoising performance. For better tradeoff between speed and performance, Zhang et al. <ref type="bibr" target="#b57">[58]</ref> present a 7-layer FCN with dilated filtering. Santhanam et al. <ref type="bibr" target="#b42">[43]</ref> introduce a recursively branched deconvolutional network (RBDN), where pooling/unpooling is adopted to obtain and aggregate multi-context representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Single image super-resolution</head><p>The application of CNN in SISR begins with SR-CNN <ref type="bibr" target="#b15">[16]</ref>, which adopts a 3-layer FCN without pooling and has a small receptive field. Subsequently, very deep network <ref type="bibr" target="#b28">[29]</ref>, residual units <ref type="bibr" target="#b31">[32]</ref>, Laplacian pyramid <ref type="bibr" target="#b30">[31]</ref>, and recursive architecture <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref> have also been suggested to enlarge receptive field. These methods, however, enlarge the receptive field at the cost of either increasing computational cost or loss of information. Due to the speciality of SISR, one effective approach is to take the low-resolution (LR) image as input to CNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref> for better tradeoff between receptive field size and efficiency. In addition, generative adversarial networks (GANs) have also been introduced to improve the visual quality of SISR <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">JPEG image artifacts removal</head><p>Due to high compression rate, JPEG image usually suffers from blocking effect and results in unpleasant visual quality. In <ref type="bibr" target="#b14">[15]</ref>, Dong et al. adopt a 4-layer ARCNN for JPEG image deblocking. By taking the degradation model of JPEG compression into account <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51]</ref>, Guo et al. <ref type="bibr" target="#b17">[18]</ref> suggest a dual-domain convolutional network to combine the priors in both DCT and pixel domains. GAN has also been introduced to generate more realistic result <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Other restoration tasks</head><p>Due to the similarity of image denoising, SISR, and JPEG artifacts removal, the model suggested for one task may be easily extended to the other tasks simply by retraining. For example, both DnCNN <ref type="bibr" target="#b56">[57]</ref> and MemNet <ref type="bibr" target="#b47">[48]</ref> have been evaluated on all the three tasks. Moreover, CNN denoisers can also serve as a kind of plug-and-play prior. By incorporating with unrolled inference, any restoration tasks can be tackled by sequentially applying the CNN denoisers <ref type="bibr" target="#b57">[58]</ref>. <ref type="bibr">Romano et al. [40]</ref> further propose a regularization by denoising framework, and provide an explicit functional for defining the regularization induced by denoisers. These methods not only promote the application of CNN in low level vision, but also present many solutions to exploit CNN denoisers for other image restoration tasks.</p><p>Several studies have also been given to incorporate wavelet transform with CNN. Bae et al. <ref type="bibr" target="#b4">[5]</ref> find that learning CNN on wavelet subbands benefits CNN learning, and suggest a wavelet residual network (WavResNet) for image denoising and SISR. Similarly, Guo et al. <ref type="bibr" target="#b19">[20]</ref> propose a deep wavelet super-resolution (DWSR) method to recover missing details on subbands. Subsequently, deep convolutional framelets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref> have been developed to extend convolutional framelets for low-dose CT. However, both of WavResNet and DWSR only consider one level wavelet decomposition. Deep convolutional framelets independently processes each subband from decomposition perspective, which ignores the dependency between these subbands. In contrast, multi-level wavelet transform is considered by our MWCNN to enlarge receptive field without information loss. Taking all the subbands as inputs after each transform, our MWCNN can embed DWT to any CNNs with pooling, and owns more power to model both spatial context and inter-subband dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first introduce the multi-level wavelet packet transform (WPT). Then we present our MWCNN motivated by multi-level WPT, and describe its network architecture. Finally, discussion is given to analyze the connection of MWCNN with dilated filtering and subsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">From multi-level WPT to MWCNN</head><p>In 2D discrete wavelet transform (DWT), four filters, i.e. f LL , f LH , f HL , and f HH , are used to convolve with an image x <ref type="bibr" target="#b35">[36]</ref>. The convolution results are then downsampled to obtain the four subband images x 1 , x 2 , x 3 , and x 4 . For example, x 1 is defined as (f LL ⊗ x) ↓ 2 . Even though the downsampling operation is deployed, due to the biorthogonal property of DWT, the original image x can be accurately reconstructed by the inverse wavelet transform (IWT), i.e.,</p><formula xml:id="formula_0">x = IW T (x 1 , x 2 , x 3 , x 4 ).</formula><p>In multi-level wavelet packet transform (WPT) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref>, the subband images x 1 , x 2 , x 3 , and x 4 are further processed with DWT to produce the decomposition results. For two-level WPT, each subband image x i (i = 1, 2, 3, or 4) is decomposed into four subband images x i,1 , x i,2 , x i,3 , and x i,4 . Recursively, the results of three or higher levels WPT can be attained. <ref type="figure" target="#fig_1">Figure 2</ref> then adopted as the pooling operator. In the reconstruction stage, the four subband images are first upsampled and then convolved with the corresponding filters to produce the reconstruction result at the current level. Finally, the original image x can be accurately reconstructed by inverse WPT.</p><p>In image denoising and compression, some operations, e.g., soft-thresholding and quantization, usually are required to process the decomposition result <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref>. These operations can be treated as some kind of nonlinearity tailored to specific task. In this work, we further extend WPT to multi-level wavelet-CNN (MWCNN) by adding a CNN block between any two levels of DWTs, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b). After each level of transform, all the subband images are taken as the inputs to a CNN block to learn a compact representation as the inputs to the subsequent level of transform. It is obvious that MWCNN is a generalization of multi-level WPT, and degrades to WPT when each CNN block becomes the identity mapping. Due to the biorthogonal property of WPT, our MWCNN can use subsampling operations safely without information loss. Moreover, compared with conventional CNN, the frequency and location characteristics of DWT is also expected to benefit the preservation of detailed texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>The key of our MWCNN architecture is to design the CNN block after each level of DWT. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, each CNN block is a 4-layer FCN without pooling, and takes all the subband images as inputs. In contrast, different CNNs are deployed to low-frequency and high-frequency bands in deep convolutional framelets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54]</ref>. We note that the subband images after DWT are still dependent, and the ignorance of their dependence may be harmful to the restoration performance. Each layer of the CNN block is composed of convolution with 3 × 3 filters (Conv), batch normalization (BN), and rectified linear unit (ReLU) operations. As to the last layer of the last CNN block, Conv without BN and ReLU is adopted to predict residual image. <ref type="figure" target="#fig_2">Figure 3</ref> shows the overall architecture of MWCNN which consists of a contracting subnetwork and an expanding subnetwork. Generally, MWCNN modifies U-Net from three aspects. (i) For downsampling and upsampling, max- pooling and up-convolution are used in conventional U-Net <ref type="bibr" target="#b40">[41]</ref>, while DWT and IWT are utilized in MWCNN. (ii) For MWCNN, the downsampling results in the increase of feature map channels. Except the first one, the other CNN blocks are deployed to reduce the feature map channels for compact representation. In contrast, for conventional U-Net, the downsampling has no effect on feature map channels, and the subsequent convolution layers are used to increase feature map channels. (iii) In MWCNN, elementwise summation is used to combine the feature maps from the contracting and expanding subnetworks. While in conventional U-Net concatenation is adopted. Then our final network contains 24 layers. For more details on the setting of MWCNN, please refer to <ref type="figure" target="#fig_2">Figure 3</ref>. In our implementation, Haar wavelet is adopted as the default in MWCNN. Other wavelets, e.g., Daubechies 2 (DB2), are also considered in our experiments.</p><p>Denote by Θ the network parameters of MWCNN, and F (y; Θ) be the network output. Let {(y i , x i )} N i=1 be a training set, where y i is the i-th input image, x i is the corresponding ground-truth image. The objective function for learning MWCNN is then given by</p><formula xml:id="formula_1">L(Θ) = 1 2N N i=1 F (y i ; Θ) − x i 2 F .<label>(1)</label></formula><p>The ADAM algorithm <ref type="bibr" target="#b29">[30]</ref> is adopted to train MWCNN by minimizing the objective function. Different from VDSR <ref type="bibr" target="#b28">[29]</ref> and DnCNN <ref type="bibr" target="#b56">[57]</ref>, we do not adopt the residual learning formulation for the reason that it can be naturally embedded in MWCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussion</head><p>The DWT in MWCNN is closely related with the pooling operation and dilated filtering. By using the Haar wavelet as an example, we explain the connection between DWT and sum-pooling. In 2D Haar wavelet, the low-pass filter f LL is defined as,</p><formula xml:id="formula_2">f LL = 1 1 1 1 .<label>(2)</label></formula><p>One can see that (f LL ⊗ x) ↓ 2 actually is the sum-pooling operation. When only the low-frequency subband is consid-ered, DWT and IWT will play the roles of pooling and upconvolution in MWCNN, respectively. When all the subbands are taken into account, MWCNN can avoid the information loss caused by conventional subsampling, and may benefit restoration result.</p><p>To illustrate the connection between MWCNN and dilated filtering with factor 2, we first give the definition of f LH , f HL , and f HH ,</p><formula xml:id="formula_3">f LH = −1 −1 1 1 , f HL = −1 1 −1 1 , f HH = 1 −1 −1 1 .<label>(3)</label></formula><p>Given an image x with size of m × n, the (i, j)-th value of x 1 after 2D Haar transform can be written as</p><formula xml:id="formula_4">x 1 (i, j) = x(2i − 1, 2j − 1) + x(2i − 1, 2j) + x(2i, 2j − 1) + x(2i, 2j). And x 2 (i, j), x 3 (i, j)</formula><p>, and x 4 (i, j) can be defined analogously. We also have</p><formula xml:id="formula_5">x(2i − 1, 2j − 1) = (x 1 (i, j) − x 2 (i, j) − x 3 (i, j) + x 4 (i, j)) /4. The dilated</formula><p>filtering with factor 2 on the position (2i − 1, 2j − 1) of x can be written as</p><formula xml:id="formula_6">(x ⊗ 2 k)(2i − 1, 2j − 1) = p+2s = 2i−1, q +2t = 2j −1 x(p, q)k(s, t),<label>(4)</label></formula><p>where k is the 3×3 convolution kernel. Actually, it also can be obtained by using the 3×3 convolution with the subband images,</p><formula xml:id="formula_7">(x⊗ 2 k)(2i−1, 2j−1)=((x 1 −x 2 −x 3 +x 4 )⊗k) (i, j)/4. (5)</formula><p>Analogously, we can analyze the connection between dilated filtering and MWCNN for (</p><formula xml:id="formula_8">x ⊗ 2 k)(2i − 1, 2j), (x ⊗ 2 k)(2i, 2j − 1), (x ⊗ 2 k)(2i, 2j</formula><p>). Therefore, the 3 × 3 dilated convolution on x can be treated as a special case of 4 × 3 × 3 convolution on the subband images. Compared with dilated filtering, MWCNN can also avoid the gridding effect. After several layers of dilated filtering, it only considers a sparse sampling of locations with the checkerboard pattern, resulting in large portion of information loss (see <ref type="figure" target="#fig_3">Figure 4</ref>(a)). Another problem with dilated filtering is that the two neighbored pixels may be based on information from totally non-overlapped locations (see <ref type="figure" target="#fig_3">Figure 4</ref>(b)), and may cause the inconsistence of local information. In contrast, <ref type="figure" target="#fig_3">Figure 4</ref>(c) illustrates the receptive field of MWCNN. One can see that MWCNN is able to well address the sparse sampling and inconsistence of local information, and is expected to benefit restoration performance quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experiments are conducted for performance evaluation on three tasks, i.e., image denoising, SISR, and compression artifacts removal. Comparison of several MWCNN variants is also given to analyze the contribution of each component. The code and pre-trained models will be given at https://github.com/lpj0/MWCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setting 4.1.1 Training set</head><p>To train our MWCNN, a large training set is constructed by using images from three dataset, i.e. Berkeley Segmentation Dataset (BSD) <ref type="bibr" target="#b37">[38]</ref>, DIV2K <ref type="bibr" target="#b2">[3]</ref> and Waterloo Exploration Database (WED) <ref type="bibr" target="#b34">[35]</ref>. Concretely, we collect 200 images from BSD, 800 images from DIV2K, and 4, 744 images from WED. Due to the receptive field of MWCNN is not less than 226 × 226, in the training stage N = 24 × 6, 000 patches with the size of 240 × 240 are cropped from the training images.</p><p>For image denoising, Gaussian noise with specific noise level is added to clean patch, and MWCNN is trained to learn a mapping from noisy image to denoising result. Following <ref type="bibr" target="#b56">[57]</ref>, we consider three noise levels, i.e., σ = 15, 25 and 50. For SISR, we take the result by bicubic upsampling as the input to MWCNN, and three specific scale factors, i.e., ×2, ×3 and ×4, are considered in our experiments. For JPEG image artifacts removal, we follow <ref type="bibr" target="#b14">[15]</ref> by considering four compression quality settings Q = 10, 20, 30 and 40 for the JPEG encoder. Both JPEG encoder and JPEG image artifacts removal are only applied on the Y channel <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Network training</head><p>A MWCNN model is learned for each degradation setting. The network parameters are initialized based on the method described in <ref type="bibr" target="#b21">[22]</ref>. We use the ADAM algorithm <ref type="bibr" target="#b29">[30]</ref> with α = 0.01, β 1 = 0.9, β 2 = 0.999 and = 10 −8 for optimizing and a mini-batch size of 24. As to the other hyper-parameters of ADAM, the default setting is adopted. The learning rate is decayed exponentially from 0.001 to 0.0001 in the 40 epochs. Rotation or/and flip based data augmentation is used during mini-batch learning. We use the MatConvNet package <ref type="bibr" target="#b48">[49]</ref> with cuDNN 6.0 to train our MWCNN. All the experiments are conducted in the Matlab (R2016b) environment running on a PC with Intel(R) Core(TM) i7-5820K CPU 3.30GHz and an Nvidia GTX1080 GPU. The learning algorithm converges very fast and it takes about two days to train a MWCNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative and qualitative evaluation</head><p>In this subsection, all the MWCNN models use the same network setting described in Sec. 3.2, and 2D Haar wavelet is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Image denoising</head><p>Except CBM3D <ref type="bibr" target="#b10">[11]</ref> and CDnCNN <ref type="bibr" target="#b56">[57]</ref>, most denoising methods are only tested on gray images. Thus, we train our MWCNN by using the gray images, and compare with six competing denoising methods, i.e., BM3D <ref type="bibr" target="#b10">[11]</ref>, TNRD <ref type="bibr" target="#b9">[10]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref>, IRCNN <ref type="bibr" target="#b57">[58]</ref>, RED30 <ref type="bibr" target="#b36">[37]</ref>, and MemNet <ref type="bibr" target="#b47">[48]</ref>. We evaluate the denoising methods on three test datasets, i.e., Set12 <ref type="bibr" target="#b56">[57]</ref>, BSD68 <ref type="bibr" target="#b37">[38]</ref>, and Ur-ban100 <ref type="bibr" target="#b22">[23]</ref>. <ref type="table">Table 1</ref> lists the average PSNR/SSIM results of the competing methods on these three datasets. We note that our MWCNN only slightly outperforms DnCNN by about 0.1 ∼ 0.3dB in terms of PSNR on BSD68. As to other datasets, our MWCNN generally achieves favorable performance when compared with the competing methods. When the noise level is high (e.g., σ = 50), the average PSNR by our MWCNN can be 0.5dB higher than that by DnCNN on Set12, and 1.2dB higher on Urban100. <ref type="figure">Figure 5</ref> shows the denoising results of the images Test011 from Set68 with the noise level σ = 50. One can see that our MWCNN is promising in recovering image details and structures, and can obtain visually more pleasant result than the competing methods. Please refer to the supplementary materials for more results on Set12 and Urban100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Single image super-resolution</head><p>Following <ref type="bibr" target="#b28">[29]</ref>, SISR is only applied to the luminance channel, i.e. Y in YCbCr color space. We test MWCNN on four datasets, i.e., Set5 <ref type="bibr" target="#b6">[7]</ref>, Set14 <ref type="bibr" target="#b55">[56]</ref>, BSD100 <ref type="bibr" target="#b37">[38]</ref>, and Urban100 <ref type="bibr" target="#b22">[23]</ref>, because they are widely adopted to   <ref type="bibr" target="#b28">[29]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref>, RED30 <ref type="bibr" target="#b36">[37]</ref>, SRResNet <ref type="bibr" target="#b31">[32]</ref>, LapSRN <ref type="bibr" target="#b30">[31]</ref>, DRRN <ref type="bibr" target="#b46">[47]</ref>, and MemNet <ref type="bibr" target="#b47">[48]</ref>. Due to the source code of SRResNet is not released, its results are from <ref type="bibr" target="#b31">[32]</ref> and are incomplete. <ref type="table">Table 2</ref> lists the average PSNR/SSIM results of the competing methods on the four datasets. Our MWCNN performs favorably in terms of both PSNR and SSIM indexes. Compared with VDSR, our MWCNN achieves a notable gain of about 0.4dB by PSNR on Set5 and Set14. On Urban100, our MWCNN outperforms VDSR by about 0.9∼1.4dB. Obviously, WaveResNet et al. <ref type="bibr" target="#b4">[5]</ref> sightly outperform VDSR, and also is still inferior to MWCNN. We note that the network depth of SRResNet is 34, while that of MWCNN is 24. Moreover, SRResNet is trained with a much larger training set than MWCNN. Even so, when the scale factor is 4, MWCNN achieve slightly higher PSNR values on Set5 and BSD100, and is comparable to SR-ResNet on Set14. <ref type="figure" target="#fig_5">Figure 6</ref> shows the visual comparisons of the competing methods on the images Barbara from Set14. Thanks to the frequency and location characteristics of DWT, our MWCNN can correctly recover the fine and detailed textures, and produce sharp edges. Furthermore, for Track 1 of NTIRE 2018 SR challenge (×8 SR) <ref type="bibr" target="#b0">[1]</ref>, our improved MWCNN is lower than the Top-1 method by 0.37dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">JPEG image artifacts removal</head><p>In JPEG compression, an image is divided into nonoverlapped 8 × 8 blocks. Discrete cosine transform (DCT) and quantization are then applied to each block, thus introducing the blocking artifact. The quantization is determined by a quality factor Q to control the compression rate. Following <ref type="bibr" target="#b14">[15]</ref>, we consider four settings on quality factor, e.g., Q = 10, 20, 30 and 40, for the JPEG encoder. Both JPEG encoder and JPEG image artifacts removal are only applied to the Y channel. In our experiments, MWCNN is compared with four competing methods, i.e., ARCNN <ref type="bibr" target="#b14">[15]</ref>, TNRD <ref type="bibr" target="#b9">[10]</ref>, DnCNN <ref type="bibr" target="#b56">[57]</ref>, and MemNet <ref type="bibr" target="#b47">[48]</ref> on the two datasets, i.e., Classic5 and LIVE1 <ref type="bibr" target="#b38">[39]</ref>. We do not consider <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> due to their source codes are unavailable. <ref type="table" target="#tab_1">Table 3</ref> lists the average PSNR/SSIM results of the competing methods on Classic5 and LIVE1. For any of the four quality factors, our MWCNN performs favorably in terms of quantitative metrics on the two datasets. On Classic5 and LIVE1, the PSNR values of MWCNN can be 0.2∼0.3dB higher than those of the second best method (i.e., Mem-Net <ref type="bibr" target="#b47">[48]</ref>) for the quality factor of 10 and 20. <ref type="figure" target="#fig_6">Figure 7</ref> shows  <ref type="figure">Figure 5</ref>. Image denoising results of "T est011" (BSD68) with noise level 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Ground  <ref type="figure" target="#fig_5">Figure 6</ref>. Single image super-resolution results of "barbara" (Set14) with upscaling factor ×4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Ground  <ref type="figure" target="#fig_6">Figure 7</ref>. JPEG image artifacts removal results of "womanhat" (LIVE1) with quality factor 10.</p><p>Net <ref type="bibr" target="#b47">[48]</ref>) for the quality factor of 10 and 20. <ref type="figure" target="#fig_6">Figure 7</ref> shows the results on the image womanhat from LIVE1 with the quality factor 10. One can see that MWCNN is effective in restoring detailed textures and sharp salient edges. <ref type="table" target="#tab_5">Table 4</ref> lists the GPU run time of the competing methods for the three tasks. The Nvidia cuDNN-v6.0 deep learning library is adopted to accelerate the GPU computation under Ubuntu 16.04 system. Specifically, only the CNNbased methods with source codes are considered in the comparison. For three tasks, the run time of MWCNN is far less than several state-of-the-art methods, including RED30 <ref type="bibr" target="#b36">[37]</ref>, MemNet <ref type="bibr" target="#b46">[47]</ref> and DRRN <ref type="bibr" target="#b46">[47]</ref>. Note that the three methods also perform poorer than MWCNN in terms of PSNR/SSIM metrics. In comparison to the other methods, MWCNN is moderately slower by speed but can achieve higher PSNR/SSIM indexes. The result indicates that, instead of the increase of network depth/width, the effectiveness of MWCNN should be attributed to the incorporation of CNN and DWT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Run time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of MWCNN variants</head><p>Using image denoising and JPEG image artifacts as examples, we compare the PSNR results by three MWC-  Net <ref type="bibr" target="#b47">[48]</ref>) for the quality factor of 10 and 20. <ref type="figure" target="#fig_6">Figure 7</ref> shows the results on the image womanhat from LIVE1 with the quality factor 10. One can see that MWCNN is effective in restoring detailed textures and sharp salient edges. <ref type="table" target="#tab_5">Table 4</ref> lists the GPU run time of the competing methods for the three tasks. The Nvidia cuDNN-v6.0 deep learning library is adopted to accelerate the GPU computation under Ubuntu 16.04 system. Specifically, only the CNNbased methods with source codes are considered in the comparison. For three tasks, the run time of MWCNN is far less than several state-of-the-art methods, including RED30 <ref type="bibr" target="#b36">[37]</ref>, MemNet <ref type="bibr" target="#b46">[47]</ref> and DRRN <ref type="bibr" target="#b46">[47]</ref>. Note that the three methods also perform poorer than MWCNN in terms of PSNR/SSIM metrics. In comparison to the other methods, MWCNN is moderately slower by speed but can achieve higher PSNR/SSIM indexes. The result indicates that, instead of the increase of network depth/width, the effectiveness of MWCNN should be attributed to the incorporation of CNN and DWT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Run time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of MWCNN variants</head><p>Using image denoising and JPEG image artifacts as examples, we compare the PSNR results by three MWC- <ref type="table" target="#tab_5">Table 4</ref>. Run time (in seconds) of the competing methods for the three tasks on images of size 256×256, 512×512 and 1024×1024: image denosing is tested on noise level 50, SISR is tested on scale ×2, and JPEG image deblocking is tested on quality factor 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Denoising</head><p>Size TNRD <ref type="bibr">[</ref>    <ref type="figure">Figure 5</ref>. Image denoising results of "T est011" (BSD68) with noise level 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Ground  <ref type="figure" target="#fig_5">Figure 6</ref>. Single image super-resolution results of "barbara" (Set14) with upscaling factor ×4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Ground  <ref type="figure" target="#fig_6">Figure 7</ref>. JPEG image artifacts removal results of "womanhat" (LIVE1) with quality factor 10.</p><p>Net <ref type="bibr" target="#b47">[48]</ref>) for the quality factor of 10 and 20. <ref type="figure" target="#fig_6">Figure 7</ref> shows the results on the image womanhat from LIVE1 with the quality factor 10. One can see that MWCNN is effective in restoring detailed textures and sharp salient edges. <ref type="table" target="#tab_5">Table 4</ref> lists the GPU run time of the competing methods for the three tasks. The Nvidia cuDNN-v6.0 deep learning library is adopted to accelerate the GPU computation under Ubuntu 16.04 system. Specifically, only the CNNbased methods with source codes are considered in the comparison. For three tasks, the run time of MWCNN is far less than several state-of-the-art methods, including RED30 <ref type="bibr" target="#b36">[37]</ref>, MemNet <ref type="bibr" target="#b46">[47]</ref> and DRRN <ref type="bibr" target="#b46">[47]</ref>. Note that the three methods also perform poorer than MWCNN in terms of PSNR/SSIM metrics. In comparison to the other methods, MWCNN is moderately slower by speed but can achieve higher PSNR/SSIM indexes. The result indicates that, instead of the increase of network depth/width, the effectiveness of MWCNN should be attributed to the incorporation of CNN and DWT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Run time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of MWCNN variants</head><p>Using image denoising and JPEG image artifacts as examples, we compare the PSNR results by three MWC-  the results on the image womanhat from LIVE1 with the quality factor 10. One can see that MWCNN is effective in restoring detailed textures and sharp salient edges. <ref type="table" target="#tab_5">Table 4</ref> lists the GPU run time of the competing methods for the three tasks. The Nvidia cuDNN-v6.0 deep learning library is adopted to accelerate the GPU computation under Ubuntu 16.04 system. Specifically, only the CNNbased methods with source codes are considered in the comparison. For three tasks, the run time of MWCNN is far less than several state-of-the-art methods, including RED30 <ref type="bibr" target="#b36">[37]</ref>, MemNet <ref type="bibr" target="#b46">[47]</ref> and DRRN <ref type="bibr" target="#b46">[47]</ref>. Note that the three methods also perform poorer than MWCNN in terms of PSNR/SSIM metrics. In comparison to the other methods, MWCNN is moderately slower by speed but can achieve higher PSNR/SSIM indexes. The result indicates that, instead of the increase of network depth/width, the effectiveness of MWCNN should be attributed to the incorporation of CNN and DWT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Run time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison of MWCNN variants</head><p>Using image denoising and JPEG image artifacts as examples, we compare the PSNR results by three   <ref type="bibr" target="#b49">[50]</ref> to suppress the gridding effect, and (ii) Dilated-2: the dilate factor of all layers is set to 2. The WaveResNet method in <ref type="bibr" target="#b4">[5]</ref> is provided to be compared. Moreover, due to its code is unavailable, a self-implementation of deep convolutional framelets (DCF) <ref type="bibr" target="#b53">[54]</ref> is also considered in the experiments. <ref type="table" target="#tab_5">Table 4</ref> lists the PSNR and run time results of these methods. And we have the following observations. (i) The gridding effect with the sparse sampling and inconsistence of local information authentically has adverse influence on restoration performance. (ii) The ablation experiments indicate that using sum connection instead of concatenation can improve efficiency without decreasing PNSR. Due to the special group of filters with the biorthogonal and timefrequency localization property in wavelet, our embedded wavelet own more puissant ability for image restoration than pooling operation and learnable downsamping filters. The worse performance of DCF also indicates that independent processing of subbands harms final result. (iii) Compared to MWCNN (DB2) and MWCNN (HD), using Haar wavelet for downsampling and upsampling in network is the best choice in terms of quantitative and qualitative evaluation. MWCNN (Haar) has similar run time with dilated CNN and U-Net but achieves higher PSNR results, which demonstrates the effectiveness of MWCNN for tradeoff between performance and efficiency.</p><p>Note that our MWCNN is quite different with DCF <ref type="bibr" target="#b53">[54]</ref>: DCF incorporates CNN with DWT in the view of decomposition, where different CNNs are deployed to each subband. However, the results in <ref type="table" target="#tab_12">Table 5</ref> indicates that independent processing of subbands is not suitable for image restoration. On the contrary, MWCNN combines DWT to CNN from perspective of enlarging receptive field without information loss, allowing to embed DWT with any CNNs with pooling. Moreover, our embedded DWT can be treated as predefined parameters to ease network learning, and the dynamic range of subbands can be jointly adjusted by the CNN blocks. Taking all subbands as input, MWCNN is more powerful in modeling inter-band dependency.</p><p>As described in Sec. 3.2, our MWCNN can be extended to higher level of wavelet decomposition. Nevertheless, higher level inevitably results in deeper network and heavier computational burden. Thus, a suitable level is required to balance efficiency and performance. <ref type="table">Table 6</ref> reports the <ref type="table">Table 6</ref>. Average PSNR (dB) and run time (in seconds) of MWC-NNs with different levels on Gaussian denoising with the noise level of 50. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a multi-level wavelet-CNN (MWCNN) architecture for image restoration, which consists of a contracting subnetwork and a expanding subnetwork. The contracting subnetwork is composed of multiple levels of DWT and CNN blocks, while the expanding subnetwork is composed of multiple levels of IWT and CNN blocks. Due to the invertibility, frequency and location property of DWT, MWCNN is safe to perform subsampling without information loss, and is effective in recovering detailed textures and sharp structures from degraded observation. As a result, MWCNN can enlarge receptive field with better tradeoff between efficiency and performance. Extensive experiments demonstrate the effectiveness and efficiency of MWCNN on three restoration tasks, i.e., image denoising, SISR, and JPEG compression artifact removal.</p><p>In future work, we will extend MWCNN for more general restoration tasks such as image deblurring and blind deconvolution. Moreover, our MWCNN can also be used to substitute the pooling operation in the CNN architectures for high-level vision tasks such as image classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) illustrates the decomposition and reconstruction of an image with WPT. Actually, WPT is a special case of FCN without the nonlinearity layers. In the decomposition stage, four pre-defined filters are deployed to each (subband) image, and downsampling is From WPT to MWCNN. Intuitively, WPT can be seen as a special case of our MWCNN without CNN blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Multi-level wavelet-CNN architecture. It consists two parts: the contracting and expanding subnetworks. Each solid box corresponds to a multi-channel feature map. And the number of channels is annotated on the top of the box. The network depth is 24. Moreover, our MWCNN can be further extended to higher level (e.g., ≥ 4) by duplicating the configuration of the 3rd level subnetwork.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the gridding effect. Taken 3-layer CNNs as an example: (a) the dilated filtering with factor 2 surfers large portion of information loss, (b) and the two neighbored pixels are based on information from totally non-overlapped locations, (c) while our MWCNN can perfectly avoid underlying drawbacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 5 .Figure 6 .Figure 7 .</head><label>5567</label><figDesc>NN variants, including: (i) MWCNN (Haar): the default MWCNN with Haar wavelet, (ii) MWCNN (DB2): MWCNN with Daubechies-2 wavelet, and (iii) MWCN-N (HD): MWCNN with Haar in contracting subnetwork and Daubechies-2 in expanding subnetwork. Then, ablation experiments are provided for verifying the effectiveness of additionally embedded wavelet: (i) the default U-Net with same architecture to MWCNN, (ii) U-Net+S: using sum connection instead of concatenation, and (iii) U-Net+D: adopting learnable conventional downsamping fil-Image denoising results of "T est011" (BSD68) with noise level 50. Image denoising results of "T est011" (BSD68) with noise level 50. Ground Truth Ground Truth (PSNR / SSIM) VDSR (25.79 / 0.7403) DnCNN (25.92 / 0.7417) RED30 (25.99 / 0.7468) SRResNet (25.93 / 0.746) LapSRN (25.77 / 0.7384) DRRN (25.75 / 0.7404) MemNet (25.69 / 0.7414) WaveResNet (25.63 / 0.7372) MWCNN (26.46 / 0.7629) Single image super-resolution results of "barbara" (Set14) with upscaling factor ×4. Ground Truth Ground Truth (PSNR / SSIM) ARCNN (31.81 / 0.8109) TNRD (31.70 / 0.8076)) DnCNN (31.79 / 0.8107) MemNet (32.08 / 0.8178) MWCNN (32.43 / 0.8257) JPEG image artifacts removal results of "womanhat" (LIVE1) with quality factor 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>NN variants, including: (i) MWCNN (Haar): the default MWCNN with Haar wavelet, (ii) MWCNN (DB2): MWCNN with Daubechies-2 wavelet, and (iii) MWCN-N (HD): MWCNN with Haar in contracting subnetwork and Daubechies-2 in expanding subnetwork. Then, ablation experiments are provided for verifying the effectiveness of additionally embedded wavelet: (i) the default U-Net with same architecture to MWCNN, (ii) U-Net+S: using sum connection instead of concatenation, and (iii) U-Net+D: adopting learnable conventional downsamping fil-Single image super-resolution results of "barbara" (Set14) with upscaling factor ×4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>NN variants, including: (i) MWCNN (Haar): the default MWCNN with Haar wavelet, (ii) MWCNN (DB2): MWCNN with Daubechies-2 wavelet, and (iii) MWCN-N (HD): MWCNN with Haar in contracting subnetwork and Daubechies-2 in expanding subnetwork. Then, ablation experiments are provided for verifying the effectiveness of additionally embedded wavelet: (i) the default U-Net with same architecture to MWCNN, (ii) U-Net+S: using sum connection instead of concatenation, and (iii) U-Net+D: adopting learnable conventional downsamping fil-JPEG image artifacts removal results of "womanhat" (LIVE1) with quality factor 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Average PSNR(dB)/SSIM results of the competing methods for image denoising with noise levels σ = 15, 25 and 50 on datasets Set14, BSD68 and Urban100. Red color indicates the best performance. Average PSNR(dB) / SSIM results of the competing methods for SISR with scale factors S = 2, 3 and 4 on datasets Set5, Set14, BSD100 and Urban100. Red color indicates the best performance.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>σ</cell><cell></cell><cell>BM3D [11]</cell><cell>TNRD [10]</cell><cell></cell><cell cols="2">DnCNN [57]</cell><cell></cell><cell>IRCNN [58]</cell><cell>RED30 [37]</cell><cell>MemNet [48]</cell><cell>MWCNN</cell></row><row><cell>Set12</cell><cell></cell><cell>15 25 50</cell><cell cols="2">32.37 / 0.8952 29.97 / 0.8505 26.72 / 0.7676</cell><cell cols="2">32.50 / 0.8962 30.05 / 0.8515 26.82 / 0.7677</cell><cell cols="2">32.86 / 0.9027 30.44 / 0.8618 27.18 / 0.7827</cell><cell cols="2">32.77 / 0.9008 30.38 / 0.8601 27.14 / 0.7804</cell><cell cols="2">--27.34 / 0.7897</cell><cell>--27.38 / 0.7931</cell><cell>33.15 / 0.9088 30.79 / 0.8711 27.74 / 0.8056</cell></row><row><cell>BSD68</cell><cell></cell><cell>15 25 50</cell><cell cols="2">31.08 / 0.8722 28.57 / 0.8017 25.62 / 0.6869</cell><cell cols="2">31.42 / 0.8822 28.92 / 0.8148 25.97 / 0.7021</cell><cell cols="2">31.73 / 0.8906 29.23 / 0.8278 26.23 / 0.7189</cell><cell cols="2">31.63 / 0.8881 29.15 / 0.8249 26.19 / 0.7171</cell><cell cols="2">--26.35 / 0.7245</cell><cell>--26.35 / 0.7294</cell><cell>31.86 / 0.8947 29.41 / 0.8360 26.53 / 0.7366</cell></row><row><cell>Urban100</cell><cell></cell><cell>15 25 50</cell><cell cols="2">32.34 / 0.9220 29.70 / 0.8777 25.94 / 0.7791</cell><cell cols="2">31.98 / 0.9187 29.29 / 0.8731 25.71 / 0.7756</cell><cell cols="2">32.67 / 0.9250 29.97 / 0.8792 26.28 / 0.7869</cell><cell cols="2">32.49 / 0.9244 29.82 / 0.8839 26.14 / 0.7927</cell><cell cols="2">--26.48 / 0.7991</cell><cell>--26.64 / 0.8024</cell><cell>33.17 / 0.9357 30.66 / 0.9026 27.42 / 0.8371</cell></row><row><cell>Dataset</cell><cell>S</cell><cell>RCN [46]</cell><cell></cell><cell>VDSR [29]</cell><cell>DnCNN [57]</cell><cell></cell><cell>RED30 [37]</cell><cell cols="3">SRResNet [32] LapSRN [31]</cell><cell>DRRN [47]</cell><cell>MemNet [48]</cell><cell>WaveResNet [5]</cell><cell>MWCNN</cell></row><row><cell>Set5 Set14 BSD100 Urban100</cell><cell>×2 ×3 ×4 ×2 ×3 ×4 ×2 ×3 ×4 ×2 ×3 ×4</cell><cell cols="2">37.17 / 0.9583 33.45 / 0.9175 31.11 / 0.8736 32.77 / 0.9109 29.63 / 0.8269 27.79 / 0.7594 ------</cell><cell>37.53 / 0.9587 33.66 / 0.9213 31.35 / 0.8838 33.03 / 0.9124 29.77 / 0.8314 28.01 / 0.7674 31.90 / 0.8960 28.82 / 0.7976 27.29 / 0.7251 30.76 / 0.9140 27.14 / 0.8279 25.18 / 0.7524</cell><cell>37.58 / 0.9593 33.75 / 0.9222 31.40 / 0.8845 33.04 / 0.9118 29.76 / 0.8349 28.02 / 0.7670 31.85 / 0.8942 28.80 / 0.7963 27.23 / 0.7233 30.75 / 0.9133 27.15 / 0.8276 25.20 / 0.7521</cell><cell cols="2">37.66 / 0.9599 33.82 / 0.9230 31.51 / 0.8869 32.94 / 0.9144 29.61 / 0.8341 27.86 / 0.7718 31.98 / 0.8974 28.92 / 0.7993 27.39 / 0.7286 30.91 / 0.9159 27.31 / 0.8303 25.35 / 0.7587</cell><cell cols="2">--32.05 / 0.8902 --28.49 / 0.7783 --27.56 / 0.7354 --26.07 / 0.7839</cell><cell>37.52 / 0.9590 -31.54 / 0.8850 33.08 / 0.9130 -28.19 / 0.7720 31.80 / 0.8950 -27.32 / 0.7280 30.41 / 0.9100 -25.21 / 0.7560</cell><cell>37.74 / 0.9591 34.03 / 0.9244 31.68 / 0.8888 33.23 / 0.9136 29.96 / 0.8349 28.21 / 0.7720 32.05 / 0.8973 28.95 / 0.8004 27.38 / 0.7284 31.23 / 0.9188 27.53 / 0.8378 25.44 / 0.7638</cell><cell>37.78 / 0.9597 34.09 / 0.9248 31.74 / 0.8893 33.28 / 0.9142 30.00 / 0.8350 28.26 / 0.7723 32.08 / 0.8978 28.96 / 0.8001 27.40 / 0.7281 31.31 / 0.9195 27.56 / 0.8376 25.50 / 0.7630</cell><cell>37.57 / 0.9586 33.86 / 0.9228 31.52 / 0.8864 33.09 / 0.9129 29.88 / 0.8331 28.11 / 0.7699 32.15 / 0.8995 28.86 / 0.7987 27.32 / 0.7266 30.96 / 0.9169 27.28 / 0.8334 25.36 / 0.7614</cell><cell>37.91 / 0.9600 34.17 / 0.9271 32.12 / 0.8941 33.70 / 0.9182 30.16 / 0.8414 28.41 / 0.7816 32.23 / 0.8999 29.12 / 0.8060 27.62 / 0.7355 32.30 / 0.9296 28.13 / 0.8514 26.27 / 0.7890</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Q</cell><cell>JPEG</cell><cell>ARCNN [15]</cell><cell>TNRD [10]</cell><cell>DnCNN [57]</cell><cell>MemNet [48]</cell><cell>MWCNN</cell></row><row><cell>Classic5</cell><cell>10 20 30 40</cell><cell>27.82 / 0.7595 30.12 / 0.8344 31.48 / 0.8744 32.43 / 0.8911</cell><cell>29.03 / 0.7929 31.15 / 0.8517 32.51 / 0.8806 33.34 / 0.8953</cell><cell>29.28 / 0.7992 31.47 / 0.8576 32.78 / 0.8837 -</cell><cell>29.40 / 0.8026 31.63 / 0.8610 32.91 / 0.8861 33.77 / 0.9003</cell><cell>29.69 / 0.8107 31.90 / 0.8658 --</cell><cell>30.01 / 0.8195 32.16 / 0.8701 33.43 / 0.8930 34.27 / 0.9061</cell></row><row><cell>LIVE1</cell><cell>10 20 30 40</cell><cell>27.77 / 0.7730 30.07 / 0.8512 31.41 / 0.9000 32.35 / 0.9173</cell><cell>28.96 / 0.8076 31.29 / 0.8733 32.67 / 0.9043 33.63 / 0.9198</cell><cell>29.15 / 0.8111 31.46 / 0.8769 32.84 / 0.9059 -</cell><cell>29.19 / 0.8123 31.59 / 0.8802 32.98 / 0.9090 33.96 / 0.9247</cell><cell>29.45 / 0.8193 31.83 / 0.8846 --</cell><cell>29.69 / 0.8254 32.04 / 0.8885 33.45 / 0.9153 34.45 / 0.9301</cell></row><row><cell cols="4">evaluate SISR performance. Our MWCNN is compared with eight CNN-based SISR methods, including RCN [46], VDSR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Average PSNR(dB) / SSIM results of the competing methods for JPEG image artifacts removal with quality factors Q = 10, 20, 30 and 40 on datasets Classic5 and LIVE1. Red color indicates the best performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Run time (in seconds) of the competing methods for the three tasks on images of size 256×256, 512×512 and 1024×1024: image denosing is tested on noise level 50, SISR is tested on scale ×2, and JPEG image deblocking is tested on quality factor 10.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Denoising</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>TNRD [10]</cell><cell cols="4">DnCNN [57] RED30 [37] MemNet [47] MWCNN</cell></row><row><cell>256×256</cell><cell>0.010</cell><cell>0.0143</cell><cell>1.362</cell><cell>0.8775</cell><cell>0.0586</cell></row><row><cell>512×512</cell><cell>0.032</cell><cell>0.0487</cell><cell>4.702</cell><cell>3.606</cell><cell>0.0907</cell></row><row><cell>1024×1024</cell><cell>0.116</cell><cell>0.1688</cell><cell>15.77</cell><cell>14.69</cell><cell>0.3575</cell></row><row><cell></cell><cell></cell><cell cols="2">Single Image Super-Resolution</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>VDSR [29]</cell><cell cols="2">LapSRN [31] DRRN [47]</cell><cell cols="2">MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0172</cell><cell>0.0229</cell><cell>3.063</cell><cell>0.8774</cell><cell>0.0424</cell></row><row><cell>512×512</cell><cell>0.0575</cell><cell>0.0357</cell><cell>8.050</cell><cell>3.605</cell><cell>0.0780</cell></row><row><cell>1024×1024</cell><cell>0.2126</cell><cell>0.1411</cell><cell>25.23</cell><cell>14.69</cell><cell>0.3167</cell></row><row><cell></cell><cell></cell><cell cols="2">JPEG Image Artifacts Removal</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell cols="2">ARCNN [15] TNRD [10]</cell><cell cols="3">DnCNN [57] MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0277</cell><cell>0.009</cell><cell>0.0157</cell><cell>0.8775</cell><cell>0.0531</cell></row><row><cell>512×512</cell><cell>0.0532</cell><cell>0.028</cell><cell>0.0568</cell><cell>3.607</cell><cell>0.0811</cell></row><row><cell>1024×1024</cell><cell>0.1613</cell><cell>0.095</cell><cell>0.2012</cell><cell>14.69</cell><cell>0.2931</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Run time (in seconds) of the competing methods for the three tasks on images of size 256×256, 512×512 and 1024×1024: image denosing is tested on noise level 50, SISR is tested on scale ×2, and JPEG image deblocking is tested on quality factor 10.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Denoising</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>TNRD [10]</cell><cell cols="4">DnCNN [57] RED30 [37] MemNet [47] MWCNN</cell></row><row><cell>256×256</cell><cell>0.010</cell><cell>0.0143</cell><cell>1.362</cell><cell>0.8775</cell><cell>0.0586</cell></row><row><cell>512×512</cell><cell>0.032</cell><cell>0.0487</cell><cell>4.702</cell><cell>3.606</cell><cell>0.0907</cell></row><row><cell>1024×1024</cell><cell>0.116</cell><cell>0.1688</cell><cell>15.77</cell><cell>14.69</cell><cell>0.3575</cell></row><row><cell></cell><cell></cell><cell cols="2">Single Image Super-Resolution</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>VDSR [29]</cell><cell cols="2">LapSRN [31] DRRN [47]</cell><cell cols="2">MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0172</cell><cell>0.0229</cell><cell>3.063</cell><cell>0.8774</cell><cell>0.0424</cell></row><row><cell>512×512</cell><cell>0.0575</cell><cell>0.0357</cell><cell>8.050</cell><cell>3.605</cell><cell>0.0780</cell></row><row><cell>1024×1024</cell><cell>0.2126</cell><cell>0.1411</cell><cell>25.23</cell><cell>14.69</cell><cell>0.3167</cell></row><row><cell></cell><cell></cell><cell cols="2">JPEG Image Artifacts Removal</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell cols="2">ARCNN [15] TNRD [10]</cell><cell cols="3">DnCNN [57] MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0277</cell><cell>0.009</cell><cell>0.0157</cell><cell>0.8775</cell><cell>0.0531</cell></row><row><cell>512×512</cell><cell>0.0532</cell><cell>0.028</cell><cell>0.0568</cell><cell>3.607</cell><cell>0.0811</cell></row><row><cell>1024×1024</cell><cell>0.1613</cell><cell>0.095</cell><cell>0.2012</cell><cell>14.69</cell><cell>0.2931</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc>Run time (in seconds) of the competing methods for the three tasks on images of size 256×256, 512×512 and 1024×1024: image denosing is tested on noise level 50, SISR is tested on scale ×2, and JPEG image deblocking is tested on quality factor 10.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Denoising</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>TNRD [10]</cell><cell cols="4">DnCNN [57] RED30 [37] MemNet [47] MWCNN</cell></row><row><cell>256×256</cell><cell>0.010</cell><cell>0.0143</cell><cell>1.362</cell><cell>0.8775</cell><cell>0.0586</cell></row><row><cell>512×512</cell><cell>0.032</cell><cell>0.0487</cell><cell>4.702</cell><cell>3.606</cell><cell>0.0907</cell></row><row><cell>1024×1024</cell><cell>0.116</cell><cell>0.1688</cell><cell>15.77</cell><cell>14.69</cell><cell>0.3575</cell></row><row><cell></cell><cell></cell><cell cols="2">Single Image Super-Resolution</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell>VDSR [29]</cell><cell cols="2">LapSRN [31] DRRN [47]</cell><cell cols="2">MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0172</cell><cell>0.0229</cell><cell>3.063</cell><cell>0.8774</cell><cell>0.0424</cell></row><row><cell>512×512</cell><cell>0.0575</cell><cell>0.0357</cell><cell>8.050</cell><cell>3.605</cell><cell>0.0780</cell></row><row><cell>1024×1024</cell><cell>0.2126</cell><cell>0.1411</cell><cell>25.23</cell><cell>14.69</cell><cell>0.3167</cell></row><row><cell></cell><cell></cell><cell cols="2">JPEG Image Artifacts Removal</cell><cell></cell><cell></cell></row><row><cell>Size</cell><cell cols="2">ARCNN [15] TNRD [10]</cell><cell cols="3">DnCNN [57] MemNet [37] MWCNN</cell></row><row><cell>256×256</cell><cell>0.0277</cell><cell>0.009</cell><cell>0.0157</cell><cell>0.8775</cell><cell>0.0531</cell></row><row><cell>512×512</cell><cell>0.0532</cell><cell>0.028</cell><cell>0.0568</cell><cell>3.607</cell><cell>0.0811</cell></row><row><cell>1024×1024</cell><cell>0.1613</cell><cell>0.095</cell><cell>0.2012</cell><cell>14.69</cell><cell>0.2931</cell></row><row><cell cols="6">MWCNN variants, including: (i) MWCNN (Haar): the de-fault MWCNN with Haar wavelet, (ii) MWCNN (DB2): MWCNN with Daubechies-2 wavelet, and (iii) MWCNN (HD): MWCNN with Haar in contracting subnetwork and Daubechies-2 in expanding subnetwork. Then, ablation ex-periments are provided for verifying the effectiveness of ad-ditionally embedded wavelet: (i) the default U-Net with same architecture to MWCNN, (ii) U-Net+S: using sum connection instead of concatenation, and (iii) U-Net+D: adopting learnable conventional downsamping filters in-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison in terms of average PSNR (dB) and run time (in seconds): image denosing is tested on noise level 50 and JPEG image deblocking is tested on quality factor 10.</figDesc><table><row><cell>Dataset</cell><cell>Dilated [55]</cell><cell>Dilated-2</cell><cell>U-Net [41]</cell><cell>U-Net+S</cell><cell>U-Net+D</cell><cell>DCF [21]</cell><cell>WaveResNet [5]</cell><cell>MWCNN (Haar)</cell><cell>MWCNN (DB2)</cell><cell>MWCNN (HD)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">JPEG Image Artifacts Removal (PC=10)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classic5 LIVE1</cell><cell>29.72 / 0.287 29.49 / 0.354</cell><cell>29.49 / 0.302 29.26 / 0.376</cell><cell>29.61 / 0.093 29.36 / 0.112</cell><cell>29.60 / 0.082 29.36 / 0.109</cell><cell>29.68 / 0.097 29.43 / 0.120</cell><cell>29.57 / 0.104 29.38 / 0.155</cell><cell>-/ --/ -</cell><cell>30.01 / 0.088 29.69 / 0.112</cell><cell>30.04 / 0.195 29.70 / 0.265</cell><cell>29.97 / 0.136 29.66 / 0.187</cell></row><row><cell cols="6">stead of Max pooling. Two 24-layer dilated CNNs are also considered: (i) Dilated: the hybrid dilated convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>PSNR and run time results of MWCNNs with the levels of 1 to 4 (i.e., MWCNN-1 ∼ MWCNN-4). It can be observed that MWCNN-3 with 24-layer architecture performs much better than MWCNN-1 and MWCNN-2, while MWCNN-4 only performs negligibly better than MWCNN-3 in terms of the PSNR metric. Moreover, the speed of MWCNN-3 is also moderate compared with other levels. Taking both efficiency and performance gain into account, we choose MWCNN-3 as the default setting.</figDesc><table><row><cell>Dataset</cell><cell>MWCNN-1</cell><cell>MWCNN-2</cell><cell>MWCNN-3</cell><cell>MWCNN-4</cell></row><row><cell>Set12</cell><cell>27.14 / 0.047</cell><cell>27.62 / 0.068</cell><cell>27.74 / 0.082</cell><cell>27.74 / 0.091</cell></row><row><cell>BSD68</cell><cell>26.16 / 0.044</cell><cell>26.45 / 0.063</cell><cell>26.53 / 0.074</cell><cell>26.54 / 0.084</cell></row><row><cell>Urban100</cell><cell>26.08 / 0.212</cell><cell>27.10 / 0.303</cell><cell>27.42 / 0.338</cell><cell>27.44 / 0.348</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partially supported by the National Science Foundation of China (NSFC) (grant No.s 61671182 and 61471146).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<ptr target="http://vision.ee.ethz.ch/ntire18" />
	</analytic>
	<monogr>
		<title level="m">Ntire 2018 super resolution challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust image denoising with multi-column deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1493" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multiresolution signal decomposition: transforms, subbands, and wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Akansu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Haddad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond deep residual learning for image restoration: Persistent homology-guided manifold simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1141" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Digital image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Banham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The wavelet transform, time-frequency localization and signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="961" to="1005" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ten lectures on wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accelerating the superresolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One-to-many network for visually pleasing compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep wavelet prediction for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Monga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Framing U-Net via deep convolutional framelets: Application to sparse-view CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08333</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single image superresolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Digital image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image compression using the 2-d wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="250" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning convolutional networks for content-weighted image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Waterloo exploration database: New challenges for image quality assessment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1004" to="1016" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A theory for multiresolution signal decomposition: the wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="674" to="693" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual importance pooling for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of selected topics in signal processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The little engine that could: Regularization by denoising (red)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02862</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EnhanceNet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized deep image to image regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5609" to="5619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Structurepreserving image super-resolution via contextualized multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of jpeg-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep convolutional framelets: A general deep learning for inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
