<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Time-aware Large Kernel Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lioutas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
						</author>
						<title level="a" type="main">Time-aware Large Kernel Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of O(n 2 ). Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size k acting as a limited-window self-attention, resulting in time complexity of O(k·n). In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sequence modeling has seen some great breakthroughs through recent years with the introduction of the use of neural networks. Recurrent neural network methods <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b63">Wu et al., 2016)</ref>, convolution methods <ref type="bibr" target="#b24">(Kim, 2014;</ref><ref type="bibr" target="#b22">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b16">Gehring et al., 2017;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref>, and self-attention approaches <ref type="bibr" target="#b43">(Paulus et al., 2018;</ref><ref type="bibr" target="#b59">Vaswani et al., 2017;</ref><ref type="bibr" target="#b11">Dai et al., 2019;</ref><ref type="bibr" target="#b26">Kitaev et al., 2020)</ref> have all yielded state-ofthe-art results in various NLP tasks such as neural machine translation (NMT) <ref type="bibr" target="#b63">Wu et al., 2016;</ref><ref type="bibr" target="#b4">Britz et al., 2017;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019)</ref>, language modeling <ref type="bibr" target="#b54">(Sundermeyer et al., 2012;</ref><ref type="bibr" target="#b58">Tran et al., 2016;</ref><ref type="bibr" target="#b14">Devlin</ref> Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). <ref type="bibr" target="#b44">Radford et al., 2019)</ref>, automatic summarization <ref type="bibr" target="#b43">(Paulus et al., 2018;</ref><ref type="bibr" target="#b15">Fan et al., 2018;</ref><ref type="bibr" target="#b6">Celikyilmaz et al., 2018)</ref>, named entity recognition <ref type="bibr" target="#b29">(Lample et al., 2016;</ref><ref type="bibr" target="#b14">Devlin et al., 2019)</ref> and sentiment analysis <ref type="bibr" target="#b64">(Xu et al., 2016;</ref><ref type="bibr" target="#b48">Sachan et al., 2019)</ref>.</p><p>Seemingly all modern approaches of sequence encoding rely on the use of attention to "filter" the excessive information given at a current time-step. Attention can be expressed as the weighted sum over context representations using attention weights that are usually generated from the context representations (self-attention) <ref type="bibr" target="#b8">(Cheng et al., 2016)</ref>. The transformer network <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref> assigns attention weights for a given time-step to all available context token representations, while the newly proposed dynamic convolution  only computes an attention over a fixed context window.</p><p>Self-attention over all context tokens is computationally very expensive. Specifically, the transformer network has a time complexity of O(n 2 ) where n is the length of the input sequence. Thus, modeling long-range dependencies becomes very challenging and the practicality of the selfattention method has been questioned. The more recent approach of dynamic convolutions  successfully reduced the time complexity to O(k·n) where k is the kernel size specified for each layer.</p><p>In this paper, we introduce a novel type of adaptive convolution, Time-aware Large Kernel (TaLK) convolutions, that learns the kernel size of a summation kernel for each time-step instead of learning the kernel weights as in a typical convolution operation. For each time-step, a function is responsible for predicting the appropriate size of neighbor representations to use in the form of left and right offsets relative to the time-step. The result is an efficient encoding method that reduces the time complexity to O(n) and uses fewer parameters than all other methods. The method employs the fast Parallel Prefix Sum <ref type="bibr" target="#b28">(Ladner &amp; Fischer, 1980;</ref><ref type="bibr" target="#b61">Vishkin, 2003)</ref> operation which has a time complexity of O(log(n)) to compute the integral image <ref type="bibr" target="#b31">(Lewis, 1994)</ref>, also known as summed-area table in the Computer Vision literature. This needs to be computed only once and can be used to calculate any summation between two boundary tokens in O(1). Applying it on a sequence with length n only needs O(n) time. To summarize, the contributions of arXiv:2002.03184v2 <ref type="bibr">[cs.</ref>LG] 19 Jun 2020 this work are three-fold:</p><p>• We introduce a novel adaptive convolution based on summation kernel for sequence encoding.</p><p>• We show both analytically and empirically that the proposed kernel method has a smaller time complexity; it is faster than previous state-of-the-art approaches and is able to encode longer sentences quicker and with a smaller running memory footprint.</p><p>• We evaluate our method on three NLP tasks, machine translation, abstractive summarization and language modeling. We show that the proposed method can get comparative performance with previous methods on WMT En-De and WMT En-Fr benchmarks in machine translation, and set a new state-of-the-art result on the IWSLT De-En and CNN-DailyMail datasets, while in language modeling our method is able to perform comparatively with self-attention and outperform dynamic convolutions on the WikiText-103 benchmark dataset.</p><p>Our code and pre-trained models are available at github.com/lioutasb/TaLKConvolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we provide a brief review over various related sequence modeling methods, and related methods that enlarge the receptive filed of a convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sequence Modeling</head><p>Sequence modeling is an important task in machine learning. An effective system should be able to comprehend and generate sequences similar to real data. Traditional approaches typically rely on the use of various kinds of recurrent neural networks such as long-short term memory networks <ref type="bibr" target="#b21">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b55">Sutskever et al., 2014;</ref><ref type="bibr" target="#b32">Li et al., 2016;</ref> and gated recurrent unit networks <ref type="bibr" target="#b9">(Cho et al., 2014;</ref><ref type="bibr" target="#b39">Nabil et al., 2016)</ref>. These recurrent approaches are auto-regressive, which slows the process down for long sequences since they linearly depend on their own previous output tokens. Recent work is focused on exploring convolutional neural networks (CNN) methods <ref type="bibr" target="#b23">(Kalchbrenner et al., 2016;</ref><ref type="bibr" target="#b16">Gehring et al., 2017;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref> or self-attention methods <ref type="bibr" target="#b59">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b66">Zhang et al., 2018;</ref><ref type="bibr" target="#b11">Dai et al., 2019;</ref><ref type="bibr" target="#b26">Kitaev et al., 2020)</ref> which both facilitate the parallelization of the encoding process. In addition, since they are not auto-regressive, they allow the encoding process to capture stronger global and local dependencies.</p><p>Recently, <ref type="bibr" target="#b62">Wu et al. (2019)</ref> proposed an alternative method to the original self-attention approach. Their window-based attention method with window size k can perform comparatively with self-attention modules that have access to all available tokens at each time-step. They utilize a depthwise convolution with a generated softmax normalized kernel of size k for every time-step. This brings down the time complexity to O(k·n) from the quadratic complexion of the original self-attention model. However, this method has the drawback of being memory intensive for long sequences depending on the implementation used. Moreover, supporting larger kernel sizes can have a negative impact to running time. In another work, <ref type="bibr" target="#b51">Shen et al. (2018)</ref> proposed computing intra-block self-attention weights within blocks of the input sequence and inter-block attention between all blocks to reduce the running memory of the full self-attention approach.</p><p>Our method differs from all these previous approaches in two main aspects. Specifically, instead of having all (or some) tokens available and then deciding which ones are needed to encode a time-step, we start from the current timestep representation and try to expand to the neighbor tokens in an adaptive manner. Additionally, instead of using attention for filtering the tokens used for encoding a time-step, we use all the information available in an adaptively decided window by utilizing a summation convolution kernel with summed-area tables, which improves upon previously proposed methodology by allowing us to reduce the complexity to O(n), to produce a smaller running memory footprint, and to use less parameters than all other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamically Sized Receptive Field</head><p>Increasing the receptive field of a convolution layer without adding a computation overhead is a challenging task. By making deeper CNN models, we may be able to accumulate many fixed-sized receptive fields, however this comes at the cost of high computational demands. Nevertheless, this approach is shown to be successful in multiple state-of-theart vision models <ref type="bibr" target="#b18">(He et al., 2016;</ref><ref type="bibr" target="#b56">Szegedy et al., 2016)</ref>. The overhead issue is often mitigated using a form of downsampling, either via pooling layers <ref type="bibr" target="#b30">(Lecun et al., 1998)</ref> or strided convolutions <ref type="bibr" target="#b52">(Springenberg et al., 2015)</ref>. <ref type="bibr" target="#b65">Yu &amp; Koltun (2016)</ref> proposed dilated convolutions, a method for enlarging the convolution kernel size by skipping intermediate pixels and thus, requiring less multadds operations.</p><p>The first work that suggested the use of learnable sized convolution kernels was box convolutions <ref type="bibr" target="#b5">(Burkov &amp; Lempitsky, 2018)</ref>. The idea of using box filters with summed-area tables <ref type="bibr" target="#b10">(Crow, 1984)</ref>, commonly known as integral images dates back many years and it is well-known to the Computer Vision community, as it became particularly popular with the work of <ref type="bibr" target="#b60">Viola &amp; Jones (2001)</ref> in object detection. The summed-area table can be efficiently parallelized using the Parallel Prefix Sum method <ref type="bibr" target="#b28">(Ladner &amp; Fischer, 1980)</ref>. This operation can be further accelerated as a hardware functional unit dedicated to compute the multi-parameter prefix-sum operation <ref type="bibr" target="#b61">(Vishkin, 2003)</ref>. <ref type="bibr" target="#b5">Burkov &amp; Lempitsky (2018)</ref> method is optimizing the kernel size parameters using approximate gradients by normalizing the sum by the area of the box. <ref type="bibr" target="#b67">Zhang et al. (2019)</ref> extended this idea by using interpolation to exploit non-integer coordinates. Inspired by this idea, we develop the proposed method for one-dimensional case of sequences. In contrast to the two previous methods, instead of using a fixed number of learnable sized kernels, we adaptively condition the size of the kernel on each input representation, effectively generating a different kernel size for each time-step token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we present the proposed adaptive Timeaware Large Kernel (TaLK) Convolution method. First, we introduce the approach that computes a convolution operation using large kernels in O(n) time, which assumes that left and right offsets are given. Next, we present our proposed method for generating offsets dynamically for each time-step. We then expand upon our method to use multiple heads and normalize the summed output vector. Next, we explain how to use TaLK Convolutions for decoding. Finally, we present the computational complexity analysis and comparison for the proposed method. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the Time-aware Large Kernel Convolution operation for a specific time-step during encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">One-dimensional Large Kernel Convolution</head><p>Let X = {x 1 , x 2 , . . . , x n } denote an input sequence, where n is the length of the sequence, x i ∈ R d is the current input representation for the i-th word (i.e., the i-th time-step) and d denotes the dimensionality of the vector representation (i.e., the number of channels).</p><p>The goal of this paper is to reduce the encoding time complexity for sequence modeling to O(n). In other words, we set out to make the encoding at each time-step independent of the size of the receptive field. In addition, we want to explore alternative methods to the successful self-attention mechanism by equally using the number of neighbor tokens to represent a time-step instead of generating an attention distribution over the tokens. Specifically, we assume that simply summing the appropriate number of token representations is enough to represent the current time-step. That is, we encode the representation at the i-th time-step by</p><formula xml:id="formula_0">o i = α r i j=α l i x j ,<label>(1)</label></formula><p>where 1 ≤ α l i ≤ i ≤ α r i ≤n are the lower (left offset) and upper (right offset) bounds of the kernel size.</p><p>Applying Equation 1 for each time-step i separately is inefficient since we do repetitive summations over the same representations. <ref type="bibr" target="#b67">Zhang et al. (2019)</ref> showed that using the summed-area table <ref type="bibr" target="#b10">(Crow, 1984)</ref>, we can accelerate a summation convolution operation to any kernel size. Specifically, let S = {S 0 , S 1 , S 2 , . . . , S n } be the summed-area table computed using</p><formula xml:id="formula_1">S 0 = 0, S i = S i−1 + x i , 1 ≤ i ≤ n.</formula><p>(2)</p><p>Given the left offset α l i and the right offset α r i , we can compute the summation denoted as o i of the features between these offsets using the summed-area table</p><formula xml:id="formula_2">o i = S a r i − S a l i −1 (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Time-aware Large Kernel Generation</head><p>Given the one-dimensional large kernel convolution above, it is important to determine the left and right offsets for computing representations at each time-step. The key of the proposed method is an adaptive time-aware large kernel convolution operation which has kernel sizes that vary over time as a learned function of the individual time steps; that is, we propose to learn the offsets of the summation kernel above for each time-step.</p><p>Specifically, we propose to use a function f {l,r} : R d → R to generate for each x i the leftã l i and rightã r i relative offsets, whereã</p><formula xml:id="formula_3">{l,r} i = σ(f {l,r} (x i )) ∈ [0, 1].</formula><p>For each a {l,r} i relative offset, we convert it to the absolute offset counterpart in the following way</p><formula xml:id="formula_4">a l i = i −ã l i · l max a r i = i +ã r i · r max ,<label>(4)</label></formula><p>where l max ∈ Z ≥0 is the maximum allowed tokens to the left and r max ∈ Z ≥0 is the maximum allowed tokens to the right.</p><p>The absolute offsets up to this point represent real positive numbers. In the next step, we need to convert these numbers to integer indexes so we can select from the summed-area table using the Equation <ref type="formula">(3)</ref>. Inspired by <ref type="bibr" target="#b67">Zhang et al. (2019)</ref>, we use one-dimensional interpolation to sample from the summed-area table by using the positive real-valued offsets a l i , a r i as follows</p><formula xml:id="formula_5">S a l i −1 = γ l · S a l i −1 + (1 − γ l ) · S a l i −1 , S a r i = (1 − γ r ) · S a r i + γ r · S a r i ,<label>(5)</label></formula><p>where . and . are the floor and ceiling operators, γ l = a l i − a l i and γ r = a r i − a r i . The above equation is continuous and differentiable in the interpolation neighborhood. The partial derivatives of S a {l,r} i with respect toã {l,r} i are given by</p><formula xml:id="formula_6">∂S a l i −1 ∂ã l i = l max (S a l i −1 − S a l i −1 ), ∂S a r i ∂ã r i = r max (S a r i − S a r i ).<label>(6)</label></formula><p>The partial derivatives of S a {l,r} i with respect to S a {l,r} i and S a {l,r} i tokens are given by</p><formula xml:id="formula_7">∂S a l i −1 ∂S a l i −1 = γ l , ∂S a l i −1 ∂S a l i −1 = (1 − γ l ), ∂S a r i ∂S a r i = (1 − γ r ), ∂S a r i ∂S a r i = γ r .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Output Normalization and Offsets Dropout</head><p>The idea of summing all the features in a window of size [a l i , a r i ] works well for shallow models. However, as the representation vectors at different time-steps are computed from summations over different numbers of neighbors, their magnitudes of values can be different. As we introduce more layers, the disproportional magnitude of the inputs makes learning harder for the nodes in the layers that follow. To address this problem, we propose to normalize the output representations of TaLK Convolutions as follows</p><formula xml:id="formula_8">o i = o i · 1 l max + r max + 1 .<label>(8)</label></formula><p>Such a simple window size based normalization can effectively get rid of the output magnitude differentiation problem resulted from summation kernels.</p><p>In addition, we regularize the predicted offsetsã {l,r} i using Dropout <ref type="bibr" target="#b20">(Hinton et al., 2012;</ref><ref type="bibr" target="#b53">Srivastava et al., 2014)</ref>. Specifically, during training we drop out every predicted offset with probability p. This helps to prevent the model from quickly optimizing towards a specific window size and be able to generate more diverse offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-headed Kernels</head><p>Although the offset computation above provides a mechanism that offers adaptive receptive fields for summation kernels at different time steps, a single pair of left and right offsets for all d dimensions cannot yield good results, as different features might be related to their counterpart in the neighbor tokens in different way. Inspired by the idea of multi-head attention <ref type="bibr" target="#b59">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref>, we further propose to extend our proposed convolution kernel into a multi-head version by allowing different representation features, i.e., channels, to have different left and right offsets for each time-step. Moreover, instead of having entirely different convolution offsets across multiple channels, we adopt a depthwise version by separating the feature channels into multiple groups, each of which share the same pair of left and right offsets.</p><p>Specifically, we tie every subsequent number of R = d H channels together and group the channels into H groups for each x i , where H is the number of heads. This results tô X = {x 1 ,x 2 , . . . ,x n }, wherex i ∈ R H×R . Then we use a function f {l,r} : R H×R → R H to generate for eachx i a vector of H left relative offsetsα l i or right relative offsets α r i viaα</p><formula xml:id="formula_9">{l,r} i = σ(f {l,r} (x i )) ∈ [0, 1] H .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Decoding Using TaLK Convolutions</head><p>In an encoder/decoder sequence generation scheme , the encoder part of the model has access to both past and future tokens. The decoding part, however, must have access only to past tokens that are generated so far. Enforcing this with TaLK Convolutions is straightforward by setting the r max value to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Module Architecture and Implementation</head><p>For sequence modeling, we follow a similar module architecture as described in <ref type="bibr" target="#b62">Wu et al. (2019)</ref>. Specifically, we apply a linear layer to project the input embedding tokens from d to 2d and then we apply a gated linear unit (GLU) . Next, we apply the TaLK Convolution operation as described in Section 3.2. Finally, we apply a projection layer to the output representations from TaLK Convolution with size W ∈ R d×d . <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the <ref type="table">Table 1</ref>. Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and n buckets is the number of hash buckets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Type</head><p>Complexity per Layer Sequential Maximum Path Length Operations Recurrent  O <ref type="bibr" target="#b23">(Kalchbrenner et al., 2016;</ref><ref type="bibr" target="#b16">Gehring et al., 2017)</ref> O  O(k · n · d) O(1) O(n/k) Reformer <ref type="bibr" target="#b26">(Kitaev et al., 2020)</ref> O(n · log(n) · d) O(log <ref type="formula">(</ref>  TaLK Convolution unit. In addition, we substitute all ReLU activation functions with the Swish function <ref type="bibr" target="#b47">(Ramachandran et al., 2017)</ref>. Similar to <ref type="bibr" target="#b59">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref>, the overall architecture is composed by the TaLK Convolution unit which replaces the self-attention or the dynamic convolution unit followed by the position-wise feed-forward network (FFN) unit.</p><formula xml:id="formula_10">(n · d 2 ) O(n) O(n) Convolutional</formula><formula xml:id="formula_11">(k · n · d 2 ) O(1) O(log k (n)) or O(n/k) Self-Attention (Vaswani et al., 2017) O(n 2 · d) O(1) O(1) Dynamic Convolutions</formula><p>The summed-area table (Equation 2) can be efficiently computed on a GPU by performing a fast Parallel Prefix Sum <ref type="bibr" target="#b28">(Ladner &amp; Fischer, 1980)</ref> over the token dimension. This operation is usually efficiently implemented on modern deep learning frameworks under the name of cumulative sum. Applying the relative offsets to the summed-area table using core functions from deep learning frameworks is not a trivial task. Such an implementation is usually very inefficient leading to slower computation and memory overhead. For this reason, we implemented the operation using CUDA kernels that enabled us to parallelize the computation for each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Computational Complexity</head><p>In this section we compare the complexity of the TaLK Convolution operation against different modules for encoding an input sequence of representations. This comparison is shown on <ref type="table">Table 1</ref>. We follow a similar comparison as analyzed by <ref type="bibr" target="#b59">Vaswani et al. (2017)</ref>. Our comparison is based on three criteria: the time complexity of the operation, the amount of computations that can be executed in parallel and the path length between long-range dependencies. <ref type="table">Table 1</ref>, our proposed method requires the least number of operations. Specifically, it has a linear time complexity to encode a sequence and does not depend on hyper-parameter decisions such as the kernel size. In terms of the number of computations that can be parallelized, our method needs logarithmic time to compute the summed-area table (Equation 2). It is true that our method does not have a constant number of sequentially executed operations like all the other non-autoregressive counterpart methods, but the logarithmic time our method is requiring is inexpensive to compute even for very long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>It is shown by <ref type="bibr" target="#b27">Kolen &amp; Kremer (2001)</ref> that a short path between any combination of token positions in the input and output sequences makes it easier to learn long-range dependencies. In practice, doubts have been cast over the ability of self-attention to model long-range dependencies <ref type="bibr" target="#b57">(Tang et al., 2018;</ref><ref type="bibr" target="#b62">Wu et al., 2019)</ref>. <ref type="bibr" target="#b62">Wu et al. (2019)</ref> showed that using a limited context window can outperform selfattention. Our method has the advantage that the number of required computations is independent of the maximum window size and thus, it can be tuned or learned without extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation</head><p>We evaluated our proposed encoding technique on machine translation, abstractive summarization and language mod- eling. These three tasks are considered touchstone and challenging in the NLP field.</p><p>Machine Translation On the machine translation task, we report results on three mainstream benchmark datasets: WMT English to German (En-De), WMT English to French (En-Fr) and IWSLT German to English (De-En).</p><p>For all datasets, we replicated the pre-processing steps mentioned in <ref type="bibr" target="#b62">Wu et al. (2019)</ref>. Specifically, for the WMT En-De we used the WMT'16 training data that consists of 4.5M sentence pairs. We validated on newstest2013 and tested on newstest2014. We employed byte-pair encoding (BPE) <ref type="bibr" target="#b49">(Sennrich et al., 2016)</ref> to the sentences, with a 32K joint source and target vocabulary. For the WMT En-Fr, we used 36M training sentence pairs from WMT'14. We validated on newstest2012+2013 and tested on newstest2014 evaluation datasets. Using BPE, we generated a joint vocabulary between the source and the target languages of size 40K tokens. The IWSLT De-En consists of 160K training sentence pairs. We lower cased all sentences and used a 10K joint BPE vocabulary.</p><p>For all datasets, we measured case-sensitive tokenized BLEU scores using multi-bleu 1 . Similarly to Vaswani 1 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi%2Dbleu.perl et al. <ref type="formula" target="#formula_0">(2017)</ref>, we applied compound splitting for WMT En-De. We trained five random initializations of each model configuration and report test accuracy of the seed which resulted in the highest validation BLEU score. For all datasets, we used beam search with beam width 5. Similar to <ref type="bibr" target="#b62">Wu et al. (2019)</ref>, we tuned a length penalty as well as the number of checkpoints to average on the validation set.</p><p>Abstractive Summarization For the abstractive summarization task, we decided to experiment with the CNN-DailyMail <ref type="bibr" target="#b19">(Hermann et al., 2015;</ref><ref type="bibr" target="#b40">Nallapati et al., 2016)</ref> dataset. The dataset is composed by approximately 280K news articles with associated multi-sentence summaries. We followed the same pre-processing steps as described by <ref type="bibr" target="#b62">Wu et al. (2019)</ref>. The BPE vocabulary consists of 30K subword tokens. We report results using the F1-Rouge (Rouge-1, Rouge-2 and Rouge-L) metric <ref type="bibr" target="#b34">(Lin, 2004)</ref>. For generating the summaries, similar to <ref type="bibr" target="#b62">Wu et al. (2019)</ref> we tuned the maximum output length, disallowing repeating the same trigram, and we apply a stepwise length penalty.</p><p>Language Modeling We experimented on the WikiText-103 <ref type="bibr" target="#b36">(Merity et al., 2017)</ref> benchmark dataset. The training data contains approximately 100M tokens. A vocabulary of about 260K tokens was used, by discarding all tokens with a frequency below 3 as described in <ref type="bibr" target="#b36">Merity et al. (2017)</ref>. We followed  and applied adaptive input representations. We replicated their setup and partition the training data into blocks of 512 contiguous tokens while ignoring document boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment Details</head><p>Hyper-Parameters For the machine translation models, we followed the same hyper-parameter setup as described in <ref type="bibr" target="#b62">Wu et al. (2019)</ref>. Specifically, we follow for WMT En-De and WMT En-Fr datasets the model hidden size d was set to 1024, the feed-forward hidden size d ff was set to 4096 and the number of layers for the encoder and the decoder was set to 7 and 6 respectively. The number of heads was set to 16 and the l max , r max values to 3, 7, 15, 31×4 for each layer. For IWSLT De-En, the model hidden size d was set to 512, the feed-forward hidden size d ff was set to 1024 and the number of layers for the encoder and the decoder was set to 7 and 6 respectively. The number of heads was set to 4 and the l max , r max values to 1, 3, 7, 15×4 for each layer.</p><p>For the abstractive summarization models, we tested our method on two types of model configurations, the Standard and the Deep configurations. Both settings have a hidden size d of 512, a feed-forward hidden size d ff of 2048 and number of heads to be 8. The Standard model has 7 encoder and 6 decoder layers with the l max , r max values be 3, 7, 15, 31×4 for each layer. The Deep model has 10 layers for both the encoder and decoder with the l max values be 3, 7, 15, 31×7 and the r max be 3, 7, 31×8.</p><p>For the language model, we followed the same configuration as . We used 17 decoding layers, each layer with a 1024 hidden size, a 4096 feed-forward hidden size and 8 heads. The adaptive input factor was set to 4. The l max values were set to 3, 7, 15, 31, 63×12 and r max to zero for each layer.</p><p>Optimization We used the Adam optimizer <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2015)</ref> with default values. In addition, our models were optimized using the cosine learning rate schedule <ref type="bibr" target="#b35">(Loshchilov &amp; Hutter, 2017)</ref>. We linearly warmed up for 10K steps from 10 −7 to 10 −3 . For IWSLT De-En, we used the inverse square root learning rate schedule <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref>. We set the dropout to 0.3 for WMT En-De and IWSLT De-En and 0.1 for WMT En-Fr.</p><p>For the WMT En-De and WMT En-Fr benchmarks, the batch size was set to 3,584 tokens per batch, per GPU. We accumulated the gradients for 16 batches before applying an update which results in an effective batch size of 450K tokens. We trained the WMT En-De model for 30K steps and the WMT En-Fr for 80K steps. For IWSLT De-En, we trained on a single GPU with 4,000 maximum tokens per batch for 50K steps. For the abstractive summarization models we followed the same setup as in <ref type="bibr" target="#b62">Wu et al. (2019)</ref> and for the language model the same setup as in .</p><p>Hardware Details We trained the WMT En-De, WMT En-Fr, CNN-DailyMail and WikiText-103 models on 8 NVIDIA RTX 2080 Ti GPUs using mixed-precision training <ref type="bibr" target="#b38">(Micikevicius et al., 2018)</ref> and the IWSLT De-En model using a single GPU. We employed our own CUDA implementation, wrapped as a standalone PyTorch layer for the TaLK Convolution operation. All experiments were run using the Fairseq <ref type="bibr" target="#b42">(Ott et al., 2019)</ref> toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Machine Translation</head><p>We demonstrate the effectiveness of our model in the WMT En-De and WMT En-Fr translation benchmarks. <ref type="table" target="#tab_0">Table 2</ref> shows that our method is able to achieve comparable results to current state-of-the-art methods. Specifically, our method was able to match the state-of-the-art score on WMT En-Fr, a benchmark dataset that is considered indicative for the effectiveness of a method due to the large number of training examples (36M) it contains. Additionally for WMT En-De, our method is only 0.1 BLEU points behind the current state- <ref type="table">Table 6</ref>. Throughput and memory consumption decrease measured for different sequence lengths (n) on a batch of size 10 with each token being represented with d = 1024 and H = 16. Throughput is calculated across 100K iterations of a single input encoding execution for each method. Memory decrease is computed as how many times less memory we need to encoding the input embedding compared to Self-Attention. Larger numbers indicate better performance.</p><p>Method n = 10 n = 100 n = 1, 000 n = 10, 000 iter/sec Mem. ↓ iter/sec Mem. ↓ iter/sec Mem. ↓ iter/sec Mem. ↓ of-the-art score. It is important to underline, however, that our method uses the least number of parameters compared to the other counterpart methods. <ref type="table">Table 3</ref> shows results for IWSLT De-En benchmark dataset. Following <ref type="bibr" target="#b62">Wu et al. (2019)</ref>, we employed a smaller model with less parameters to reflect the size of the dataset. Specifically, we set d to 512, d ff to 1024 and H to 4. Furthermore, we disabled the GLU unit that is described in Section 3.6 and made the input projection layer to size W ∈ R d×d . Our method was able to outperform all other methods setting a new state-of-the-art result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Abstractive Summarization</head><p>We evaluated the proposed method on the task of abstractive summarization. We test the method's ability to process long documents on the CNN-DailyMail dataset. We encode an article of up to 400 sub-words and we generate a summarization composed from multiple sentences. <ref type="table" target="#tab_1">Table 4</ref> shows the results of our experiments. Our Standard model is able to achieve better results on the Rouge-1 and Rouge-2 metrics than previous methods. In addition, the Standard model is using significantly less parameters, approximately 30M parameters less. The Deep model uses more layers to closely match the number of parameters and is able to outperform all previous models. This shows that our method is able to encode long sequences successfully without having the need to have access to all context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on Language Modeling</head><p>We evaluated our method on the task of language modeling. We considered the WikiText-103 benchmark dataset and we compared against recent methods in the literature. Particularly, we followed the setup that was implemented in the adaptive inputs baseline . This work suggest the use of self-attention with adaptive input representations. We substituted the self-attention module with our method. In order to assimilate the number of parameters used in their experiments, we increased the number of layers by one. As seen on <ref type="table">Table 5</ref>, our method yields better perplexity than dynamic convolutions when trained using the same settings, including the same maximum kernel size. In addition, we get comparative performance with self-attention. Moreover, we use less number of parameters than the best comparison methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Encoding Inference Speed Comparison</head><p>We also compared our method against other nonautoregressive methods in terms of encoding inference speed and memory consumption. We measured the speed using a single NVIDIA RTX 2080 Ti GPU with full precision floating-point arithmetic (FP32). Specifically, we measured the throughput of encoding a batch of size B = 10, d = 1024 and H = 16. For each method, we only took into consideration the time it takes to process using the core approach of each encoding method.</p><p>For self-attention <ref type="bibr" target="#b59">(Vaswani et al., 2017)</ref>, we only timed the attention operation softmax( QK T √ d k )V . For dynamic convolutions , we only timed the operation DepthwiseConv(X, softmax(W dyn ), i, c) where W dyn ∈ R n·B·H×K is the generated kernel for each time-step. The authors of dynamic convolutions proposed two ways of implementing their method. The first method uses the standard convolution unfolding function which is faster for longer sequences. The second approach is the band matrix trick method which copies and expands the normalized weights matrix into a band matrix. This second approach yields faster execution time for shorter sequences but is more memory intensive. In order to be fair, in our experiments we used unfolding to sequences longer than 500 tokens and band matrices for shorter sequences. We also set K to 3 and 31, the first being the smallest kernel size dynamic convolutions use and the second being the largest. Finally, for our method we measured the time to compute the large kernel convolution operation given the relative offsets. We evaluated for 100K iterations across four different sequence lengths n. <ref type="table">Table 6</ref> shows that our method yields much better through-  put than all other methods. Specifically, the number of iterations of self-attention per second is comparable to dynamic convolutions for short sentences (n &lt; 500). Our method allows for more sentences to be processed each second, leading to a much higher throughput. For longer sentences, self-attention is notably slower than our method and for the case of n = 10, 000, self-attention was running out-of-memory and was not able to execute an iteration.</p><p>Although our method has a logarithmic time complexity for computing the summed-area table (Section 3.7), the fact that we are computing a much more inexpensive in terms of complexity operation, specifically we only utilize additions, other methods with O(1) complexity are less efficient due to the employment of both multiplication as well as addition operations. Therefore, our method has a considerably higher throughput compared to dynamic convolutions.</p><p>Furthermore, we examined the running memory requirements for all three different non-autoregressive methods. We compared dynamic convolutions and our proposed method against self-attention and report the number of times we reduced the running memory compared to self-attention. For all sequence length cases, our method requires less memory than dynamic convolutions when compared to the "expensive" self-attention operation. The times we were able to decrease the memory consumption can be seen on <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Model Ablation</head><p>In order to evaluate the importance of the different choices for the TaLK Convolutions, we varied our baseline model, described in Section 3.2, using the different proposed extensions mentioned in Sections 3.3 and 3.4. We measured the performance on the validation set of the IWSLT De-En translation benchmark dataset. We used beam search as described in Section 4.1. We report the results in <ref type="table" target="#tab_4">Table 7</ref>.</p><p>Initially, we modified the baseline model with the addition of the output normalization (Section 3.3). As seen in <ref type="table" target="#tab_4">Table  7</ref>, the original method is not able to converge. This validates our intuition that since we are summing the available information inside the kernel, not normalized outputs make learning difficult for the layers that follow. Next, we increased the values l max , r max to allow larger adaptive kernel sizes which yielded a higher performance without additional computation cost. Further, we introduced a dropout unit with probability p = 0.1 on the generated relative offsets. This allowed for the performance to increase further as we stopped the model from overfitting over the same window size. Next, we increased the number of heads H from 1 to 512 (all available dimensions) and we called this fully-head TaLK Convolution. We can see that by treating each of the 512 dimensions separately and generating 512 relative offsets, we were able to increase the performance. However, we believe that by having each dimension generate its own offsets actually brings some noise. Thus, we reduced the number of heads to H = 4 which increased the performance even more. Finally, we show that by substituting the Swish activation function with the ReLU function the performance drops which justifies our decision to use the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we presented Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution method based on summation kernel for sequence representation and encoding. It learns to predict the kernel boundaries for each time-step of the sequence. In contrast to all other nonautoregressive methods, this approach needs true linear time O(n) with respect to the sequence length, while being able to successfully encode a sequence without using the notion of attention. We validated the proposed method on three NLP tasks, machine translation, abstractive summarization and language modeling, and achieved a comparative performance. Moreover, we showed both analytically and empirically that the proposed method is faster than previous approaches and that it is able to encode longer sentences quicker and with a smaller running memory footprint. For future work, we plan to apply our method to other sequential tasks such as question answering and the PG-19 benchmark dataset <ref type="bibr" target="#b46">(Rae et al., 2020)</ref>. We will also explore this novel convolution mechanism in the area of computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The Time-aware Large Kernel convolution operation. For the current time-step, we compute the left and right offsets for each head, and then sum all the representation vectors inside these boundaries. This operation can be efficiently computed using summed-area tables with time complexity O(log(n)) and compute the output representation for each time-step in O(n) time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The proposed TaLK Convolution unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Machine translation accuracy in terms of BLEU for WMT En-De and WMT En-Fr on newstest2014.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Param (En-De) WMT En-De WMT En-Fr</cell></row><row><cell cols="2">Gehring et al. (2017)</cell><cell>216M</cell><cell>25.2</cell><cell>40.5</cell></row><row><cell cols="2">Vaswani et al. (2017)</cell><cell>213M</cell><cell>28.4</cell><cell>41.0</cell></row><row><cell cols="2">Ahmed et al. (2017)</cell><cell>213M</cell><cell>28.9</cell><cell>41.4</cell></row><row><cell cols="2">Chen et al. (2018)</cell><cell>379M</cell><cell>28.5</cell><cell>41.0</cell></row><row><cell cols="2">Shaw et al. (2018)</cell><cell>-</cell><cell>29.2</cell><cell>41.5</cell></row><row><cell cols="2">Ott et al. (2018)</cell><cell>210M</cell><cell>29.3</cell><cell>43.2</cell></row><row><cell cols="2">Wu et al. (2019)</cell><cell>213M</cell><cell>29.7</cell><cell>43.2</cell></row><row><cell cols="2">Kitaev et al. (2020)</cell><cell>213M</cell><cell>29.1</cell><cell>-</cell></row><row><cell cols="2">TaLK Convolution (Ours)</cell><cell>209M</cell><cell>29.6</cell><cell>43.2</cell></row><row><cell cols="3">Table 3. Machine translation accuracy in terms of BLEU on</cell><cell></cell></row><row><cell>IWSLT De-En.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Param IWSLT De-En</cell><cell></cell></row><row><cell>Deng et al. (2018)</cell><cell>-</cell><cell>33.1</cell><cell></cell></row><row><cell>Vaswani et al. (2017)</cell><cell>47M</cell><cell>34.4</cell><cell></cell></row><row><cell>Wu et al. (2019)</cell><cell>43M</cell><cell>35.2</cell><cell></cell></row><row><cell>TaLK Convolution (Ours)</cell><cell>42M</cell><cell>35.5</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Results on CNN-DailyMail abstractive summarization.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="3">Param Rouge-1 Rouge-2 Rouge-L</cell></row><row><cell cols="2">LSTM (Paulus et al., 2018)</cell><cell></cell><cell>-</cell><cell>38.30</cell><cell>14.81</cell><cell>35.49</cell></row><row><cell cols="2">CNN (Fan et al., 2018)</cell><cell></cell><cell>-</cell><cell>39.06</cell><cell>15.38</cell><cell>35.77</cell></row><row><cell cols="3">Self-Attention Baseline (Wu et al., 2019)</cell><cell>90M</cell><cell>39.26</cell><cell>15.98</cell><cell>36.35</cell></row><row><cell cols="3">Lightweight Convolution (Wu et al., 2019)</cell><cell>86M</cell><cell>39.52</cell><cell>15.97</cell><cell>36.51</cell></row><row><cell cols="3">Dynamic Convolution (Wu et al., 2019)</cell><cell>87M</cell><cell>39.84</cell><cell>16.25</cell><cell>36.73</cell></row><row><cell cols="3">TaLK Convolution (Standard)</cell><cell>59M</cell><cell>40.03</cell><cell>18.45</cell><cell>36.13</cell></row><row><cell cols="2">TaLK Convolution (Deep)</cell><cell></cell><cell>83M</cell><cell>40.59</cell><cell>18.97</cell><cell>36.81</cell></row><row><cell cols="3">Table 5. Test perplexity on WikiText-103. We used adaptive inputs</cell><cell></cell><cell></cell></row><row><cell cols="3">similar to Baevski &amp; Auli (2019) and show that our method yields</cell><cell></cell><cell></cell></row><row><cell cols="3">better perplexity than dynamic convolutions and comparative per-</cell><cell></cell><cell></cell></row><row><cell>formance with self-attention.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Param Test</cell><cell></cell><cell></cell></row><row><cell>Grave et al. (2017)</cell><cell>-</cell><cell>40.8</cell><cell></cell><cell></cell></row><row><cell>Dauphin et al. (2017)</cell><cell cols="2">229M 37.2</cell><cell></cell><cell></cell></row><row><cell>Merity et al. (2018)</cell><cell cols="2">151M 33.0</cell><cell></cell><cell></cell></row><row><cell>Rae et al. (2018)</cell><cell>-</cell><cell>29.2</cell><cell></cell><cell></cell></row><row><cell>Baevski &amp; Auli (2019)</cell><cell cols="2">247M 20.5</cell><cell></cell><cell></cell></row><row><cell>Dynamic Convolutions</cell><cell cols="2">255M 25.0</cell><cell></cell><cell></cell></row><row><cell cols="3">TaLK Convolution (Ours) 240M 23.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Time-aware Large Kernel Convolutions</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Ablation on IWSLT De-En validation set. (+) indicates that a result includes all preceding features.</figDesc><table><row><cell>Model</cell><cell>Param</cell><cell>BLEU</cell></row><row><cell>TaLK Convolution (a l i , a r i =1x7, H=1)</cell><cell>42M</cell><cell>diverges</cell></row><row><cell>+ Output Normalization</cell><cell>42M</cell><cell>35.70 ± 0.1</cell></row><row><cell>+ Increasing Max Offsets (a l i , a r i =1,3,7,15x4)</cell><cell>42M</cell><cell>36.23 ± 0.1</cell></row><row><cell>+ Offsets Dropout (p=0.1)</cell><cell>42M</cell><cell>36.37 ± 0.05</cell></row><row><cell>+ Fully-headed Kernels (H=512)</cell><cell>47M</cell><cell>36.51 ± 0.07</cell></row><row><cell>+ Multi-headed Kernels (H=4)</cell><cell>42M</cell><cell>36.65 ± 0.05</cell></row><row><cell>Replacing Swish with ReLU</cell><cell>42M</cell><cell>36.21 ± 0.05</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">School of Computer Science, Carleton University, Canada. Correspondence to: Vasileios Lioutas &lt;contact@vlioutas.com&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the Canada Research Chairs program and the NSERC Discovery grant. We would like to express our gratitude to our anonymous reviewers for their valuable comments and feedback. A special thank you to Vasileia Karasavva for editing and proofreading the final manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Weighted transformer network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711.02132" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks with box convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Summed-area tables for texture mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 11th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent alignment and variational attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1207.0580" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.10099" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gradient Flow in Recurrent Nets: The Difficulty of Learning LongTerm Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley-IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel prefix computation. J. ACM</title>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fast template matching. Vis. Interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A generative model for category text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">An analysis of neural language modeling at multiple scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1803.08240" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Mixed precision training. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CUFE at SemEval-2016 task 4: A gated recurrent model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atyia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequenceto-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast parametric learning with activation memorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compressive transformers for longrange sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.05941" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting lstm networks for semi-supervised text classification via mixed objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bi-directional block self-attention for fast and memoryefficient sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Why selfattention? a targeted evaluation of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recurrent memory networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Prefix sums and an application thereof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Vishkin</surname></persName>
		</author>
		<idno>/04/01/ 2003</idno>
		<ptr target="http://www.google.com/patents?id=qCAPAAAAEBAJ" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1609.08144" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cached long short-term memory neural networks for document-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Accelerating neural transformer via an average attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Accelerating large-kernel convolution using summed-area tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.11367" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
