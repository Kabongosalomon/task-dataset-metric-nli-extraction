<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Make Predictions on Graphs with Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phi</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Strategic Innovation Group Booz</orgName>
								<address>
									<settlement>Allen | Hamilton San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Strategic Innovation Group Booz</orgName>
								<address>
									<settlement>Allen | Hamilton San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Make Predictions on Graphs with Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-network embedding</term>
					<term>link prediction</term>
					<term>semi- supervised learning</term>
					<term>multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We examine two fundamental tasks associated with graph representation learning: link prediction and semisupervised node classification. We present a novel autoencoder architecture capable of learning a joint representation of both local graph structure and available node features for the multitask learning of link prediction and node classification. Our autoencoder architecture is efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification, whereas previous related methods require multiple training steps that are difficult to optimize. We provide a comprehensive empirical evaluation of our models on nine benchmark graph-structured datasets and demonstrate significant improvement over related methods for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning.</p><p>In [9]:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction</head><p>Link prediction attempts to answer the principal question: given two entities, should there be a link between them? One can view link prediction as a graph/matrix completion problem, where the goal is to predict missing links using data from known, observed positive and negative links. We approach the task of link prediction through two stages of supervised machine learning: matrix factorization and linear (multiclass) classification. Matrix factorization learns and extracts low dimensional latent features from the global topology of the graph. A linear classifier can combine latent features with observed features on graph nodes and edges to learn a decision function that can predict link propensity for any pair of nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out[9]:</head><p># compute 10 community clusters using the Girvan-Newman edge betweenness community detection algorithm community = g.community_edge_betweenness(clusters=10).as_clustering() ig.plot (community, layout=layout, vertex_size=20, edge_arrow_width=0.75, edge_arrow_size=0.75)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A S the world is becoming increasingly interconnected, graph-structured data are also growing in ubiquity. In this work, we examine the task of learning to make predictions on graphs for a broad range of real-world applications. Specifically, we study two canonical subtasks associated with graph-structured datasets: link prediction and semi-supervised node classification (LPNC). A graph is a partially observed set of edges and nodes (or vertices), and the learning task is to predict the labels for edges and nodes. In real-world applications, the input graph is a network with nodes representing unique entities, and edges representing relationships (or links) between entities. Further, the labels of nodes and edges in a graph are often correlated, exhibiting complex relational structures that violate the general assumption of independent and identical distribution fundamental in traditional machine learning <ref type="bibr" target="#b9">[10]</ref>. Therefore, models capable of exploiting topological structures of graphs have been shown to achieve superior predictive performances on many LPNC tasks <ref type="bibr" target="#b22">[23]</ref>.</p><p>We present a novel densely connected autoencoder architecture capable of learning a shared representation of latent node embeddings from both local graph topology and available explicit node features for LPNC. The resulting autoencoder models are useful for many applications across multiple domains, including analysis of metabolic networks for drugtarget interaction <ref type="bibr" target="#b4">[5]</ref>, bibliographic networks <ref type="bibr" target="#b24">[25]</ref>, social networks such as Facebook ("People You May Know"), terrorist networks <ref type="bibr" target="#b37">[38]</ref>, communication networks <ref type="bibr" target="#b10">[11]</ref>, cybersecurity <ref type="bibr" target="#b5">[6]</ref>, recommender systems <ref type="bibr" target="#b15">[16]</ref>, and knowledge bases such as DBpedia and Wikidata <ref type="bibr" target="#b34">[35]</ref>.</p><p>There are a number of technical challenges associated with learning to make meaningful predictions on complex graphs:</p><p>• Extreme class imbalance: in link prediction, the number of known present (positive) edges is often significantly less than the number of known absent (negative) edges, making it difficult to reliably learn from rare examples; • Learn from complex graph structures: edges may be directed or undirected, weighted or unweighted, highly sparse in occurrence, and/or consisting of multiple types. A useful model should be versatile to address a variety of graph types, including bipartite graphs; • Incorporate side information: nodes (and maybe edges) are sometimes described by a set of features, called side information, that could encode information complementary to topological features of the input graph. Such explicit data on nodes and edges are not always readily available and are considered optional. A useful model should be able to incorporate optional side information about nodes and/or edges, whenever available, to potentially improve predictive performance; • Efficiency and scalability: real-world graph datasets contain large numbers of nodes and/or edges. It is essential for a model to be memory and computationally efficient to achieve practical utility on real-world applications.</p><p>Our contribution in this work is a simple, yet versatile autoencoder architecture that addresses all of the above technical challenges. We demonstrate that our autoencoder models: 1) can handle extreme class imbalance common in link prediction problems; 2) can learn expressive latent features for nodes from topological structures of sparse, bipartite graphs that may have directed and/or weighted edges; 3) is flexible to incorporate explicit side features about nodes as an optional component to improve predictive performance; and 4) utilize extensive parameter sharing to reduce memory footprint and computational complexity, while leveraging available GPUbased implementations for increased scalability. Further, the autoencoder architecture has the novelty of being efficiently trained end-to-end for the joint, multi-task learning (MTL) of both link prediction and node classification tasks. To the <ref type="figure">Fig. 1</ref>. Schematic depiction of the Local Neighborhood Graph Autoencoder (LoNGAE) architecture. Left: A partially observed input graph with known positive links (solid lines) and known negative links (dashed lines) between two nodes; pairs of nodes not yet connected have unknown status links. Middle: A symmetrical, densely connected autoencoder with parameter sharing (tied weights) is trained end-to-end to learn node embeddings from the adjacency vector for graph representation. Right: Exemplar multi-task output for link prediction and node classification. best of our knowledge, this is the first architecture capable of performing simultaneous link prediction and node classification in a single learning stage, whereas previous related methods require multiple training stages that are difficult to optimize. Lastly, we conduct a comprehensive evaluation of the proposed autoencoder architecture on nine challenging benchmark graph-structured datasets comprising a wide range of LPNC applications. Numerical experiments validate the efficacy of our models by showing significant improvement on multiple evaluation measures over related methods designed for link prediction and/or node classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. AUTOENCODER ARCHITECTURE FOR LINK PREDICTION AND NODE CLASSIFICATION</head><p>We now characterize our proposed autoencoder architecture, schematically depicted in <ref type="figure">Figure 1</ref>, for LPNC and formalize the notation used in this paper. The input to the autoencoder is a graph G = (V, E) of N = |V| nodes. Graph G is represented by its adjacency matrix A ∈ R N ×N . For a partially observed graph, A ∈ {1, 0, UNK} N ×N , where 1 denotes a known present positive edge, 0 denotes a known absent negative edge, and UNK denotes an unknown status (missing or unobserved) edge. In general, the input to the autoencoder can be directed or undirected, weighted or unweighted, and/or bipartite graphs. However, for the remainder of this paper and throughout the numerical experiments, we assume undirected and symmetric graphs with binary edges to maintain parity with previous related work.</p><p>Optionally, we are given a matrix of available explicit node features, i.e. side information X ∈ R N ×F . The aim of the autoencoder model h(A, X) is to learn a set of lowdimensional latent variables for the nodes Z ∈ R N ×D that can produce an approximate reconstruction outputÂ such that the error between A andÂ is minimized, thereby preserving the global graph structure. In this paper, we use capital variables (e.g., A) to denote matrices and lower-case variables (e.g., a) to denote row vectors. For example, we use a i to mean the ith row of the matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Link Prediction</head><p>Research on link prediction attempts to answer the principal question: given two entities, should there be a connection between them? We focus on the structural link prediction problem, where the task is to compute the likelihood that an unobserved or missing edge exists between two nodes in a partially observed graph. For a comprehensive survey on link prediction, to include structural and temporal link prediction using unsupervised and supervised models, see <ref type="bibr" target="#b32">[33]</ref>.</p><p>Link Prediction from Graph Topology Let a i ∈ R N be an adjacency vector of A that contains the local neighborhood of the ith node. Our proposed autoencoder architecture comprises a set of non-linear transformations on a i summarized in two component parts: encoder g(a i ) : R N → R D , and decoder f (z i ) : R D → R N . We stack two layers of the encoder part to derive D-dimensional latent feature representation of the ith node z i ∈ R D , and then stack two layers of the decoder part to obtain an approximate reconstruction outputâ i ∈ R N , resulting in a four-layer autoencoder architecture. Note that a i is highly sparse, with up to 90 percent of the edges missing at random in some of our experiments, and the dense reconstructed outputâ i contains the predictions for the missing edges. The hidden representations for the encoder and decoder parts are computed as follows:</p><formula xml:id="formula_0">z i = g (a i ) = ReLU W · ReLU Va i + b (1) + b (2)</formula><p>Encoder Part,</p><formula xml:id="formula_1">a i = f (z i ) = V T · ReLU W T z i + b (3) + b (4) Decoder Part, a i = h (a i ) = f (g (a i ))</formula><p>Autoencoder.</p><p>The choice of non-linear, element-wise activation function is the rectified linear unit ReLU(x) = max(0, x) <ref type="bibr" target="#b20">[21]</ref>. The last decoder layer computes a linear transformation to score the missing links as part of the reconstruction. We constrain the autoencoder to be symmetrical with shared parameters for {W, V} between the encoder and decoder parts, resulting in almost 2× fewer parameters than an unconstrained architecture. Parameter sharing is a powerful form of regularization that helps improve learning and generalization, and is also the main motivation for MTL, first explored in <ref type="bibr" target="#b1">[2]</ref> and most recently in <ref type="bibr" target="#b36">[37]</ref>. Notice the bias units b do not share parameters, and W T , V T are transposed copies of {W, V}. For brevity of notation, we summarize the parameters to be learned in θ = W, V, b (k) , k = 1, ..., 4. Since our autoencoder learns node embeddings from local neighborhood structures of the graph, we refer to it as LoNGAE for Local Neighborhood Graph Autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Link Prediction with Node Features</head><p>Optionally, if a matrix of explicit node features X ∈ R N ×F is available, then we concatenate (A, X) to obtain an augmented adjacency matrixĀ ∈ R N ×(N +F ) and perform the above encoderdecoder transformations onā i for link prediction. We refer to this variant as αLoNGAE. Notice the augmented adjacency matrix is no longer square and symmetric. The intuition behind the concatenation of node features is to enable a shared representation of both graph and node features throughout the autoencoding transformations by way of the tied parameters {W, V}. This idea draws inspiration from recent work by Vukotić et al. <ref type="bibr" target="#b31">[32]</ref>, where they successfully applied symmetrical autoencoders with parameter sharing for multi-modal and cross-modal representation learning of textual and visual features.</p><p>The training complexity of αLoNGAE is O((N + F )DI), where N is the number of nodes, F is the dimensionality of node features, D is the size of the hidden layer, and I is the number of iterations. In practice, F , D N , and I are independent of N . Thus, the overall complexity of the autoencoder is O(N ), linear in the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inference and Learning</head><p>During the forward pass, or inference, the model takes as input an adjacency vector a i and computes its reconstructed outputâ i = h(a i ) for link prediction. The parameters θ are learned via backpropagation. During the backward pass, we estimate θ by minimizing the Masked Balanced Cross-Entropy (MBCE) loss, which only allows for the contributions of those parameters associated with observed edges, as in <ref type="bibr" target="#b23">[24]</ref>. Moreover, a i can exhibit extreme class imbalance between known present and absent links, as is common in link prediction problems. We handle class imbalance by defining a weighting factor ζ ∈ [0, 1] to be used as a multiplier for the positive class in the cross-entropy loss formulation. This approach is referred to as balanced cross-entropy. Other approaches to class imbalance include optimizing for a ranking loss <ref type="bibr" target="#b19">[20]</ref> and the recent work on focal loss by Lin et al. <ref type="bibr" target="#b17">[18]</ref>. For a single example a i and its reconstructed outputâ i , we compute the MBCE loss as follows:</p><formula xml:id="formula_2">L BCE = −a i log (σ (â i )) · ζ − (1 − a i ) log (1 − σ (â i )) , L MBCE = m i L BCE m i .</formula><p>Here, L BCE is the balanced cross-entropy loss with weighting factor ζ = 1 − # present links # absent links , σ(·) is the sigmoid function, is the Hadamard (element-wise) product, and m i is the boolean function:</p><formula xml:id="formula_3">m i = 1 if a i = UNK, else m i = 0.</formula><p>The same autoencoder architecture can be applied to a row vectorā i ∈ R N +F in the augmented adjacency matrixĀ. However, at the final decoder layer, we slice the reconstruction h(ā i ) into two outputs:â i ∈ R N corresponding to the reconstructed example in the original adjacency matrix, and x i ∈ R F corresponding to the reconstructed example in the matrix of node features. During learning, we optimize θ on the concatenation of graph topology and side node features (a i , x i ), but compute the losses for the reconstructed outputs (â i ,x i ) separately with different loss functions. The motivation behind this design is to maintain flexibility to handle different input formats; the input a i is usually binary, but the input x i can be binary, real-valued, or both. In this work, we enforce both inputs (a i , x i ) to be in the range [0, 1] for simplicity and improved performance, and compute the augmented αMBCE loss as follows:</p><formula xml:id="formula_4">L αMBCE = L MBCE(ai,âi) + L CE(xi,xi) , where L CE(xi,xi) = −x i log (σ (x i ))−(1−x i ) log (1 − σ (x i ))</formula><p>is the standard cross-entropy loss with sigmoid function σ(·). At inference time, we use the reconstructed outputâ i for link prediction and disregard the outputx i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semi-Supervised Node Classification</head><p>The αLoNGAE model can also be used to perform efficient information propagation on graphs for the task of semisupervised node classification. Node classification is the task of predicting the labels or types of entities in a graph, such as the types of molecules in a metabolic network or document categories in a citation network.</p><p>For a given augmented adjacency vectorā i , the autoencoder learns the corresponding node embeddings z i to obtain an optimal reconstruction. Intuitively, z i encodes a vector of latent features derived from the concatenation of both graph and node features, and can be used to predict the label of the ith node. For multi-class classification, we can decode z i using the softmax activation function to learn a probability distribution over node labels. More precisely, we predict node labels via the following transformation: <ref type="bibr" target="#b4">(5)</ref> .</p><formula xml:id="formula_5">ŷ i = softmax(z i ) = 1 Z exp(z i ), where Z = exp(z i ) andz i = U · ReLU W T z i + b (3) + b</formula><p>In many applications, only a small fraction of the nodes are labeled. For semi-supervised learning, it is advantageous to utilize unlabeled examples in conjunction with labeled instances to better capture the underlying data patterns for improved learning and generalization. We achieve this by jointly training the autoencoder with a masked softmax classifier to collectively learn node labels from minimizing their combined losses:</p><formula xml:id="formula_6">L MULTI-TASK = −MASK i c∈C y ic log(ŷ ic ) + L MBCE ,</formula><p>where C is the set of node labels, y ic = 1 if node i belongs to class c,ŷ ic is the softmax probability that node i belongs to class c, L MBCE is the loss defined for the autoencoder, and the boolean function MASK i = 1 if node i has a label, otherwise MASK i = 0. Notice in this configuration, we can perform multi-task learning for both link prediction and semisupervised node classification, simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>The field of graph representation learning is seeing a resurgence of research interest in recent years, driven in part by the latest advances in deep learning. The aim is to learn a mapping that encodes the input graph into low-dimensional feature embeddings while preserving its original global structure. Hamilton et al. <ref type="bibr" target="#b8">[9]</ref> succinctly articulate the diverse set of previously proposed approaches for graph representation learning, or graph embedding, as belonging within a unified encoder-decoder framework. In this section, we summarize three classes of encoder-decoder models most related to our work: matrix factorization (MF), autoencoders, and graph convolutional networks (GCNs).</p><p>MF has its roots in dimensionality reduction and gained popularity with extensive applications in collaborative filtering (CF) and recommender systems <ref type="bibr" target="#b15">[16]</ref>. MF models take an input matrix M, learn a shared linear latent representation for rows (r i ) and columns (c j ) during an encoder step, and then use a bilinear (pairwise) decoder based on the inner product r i c j to produce a reconstructed matrixM. CF is mathematically similar to link prediction, where the goal is essentially matrix completion. Menon and Elkan <ref type="bibr" target="#b19">[20]</ref> proposed an MF model capable of incorporating side information about nodes and/or edges to demonstrate strong link prediction results on several challenging network datasets. Other recent approaches similar to MF that learn node embeddings via some encoder transformation and then use a bilinear decoder for the reconstruction include DeepWalk <ref type="bibr" target="#b21">[22]</ref> and its variants LINE <ref type="bibr" target="#b29">[30]</ref> and node2vec <ref type="bibr" target="#b7">[8]</ref>. DeepWalk, LINE, and node2vec do not support external node/edge features.</p><p>Our work is inspired by recent successful applications of autoencoder architectures for collaborative filtering that outperform popular matrix factorization methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and is related to Structural Deep Network Embedding (SDNE) <ref type="bibr" target="#b33">[34]</ref> for link prediction. Similar to SDNE, our models rely on the autoencoder to learn non-linear node embeddings from local graph neighborhoods. However, our models have several important distinctions: 1) we leverage extensive parameter sharing between the encoder and decoder parts to enhance representation learning; 2) our αLoNGAE model can optionally concatenate side node features to the adjacency matrix for improved link prediction performance; and 3) the αLoNGAE model can be trained end-to-end in a single stage for multi-task learning of link prediction and semi-supervised node classification. On the other hand, training SDNE requires multiple steps that are difficult to jointly optimize: i) pretraining via a deep belief network; and ii) utilizing a separate downstream classifier on top of node embeddings for LPNC.</p><p>Lastly, GCNs <ref type="bibr" target="#b13">[14]</ref> are a recent class of algorithms based on convolutional encoders for learning node embeddings. The GCN model is motivated by a localized first-order approximation of spectral convolutions for layer-wise information propagation on graphs. Similar to our αLoNGAE model, the GCN model can learn hidden layer representations that encode both local graph structure and features of nodes. The choice of the decoder depends on the task. For link prediction, the bilinear inner product is used in the context of the variational graph autoencoder (VGAE) <ref type="bibr" target="#b14">[15]</ref>. For semi-supervised node classification, the softmax activation function is employed. The GCN model provides an end-to-end learning framework that scales linearly in the number of graph edges and has been shown to achieve strong LPNC results on a number of graph-structured datasets. However, the GCN model has a drawback of being memory intensive because it is trained on the full dataset using batch gradient descent for every training iteration. We show that our models outperform GCN-based models for LPNC while consuming a constant memory budget by way of mini-batch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL DESIGN</head><p>In this section, we expound our protocol for the empirical evaluation of our models' capability for learning and generalization on the tasks of link prediction and semi-supervised node classification. Secondarily, we also present results of the models' representation capacity on the task of network reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Baselines</head><p>We evaluate our proposed autoencoder models on nine graph-structured datasets, spanning multiple application domains, from which previous graph embedding methods have achieved strong results for LPNC. The datasets are summarized in <ref type="table" target="#tab_1">Table I</ref>   <ref type="bibr" target="#b19">[20]</ref>. {Cora, Citeseer, Pubmed} are from <ref type="bibr" target="#b24">[25]</ref> and reported in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. And {Arxiv-GRQC, BlogCatalog} are reported in <ref type="bibr" target="#b33">[34]</ref>.</p><p>We empirically compare our autoencoder models against four strong baselines summarized in <ref type="table" target="#tab_1">Table II</ref>, which were designed specifically for link prediction and/or node classification. We begin our empirical evaluation with the SDNE <ref type="bibr" target="#b33">[34]</ref> baseline, where we compare the representation capacity of our models on the network reconstruction task using the Baseline Task Metric SDNE <ref type="bibr" target="#b33">[34]</ref> Reconstruction Precision@k MF <ref type="bibr" target="#b19">[20]</ref> Link Prediction AUC VGAE <ref type="bibr" target="#b14">[15]</ref> Link Prediction AUC, AP GCN <ref type="bibr" target="#b13">[14]</ref> Node Classification Accuracy</p><p>Arxiv-GRQC and BlogCatalog datasets. For the MF baseline, we closely follow the experimental protocol in <ref type="bibr" target="#b19">[20]</ref>, where we randomly sample 10 percent of the observed links for training and evaluate link prediction performance on the other disjoint 90 percent for the {Protein, Metabolic, Conflict} datasets. For PowerGrid, we use 90 percent of observed links for training and evaluate on the remaining 10 percent. And for the VGAE and GCN baselines, we use the same train/validation/test segments described in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b13">[14]</ref> for link prediction and node classification, respectively, on the {Cora, Citeseer, Pubmed} citation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We implement the autoencoder architecture using Keras [4] on top of the GPU-enabled TensorFlow <ref type="bibr" target="#b0">[1]</ref> backend, along with several additional details. The diagonal elements of the adjacency matrix are set to 1 with the interpretation that every node is connected to itself. We impute missing or UNK elements in the adjacency matrix with 0. Note that imputed edges are not observed elements in the adjacency matrix and hence do not contribute to the masked loss computations during training. We are free to impute any values for the missing edges, but through cross-validation we found that the uniform value of 0 produces the best results.</p><p>Hyper-parameter tuning is performed via cross-validation or on the available validation set. Key hyper-parameters include mini-batch size, dimensionality of the hidden layers, and the percentage of dropout regularization. In general, we strive to keep a similar set of hyper-parameters across datasets to highlight the consistency of our models. In all experiments, the dimensionality of the hidden layers in the autoencoder architecture is fixed at N -256-128-256-N . For reconstruction and link prediction, we train for 50 epochs using mini-batch size of 8 samples. For node classification, we train for 100 epochs using mini-batch size of 64 samples. We utilize early stopping as a form of regularization in time when the model shows signs of overfitting on the validation set.</p><p>We apply mean-variance normalization (MVN) after each ReLU activation layer to help improve link prediction performance, where it compensates for noise between train and test instances by normalizing the activations to have zero mean and unit variance. MVN enables efficient learning and has been shown effective in cardiac semantic segmentation <ref type="bibr" target="#b30">[31]</ref> and speech recognition <ref type="bibr" target="#b11">[12]</ref>.</p><p>During training, we apply dropout regularization <ref type="bibr" target="#b26">[27]</ref> throughout the architecture to mitigate overfitting, depending on the sparsity of the input graph. For link prediction, dropout is also applied at the input layer to produce an effect similar to using a denoising autoencoder. This denoising technique was previously employed for link prediction in <ref type="bibr" target="#b2">[3]</ref>. We initialize weights according to the Xavier scheme described in <ref type="bibr" target="#b6">[7]</ref>. We do not apply weight decay regularization.</p><p>We employ the Adam algorithm <ref type="bibr" target="#b12">[13]</ref> for gradient descent optimization with a fixed learning rate of 0.001. As part of our experimental design, we also performed experiments without parameter sharing between the encoder and decoder parts of the architecture and found severely degraded predictive performance. This observation is consistent with prior findings that parameter sharing helps improve generalization by providing additional regularization to mitigate the adverse effects of overfitting and enhance representation learning <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstruction</head><p>Results of the reconstruction task for the Arxiv-GRQC and BlogCatalog network datasets are illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. In this experiment, we compare the results obtained by our LoNGAE model to those obtained by the related autoencoder-based SDNE model <ref type="bibr" target="#b33">[34]</ref>. The evaluation metric is precision@k, which is a rank-based measure used in information retrieval and is defined as the proportion of retrieved edges/links in the top-k set that are relevant. We use precision@k to evaluate the model's ability to retrieve edges known to be present (positive edges) as part of the reconstruction.</p><p>In comparison to SDNE, we show that our LoNGAE model achieves better precision@k performance for all k values, up to k = 10, 000 for Arxiv-GRQC and k = 100, 000 for BlogCatalog, when trained on the complete datasets. We also systematically test the capacity of the LoNGAE model to reconstruct the original networks when up to 80 percent of the edges are randomly removed, akin to the link prediction task. We show that the LoNGAE model only gets worse precision@k performance than SDNE on the Arxiv-GRQC dataset when more than 40 percent of the edges are missing at random. On the BlogCatalog dataset, the LoNGAE   Link Prediction <ref type="table" target="#tab_1">Table III</ref> shows the comparison between our autoencoder models and the matrix factorization (MF) model proposed in <ref type="bibr" target="#b19">[20]</ref> for link prediction with and without node features. Recall that our goal is to recover the statuses of the missing or unknown links in the input graph. As part of the experimental design, we pretend that a randomly selected set of elements in the adjacency matrix are missing and collect their indices to be used as a validation set. Our task is to train the autoencoder to produce a set of predictions, a list of ones and zeros, on those missing indices and see how well the model performs when compared to the ground-truth. The evaluation metric is the area under the ROC curve (AUC).</p><p>Results are reported as mean AUC and standard deviation over 10-fold cross-validation. The datasets under consideration for link prediction exhibit varying degrees of class imbalance. For featureless link prediction, our LoNGAE model marginally outperforms MF on {Protein, Metabolic, Conflict} and is significantly better than MF on PowerGrid. Consistent with MF results, we observe that incorporating external node features provides a boost in link prediction accuracy, especially for the Protein dataset where we achieve a 6 percent increase in performance. Metabolic and Conflict also come with external edge features, which were exploited by the MF model for further performance gains. We leave the task of combining edge features for future work. Each node in Conflict only has three features, which are unable to significantly boost link prediction accuracy. PowerGrid does not have node features so there are no results for the respective rows. <ref type="table" target="#tab_1">Table IV</ref> summarizes the performances between our autoencoder models and related graph embedding methods for link prediction with and without node features. Following the protocol described in <ref type="bibr" target="#b14">[15]</ref>, we report AUC and average precision (AP) scores for each model on the held-out test set containing 10 percent of randomly sampled positive links and the same number of negative links. We show mean AUC and AP with standard error over 10 runs with random weight initializations on fixed data splits. Results for the baseline methods are taken from Kipf and Welling <ref type="bibr" target="#b14">[15]</ref>, where we pick the best performing models for comparison. Similar to the MF model, the graph embedding methods that can combine side node features always produce a boost in link prediction accuracy. In this comparison, we significantly outperform the best graph embedding methods by as much as 10 percent, with and without node features. Our αLoNGAE model achieves competitive link prediction performance when compared against the best model presented in <ref type="bibr" target="#b14">[15]</ref> on the Pubmed dataset.   <ref type="bibr" target="#b13">[14]</ref> 0.815 0.703 0.790 Planetoid <ref type="bibr" target="#b35">[36]</ref> 0.757 0.647 0.772 ICA <ref type="bibr" target="#b18">[19]</ref> 0.751 0.691 0.739 DeepWalk <ref type="bibr" target="#b21">[22]</ref> 0.672 0.432 0.653</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task Learning</head><p>Lastly, we report LPNC results obtained by our αLoNGAE model in the MTL setting over 10 runs with random weight initializations. In the MTL scenario, the αLoNGAE model takes as input an incomplete graph with 10 percent of the positive edges, and the same number of negative edges, missing at random and all available node features to simultaneously produce predictions for the missing edges and labels for the nodes. <ref type="table" target="#tab_1">Table VI</ref> shows the efficacy of the αLoNGAE model for MTL when compared against the best performing task-specific link prediction and node classification models, which require the complete adjacency matrix as input. For link prediction, multi-task αLoNGAE achieves competitive performance against task-specific αLoNGAE, and significantly outperforms the best VGAE model from Kipf and Welling <ref type="bibr" target="#b14">[15]</ref> on Cora and Citeseer datasets. For node classification, multi-task αLoNGAE is the best performing model across the board, only trailing behind the GCN model on the Cora dataset.  <ref type="bibr" target="#b13">[14]</ref> 0.815 0.703 0.790</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In our experiments, we show that a simple autoencoder architecture with parameter sharing consistently outperforms previous related methods on a range of challenging graphstructured benchmarks for three separate tasks: reconstruction, link prediction, and semi-supervised node classification. For the reconstruction task, our LoNGAE model achieves superior precision@k performance when compared to the related SDNE model. Although both models leverage a deep autoencoder architecture for graph representation learning, the SDNE model lacks several key implementations necessary for enhanced representation capacity, namely parameter sharing between the encoder-decoder parts and end-to-end training of deep architectures.</p><p>For link prediction, we observe that combining available node features always produces a significant boost in predictive performance. This observation was previously reported in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b14">[15]</ref>, among others. Intuitively, we expect topological graph features provide complementary information not present in the node features, and the combination of both feature sets should improve predictive power. Although explicit node features may not always be readily available, a link prediction model capable of incorporating optional side information has broader applicability.</p><p>Our αLoNGAE model also performs favorably well on the task of semi-supervised node classification. The model is capable of encoding non-linear node embeddings from both local graph structure and explicit node features, which can be decoded by a softmax activation function to yield accurate node labels. The efficacy of the proposed αLoNGAE model is evident especially on the Pubmed dataset, where the label rate is only 0.003. This efficacy is attributed to parameter sharing being used in the autoencoder architecture, which provides regularization to help improve representation learning and generalization.</p><p>Our autoencoder architecture naturally supports multi-task learning, where a joint representation for both link prediction and node classification is enabled via parameter sharing. MTL aims to exploit commonalities and differences across multiple tasks to find a shared representation that can result in improved performance for each task-specific metric. In this work, we show that our multi-task αLoNGAE model improves node classification accuracy by learning to predict missing edges at the same time. Our multi-task model has broad practical utility to address real-world applications where the input graphs may have both missing edges and node labels.</p><p>Finally, we address one major limitation associated with our autoencoder models having complexity scale linearly in the number of nodes. Hamilton et al. <ref type="bibr" target="#b8">[9]</ref> express that the complexity in nodes may limit the utility of the models on massive graphs with hundreds of millions of nodes. In practice, we would implement our models to leverage data parallelism <ref type="bibr" target="#b25">[26]</ref> across commodity CPU and/or GPU resources for effective distributed learning on massive graphs. Data parallelism is possible because our models learn node embeddings from each row vector of the adjacency matrix independently. Nevertheless, the area of improvement in future work is to take advantage of the sparsity of edges in the graphs to scale our models linearly in the number of observed edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we presented a new autoencoder architecture for link prediction and semi-supervised node classification, and showed that the resulting models outperform related methods in accuracy performance on a range of real-world graphstructured datasets. The success of our models is primarily attributed to extensive parameter sharing between the encoder and decoder parts of the architecture, coupled with the capability to learn expressive non-linear latent node representations from both local graph neighborhoods and explicit node features. Further, our novel architecture is capable of simultaneous multi-task learning of both link prediction and node classification in one efficient end-to-end training stage.</p><p>Our work provides a useful framework to make accurate and meaningful predictions on a diverse set of complex graph structures for a wide range of real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author thanks Edward Raff and Jared Sylvester for insightful discussions, and gracious reviewers for constructive feedback on the paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of precision@k performance between our LoNGAE model and the related autoencoder-based SDNE model for the reconstruction task on the Arxiv-GRQC and BlogCatalog network datasets. The parameter k indicates the total number of retrieved edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and include networks for Protein interactions, Metabolic pathways, military Conflict between countries, the U.S. PowerGrid, collaboration between users on the BlogCatalog social website, and publication citations from the Cora, Citeseer, Pubmed,</figDesc><table><row><cell>Arxiv-GRQC</cell><cell>databases.</cell><cell>{Protein, Metabolic,</cell></row><row><cell cols="3">Conflict, PowerGrid} are reported in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>STATISTICS OF DATASETS USED IN EMPIRICAL EVALUATION. THE NOTATION |O + |:|O − | DENOTES THE RATIO OF OBSERVED PRESENT (POSITIVE) EDGES TO ABSENT (NEGATIVE) EDGES AND IS A MEASURE OF CLASS IMBALANCE.LABEL RATE IS DEFINED AS THE NUMBER OF NODES LABELED FOR TRAINING DIVIDED BY THE TOTAL NUMBER OF NODES.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell cols="2">Average |O + |:|O − | Degree Ratio</cell><cell cols="2">Node Features Classes Node</cell><cell>Label Rate</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>4.5</cell><cell>1 : 4384</cell><cell>500</cell><cell>3</cell><cell>0.003</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>2.8</cell><cell>1 : 1198</cell><cell>3,703</cell><cell>6</cell><cell>0.036</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>3.9</cell><cell>1 : 694</cell><cell>1,433</cell><cell>7</cell><cell>0.052</cell></row><row><cell>Protein</cell><cell>2,617</cell><cell>9.1</cell><cell>1 : 300</cell><cell>76</cell><cell>-</cell><cell>-</cell></row><row><cell>Metabolic</cell><cell>668</cell><cell>8.3</cell><cell>1 : 80</cell><cell>325</cell><cell>-</cell><cell>-</cell></row><row><cell>Conflict</cell><cell>130</cell><cell>2.5</cell><cell>1 : 52</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell>PowerGrid</cell><cell>4,941</cell><cell>2.7</cell><cell>1 : 1850</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Arxiv-GRQC</cell><cell>5,242</cell><cell>5.5</cell><cell>1 : 947</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BlogCatalog</cell><cell>10,312</cell><cell>64.8</cell><cell>1 : 158</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">SUMMARY OF BASELINES USED IN EMPIRICAL EVALUATION. ACRONYMS:</cell></row><row><cell cols="7">AUC -AREA UNDER ROC CURVE; AP -AVERAGE PRECISION.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF AUC PERFORMANCE BETWEEN OUR AUTOENCODER MODELS AND THE BEST PREVIOUS MATRIX FACTORIZATION MODEL FOR LINK PREDICTION. NUMBER FORMAT: MEAN VALUE (STANDARD DEVIATION).</figDesc><table><row><cell>Method</cell><cell>Node Features</cell><cell>Protein</cell><cell>Metabolic</cell><cell>Conflict</cell><cell>PowerGrid</cell></row><row><cell>LoNGAE (this work)</cell><cell>No</cell><cell>0.798 (0.004)</cell><cell>0.703 (0.009)</cell><cell cols="2">0.698 (0.025) 0.781 (0.007)</cell></row><row><cell>Matrix Factorization [20]</cell><cell>No</cell><cell>0.795 (0.005)</cell><cell>0.696 (0.001)</cell><cell>0.692 (0.040)</cell><cell>0.754 (0.014)</cell></row><row><cell>αLoNGAE (this work)</cell><cell>Yes</cell><cell>0.861 (0.003)</cell><cell>0.750 (0.011)</cell><cell>0.699 (0.021)</cell><cell>-</cell></row><row><cell>Matrix Factorization [20]</cell><cell>Yes</cell><cell cols="2">0.813 (0.002)  *  0.763 (0.006)</cell><cell>*  0.890 (0.017)</cell><cell>-</cell></row><row><cell cols="3">model achieves better precision@k performance than SDNE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">for large k values even when 80 percent of the edges are</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">missing at random. This experiment demonstrates the superior</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">representation capacity of our LoNGAE model compared to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SDNE.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* THESE RESULTS INCORPORATED ADDITIONAL edge features FOR LINK PREDICTION, WHICH WE LEAVE FOR FUTURE WORK.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF AUC AND AP PERFORMANCES BETWEEN OUR AUTOENCODER MODELS AND RELATED GRAPH EMBEDDING METHODS FOR LINK PREDICTION. NUMBER FORMAT: MEAN VALUE (STANDARD DEVIATION). § DENOTES THE BEST PERFORMING MODEL PRESENTED IN<ref type="bibr" target="#b14">[15]</ref>. αLoNGAE model on the same data splits over 10 runs with random weight initializations and report mean accuracy. Kipf and Welling<ref type="bibr" target="#b13">[14]</ref> report their mean GCN and ICA results on the same data splits over 100 runs with random weight initializations. The other baseline methods are taken from Yang et al.<ref type="bibr" target="#b35">[36]</ref>. In this comparison, our αLoNGAE model achieves competitive performance when compared against the GCN model on the Cora dataset, but outperforms GCN and all other baseline methods on the Citeseer and Pubmed datasets.</figDesc><table><row><cell>Method</cell><cell>Node Features</cell><cell>AUC</cell><cell cols="2">Cora</cell><cell>AP</cell><cell>AUC</cell><cell cols="2">Citeseer</cell><cell>AP</cell><cell>AUC</cell><cell>Pubmed</cell><cell>AP</cell></row><row><cell>LoNGAE (this work)</cell><cell>No</cell><cell cols="2">0.896 (0.003)</cell><cell cols="8">0.915 (0.001) 0.860 (0.003) 0.892 (0.003) 0.926 (0.001)</cell><cell>0.930 (0.002)</cell></row><row><cell>Spectral Clustering [29]</cell><cell>No</cell><cell cols="2">0.846 (0.01)</cell><cell cols="2">0.885 (0.00)</cell><cell cols="2">0.805 (0.01)</cell><cell cols="2">0.850 (0.01)</cell><cell cols="2">0.842 (0.01)</cell><cell>0.878 (0.01)</cell></row><row><cell>DeepWalk [22]</cell><cell>No</cell><cell cols="2">0.831 (0.01)</cell><cell cols="2">0.850 (0.00)</cell><cell cols="2">0.805 (0.02)</cell><cell cols="2">0.836 (0.01)</cell><cell cols="2">0.844 (0.00)</cell><cell>0.841 (0.00)</cell></row><row><cell>VGAE  § [15]</cell><cell>No</cell><cell cols="2">0.843 (0.02)</cell><cell cols="2">0.881 (0.01)</cell><cell cols="2">0.789 (0.03)</cell><cell cols="2">0.841 (0.02)</cell><cell cols="2">0.827 (0.01)</cell><cell>0.875 (0.01)</cell></row><row><cell>αLoNGAE (this work)</cell><cell>Yes</cell><cell cols="2">0.943 (0.003)</cell><cell cols="6">0.952 (0.002) 0.956 (0.003) 0.964 (0.002)</cell><cell cols="2">0.960 (0.003)</cell><cell>0.963 (0.002)</cell></row><row><cell>VGAE  § [15]</cell><cell>Yes</cell><cell cols="2">0.914 (0.01)</cell><cell cols="2">0.926 (0.01)</cell><cell cols="2">0.908 (0.02)</cell><cell cols="2">0.920 (0.02)</cell><cell cols="2">0.964 (0.00)</cell><cell>0.965 (0.00)</cell></row><row><cell cols="6">Node Classification Results of semi-supervised node classi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">fication for the {Cora, Citeseer, Pubmed} datasets are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">summarized in Table V. In this context of citation networks,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">node classification is equivalent to the task of document classi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">fication. We closely follow the experimental setup of Kipf and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Welling [14], where we use their provided train/validation/test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">splits for evaluation. Accuracy performance is measured on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">held-out test set of 1,000 examples. We tune hyper-parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">on the validation set of 500 examples. The train set only</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">contains 20 examples per class. All methods use the complete</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">adjacency matrix, and available node features, to learn latent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">embeddings for node classification. For comparison, we train</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>and test our</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF ACCURACY PERFORMANCE BETWEEN OUR αLONGAE MODEL AND RELATED GRAPH EMBEDDING METHODS FOR SEMI-SUPERVISED NODE CLASSIFICATION.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>αLoNGAE (this work)</cell><cell>0.783</cell><cell>0.716</cell><cell>0.794</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF LINK PREDICTION AND NODE CLASSIFICATION PERFORMANCES OBTAINED BY THE αLONGAE MODEL IN THE MULTI-TASK LEARNING SETTING. LINK PREDICTION PERFORMANCE IS REPORTED AS THE COMBINED AVERAGE OF AUC AND AP SCORES. ACCURACY IS USED FOR NODE CLASSIFICATION PERFORMANCE.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell cols="2">Link Prediction</cell><cell></cell><cell></cell></row><row><cell>Multi-task αLoNGAE</cell><cell>0.946</cell><cell>0.949</cell><cell>0.944</cell></row><row><cell>Task-specific αLoNGAE</cell><cell>0.948</cell><cell>0.960</cell><cell>0.962</cell></row><row><cell>Task-specific VGAE [15]</cell><cell>0.920</cell><cell>0.914</cell><cell>0.965</cell></row><row><cell cols="3">Node Classification</cell><cell></cell></row><row><cell>Multi-task αLoNGAE</cell><cell>0.790</cell><cell>0.718</cell><cell>0.804</cell></row><row><cell>Task-specific αLoNGAE</cell><cell>0.783</cell><cell>0.716</cell><cell>0.794</cell></row><row><cell>Task-specific GCN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning: a knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A marginalized denoising method for link prediction in relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network-based drugtarget interaction prediction with Probabilistic Soft Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Raschid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="775" to="787" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collective spammer detection in evolving multi-relational social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 9</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey on statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bahareh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Canadian Conference on Advances in Artificial Intelligence</title>
		<meeting>the 23rd Canadian Conference on Advances in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The time-series link prediction problem with applications in communication surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="286" to="303" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modified mean and variance normalization: transforming to utterance-specific estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circuits, Systems, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1593" to="1609" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01715</idno>
		<title level="m">Training deep autoencoders for collaborative filtering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<title level="m">Focal loss for dense object detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Link prediction via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting>the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transforming graph data for statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="363" to="441" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AutoRec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A data and model-parallel, distributed and scalable framework for training of deep networks in Apache Spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayadeva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05840</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gaudel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00806</idno>
		<title level="m">Hybrid collaborative filtering with autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Leveraging social media networks for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Line: largescale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A fully convolutional neural network for cardiac segmentation in short-axis MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00494</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional joint representation learning with symmetrical deep neural networks for multimodal and crossmodal applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vukotić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<meeting>the 2016 ACM International Conference on Multimedia Retrieval (ICMR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5118</idno>
		<title level="m">Link prediction in social networks: the state-of-the-art</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04038</idno>
		<title level="m">Trace norm regularised deep multi-task learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Entity and relationship labeling in affiliation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
