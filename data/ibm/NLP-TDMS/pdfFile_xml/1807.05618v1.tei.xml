<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Quispe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<postCode>13083-852</postCode>
									<settlement>Campinas</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">University of Campinas</orgName>
								<address>
									<postCode>13083-852</postCode>
									<settlement>Campinas</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>person re-identification</term>
					<term>deep learning</term>
					<term>multi-clue guided learning</term>
					<term>human semantic parsing</term>
					<term>saliency detection</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a video or an image of a person acquired from a camera, person re-identification is the process of retrieving all instances of the same person from videos or images taken from a different camera with non-overlapping view. This task has applications in various fields, such as surveillance, forensics, robotics, multimedia. In this paper, we present a novel framework, named Saliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of the capabilities of both clues: saliency and semantic parsing maps, to guide a backbone convolutional neural network (CNN) to learn complementary representations that improves the results over the original backbones. The insight of fusing multiple clues is based on specific scenarios in which one response is better than another, thus favoring the combination of them to increase performance. Due to its definition, our framework can be easily applied to a wide variety of networks and, in contrast to other competitive methods, our training process follows simple and standard protocols. We present extensive evaluation of our approach through five backbones and three benchmarks. Experimental results demonstrate the effectiveness of our person re-identification framework. In addition, we combine our framework with re-ranking techniques to achieve state-of-the-art results on three benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) is a very challenging problem that aims to find all entities that have the same identification (ID) across cameras with respect to a gallery of individuals for a given probe (query). The probe and gallery are recorded from different camera views.</p><p>Some challenges associated with the Re-ID problem include occlusions, complex background, illumination conditions. However, the most difficult scenario is the occurrence of extreme changes in pose/viewpoint. Re-ID is typically defined in a setting where no high resolution images are available (for example, security cameras installed in universities and airports). Since methods based on face recognition are not effective to be applied, current approaches are based on the appearance of people. More recently, the use of Convolutional Neural Networks (CNN) has become popular in this task.</p><p>All of the previously mentioned problems and constraints make Re-ID a difficult task, even for humans.</p><p>Email address: helio@ic.unicamp.br (Helio Pedrini) Consider a scenario in which two different people are wearing similar clothing, with only a few difference in the colors of their belts and shoes. These details can be key clues to distinguishing people. In a security camera, small detail, such as the color of shoes or belt, may not be sufficiently clear so that they may be ignored by a human operator, where the two people would be considered as the same person. This realistic situation occurs in several Re-ID datasets, which present low resolution images, scaling changes, or misalignment in bounding boxes.</p><p>It is worth mentioning that the Re-ID task considers as input the bounding boxes around the people in the scene, which can be a sequence of images or videos. In this work, we focus on images, however, our framework can be easily extended to the video person Re-ID.</p><p>Since pose/viewpoint change are crucial issues for Re-ID, several approaches are based on separate horizontal-stripe images and compare people based on them, but such a method is not a complete solution. As pointed out by Kalayeh et al. <ref type="bibr" target="#b0">[1]</ref>, semantic parsing is a natural improvement for horizontal stripes because it provides labels at the pixel level, so we decided to use this insight in our framework. In addition, we realized that not every part of people is equally informative; in some cases, a backpack with a bright color or other salient objects may be a clue to the Re-ID. Thus, we designed a unified framework that unities semantic parsing and saliency to improve performance. The idea of combining multiple clues is natural to this problem, because each subnet stream of the framework can learn to solve different scenarios.</p><p>The main contributions of this paper are summarized as follows. Initially, we introduce a novel framework using saliency and semantic parsing. To the best of our knowledge, this is the first work that combines these two clues for Re-ID. Extensive experiments on three datasets and five backbones show the ability of our method to improve results and suggest that it can be used with many other backbones because of its definition. Different from other competitive methods, our framework takes full advantage of pretrained models and require a minimum number of fine-tuning epochs to reach competitive results. Moreover, our training process does not need to combine multiple Re-ID benchmarks.</p><p>Our framework, combined with re-ranking techniques, achieved state-of-the-art results on the three most widely used and challenging Re-ID datasets. We compared our work with the most competitive approaches available in the literature, yielding improvements of up to 4.1% in mAP and 1.8% rank-1.</p><p>The remainder of this paper is organized as follows. Section 2 briefly reviews saliency detection, semantic parsing detection, as well as methods that use these concepts in the context of person re-identification. Section 3 defines the re-identification problem and models that can be used as backbones of our framework, then it describes our method. Section 4 offers implementation details, validation protocols, evaluation and comparison with the state-of-art. Section 5 concludes the paper with some final remarks and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>This section reviews some relevant concepts and works associated with the research topic investigated in this work. Techniques for salient object detection, human semantic parsing, and person re-identification are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Salient Object Detection</head><p>Saliency detection is a task that aims to identify the fixation points that a human viewer would focus at the first glance. It has applications in various vision tasks, such as image segmentation, object detection, video summarization, compression, just to mention a few of them <ref type="bibr" target="#b1">[2]</ref>.</p><p>Early approaches to saliency detection were driven through local low-level features -such as intensity, color, orientation and texture -or global features based on finding regions in the image, which implies unique frequencies in the Fourier domain <ref type="bibr" target="#b2">[3]</ref>. In the last years, deep models have become the mainstream solution due to the CNNs capacity for representing multi-scale and multi-level features. Some current approaches include Multi-Layer Perceptrons (MLP) and Fully Convolutional Neural Network (FCNN) <ref type="bibr" target="#b3">[4]</ref>.</p><p>The first work that used the concept of saliency in the context of person re-identification was proposed by Zhao et al. <ref type="bibr" target="#b4">[5]</ref>. Their approach is based on a patch matching-based method. Each image patch has an associated saliency that is computed in an unsupervised fashion, then matching is computed inside the patchneighborhood using hand-crafted features. A matching between patches with too different saliency brings a penalty to the model. Thus, the model is fitted to minimize the total cost of patch matching. Differently to this work, we do not use any patch matching-based approach and use deep features to encode person characteristics.</p><p>Liu et al. <ref type="bibr" target="#b5">[6]</ref> proposed an attentive-based method, named HydraPlus-Net. Although the authors do not use the concept of saliency, the idea is related because they guide their network to focus more on specific regions of the image. Differently from this work, our approach first computes the salient object map from the input image and then uses this map to weigh an intermediate layer of the CNN backbone. Another difference is that our training pipeline does not include the saliency detection step as part of its process. Finally, our framework is designed to be capable to use different type of backbones (ResNet <ref type="bibr" target="#b6">[7]</ref>, DenseNet <ref type="bibr" target="#b7">[8]</ref>, among others), whereas HydraPlus-Net is designed to use Inception <ref type="bibr" target="#b8">[9]</ref> blocks for its construction.</p><p>Similar to HydraPlus-Net, Zhou et al. <ref type="bibr" target="#b9">[10]</ref> proposed to learn saliency maps and Re-ID at the same time. They introduced a weighted version of bilinear coding <ref type="bibr" target="#b10">[11]</ref> to encode higher-order channel-wise interactions. The main difference from our framework is that saliency maps are computed through the raw image in our pipeline, whereas the work by Zhou et al. <ref type="bibr" target="#b9">[10]</ref> uses the output of the GoogLeNet <ref type="bibr" target="#b8">[9]</ref> as input to their saliency Part-Net.</p><p>Qian et al. <ref type="bibr" target="#b11">[12]</ref> proposed a network that learns saliency from their Re-ID pipeline. They accurately pointed out that features at different scales are not a well-solved problem for Re-ID. Their proposal, named MuDeep Net, is a network capable of learning features at different scales and creating saliency masks to emphasize channels with highly discriminative features. In our framework, we guide the network to learn from saliency and semantic parsing maps, without using multi-scale information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Human Semantic Parsing</head><p>Human semantic parsing aims to segment human image into regions with fine-grained meaning, which has applications in Re-ID and human behavior analysis <ref type="bibr" target="#b12">[13]</ref>. In its general form, semantic parsing has applications in several other domains, such as image montage, object colorization, stereo scene parsing, and medical segmentation <ref type="bibr" target="#b13">[14]</ref>.</p><p>Kalayeh et al. <ref type="bibr" target="#b0">[1]</ref> demonstrated that the use of semantic parsing can boost up results in Re-ID. They proposed to use an Inception-based network <ref type="bibr" target="#b8">[9]</ref> that computes semantic maps and generates features for global representation of the input. Then, the feature map before the last average pooling is multiplied by the parsing maps to create a local representation. Our framework presents similarities with their method in the sense that we also employ human semantic parsing in Re-ID, but there are some key differences: the first one is that we use intermediate layers instead of the last one since we consider that very deep layer representation encodes too abstract information and it is not intuitive to combine it in a meaningful way with semantic and saliency maps. Second, we introduce saliency in our framework, as our experiments indicated, saliency and parsing maps contain complementary information able to enhance results. In fact, when integrated with re-ranking techniques, we achieved state-of-the-art performance. Finally, our training process does not require to combine various benchmarks for creating a huge training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Person Re-Identification</head><p>Person re-identification (Re-ID) is defined as the task of matching all instances of the same person across multiple cameras, that is, comparing a person of interest, named probe, against a gallery of candidates previously captured. Re-ID has applications related to surveillance of public areas/events for preventing dangerous events (such as terrorism and murder). For this reason, it has received attention from the computer vision community and has been widely studied over the last years.</p><p>Early works focused on handcrafted features such as color and texture, however, due to extreme viewpoint and illumination changes, these types of characteristics are not sufficiently discriminative. Currently, Deep Learning has established a new paradigm in the Re-ID problem.</p><p>Chang et al. <ref type="bibr" target="#b14">[15]</ref> proposed Multi-Level Factorization Net (MLFN) that encodes features at multiple semanticlevels. MLFN is composed of various stacked blocks with the same architecture and block selection modules that learn to interpret content of input images. The insight behind the selection blocks is to control and specialize the features that each block is learning.</p><p>Zhao et al. <ref type="bibr" target="#b15">[16]</ref> uses GoogLeNet <ref type="bibr" target="#b8">[9]</ref> to extract features. Then, a multi-branch architecture uses these features to detect discriminative regions and create a partaligned representation. From this idea, they were able to overcome misalignment and pose changes. Differently from this work, Su et al. <ref type="bibr" target="#b16">[17]</ref> extracted person parts directly from the input images through a pose estimator trained independently. Then, they extracted features from complete images and parts. In the case of local clues, their architecture considers affine transformations. Finally, because pose estimation may be affected by pose changes or occlusions, they combined parts and global features using a weighted sub-net.</p><p>Li et al. <ref type="bibr" target="#b17">[18]</ref> proposed Harmonious Attention Network (HA-CNN), which focuses on learning finegrained relevant pixels and coarse latent regions at the same time. HA-CNN is based on Inception <ref type="bibr" target="#b8">[9]</ref> blocks in a multi-branch structure for global and local representation. They further introduced a method for combining these representations in a harmonious way.</p><p>To compensate for viewpoint changes, Sarfraz et al. <ref type="bibr" target="#b18">[19]</ref> proposed to create a pose-discriminative embedding. They trained a network that learns specialized features depending on the pose of the input: front, back or side. They also used body joint keypoints in order to guide the CNNs attention. Moreover, they proposed a new re-ranking technique, named Expanded Cross Neighborhood. Results were improved based on distance between gallery and probe features. Zhong et al. <ref type="bibr" target="#b19">[20]</ref> proposed a re-ranking method based on kreciprocal nearest neighbors and Jaccard distance. It is worth mentioning that re-ranking methods are unsupervised and do not need any human interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we describe the person reidentification problem more formally and present our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We consider Re-ID as a retrieval process, that is, given a query person x p with ID y p and a gallery of m people X = {x 1 , x 2 , . . . , x m } with IDs Y = {y 1 , y 2 , . . . , y m }, then Re-ID aims to recover all</p><formula xml:id="formula_0">x i , (1 ≤ i ≤ m) such that y i = y p .</formula><p>Suppose that a model M(θ) with learned parameters θ is capable of representing x p and people in X with feature maps f p and F = { f 1 , f 2 , . . . , f m }, respectively. Thus, we can use Euclidean distance to compare f p against each element of F and construct a ranked list based on the similarity of the feature maps. Depending on the application and context in which Re-ID is used, this ranked list may be cut off in the top 1, 5 or more. The resulting list L (also referred as ranked list) is employed to represent only people that have identity equal to y p 1 . In this work, we create a model M that uses M as backbone, such that the list L generated by M is better than L. The quantitative definition of better is based on the mean Average Precision (mAP) and cumulated matching characteristics (CMC). Both metrics are explained in the experiment section. Due to the definition of M(θ), our framework can be applied to many different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person Re-ID Framework</head><p>Based on the fact that a challenging issue for the reidentification task is caused by dramatic pose/viewpoint changes, we propose to combine global representation with saliency and semantic parsing masks. As shown in our experiments, these two types of masks generate complementary feature maps that improve results over the original CNN backbones. Saliency is important for Re-ID because in specific scenarios <ref type="figure" target="#fig_0">(Figure 1)</ref> where people have certain items that can guide the reidentification process.</p><p>However, saliency is not a complete solution to the problem because it focuses on some areas of the image and may be affected by occlusions. Thus, we use semantic parsing to encode every part of the person and overcome misalignment in the bounding box detection and occlusions ( <ref type="figure" target="#fig_1">Figure 2)</ref>.</p><p>We propose the Saliency-Semantic Parsing (SSP-ReID) framework, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which is composed of two streams. Both of them have the same backbone architecture, however, without sharing weights. One of the streams (named S-ReID subnetwork) focuses on getting global-saliency features, whereas the other (named SP-ReID subnetwork) focuses on getting global-semantic-parsing features. The output of our framework is a feature map that is used to compare query and gallery images. <ref type="bibr" target="#b0">1</ref> L may not be totally correct, since M(θ) may not be perfect.  Given the input image, we compute the saliency and semantic parsing maps using off-the-shelf deep methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>. For the semantic parsing, we consider 5 semantic areas 2 . Then, we use a CNN to create a global representation of the person. Moreover, we take the feature map from an intermediate layer and join it with saliency map in one stream, and semantic parsing maps in the other. We decide to use an intermediate layer since it is widely known that CNNs encode more abstract and higher semantic-level features as they go deeper (for instance, the relationship of head location between the input image and the very deep feature map <ref type="bibr" target="#b1">2</ref> Head, upper body, lower body, shoes and complete body. may not be clear at first glance). Thus, it is more intuitive to combine raw saliency/semantic parsing maps with an intermediate layer as it does not have too abstract information and, at the same time, it encodes rich information.</p><p>Given an intermediate tensor τ ∈ R h×w×c and a saliency/semantic parsing map ω ∈ R h ×w , in order to join intermediate feature tensor and saliency/semantic parsing information, we initially apply a bilinear interpolation over the tensor to transform τ ∈ R h ×w ×c . Then, we apply element-wise product between every channel of the tensor and the map. Finally, we use average pooling to obtain the feature vector v. For the saliency feature join, the output feature is inside R c , whereas for semantic parsing feature join is inside R 5c due to the 5 semantic regions considered.</p><p>In order to train our network, we consider crossentropy loss function with label smoothing regularizer (LSR) <ref type="bibr" target="#b21">[22]</ref> and triplet loss with hard positive-negative mining <ref type="bibr" target="#b22">[23]</ref>.</p><p>Cross-entropy with LSR is defined as:</p><formula xml:id="formula_1">H(q , p) = − K k=1 log p(k)q (k) = (1 − )H(q, p) + H(u, p)<label>(1)</label></formula><p>where K is the size of training batch, is a regularizer value, p(k) is the output of the model, q is the groundtruth distribution, u is the uniform distribution and q is defined as :</p><formula xml:id="formula_2">q (k) = (1 − )q(k) + K<label>(2)</label></formula><p>LSR is a change in the ground-truth labels distribution, which aims to make the model more adaptable by adding prior distribution over the labels. We consider this loss over general cross entropy in order to avoid the largest logit from becoming much larger than all others, this prevents overfit.</p><p>Triplet loss with hard positive-negative mining is de-fined as:</p><formula xml:id="formula_3">T (X) = P i=1 N a=1 [m + max p=1...N D( f (x i a ), f (x i p )) − min p=1...N n=1...N i j D( f (x i a ), f (x j n ))] +<label>(3)</label></formula><p>where X is a training batch, with P people and N images per person, f ( . ) is the output feature map of the network, x i j is the j-th image of the i-th person, D( . , . ) is a distance function (e.g. Euclidean), [σ] + denotes max(σ , 0) and m is hyperparameter named margin. Basically, this loss finds the pair of images of the same person with maximum distance and the pair of images of different people with minimum distance and guides the model to make the difference between these two at least equal to the margin m.</p><p>SP-ReID and S-ReID subnetworks are trained separately and, depending on the backbone, we use the sum of both losses or only cross-entropy with LSR. To train our network, we add a multi-class classification layer to the end of the subnetwork. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the network architecture for the training step, as well as its relation to the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, the parameter setting and datasets used in our experiments will be first introduced. Next, we will show the results obtained with our proposed method and compare them with state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>The saliency detection is performed via the off-theshelf FCNN proposed by Li et al. <ref type="bibr" target="#b20">[21]</ref>, whereas the semantic parsing detection is computed through the Joint Human Parsing and Pose Estimation Network (JJPNet) <ref type="bibr" target="#b12">[13]</ref> trained in the Look into Person (LIP) dataset <ref type="bibr" target="#b23">[24]</ref>.</p><p>We evaluate our framework using 5 different backbones. <ref type="table" target="#tab_0">Table 1</ref> summarizes the loss function used for each backbone. The initial weights of all backbones are Imaginet <ref type="bibr" target="#b24">[25]</ref> pretrained models. In the case of the intermediate layer to be combined with saliency and semantic parsing maps: (i) we use the output of layer Res5C for ResNet50 <ref type="bibr" target="#b6">[7]</ref> and ResNet50-M <ref type="bibr" target="#b25">[26]</ref>; (ii) we use the output last Inception-B block for Inception-V4 <ref type="bibr" target="#b26">[27]</ref>; (iii) we use the output of Middle Flow layer for Xception <ref type="bibr" target="#b27">[28]</ref>; and (iv) we use the output of the second composite function for DenseNet121 <ref type="bibr" target="#b7">[8]</ref>.  <ref type="bibr" target="#b21">[22]</ref>, whereas TRIP stands for Triplet Loss with hard positive-negative mining <ref type="bibr" target="#b22">[23]</ref>. Using TRIP in Inception-V4 <ref type="bibr" target="#b26">[27]</ref> and Xception <ref type="bibr" target="#b27">[28]</ref> raises exploding gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Loss function</p><p>ResNet50 <ref type="bibr" target="#b6">[7]</ref> TRIP + CROSSE Densenet121 <ref type="bibr" target="#b7">[8]</ref> TRIP + CROSSE Resnet50-M <ref type="bibr" target="#b25">[26]</ref> TRIP + CROSSE Inception-V4 <ref type="bibr" target="#b26">[27]</ref> CROSSE Xception <ref type="bibr" target="#b27">[28]</ref> CROSSE</p><p>We adjust the size of the input to 254 × 128 pixels and saliency/semantic parsing maps to 128 × 64 pixels. Adam optimizer is used with training batch of 32, initial learning rate of 0.0003, weight decay of 0.0005, and a learning rate decay factor of 0.1 every 60 epochs. We also fix the number of training epochs to 180 for every backbone. In the LSR implementation, we set = 0.1 and triplet loss as m = 0.3. Finally, we use the re-ranking method proposed by Zhong et al. <ref type="bibr" target="#b19">[20]</ref> to compare our results with state-of-the-art approaches 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Validation Protocols</head><p>We evaluate our framework on three widely used datasets. A summary of them is shown in <ref type="table" target="#tab_1">Table 2</ref>, which reports the number of people, bounding boxes and cameras present in each benchmark setup. The DukeMTMC-ReID dataset <ref type="bibr" target="#b30">[31]</ref> is a subset of the DukeMTMC dataset <ref type="bibr" target="#b31">[32]</ref> for image-based re-identification with hand-drawn bounding boxes. Bounding boxes of different sizes with outdoor scenes as background are available. For validation, we use the fixed training and testing sets proposed in the original protocol of the dataset.</p><p>The Market1501 <ref type="bibr" target="#b28">[29]</ref> was created through Deformable Part Model (DPM) in order to simulate a real-world scenario. For validation, we use the fixed training and testing sets provided with the dataset. For the triplet loss, we take the feature vector before softmax layer and use it to compare images based on the Euclidean distance. The triplet loss may be ignored depending on the CNN backbone.</p><p>The CUHK03 <ref type="bibr" target="#b29">[30]</ref> has an average of 4.8 images per view. Misalignment, occlusions and missing body parts are quite common. For validation, we use the new validation protocol <ref type="bibr" target="#b19">[20]</ref> with partition of 767/700. Moreover, we evaluate detected (CUHK03 (D)) and labeled (CUHK03 (L)) versions of the dataset.</p><p>Quantitative results for every dataset are based on mean Average Precision (mAP) and Cumulative Matching Curve (CMC). The mAP considers the order in which the gallery is sorted for a given query, defined as:</p><formula xml:id="formula_4">mAP = AP #queries<label>(4)</label></formula><p>where AP is defined as</p><formula xml:id="formula_5">AP = n k=1 P(k)· rel(k) #relevant items<label>(5)</label></formula><p>where n is the number of recovered items, rel(k) is equal to 1 if the k-th item is relevant to the query and 0 otherwise, and P(k) is defined as:</p><formula xml:id="formula_6">P(k) = k i=1 rel(i) k<label>(6)</label></formula><p>The CMC represents the probability that a correct match with the query identity will appear in variablesized ranked list:</p><formula xml:id="formula_7">CMC(r) = in(r) #queries<label>(7)</label></formula><p>where in(r) is the number of queries that have a relevant element within the first r items in the ranked list. We set r = 1 and refer to it as rank-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance in Re-ID</head><p>In this section, we evaluate and compare different aspects of our framework: backbone (e.g., ResNet <ref type="bibr" target="#b6">[7]</ref>), saliency subnet (S-ReID), semantic parsing subnet (SP-ReID) and the complete framework (SSP-ReID). Results are summarized in <ref type="table">Table 3</ref>. Overall, ResNet50-M + SSP-ReID produced the best results for all datasets, whereas there are marginal differences in using ResNet50 <ref type="bibr" target="#b6">[7]</ref> and DenseNet <ref type="bibr" target="#b7">[8]</ref> as backbones. On the other hand, Inception-V4 <ref type="bibr" target="#b26">[27]</ref> and Xception <ref type="bibr" target="#b27">[28]</ref> yielded the worse performance. Note that DenseNet <ref type="bibr" target="#b7">[8]</ref> gets interesting results despite of its lower number of parameters. In addition, our framework raises consistently an improvement over all backbones, this suggest that our framework can be used as a enhancing method in future works.</p><p>In the light of all datasets, S-ReID achieved improvements up to 1.3% for mAP and up to 4.4% for rank-1 over individual backbones, however, in general the improvements were marginal.</p><p>There are also cases where results were marginally worse. This same scenario is repeated for SP-ReID, but if we consider the complete framework (combination of S-ReID and SP-ReID), we consistently obtained better results, with improvements up to 7.4% (mAP) and 7.2% (rank-1) in Market1501, 6.8%(mAP) and 7.7%(rank-1) in CUHK03 (D), 4.9%(mAP) and 4.5%(rank-1) in CUHK03 (L), 6.7%(mAP) and 8.5% (rank-1) for DukeMTMC-reID. This suggests that <ref type="table">Table 3</ref>: Results of framework in Re-ID. ResNet + S-Reid stands for Saliency subnet using ResNet as backbone. Analogously, SP-ReID refers to Semantic Parsing subnet, whereas SSP-ReID refers to the complete framework. We highlight in red color the cases in which the subnetwork/framework is worse than the original backbone, whereas cases with better results than backbone are highlighted in blue color. S-ReID and SP-ReID are learning similar performance representation, but with complementary information, which improves the model capacity when they are combined. <ref type="figure">Figure 5</ref> shows an example of rank-2 results for DukeMTMC-ReID <ref type="bibr" target="#b30">[31]</ref>. It can be observed that the method improvement is inversely proportional to the capacity of the backbone. For better backbones (for instance, ResNet50-M <ref type="bibr" target="#b25">[26]</ref>), the improvements are smaller when compared to the lower performance backbones (for instance, Xception <ref type="bibr" target="#b27">[28]</ref>). This is related to the complexity of datasets: as we start achieving high results in mAP or rank-1, we need much more higher discriminative models that can deal with more specific and often sparse cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance Comparison</head><p>We evaluate our method with various competitive approaches available in the literature. A performance comparison is presented in <ref type="table" target="#tab_3">Table 4</ref>, where we applied re-ranking (RR) to boost our final results.</p><p>Our method was able to achieve state-of-the-art in all datasets. Overall, performance improvement is higher for mAP than rank-1, because mAP considers a greater number of elements in its definition. Thus, it is more sensitive to any change in the ranking list. SPreID <ref type="bibr" target="#b0">[1]</ref> is the method with the closest performance, however, unlike such approach, our framework does not have to be trained with 10 datasets, we only need 180 epochs in each dataset. In addition, our framework is easier to implement compared to other methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we presented SSP-ReID, a framework based on saliency and semantic parsing information, which achieved state-of-the-art results on three challenging datasets for the re-identification task. SSP-ReID is composed of two subnetworks, a saliencyguided subnet that aims to focus on learning in specific parts of the image and a semantic parsing-guided subnet for dealing with misalignments, occlusions, and other challenging issues for the person re-identification task. We conducted extensive evaluation of our framework on five different backbones and three datasets. The representation learned from the saliency-guided and semantic parsing-guided subnetworks has similar performance to that of the individual backbones, however, both combined boost the results, indicating that the learned representation is complementary.</p><p>Our framework can be easily adapted to multiple CNN backbones, further improving performance over original networks. We expect that the use of multiple clues (for instance, human pose and multi-scale strategies) inspires other re-identification works. <ref type="figure">Figure 5</ref>: Qualitative example of performance of framework modules. The first column represents the query, the second and third columns represent the results of rank-1 and rank-2, respectively (the correct results are in green, otherwise they are in red). The first, second and third rows represent the output of S-ReID, SP-ReID and SSP-ReID, respectively. For the same query, S-ReID and SP-ReID achieved different correct rank-1 results and wrong rank-2 results. On the other hand, SSP-ReID can combine the best characteristics of both subnets and reach correct rank-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of saliency detection for the same person (from left to right): original image, saliency map, and result of overlap saliency map over the original image. The focus of the saliency is on the arm and white bag. Our framework uses this information to guide the feature learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of parsing with five semantic regions of the same person with two different views. We use these maps to overcome misalignment and occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>SSP-ReID is a framework based on semantic parsing (SP-ReID subnet) and saliency (S-ReID subnet) to learn individual-similar performance representations. At the same time both representations are complementary because the union leads to an increase in performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training setup for S-ReID and SP-ReID subnetworks. When training out framework, we consider triplet and cross-entropy loss functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Loss function used for each backbone. CROSSE stands for Cross Entropy with LSR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparative summary of Re-ID datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># People # BBox # Cameras</cell></row><row><cell>Market1501 [29]</cell><cell>1501</cell><cell>32668</cell><cell>6</cell></row><row><cell>CUHK03 [30]</cell><cell>1467</cell><cell>14096</cell><cell>6</cell></row><row><cell>DukeMTMC-ReID [31]</cell><cell>1812</cell><cell>36411</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-art, in bold the best results, RR stands for re-ranking</figDesc><table><row><cell></cell><cell cols="2">Market1501</cell><cell cols="2">CUHK03 (D)</cell><cell cols="2">CUHK03 (L)</cell><cell cols="2">DukeMTMC-reID</cell></row><row><cell>Method</cell><cell cols="8">mAP(%) rank-1(%) mAP(%) rank-1(%) mAP(%) rank-1(%) mAP(%) rank-1(%)</cell></row><row><cell>SPreID [1]</cell><cell>83.3</cell><cell>93.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.3</cell><cell>85.9</cell></row><row><cell>DaRe(De)+RE+RR [33]</cell><cell>86.7</cell><cell>90.9</cell><cell>71.6</cell><cell>70.6</cell><cell>74.7</cell><cell>73.8</cell><cell>80.0</cell><cell>84.4</cell></row><row><cell>DuATM [34]</cell><cell>76.6</cell><cell>91.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64.5</cell><cell>81.8</cell></row><row><cell>HA-CNN [18]</cell><cell>75.7</cell><cell>91.2</cell><cell>41.0</cell><cell>44.4</cell><cell>38.6</cell><cell>41.7</cell><cell>63.8</cell><cell>80.5</cell></row><row><cell>ATWL [35]</cell><cell>75.6</cell><cell>89.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.4</cell><cell>79.8</cell></row><row><cell>PSE [19]</cell><cell>84.0</cell><cell>90.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.8</cell><cell>85.2</cell></row><row><cell>MLFN [15]</cell><cell>74.3</cell><cell>90.0</cell><cell>47.8</cell><cell>52.8</cell><cell>49.2</cell><cell>54.7</cell><cell>62.8</cell><cell>81.0</cell></row><row><cell>SVDNet [36]</cell><cell>62.1</cell><cell>82.3</cell><cell>37.8</cell><cell>40.9</cell><cell>37.2</cell><cell>41.5</cell><cell>56.8</cell><cell>76.7</cell></row><row><cell>ResNet + SSP-ReID</cell><cell>75.9</cell><cell>89.3</cell><cell>57.1</cell><cell>59.4</cell><cell>58.9</cell><cell>60.6</cell><cell>66.1</cell><cell>80.1</cell></row><row><cell>ResNet + SSP-ReID + RR</cell><cell>88.2</cell><cell>91.5</cell><cell>71.1</cell><cell>67.6</cell><cell>72.4</cell><cell>68.4</cell><cell>81.4</cell><cell>84.8</cell></row><row><cell>ResNet-M + SSP-ReID</cell><cell>80.1</cell><cell>92.5</cell><cell>60.5</cell><cell>63.1</cell><cell>63.3</cell><cell>65.6</cell><cell>68.6</cell><cell>81.8</cell></row><row><cell>ResNet-M + SSP-ReID + RR</cell><cell>90.8</cell><cell>93.7</cell><cell>75.0</cell><cell>72.4</cell><cell>77.5</cell><cell>74.6</cell><cell>83.7</cell><cell>86.4</cell></row><row><cell>DenseNet + SSP-ReID</cell><cell>76.7</cell><cell>90.9</cell><cell>48.1</cell><cell>48.1</cell><cell>49.5</cell><cell>49.1</cell><cell>67.1</cell><cell>82.2</cell></row><row><cell>DenseNet + SSP-ReID + RR</cell><cell>89.9</cell><cell>93.3</cell><cell>63.1</cell><cell>58.4</cell><cell>64.7</cell><cell>59.9</cell><cell>83.3</cell><cell>86.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code and models will be available upon acceptance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors are thankful to FAPESP (grant #2014/12236-1) and CNPq (grant #305169/2015-7) for their financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human Semantic Parsing for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gökmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00871</idno>
		<title level="m">Deep Learning For Video Saliency Detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-aware Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5878</idno>
		<title level="m">Salient Object Detection: A Survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person Re-identification by Salience Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis, IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Densely Connected Convolutional Networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08580</idno>
		<title level="m">Weighted Bilinear Coding over Salient Body Parts for Person Re-identification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilinear CNN Models for Fine-Grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05165</idno>
		<title level="m">Multi-scale Deep Learning Architectures for Person Re-identification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Look Into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Survey of Human Pose Estimation: The Body Parts Parsing based Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-Level Factorisation Net for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-Learned Part-Aligned Representations for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Posedriven Deep Convolutional Model for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonious Attention Network for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Re-ranking Person Reidentification with k-reciprocal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the Inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Look into Person: Joint Body Parsing &amp; Pose Estimation Network and a New Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08106</idno>
		<title level="m">The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-Resnet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<title level="m">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IEEE Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable Person Re-identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepreid: Deep Filter Pairing Neural Network for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resource Aware Person Reidentification across Multiple Resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8042" to="8051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Features for Multi-Target Multi-Camera Tracking and Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Svdnet for Pedestrian Retrievala</title>
		<imprint/>
	</monogr>
	<note>arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
