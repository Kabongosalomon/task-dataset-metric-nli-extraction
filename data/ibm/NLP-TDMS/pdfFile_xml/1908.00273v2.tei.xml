<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pyramid Real Image Denoising Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuqing</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidong</forename><surname>Men</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Ju</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">GuangDong TUS-TuWei Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pyramid Real Image Denoising Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Real Image Denoising</term>
					<term>Convolutional Neural Networks</term>
					<term>Channel Attention</term>
					<term>Pyramid Pooling</term>
					<term>Kernel Selecting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep Convolutional Neural Networks (CNNs) have shown extraordinary capability of modelling specific noise and denoising, they still perform poorly on real-world noisy images. The main reason is that the real-world noise is more sophisticated and diverse. To tackle the issue of blind denoising, in this paper, we propose a novel pyramid real image denoising network (PRIDNet), which contains three stages. First, the noise estimation stage uses channel attention mechanism to recalibrate the channel importance of input noise. Second, at the multi-scale denoising stage, pyramid pooling is utilized to extract multi-scale features. Third, the stage of feature fusion adopts a kernel selecting operation to adaptively fuse multi-scale features. Experiments on two datasets of real noisy photographs demonstrate that our approach can achieve competitive performance in comparison with state-of-the-art denoisers in terms of both quantitative measure and visual perception quality. Code is available at https://github.com/491506870/PRIDNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image denoising aims at restoring a clean image from its noisy one, which plays an essential role in low-level visual tasks. There has been considerable research on it, and nearoptimal performances have been achieved for the removal of statistical distribution-regular noise (e.g., additive white Gaussian noise (AWGN), shot noise). Nevertheless, there is still a huge difference between specific noise and real-world noise. Among the latter, the noise comes from both shooting environment and image processing pipeline, thus its forms demonstrate complexity and diversity.</p><p>Recently deep Convolutional Neural Networks (CNNs) have led to significant improvements on denoising for specific noise. Mao et al. <ref type="bibr" target="#b0">[1]</ref> present a very deep fully convolutional encoding-decoding framework with symmetric skip connections for Gaussian denoising, termed REDNet. Zhang et al. <ref type="bibr" target="#b1">[2]</ref> demonstrate that by merging residual learning and batch normalization, a denoising CNN (DnCNN) could outperform traditional non-CNN based methods. Other CNN methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> also obtain promising denoising performance.</p><p>However, once the methods targeting for the specific noise above generalize to real-world noise, the performance may be even worse than the representative traditional methods such as BM3D <ref type="bibr" target="#b4">[5]</ref>. Few blind denoising approaches especially for real noisy images are developed. By interactively setting relatively higher noise level, FFDNet <ref type="bibr" target="#b5">[6]</ref> can deal with more complex noise. CBDNet <ref type="bibr" target="#b6">[7]</ref> further utilizes a noise estimation subnetwork, so that the entire network could achieve endto-end blind denoising. Yu et al. <ref type="bibr" target="#b7">[8]</ref> propose a multi-path CNN named Path-Restore, which could dynamically select an appropriate route for each image region, especially for the varied noise distribution of a real noisy image. As a commercial plug-in for Photoshop, Neat Image (NI) is also for bind denoising. Despite these methods have significantly improved the performance of real image denoising, there still remains three issues to be noticed:</p><formula xml:id="formula_0">(a) Noisy Image (b) BM3D (c) DnCNN (d) FFDNet (e) N3Net (f) CBDNet (g) Path-Restore (h) Ours</formula><p>First, in most of CNN based denoising methods, all channelwise features are treated equally without adjustment according to their importance. In CNNs, different feature channels capture different types of noise across all regions of a single noisy image. Among them, some noise are more significant than others, thus should be assigned more weights.</p><p>Second, the previously mentioned methods, with fixed receptive fields, fail to carry diverse information. Referring to the traditional denoising method BM3D <ref type="bibr" target="#b4">[5]</ref>, it searches for similar blocks in the whole image, taking the global information into consideration. Since features are not limited to a small area, receptive fields with different scales can fully exploit hierarchical spatial features. Context information would be very helpful when the image suffers from heavy noise.</p><p>Third, for the aggregation of multi-scale features, most of existing methods simply combine them in an elementwise summation manner or just concatenate them together <ref type="bibr" target="#b0">[1]</ref>. Although containing the information of all scales, they treat features with different scales indiscriminately, ignoring  the spatial and channel specificity of scale-wise features. Therefore multi-scale features can not be adaptively expressed.</p><p>To address these issues, we propose a pyramid real image denoising network (PRIDNet) as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The main contribution of this work is three-fold:</p><p>• Channel attention: Channel attention mechanism is utilized on the extracted noise features, adaptively recalibrating the channel importance. • Multi-scale feature extraction: We design a pyramid denoising structure, in which each branch pays attention to one-scale features. Benefitted from it, we can extract global information and retain local details simultaneously, thereby making preparation for following comprehensive denoising. • Feature self-adaptive fusion: Within concatenated multiscale features, each channel represents one-scale features. We introduce a kernel selecting module. Multiple branches with different convolutional kernel sizes are fused by linear combination, allowing different feature maps to express by size-different kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED METHOD</head><p>In this section, we will formulate the proposed PRIDNet, including network architecture and three stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, our model includes three stages: noise estimation stage, multi-scale denoising stage and feature fusion stage. The input noisy image is processed by three stages sequentially. Since all the operations are spatially invariant, it is robust enough to handle input images of arbitrary size. To avoid loss of information, the output of the first stage is concatenated with its input before fed into next stage, likewise the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noise Estimation Stage</head><p>This stage focuses on extracting discriminative features from input noisy images, which is regarded as an estimation of the noise level <ref type="bibr" target="#b6">[7]</ref>. We adopt a plain five-layer fully convolutional subnetwork without pooling and batch normalization, ReLU is deployed after each convolution. In each convolution layer, the number of feature channels is set to 32 (except the last layer is 1 or 3), and the filter size is 3×3.</p><p>Before the last layer of stage, a channel attention module <ref type="bibr" target="#b8">[9]</ref> is inserted to explicitly calibrate the interdependencies between feature channels. As shown in <ref type="figure">Fig. 3</ref>, the collection of channel weight µ = [µ 1 , µ 2 , ..., µ c ] ∈ R 1×1×C is our goal, which is applied to rescale input feature maps U ∈ R H×W ×C to generate recalibrated features. We first squeeze global information of U into a channel descriptor ν ∈ R 1×1×C by using global average pooling (GAP). Then, it is followed by two fully connected layers (FC), and the number of channels in middle layer is set to 2. Above process can be formulated as µ = Sigmoid(F C 2 (ReLU (F C 1 (GAP (U ))))).</p><p>(</p><p>The final output of channel attention module (denoted as U ∈ R H×W ×C ) is obtained by</p><formula xml:id="formula_2">U = U • µ,<label>(2)</label></formula><p>where • refers to channel-wise multiplication between U i ∈ R H×W and scalar calibration weight µ i , i = 1, 2, ...C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-scale Denoising Stage</head><p>The idea of pyramid pooling is widely used in the fields of scene parsing <ref type="bibr" target="#b9">[10]</ref>, image compression and so on. While to the best of our knowledge, it has never been used in the image denoising. Zhou et al. <ref type="bibr" target="#b10">[11]</ref> show that the empirical receptive field of CNN is much smaller than the theoretical one especially on high-level layers, which means that global <ref type="figure">Fig. 3</ref>. Channel attention module architecture. The GAP denotes the global average pooling operation. F C 1 has a ReLU activation after it, and F C 2 has a Sigmoid activation after it. For concision, we omit these items. information is not fully integrated when extracting features. On the contrary, for the removal of noise covering entire image, it has great help to match goal blocks with similar content in the whole image.</p><formula xml:id="formula_3">GAP FC1 FC2 C H H W W C 1x1xC 1x1xC 1x1xC U U μ ν</formula><formula xml:id="formula_4">U U V U U U V V V H W C H W C Kernel 3x3 Kernel 7x7 5x5 GAP FC1</formula><p>To mitigate this problem, we develop a five-level pyramid. Through five parallel ways, the input feature maps are downsampled to different sizes, helping branches gain relatively scale-different receptive fields to capture original, local and global information simultaneously. Pooling kernels are set to 1×1, 2×2, 4×4, 8×8 and 16×16, respectively. Each pooled feature is then followed by U-Net <ref type="bibr" target="#b11">[12]</ref>, the network composed of deep encoding-decoding and skip connections, for studies have shown that successive upsampling and downsampling are helpful for denoising tasks. Note that five U-Nets do not share weights. At final of this stage, multi-level denoised features are upsampled by bilinear interpolation to the same size, and then concatenated together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Feature Fusion Stage</head><p>In order to choose size-different kernel for each channel within concatenated multi-scale results, inspired by <ref type="bibr" target="#b12">[13]</ref>, a kernel selecting module is introduced. Details of a kernel selecting module is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. The given feature maps U ∈ R H×W ×C are conducted by three parallel convolutions with kernel size 3, 5 and 7, respectively, to get U ∈ R H×W ×C , U ∈ R H×W ×C and U ∈ R H×W ×C . We first integrate information from all branches via element-wise summation:</p><formula xml:id="formula_5">U = U + U + U .<label>(3)</label></formula><p>Then U is shrunk and then expanded by passing through a GAP and two FCs, the same operations as in the channel attention module, but no Sigmoid at last. The three outputs of F C 2 , α ∈ R 1×1×C , β ∈ R 1×1×C and γ ∈ R 1×1×C are operated by Softmax, which is applied across branches on the channel-wise, like gating mechanism:</p><formula xml:id="formula_6">k c = e k c e α c + e β c + e γ c , k = α, β, γ,<label>(4)</label></formula><p>where α, β and γ denote the soft attention vector for U , U and U , respectively. Note that α c is the c-th element of α, likewise β c and γ c . The final output feature maps V are computed via combining various kernels with their attention weights:</p><formula xml:id="formula_7">V c = α c · U + β c · U + γ c · U ,<label>(5)</label></formula><p>where α, β and γ need to satisfy α c + β c + γ c = 1, and</p><formula xml:id="formula_8">V = [V 1 , V 2 , ..., V c ], V c ∈ R H×W .</formula><p>Finally, we utilize a 1 × 1 convolutional layer to compress the dimension to 1 or 3 for feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>For training, we utilize 320 image pairs (noisy and clean) both in raw-RGB space and sRGB space from Smartphone Image Denoising Dataset (SIDD) <ref type="bibr" target="#b13">[14]</ref>. And we set another 1280 256 × 256 crops of 40 images in SIDD as our validation data to conduct our ablation study.</p><p>For testing, we adopt two widely used benchmark datasets: DND <ref type="bibr" target="#b14">[15]</ref> and NC12 <ref type="bibr" target="#b15">[16]</ref>. DND, a benchmark of 50 real highresolution images, captured by consumer grade cameras. Since only noisy images are provided to the public, PSNR/SSIM of denoised results are obtained through the online submission system 1 . NC12 contains 12 noisy images, we only show the denoising results for qualitative evaluation as the ground-truth clean images are unavailable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We train two models, one targeting raw images and the other targeting sRGB images. The whole network is optimized with L 1 loss, and trained by Adam with β 1 = 0.9, β 2 = 0.999 and = 10 −8 . All models are trained with 4000 epochs, where the learning rate for the first 1500 epochs is 10 −4 , and then 10 −5 to finetune the model. We set patch size to 256 × 256, and batch size is set to 2 for "raw" model while 8 for "sRGB" model. All the experiments are implemented with TensorFlow on an NVIDIA GTX 1080ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State-of-the-arts</head><p>Qualitative and Quantitative Results on DND Benchmark. Following the test protocol and tools provided by the website, we process 1000 bounding boxes in 50 real noisy images. The performance of our models with respect to prior work is shown in <ref type="table" target="#tab_1">Table I</ref>. Note that we do not take unpublished or anonymous work into consideration although they have released results. We can see that although state-ofthe-art non-blind Gaussian methods, e.g., TNRD <ref type="bibr" target="#b16">[17]</ref>, BM3D <ref type="bibr" target="#b4">[5]</ref> and WNNM <ref type="bibr" target="#b18">[19]</ref>, are provided by noise level, they still have poorly performance, mainly due to the much difference between AWGN and real noise. CBDNet <ref type="bibr" target="#b6">[7]</ref> and Path-Restore <ref type="bibr" target="#b7">[8]</ref> are especially trained for blind denoising of real images, thus yield promising results. Our PRIDNet achieves large PSNR gains (i.e., ∼ 0.9dB for raw, and ∼ 0.4dB for sRGB) over the second best method. As for running time, PRIDNet takes about 0.05s to process an 512 × 512 image. The visual denoising results by the various methods are shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and the supplementary material. CBDNet <ref type="bibr" target="#b6">[7]</ref> still contains some noise. DnCNN <ref type="bibr" target="#b1">[2]</ref> suffers from edge distortion. BM3D, N3Net and Path-Restore introduce some color artifacts to the denoised results, while FFDNet <ref type="bibr" target="#b5">[6]</ref> suffers from the problems of over-smoothing and loss of details and structures. Compared with these state-of-the-arts, our method (PRIDNet) achieves a better denoising performance by removing almost all the noise while preserving more fine textural details of whole image.</p><p>Qualitative Comparisons on NC12. The methods we consider for comparison include blind denoising approaches (NC <ref type="bibr" target="#b15">[16]</ref>, NI and CBDNet <ref type="bibr" target="#b6">[7]</ref>), blind Gaussian denoising approaches <ref type="figure" target="#fig_1">(DnCNN [2]</ref>), and non-blind Gaussian denoising approaches (BM3D <ref type="bibr" target="#b4">[5]</ref> and FFDNet <ref type="bibr" target="#b5">[6]</ref>). For non-blind methods, we exploit NI to estimate the noise level std. so that they can perform the best visual quality. Due to limited space, we leave the visual comparisons of above methods in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>We conduct four ablation experiments to assess the importance of three key components in our PRIDNet. All ablation experiments are evaluated on the validation data of SIDD <ref type="bibr" target="#b13">[14]</ref>. We can conclude from <ref type="table" target="#tab_1">Table II</ref> that the design of pyramid feature processing is the most crucial for our real image denoising task, which improves 0.22dB. The combination of three key components effectively boost network performance by 0.39dB compared with the plain network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, a PRIDNet is presented for blind denoising of real images. The proposed network includes three sequential stages. The first stage explores relative importance of feature channels. At the second stage, pyramid pooling is developed to denoise multi-scale features. At the last stage, the operation of self-adaptive kernel selecting is introduced for feature fusion. Both qualitative and quantitative experiments show that our method achieves competitive performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Denoising performance of state-of-the-art methods on a DND image. Readers are encouraged to zoom in for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our proposed network (PRIDNet). The number of channels of feature maps is shown below them, for "sRGB" model it is in the parentheses, while for "raw" model it has no parentheses. The symbol indicates concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Kernel selecting module architecture. The GAP denotes the global average pooling operation. A ReLU activation after F C 1 is omitted for brief.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Denoising performance of state-of-the-art methods on another DND image. Readers are encouraged to zoom in for better visualization. Denoising performance of state-of-the-art methods on a NC12 image. Readers are encouraged to zoom in for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I QUANTITATIVE</head><label>I</label><figDesc>PERFORMANCE OF OUR MODEL ON DND COMPARED WITH OTHER PUBLISHED TECHNIQUES, AND SORTED BY SRGB PSNR.</figDesc><table><row><cell></cell><cell cols="2">Raw</cell><cell cols="2">sRGB</cell><cell></cell></row><row><cell>Algorithm</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>Blind / Non-blind</cell></row><row><cell>TNRD [17]</cell><cell>44.97</cell><cell>0.9624</cell><cell>33.65</cell><cell>0.8306</cell><cell>Non-blind</cell></row><row><cell>BM3D [5]</cell><cell>46.64</cell><cell>0.9724</cell><cell>34.51</cell><cell>0.8507</cell><cell>Non-blind</cell></row><row><cell>KSVD [18]</cell><cell>45.54</cell><cell>0.9676</cell><cell>36.49</cell><cell>0.8978</cell><cell>Non-blind</cell></row><row><cell>WNNM [19]</cell><cell>46.30</cell><cell>0.9707</cell><cell>37.56</cell><cell>0.9313</cell><cell>Non-blind</cell></row><row><cell>FFDNet [6]</cell><cell>-</cell><cell>-</cell><cell>37.61</cell><cell>0.9415</cell><cell>Non-blind</cell></row><row><cell>DnCNN [2]</cell><cell>-</cell><cell>-</cell><cell>37.90</cell><cell>0.9430</cell><cell>Blind</cell></row><row><cell>CBDNet [7]</cell><cell>-</cell><cell>-</cell><cell>38.06</cell><cell>0.9421</cell><cell>Blind</cell></row><row><cell>Path-Restore [8]</cell><cell>-</cell><cell>-</cell><cell>39.00</cell><cell>0.9542</cell><cell>Blind</cell></row><row><cell>N3Net [3]</cell><cell>47.56</cell><cell>0.9767</cell><cell>-</cell><cell>-</cell><cell>Blind</cell></row><row><cell>PRIDNet</cell><cell>48.48</cell><cell>0.9806</cell><cell>39.42</cell><cell>0.9528</cell><cell>Blind</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="6">ABLATION STUDY OF OUR MODEL ON VALIDATION DATASET OF SIDD.</cell></row><row><cell cols="6">Channel Attention Pyramid Kernel Selecting SIDD</cell></row><row><cell>--√ √ √</cell><cell></cell><cell>-√ -√ √</cell><cell></cell><cell>-√ √ -√</cell><cell>51.81 52.02 51.98 52.15 52.20</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://noise.visinf.tu-darmstadt.de/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3d collaborative filtering with grouping constraint in luminance-chrominance space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04686</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Path-restore: Learning network path selection for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-O</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recursive multistage upscaling network with discriminative fusion for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-D</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-D</forename><surname>Men</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="574" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06586</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1692" to="1700" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The noise clinic: A blind image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPOL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4311</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
