<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Deep Graph Infomax</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
							<email>yuxiang@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM Lab</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<email>chuang7@nd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
							<email>peng.dai@jd.com</email>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
							<email>liefeng.bo@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
							<email>jiawei@ifmlab.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IFM Lab</orgName>
								<orgName type="institution">Florida State University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">JD Finance America Corporation</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterogeneous Deep Graph Infomax</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can serve various downstream tasks, such as node classification and node clustering. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph representation learning problem. Inspired by emerging mutual information-based learning algorithms, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI ) for heterogeneous graph representation learning. We use the meta-path to model the structure involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture individual node local representations. By maximizing the local-global mutual information, HDGI effectively learns node representations based on the diverse information in heterogeneous graphs. Experiments show that HDGI remarkably outperforms stateof-the-art unsupervised graph representation learning methods on both node classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic regression, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Numerous real-world applications, such as social networks <ref type="bibr" target="#b39">[40]</ref>, financial platforms <ref type="bibr" target="#b21">[22]</ref> and knowledge graphs <ref type="bibr" target="#b30">[31]</ref> exhibit the favorable property of graph structure data. Meanwhile, handling graph data is very challenging. Because each node has its unique attributes, and the connections between nodes convey essential information. When learning from nodes' attributes and the connection information simultaneously, the task becomes more challenging.</p><p>Traditional machine learning methods focus on individual nodes' features but ignore the structural information, which obstructs their ability to process graph data. Graph neural networks (GNNs) learn nodes' new feature vectors through a recursive neighborhood aggregation scheme <ref type="bibr" target="#b34">[35]</ref>. With the support of sufficient training samples, a rich body of supervised graph neural network models have been developed <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>. However, labeled data is not always available in graph representation learning tasks. Those algorithms are not applicable to the unsupervised learning settings. For the purpose of alleviating the training sample scarcity problem, unsupervised graph representation learning has aroused extensive research interest. The goal of this task is to learn a low-dimensional representation of each graph node. The representation preserves graph topological structure and node  content. Meanwhile, the learned representations can be applied to conventional sample-based machine learning algorithms.</p><p>Most of the existing unsupervised graph representation learning models can be grouped into factorization-based models and edge-based models. Precisely, factorization-based models capture the global graph information by factorizing the sample affinity matrix <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Through the affinity matrix factorization, the full graph structure can be well grasped. However, those methods ignore the node attributes and local neighborhood relationships. Edge-based models exploit the local and higher-order neighborhood information by edge connections or random-walk paths. Nodes tend to have similar representations if they are connected or co-occur in the same path <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Edge-based models are prone to preserve limited order node proximity. They lack a mechanism to preserve the global graph structure. The recently proposed mutual information-based models <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b24">[25]</ref> demonstrate that mutual information maximization has attractive advantages in encouraging an encoder to considers both global and local features. For graph structure data, global and local graph structure also need to be comprehensively considered. For homogeneous graphs, DGI <ref type="bibr" target="#b29">[30]</ref> maximizes the mutual information between graph patch representations and the corresponding high-level summaries of graphs. However, the networked data in the real world usually contain complex structures (involving multiple types of nodes and edges), which can be formally modeled as heterogeneous graphs (HG). Compared with homogeneous graphs, heterogeneous graphs contain more detailed information and rich semantics among multi-typed nodes. Taking the bibliographic network in <ref type="figure" target="#fig_1">Figure 1</ref> as an example, it contains three types of nodes (Author, Paper, and Subject) as well as two types of edges (Write and Belong-to). Besides, the individual nodes themselves also carry much attribute information (e.g., paper textual contents). Due to the diversity of node and edge types, the heterogeneous graph itself becomes more complicated. The diverse (direct or indirect) connections between nodes also convey more semantic information. The models initially proposed for homogeneous graphs may encounter significant challenges to handle relations with different semantics in heterogeneous graphs.</p><p>In this paper, we explore the use of mutual information maximization for heterogeneous graph representation learning. To address the aforementioned challenges in heterogeneous graphs, we propose a novel meta-path based unsupervised graph neural network model, namely Heterogeneous Deep Graph Infomax (HDGI ). In heterogeneous graph studies, meta-path <ref type="bibr" target="#b26">[27]</ref> has been widely used to represent the composite relations with different semantics. As illustrated in <ref type="figure" target="#fig_1">Figure 1(d)</ref>, the relations between paper can be expressed by PAP and PSP, which represent papers written by the same author and papers belonging to the same subject, respectively. HDGI utilizes the structure of meta-paths to model connection semantics in heterogeneous graphs. Based on different metapaths, HDGI disassembles heterogeneous graphs into homogeneous graphs of specific semantics. In these homogeneous graphs, HDGI applies a convolutional style GNN to capture the local representation of nodes with specific semantics. After that, through a semantic-level attention mechanism, HDGI aggregates node representations of different semantics. By maximizing local-global mutual information, HDGI learns high-level representations containing graph-level structural information without any supervised label. The node attributes can be simultaneously fused into the representations in the process of maximizing local-global mutual information. In order to verify the effectiveness and competitiveness of learned representations, we perform experiments on both classification tasks and clustering tasks. The experimental results show that the performance of HDGI can often beat supervised state-ofart comparative graph neural network models.</p><p>In summary, our contributions in this paper can be summarized as follows:</p><p>• This paper presents the first model to apply mutual information maximization to representation learning in heterogeneous graphs. • Our proposed method, HDGI , is a novel unsupervised graph neural network. It handles graph heterogeneity by utilizing an attention mechanism on meta-paths and deals with the unsupervised settings by applying mutual information maximization. <ref type="bibr">•</ref> Our experiments demonstrate that the representations learned by HDGI are effective for both node classification and clustering tasks. Moreover, its performance can also beat state-of-the-art GNN models, where they have additional supervised label information.</p><p>The rest of this paper is organized as follows. We discuss the related work in Section 2. In Section 3, we present the problem formulation along with important terminologies used in our method. We present HDGI in detail in Section 4. Experimental results and analyses are provided in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph representation learning</head><p>Graph representation learning has become a non-trivial topic <ref type="bibr" target="#b2">[3]</ref> because of the ubiquity of graphs in real-world scenarios. As a data type containing rich structural information, many models <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b31">[32]</ref> acting on graphs learn the representations of nodes based on the structure of the graph. Node2vec <ref type="bibr" target="#b8">[9]</ref> learns a mapping of nodes to a lowdimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. DeepWalk <ref type="bibr" target="#b19">[20]</ref> uses the set of random walks over the graph in SkipGram to learn node embeddings. Tang et al. <ref type="bibr" target="#b27">[28]</ref> optimize a carefully designed objective function that preserves both the local and global network structures. Qu et al. <ref type="bibr" target="#b18">[19]</ref> propose preserving asymmetric transitivity by approximating high-order proximity based on asymmetric transitivity. Wang et al. <ref type="bibr" target="#b31">[32]</ref> attempt to retrieve structural information through matrix factorization incorporating the community structure. However, all the above methods are proposed for homogeneous graphs.</p><p>In order to handle graph heterogeneity, metapath2vec <ref type="bibr" target="#b3">[4]</ref> samples random walks under the guidance of meta-paths and learns node embeddings through the skip-gram. HIN2Vec <ref type="bibr" target="#b6">[7]</ref> learns the embedding vectors of nodes and meta-paths simultaneously while conducts prediction tasks. Wang et al. <ref type="bibr" target="#b32">[33]</ref> consider the attention mechanism in heterogeneous graph learning through the model HAN, where information from multiple meta-path defined connections can be learned effectively. From the perspective of attributed graphs, SHNE <ref type="bibr" target="#b37">[38]</ref> captures both structural closeness and unstructured semantic relations through joint optimization of heterogeneous SkipGram and deep semantic encoding. HGAT <ref type="bibr" target="#b20">[21]</ref> employs a hierarchical attention mechanism considering both node-level and schemalevel attention to handle graph heterogeneity. Many learning methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b30">[31]</ref> on knowledge graphs can often be applied to other heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph neural network</head><p>The core idea of GNN is aggregating the neighbors' feature information through neural networks to learn the new features <ref type="bibr" target="#b34">[35]</ref>, which combine the independent information of the node and corresponding structural information in the graph. A propagation model incorporating gated recurrent units to propagate information across all nodes is proposed in <ref type="bibr" target="#b16">[17]</ref>. Joan Bruna et al. <ref type="bibr" target="#b1">[2]</ref> extends convolution to general graphs by a novel Fourier transformation in graphs. <ref type="bibr">Kipf et al. [15]</ref> propose Graph Convolutional Network (GCN). Hamilton et al. <ref type="bibr" target="#b9">[10]</ref> introduce GraphSAGE that generates embeddings by directly aggregating features from a node's local neighborhood. Graph Attention Network (GAT) <ref type="bibr" target="#b28">[29]</ref> first imports the attention mechanism into graphs. Other successful GNNs are also based on supervised learning including SplineCNN <ref type="bibr" target="#b5">[6]</ref>, AdaGCN <ref type="bibr" target="#b25">[26]</ref> and AS-GCN <ref type="bibr" target="#b12">[13]</ref>. The unsupervised learning GNNs can be mainly divided into two categories, i.e., random walk-based <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> and mutual informationbased <ref type="bibr" target="#b29">[30]</ref>. DGI <ref type="bibr" target="#b29">[30]</ref> is a general GNN for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs. DGI is the pioneer of our work, but it can only be applied to homogeneous graphs. HDGI extends mutual information-based GNNs to heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mutual information</head><p>Information theory <ref type="bibr" target="#b23">[24]</ref> has been applied in various domains. Recently mutual information is utilized in various learning tasks successfully. Belghazi et al. <ref type="bibr" target="#b0">[1]</ref> propose a neural network-based mutual information estimator, enabling the mutual information estimation to be linearly scalable in dimensionality. Deep InfoMax (DIM) <ref type="bibr" target="#b11">[12]</ref> investigates unsupervised learning of representations by maximizing mutual information in the computer vision domain. DGI <ref type="bibr" target="#b29">[30]</ref> extends the mechanism into the graph domain and works on node representation learning. Sun et al. <ref type="bibr" target="#b24">[25]</ref> take advantage of mutual information maximization to learn graph-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>In this section, we first introduce the concept of heterogeneous graph and the problem definition of heterogeneous graph representation learning. Next, we define meta-path based adjacency matrix, which is critical in the following algorithm description. Definition 3.1 (Heterogeneous Graph (HG)): A heterogeneous graph is defined as G = (V, E) with a node type mapping function φ : V → T and an edge type mapping function ψ : E → R. Each node v ∈ V belongs to one particular type in the node type set T : φ(v) ∈ T , and each edge e ∈ E belongs to a particular edge type in the edge type set R : ψ(e) ∈ R. Heterogeneous graphs have the property that |T | + |R| &gt; 2. The attributes and content of nodes can be encoded as initial feature matrix X. Problem Definition. (Heterogeneous Graph Representation Learning): Given a heterogeneous graph G and the initial feature matrix X, the representation learning task in G is to learn the low dimensional node representations H ∈ R |V|×d which contains both structure information of G and node attributes of X. The learned representation H can be applied to downstream graph-related tasks such as node classification and node clustering, etc. We focus on learning the representation of a specific type of node in this paper. We represent the set of nodes for representation learning as the target-type nodes V t .</p><p>In a heterogeneous graph, two neighboring nodes can be connected by different types of edges. Meta-paths <ref type="bibr" target="#b26">[27]</ref>, which represent node and edge types between two neighboring nodes in a HG, have been proposed to model such rich information. Formally, a path v 1</p><formula xml:id="formula_0">R1 − − → v 2 R2 − − → · · · Rn−1 −−−→ v n is</formula><p>defined as a meta-path between nodes v 1 and v n , wherein R = R 1 • R 2 • · · · • R n−1 defines the composite relations between node v 1 and v n <ref type="bibr" target="#b3">[4]</ref>. In this paper, we intend to utilize symmetric and undirected meta paths to denote the closeness among target-type nodes V t , which can help simplify the problem setting. We represent the set of meta paths used in this paper as {Φ 1 , Φ 2 , · · · , Φ P }, where Φ i denotes the i-th meta path type. For example, in <ref type="figure" target="#fig_1">Figure 1(d)</ref>, Paper-Author-Paper (PAP) and Paper-Subject-Paper (PSP) are two types of meta-paths, which contain the semantic "papers written by the same author" and "papers belonging to the same subject" respectively.</p><p>Definition 3.2 (Meta-path based Adjacency Matrix): For the meta-path Φ i , if there exists a path instance between node v i ∈ V t and v j ∈ V t , we call that v i and v j are "connected neighbors" based on Φ i . Such neighborhood information can be represented by a meta-path based adjacent matrix</p><formula xml:id="formula_1">A Φi ∈ R |Vt|×|Vt| , where A Φi ij = A Φi ji = 1 if v i , v j are connected by meta-path Φ i and A Φi ij = A Φi ji = 0 otherwise.</formula><p>In the next section, we will propose the novel method HDGI for heterogeneous graph representation learning, which is an unsupervised GNN model and can integrate information from different meta-paths through an attention mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HDGI METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. HDGI Architecture Overview</head><p>A high-level illustration of the proposed HDGI is shown in <ref type="figure">Figure 2</ref>. We summarize the notations used for model description in <ref type="table" target="#tab_0">Table I</ref>. The input of HDGI is a heterogeneous graph G containing N target-type nodes whose initial ddimension features are denoted by X ∈ R N ×d , and meta-path</p><formula xml:id="formula_2">set {Φ i } P i=1 . Based on {Φ i } P i=1</formula><p>we can calculate the metapath set based adjacency matrices {A Φi } P i=1 . The meta-path based local representation encoding described in Section IV-B has two steps: (1) learning individual node representation H Φi from X and each A Φi , i = 1, 2.., P and (2) generating node representation H by aggregating {H Φi } P i=1 through a  semantic-level attention mechanism. A global representation encoder R is proposed to derive a graph summary vector s from H (see Section IV-C). The discriminator D will be trained with the objective to maximize mutual information between positive nodes and the graph-level summary s. At the same time, HDGI is optimized in an end-to-end manner by backpropagation with the object of mutual information maximization. In Section IV-D we elaborate the mutual information based discriminator D and the negative sample generator C.</p><formula xml:id="formula_3">i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Meta-path based local representation encoder</head><p>Meta-path based local representation encoder has a hierarchical structure. The first step is meta-path specific node representation learning, which encodes nodes in terms of each meta-path based adjacency matrix. The second step, namely heterogeneous graph node representation learning, is aggregating these representations relating to different metapaths by a semantic-level attention mechanism.</p><p>1) Meta-path specific node representation learning: We use meta-path based adjacency matrices to convey multiple different semantics due to the heterogeneity of input graphs. Given the meta-path Φ, the adjacency matrix A Φ ∈ R |Vt|×|Vt| can represent the relational information about the meta-path Φ based connections. Each of A Φi , i = 1, 2, ..P can be viewed as a homogeneous graph. The initial node feature matrix X ∈ R N ×F is constructed by stacking the feature vectors in X . At this step, our target is to derive a node representation containing the information of initial node feature X and A Φi , with a node-level encoder:</p><formula xml:id="formula_4">H Φi = a Φi (X, A Φi )<label>(1)</label></formula><p>Two kinds of the encoder are considered in this work. The first is Graph Convolutional Network (GCN) <ref type="bibr" target="#b14">[15]</ref>. GCN introduces a spectral graph convolution operator for the graph representation learning. The node representations learned by GCN should be:</p><formula xml:id="formula_5">H Φi = (D Φi − 1 2Ã Φi D Φi − 1 2 )XW Φi<label>(2)</label></formula><p>whereÃ Φi = A Φi +I, D Φi is the diagonal node degree matrix ofÃ Φi . Matrix W Φi ∈ R d×F is the filter parameter matrix, which is not shared between different A Φ . The second encoder we consider is the Graph Attention Networks (GAT) <ref type="bibr" target="#b28">[29]</ref>. GAT effectively updates the node representations by aggregating the information from their neighbors, including self-neighbor. For the m-th node, its Khead attention output can be computed as:</p><formula xml:id="formula_6">h Φi m = K k=1 σ( j∈N Φ i m α Φi,k mj W Φi x j )<label>(3)</label></formula><p>where is the concatenation operator, W Φi is the linear transformation parameter matrix and N Φi m is neighbor set defined by Φ i . α Φi,k mj is the normalized attention coefficient computed by the k-th attention mechanism.</p><p>After the meta-path specific node representation learning, we obtain the set of node representations {H Φi } P i=1 . They are aggregated to get the heterogeneous graph node representation.</p><p>2) Heterogeneous graph node representation learning: The representations learned based on the specific meta-path contain only the semantic-specific information. In order to aggregate the more general representations of the nodes, we need to combine these representations {H Φ1 , H Φ2 , . . . , H Φ P }. The vital issue to accomplish the aggregation is exploring how much each meta-path should contribute to the final representations.</p><p>Here we add a semantic-level attention layer L att to learn the weights:</p><formula xml:id="formula_7">{β Φ1 , β Φ2 , . . . , β Φ P } = L att (H Φ1 , H Φ2 , . . . , H Φ P ) (4)</formula><p>Specifically, the importance of the meta-path Φ i is calculated by</p><formula xml:id="formula_8">e Φi = 1 N N n=1 tanh( q T · [W sem · h Φi n + b])<label>(5)</label></formula><p>where W sem is a linear transformation parameter matrix. q is the learnable shared attention vector. β Φi is obtained by normalizing {e Φi } P i=1 with a softmax function:</p><formula xml:id="formula_9">β Φi = softmax(e Φi ) = exp(e Φi ) P j=1 exp(e Φj )<label>(6)</label></formula><p>The heterogeneous graph node representation H is obtained by a linear combination of</p><formula xml:id="formula_10">{H Φi } P i=1 , that is H = P i=1 β Φi · H Φi<label>(7)</label></formula><p>Our semantic attention layer is inspired by HAN <ref type="bibr" target="#b32">[33]</ref>, but there are still some differences in the learning direction. HAN utilizes classification cross-entropy as the loss function. Available labels in training set guide the learning direction. The attention weights learned in HDGI are guided by the binary cross-entropy loss, which indicates whether the node belongs to the original graph. Therefore, the weights learned in HDGI serve for the existence of a node. Because no classification label involves, the weights get no bias from the known labels.</p><p>The representations H serve as the final output local features. It should be mentioned that all parameters in the meta-path based local representation encoder are shared for positive and negative nodes. Negative nodes are generated by the negative samples generator, which we will introduce in section IV-D2. The global representation encoder leverages the representations H to output the graph-level summary, described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Global Representation Encoder</head><p>The learning objective of HDGI is to maximize the mutual information between local representations and global representation. The local representations of nodes are included in H. We need the summary vector s to represent the entire heterogeneous graph's global information. Based on H, we examined three candidate encoder functions: Averaging encoder function. Our first candidate encoder function is the averaging operator, where we simply take the mean of the node representations to output the graph-level summary vector s:</p><formula xml:id="formula_11">s = σ 1 N N i=1 h i (8)</formula><p>σ is a PReLU activation funtion. Pooling encoder function. In this pooling encoder function, each node's vector is independently fed through a fullyconnected layer. An elementwise max-pooling operator has applied to summary the information from the nodes set:</p><formula xml:id="formula_12">s pool = max(σ(W pool h i + b), i ∈ {1, 2, . . . , N }) (9)</formula><p>where max denotes the element-wise max operator and σ is a nonlinear activation function. Set2vec encoder function. The final encoder function we examine is Set2vec <ref type="bibr" target="#b17">[18]</ref>, which is based on an LSTM architecture. Because the original set2vec in <ref type="bibr" target="#b17">[18]</ref> works on ordered node sequences. However, we need a summary of the graph concluding comprehensive information from each node instead of merely graph structure. Therefore, we apply the LSTMs to a random permutation of the node's neighbor on an unordered set.</p><p>D. HDGI Learning 1) Mutual information based discriminator: It is inconvenient to calculate the mutual information between random variables directly. Belghazi et al. <ref type="bibr" target="#b0">[1]</ref> prove that the KLdivergence admits the Donsker-Varadhan representation and the f -divergence representation as dual representations. The dual representations provide a lower-bound to the mutual information of random variables X and Y :</p><formula xml:id="formula_13">MI(X; Y ) ≥ E P XY [T ω (x, y)] − log(E P X ⊗P Y [e Tω(x,y) ]) (10)</formula><p>Here, P XY is the joint distribution, and P X ⊗ P Y is the product of margins. T ω is a deep neural network based discriminator parametrized by ω. The expectations in equ.10 can be estimated using samples from P XY and P X ⊗ P Y . The expressive power of the discriminator ensures to approximate the MI with high accuracy. Following the philosophy in <ref type="bibr" target="#b11">[12]</ref>, we estimate and maximize the mutual information by training a discriminator D to distinguish positive sample set P os = [ h n , s] The sample ( h i , s) is denoted as positive as node h i belongs to the original graph (the joint distribution), and ( h j , s) is negative as the node h j is the generated fake one (the product of marginals). The discriminator D is a bilinear layer:</p><formula xml:id="formula_14">D( h i , s) = σ( h T i W D s)<label>(11)</label></formula><p>Here W D is a learnable matrix, and σ is the sigmoid activation function. Veličković et al. <ref type="bibr" target="#b29">[30]</ref> prove that the binary crossentropy loss amounts to maximizing the mutual information with theoretical guarantee. Therefore, we can maximize the </p><formula xml:id="formula_15">+ M m=1 E N eg [log(1 − D( h m , s))]<label>(12)</label></formula><p>In essence, the discriminator works to maximize the mutual information between a high-level global representation and local representations (node-level), which encourages the encoder to learn the information presenting in all globally relevant locations. The information about a class label can be one of the cases. The above loss can be optimized through gradient descent. The representations of nodes can be learned when the optimization is completed.</p><p>2) Negative samples generator: The negative sample set</p><formula xml:id="formula_16">[ h m , s] M m=1</formula><p>is composed of the samples that do not exist in the heterogeneous graph. As our target is to maximize the mutual information between positive nodes and the graphlevel summary vector, the generated negative samples will affect the structural information captured by the model. In this way, we need high-quality negative samples that keep the structural information preciseness. We extend the negative sample generation approach proposed in <ref type="bibr" target="#b29">[30]</ref> to heterogeneous graph setting. In heterogeneous graph G, we have rich and complex structural information characterized by meta-path based adjacency matrices. Our negative samples generator:</p><formula xml:id="formula_17">X, {A Φ1 , A Φ2 , . . . , A Φ P } = C(X, {A Φ1 , A Φ2 , . . . , A Φ P })<label>(13)</label></formula><p>keeps all meta-path based adjacency matrices unchanged but shuffles the rows of the initial node feature matrix X, which changes the index of nodes to corrupt the node-level connections among them. We provide a simple example to illustrate the procedure of generating negative samples in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>In this section, we evaluate the proposed HDGI framework in three real-world heterogeneous graphs. We first introduce the datasets and experimental settings. Then we report the model performance as compared to other state-of-the-art competitive methods. The evaluation results show the superiority of our developed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluate the performance of HDGI on three heterogeneous graphs and summarize their details in <ref type="table" target="#tab_0">Table II</ref>. : This is a research paper set, which contains scientific publications and the corresponding authors. The target author node can be divided into four areas: database, data mining, information retrieval, and machine learning. We use the area of authors as labels. The initial features are generated based on authors' profiles with the bag-of-words embeddings. The meta-paths we defined in DBLP are Author-Paper-Author (APA), Author-Paper-Conference-Paper-Author (APCPA), and Author-Paper-Term-Paper-Author (APTPA). • ACM <ref type="bibr" target="#b32">[33]</ref>: This is another academic paper data in which target paper nodes are categorized into 3 classes: database, wireless communication, and data Mining. We extract 2 meta-paths from this graph: Paper-Author-Paper (PAP) and Paper-Subject-Paper (PSP). The initial features are constructed from paper keywords with the TF-IDF based embedding techniques. • IMDB <ref type="bibr" target="#b33">[34]</ref>: It is a knowledge graph data about movies (target nodes) categorized into three types: Action, Comedy, and Drama. The meta-paths we choose are Movie-Actor-Movie (MAM), Movie-Director-Movie (MDM), and Movie-Keyword-Movie (MKM). The feature of a movie is composed of {color, title, language, keywords, country, rating, year} with a TF-IDF encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Setup</head><p>The most commonly used tasks to measure the quality of learned representations are node classification <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> and node clustering <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b32">[33]</ref>. We evaluate HDGI from both two types of tasks.</p><p>1) Compared Baselines: HDGI is compared with the following supervised and unsupervised methods: Unsupervised methods • Raw Feature: The initial features are used as embeddings.</p><p>• Metapath2vec (M2V) <ref type="bibr" target="#b3">[4]</ref>: A meta-path based graph embedding method for heterogeneous graph. We test all meta-paths and report the best result.</p><p>• DeepWalk (DW) <ref type="bibr" target="#b19">[20]</ref>: A random walk based graph embedding method, but it is designed to deal with homogeneous graph.</p><p>• DeepWalk+Raw Feature(DW+F): It concatenates the learned DeepWalk embeddings with the raw features as the final representations. • DGI <ref type="bibr" target="#b29">[30]</ref>: A mutual information based graph representation method for homogeneous graph. • HDGI-C: The graph convolutional network is utilized to capture local representations in HDGI . • HDGI-A: This is another variant of HDGI , which uses graph attention mechanism to learn local representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised methods</head><p>• GCN <ref type="bibr" target="#b14">[15]</ref>: A semi-supervised methods for node classification on homogeneous graphs. • RGCN <ref type="bibr" target="#b22">[23]</ref>: It performs representation learning on all nodes labeled with entity types in heterogeneous graphs.</p><p>• GAT <ref type="bibr" target="#b28">[29]</ref>: GAT applies the attention mechanism in homogeneous graphs for node classification.</p><p>• HAN <ref type="bibr" target="#b32">[33]</ref>: HAN employs node-level attention and semantic-level attention to capture the information from all meta-paths. For methods designed for homogeneous graphs, i.e., Deep-Walk, DGI, GCN, GAT, we do not consider graph heterogeneity and construct meta-path based adjacency matrix, then we report the best performance. We test all meta-paths for Metapath2vec and report the best result. For RGCN, because our task is to learn the representations of target-type nodes, the cross-entropy loss is calculated by the classification in targettype nodes only. In the node classification task, a training set is used to learn a simple classifier for unsupervised methods, while the supervised methods can output the result as end-toend. For the node clustering, we will not use any label in this unsupervised learning task and make comparison among all unsupervised learning methods.</p><p>2) Reproducibility: For the proposed HDGI , including HDGI-C and HDGI-A, we optimize the model with Adam <ref type="bibr" target="#b13">[14]</ref>. The dimension of node-level representations in HDGI-C is set as 512, and the dimension of q is set as 8. For HDGI-A, we set the dimension of node-level representations as 64, and the attention head is set as 4. The dimension of q is set as 8 as well. We employ Pytorch to implement our model and conduct experiments in the server with 4 GTX-1080ti GPUs. The detailed parameters of all other comparison methods are provided in the opensource codes: https://github.com/YuxiangRen/Heterogeneous-Deep-Graph-Infomax.</p><p>C. Performance Comparison 1) Node classification task: In the node classification task, we train a logistic regression classifier for unsupervised learning methods, while the supervised methods output the classification result as end-to-end models. We conduct the experiments with two different training set sizes (20% or 80% of full datasets). The sizes of the validation set and test set are fixed at 10% of full datasets. For unsupervised methods, the dataset division is used for training the logistic regression classifier but has no relationship with representation learning. To keep the results stable, we repeat the classification process 10 times and report the average Macro-F1 and Micro-F1 in <ref type="table">Table ?</ref>?. X, A and Y in <ref type="table">Table ?</ref>? denote initial features, the adjacency matrix and node labels, respectively. They are used to reflect the required input of different methods. We observe that HDGI-C outperforms all other unsupervised learning methods. When competing with the supervised learning methods (designed for homogeneous graphs like GCN and GAT), HDGI can perform much better. This observation proves that the type and semantic information are critical and need to be handled carefully instead of directly ignoring them in heterogeneous graphs. The result of RGCN is suboptimal because the original RGCN is a featureless approach, and we follow the code to assign a one-hot vector to each node.</p><p>In addition, the unified learning of all types of nodes in the same latent space is beneficial to entity type classification. However, it may not be applicable to label classification. HDGI is also competitive with the result reported from HAN <ref type="bibr" target="#b32">[33]</ref>, which is designed for heterogeneous graphs. The reason should be that HDGI can capture more global structural information when exploring the mutual information in reconstructing the representation. At the same time, supervised loss based GNNs overemphasize the direct neighborhoods <ref type="bibr" target="#b29">[30]</ref>. On the other hand, this observation suggests that the features learned through supervised learning in graph structures may have limitations, either from the structure or a task-based preference. These limitations can damage the representations from a more general perspective. Both HDGI-C and HDGI-A achieve stunning performance, which verifies the effectiveness of the general framework of HDGI as well.</p><p>2) Node clustering task: In the node clustering task, we use the K-Means to conduct the clustering based on the learned representations. The number of clusters K is set as the number of target node classes. We do not use any label in this unsupervised learning task and make the comparison among all unsupervised learning methods. We also repeat the clustering process 10 times and report the average NMI and ARI in <ref type="table" target="#tab_0">Table IV</ref>. DeepWalk cannot perform well because they are not able to handle the graph heterogeneity. Metapath2vec cannot handle diversified semantic information simultaneously, which makes the representations not effective enough. The verification based on node clustering tasks also demonstrates that HDGI can learn effective representations by considering the structural information, the semantic information, and the node independent content simultaneously.</p><p>3) HDGI-A vs HDGI-C: From the comparison between HDGI-C and HDGI-A in node classification tasks, the results reflect some interesting findings. HDGI-C achieves better performance than HDGI-A in all experiments, which means that the graph convolution works better than the attention mechanism in capturing local network structures. The reason might be that the graph attention mechanism is strictly limited to the direct neighbors of nodes. The graph convolution "X" learning with initial features. "A" learning with the adjacency matrix. "Y" learning with node labels. considering hierarchical dependencies can see farther. The results of the clustering task can also verify this analysis. 4) Different global representation encoder functions: We present the results of HDGI-C with different global representation encoder functions working on the node classification task in <ref type="figure">Figure 4</ref>. The simple average function performs the best. However, we can find that this advantage is very subtle. In fact, each function can perform well on experimental datasets. Nevertheless, for larger and more complex heterogeneous graphs, a specified and sophisticated function may perform better. The design of the global encoder function for heterogeneous graphs with different scales and structures is an open question, which is worthy of further discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose an unsupervised graph neural network, HDGI , which learns node representations in heterogeneous graphs. HDGI combines several state-of-the-art techniques. It employs convolution style GNNs along with a semantic-level attention mechanism to capture individual local representations of nodes. Through maximizing the local-global mutual information, HDGI learns high-level representations containing graph-level structural information. It exploits the structure of meta-path to model the connection semantics in  <ref type="figure">Figure 4</ref>: The comparison between different global representation encoder functions heterogeneous graphs. Node attributes are fused into representations through the local-global mutual information maximization simultaneously. We demonstrate the effectiveness of learned representations for both node classification and clustering tasks on three heterogeneous graphs. HDGI is incredibly competitive in node classification tasks with stateof-the-art supervised methods, where they have the additional supervised label information. We are optimistic that mutual information maximization is a promising future direction for unsupervised representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>: Paper-Author-Paper(PAP) Meta-path: Paper-Subject-Paper(PSP) (d) Meta-paths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An example of heterogeneous bibliographic graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>p &lt; l a t e x i t s h a 1 _ 1 &lt; l a t e x i t s h a 1 _ 1 &lt; l a t e x i t s h a 1 _ 1 &lt; 1 &lt; 1 &lt;</head><label>11111111</label><figDesc>b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y hi Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E pa t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt;s &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q j j J A Q F i m e y T / s N x C 5 U n + + R R Q b M = " &gt; A A A B 8 H i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H w Y I 2 V v m Y M P u 3 W V 3 j 4 R c + B U 2 F h p j 6 8 + x 8 9 + 4 w B U K v m S S l / d m M j P P j w X X x n W / n d z G 5 t b 2 T n 6 3 s L d / c H h U P D 5 p 6 i h R D B s s E p F q + 1 S j 4 C E 2 D D c C 2 7 F C K n 2 B L X 9 8 N / d b E 1 S a R + G j m c b Y k 3 Q Y 8 o A z a q z 0 V O 5 O k K V 6 V u 4 X S 2 7 F X Y C s E y 8 j J c h Q 7 x e / u o O I J R J D w w T V u u O 5 s e m l V B n O B M 4 K 3 U R j T N m Y D r F j a U g l 6 l 6 6 O H h G L q w y I E G kb I W G L N T f E y m V W k + l b z s l N S O 9 6 s 3 F / 7 x O Y o K b X s r D O D E Y s u W i I B H E R G T + P R l w h c y I q S W U K W 5 v J W x E F W X G Z l S w I X i r L 6 + T Z r X i X V W q D 9 V S 7 T a L I w 9 n c A 6 X 4 M E 1 1 O A e 6 t A A B h K e 4 R X e H O W 8 O O / O x 7 I 1 5 2 Q z p / A H z u c P Z i 6 Q I w = = &lt; / l a t e x i t &gt; D &lt;l a t e x i t s h a 1 _ b a s e 6 4 = " 3 t t f G u Z Z 2 1 m Y v + 6 l 8 y + b F g j 1 s J 4 = " &gt; A A A B 9 H i c b V B N T w I x F H z F L 8 Q v 1 K O X R j D x R H b x o E e i H j x i I k g C G 9 I t X W j o d t e 2 S 0 I 2 / A 4 v H j T G q z / G m / / G L u x B w U m a T G b e y 5 u O H w u u j e N 8 o 8 L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 y h R l L V o J C L V 8 Y l m g k v W M t w I 1 o k V I 6 E v 2 K M / v s n 8 x w l T m k f y w U x j 5 o V k K H n A K T F W 8 q q 9 k J g R J S K 9 n V X 7 5 Y p T c + b A q 8 T N S Q V y N P v l r 9 4 g o k n I p K G C a N 1 1 n d h 4 K V G G U 8 F m p V 6 i W U z o m A x Z 1 1 J J Q q a 9 d B 5 6 h s + s M s B B p O y T B s / V 3 x s p C b W e h r 6 d z D L q Z S 8 T / / O 6 i Q m u v J T L O D F M 0 s W h I B H Y R D h r A A + 4 Y t S I q S W E K m 6 z Y j o i i l B j e y r Z E t z l L 6 + S d r 3 m X t T q 9 / V K 4 z q v o w g n c A r n 4 M I l N O A O m t A C C k / w D K / w h i b o B b 2 j j 8 V o A e U 7 x / A H 6 P M H M s 6 R uA = = &lt; / l a t e x i t &gt; D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 t t f G u Z Z 2 1 m Y v + 6 l 8 y + b F g j 1 s J 4 = " &gt; A A A B 9 H i c b V B N T w I x F H z F L 8 Q v 1 K O X R j D x R H b x o E e i H j x i I k g C G 9 I t X W j o d t e 2 S 0 I 2 / A 4 v H j T G q z / G m / / G L u x B w Um a T G b e y 5 u O H w u u j e N 8 o 8 L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q 6 y h R l L V o J C L V 8 Y l m g k v W M t w I 1 o k V I 6 E v 2 K M / v s n 8 x w l T m k f y w U x j 5 o V k K H n A K T F W 8 q q 9 k J g R J S K 9 n V X 7 5 Y p T c + b A q 8 T N S Q V y N P v l r 9 4 g o k n I p K G C a N 1 1 n d h 4 K V G G U 8 F m p V 6 i W U z o m A x Z 1 1 J J Q q a 9 d B 5 6 h s + s M s B B p O y T B s / V 3 x s p C b W e h r 6 d z D L q Z S 8 T / / O 6 i Q m u v J T L O D F M 0 s W h I B H Y R D h r A A + 4 Y t S I q S W E K m 6 z Y j o i i l B j e y r Z E t z l L 6 + S d r 3 m X t T q 9 / V K 4 z q v o w g n c A r n 4 M I l N O A O m t A C C k / w D K / w h i b o B b 2 j j 8 V o A e U 7 x / A H 6 P M H M s 6 R u A = = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l G b w 8 j c J W 3 K 0 w E s h r S Q p 8 a Q G 0 q 8 = " &gt; A A A B 6 H i c b V D J S g N B E K 2 J W x K 3 q E c v j U E Q h D C j B z 0 G v X h M w C y Y D K G n U 5 O 0 6 V n o 7 h H D k C / w 4 k G R X P 0 B / 8 W b X 6 O d 5 a C J D w o e 7 1 V R V c + L B V f a t r + s z M r q 2 v p G N p f f 3 N r e 2 S 3 s 7 d d V l E i G N R a J S D Y 9 q l D w E G u a a 4 H N W C I N P I E N b 3 A 9 8 R s P K B W P w l s 9 j N E N a C / k P m d U G 6 l 6 2 i k U 7 Z I 9 B V k m z p w U y 7 l 4 f P f x + F 3 p F D 7 b 3 Y g l A Y a a C a p U y 7 F j 7 a Z U a s 4 E j v L t R G F M 2 Y D 2 s G V o S A N U b j o 9 d E S O j d I l f i R N h Z p M 1 d 8 T K Q 2 U G g a e 6 Q y o 7 q t F b y L + 5 7 U S 7 V + 6 K Q / j R G P I Z o v 8 R B A d k c n X p M s l M i 2 G h l A m u b m V s D 6 V l G m T T d 6 E 4 C y + v E z q Z y X n v H R W N W l c w Q x Z O I Q j O A E H L q A M N 1 C B G j B A e I I X e L X u r W f r z R r P W j P W f O Y A / s B 6 / w G S B p C B &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m U K s B b 8 8 / t a 0 G a 1 0 S L x F p + 4 H i N w = " &gt; A A A B 6 H i c b V D J T g J B E K 3 B D X B D P X r p S E y 8 S G b 0 o E e i F 4 + Q y B J h Q n q a G m j p W d L d Y y Q T v s C L B 4 3 h 6 g / 4 L 9 7 8 G m 2 W g 4 I v q e T l v a p U 1 f N i w Z W 2 7 S 8 r s 7 K 6 t r 6 R z e U 3 t 7 Z 3 d g t 7 + 3 U V J Z J h j U U i k k 2 P K h Q 8 x J r m W m A z l k g D T 2 D D G 1 x P / M Y D S s W j 8 F Y P Y 3 Q D 2 g u 5 z x n V R q q e d g p F u 2 R P Q Z a J M y f F c i 4 e 3 3 0 8 f l c 6 h c 9 2 N 2 J J g K F m g i r V c u x Y u y m V m j O B o 3 w 7 U R h T N q A 9 b B k a 0 g C V m 0 4 P H Z F j o 3 S J H 0 l T o S Z T 9 f d E S g O l h o F n O g O q + 2 r R m 4 j / e a 1 E + 5 d u y s M 4 0 R i y 2 S I / E U R H Z P I 1 6 X K J T I u h I Z R J b m 4 l r E 8 l Z d p k k z c h O I s v L 5 P 6 W c k 5 L 5 1 V T R p X M E M W D u E I T s C B C y j D D V S g B g w Q n u A F X q 1 7 6 9 l 6 s 8 a z 1 o w 1 n z m A P 7 D e f w C V D p C D &lt; / l a t e x i t &gt; a l a t e x i t s h a 1 _ b a s e 6 4 = " k F B b j L 4 7 V H C Y d s B 6 9 V Z 5 d h k o 2 / U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U n q Q p d F N y 4 r 2 A c 0 I U y m k 3 b o Z B L m I Z T Q 3 3 D j Q h G 3 / o w 7 / 8 Z p m 4 W 2 H r h w O O d e 7 r 0 n y j h T 2 n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p q t R I Q j s k 5 a n s R 1 h R z g T t a K Y 5 7 W e S 4 i T i t B d N 7 u Z + 7 4 l K x V L x q K c Z D R I 8 E i x m B G s r + X U c 5 n 5 7 z E J v V g + r N b f h L o D W i V e Q G h R o h 9 U v f 5 g S k 1 C h C c d K D T w 3 0 0 G O p W a E 0 1 n F N 4 p m m E z w i A 4 s F T i h K s g X N 8 / Q h V W G K E 6 l L a H R Q v 0 9 k e N E q W k S 2 c 4 E 6 7 F a 9 e b i f 9 7 A 6 P g m y J n I j K a C L B f F h i O d o n k A a M g k J Z p P L c F E M n s r I m M s M d E 2 p o o N w V t 9 e Z 1 0 m w 3 v q t F 8 a N Z a t 0 U c Z T i D c 7 g E D 6 6 h B f f Q h g 4 Q y O A Z X u H N M c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A A f 6 R A Q = = &lt; / l a t e x i t &gt; a l a t e x i t s h a 1 _ b a s e 6 4 = " k F B b j L 4 7 V H C Y d s B 6 9 V Z 5 d h k o 2/ U = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B l v B V U n q Q p d F N y 4 r 2 A c 0 I U y m k 3 b o Z B L m I Z T Q 3 3 D j Q h G 3 / o w 7 / 8 Z p m 4 W 2 H r h w O O d e 7 r 0 n y j h T 2 n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p q t R I Q j s k 5 a n s R 1 h R z g T t a K Y 5 7 W e S 4 i T i t B d N 7 u Z + 7 4 l K x V L x q K c Z D R I 8 E i x m B G s r + X U c 5 n 5 7 z E J v V g + r N b f h L o D W i V e Q G h R o h 9 U v f 5 g S k 1 C h C c d K D T w 3 0 0 G O p W a E 0 1 n F N 4 p m m E z w i A 4 s F T i h K s g X N 8 / Q h V W G K E 6 l L a H R Q v0 9 k e N E q W k S 2 c 4 E 6 7 F a 9 e b i f 9 7 A 6 P g m y J n I j K a C L B f F h i O d o n k A a M g k J Z p P L c F E M n s r I m M s M d E 2 p o o N w V t 9 e Z 1 0 m w 3 v q t F 8 a N Z a t 0 U c Z T i D c 7 g E D 6 6 h B f f Q h g 4 Q y O A Z X u H N M c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A A f 6 R A Q = = &lt; / l a t e x i t &gt; a p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l y e 4 T i v g G D i A I o Y 6 C Q p I X D J j b t M = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L U g x 6 L X j x W s B / Q h L D Z b t q l m 8 2 y u x F K 6 N / w 4 k E R r / 4 Z b / 4 b t 2 0 O 2 v p g 4 P H e D D P z I s m Z N q 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J V 6 e Z I r R D U p 6 q f o Q 1 5 U z Q j m G G 0 7 5 U F C c R p 7 1 o c j f 3 e 0 9 U a Z a K R z O V N E j w S L C Y E W y s 5 N d x m P v t M Q v l r B 5 W a 2 7 D X Q C t E 6 8 g N S j Q D q t f / j A l W U K F I R x r P f B c a Y I c K 8 M I p 7 O K n 2 k q M Z n g E R 1 Y K n B C d Z A v b p 6 h C 6 s M U Z w q W 8 K g h f p 7 I s e J 1 t M k s p 0 J N m O 9 6 s 3 F / 7 x B Z u K b I G d C Z o Y K s l w U Z x y Z F M 0 D Q E O m K D F 8 a g k m i t l b E R l j h Y m x M V V s C N 7 q y + u k 2 2 x 4 V 4 3 m Q 7 P W u i 3 i K M M Z n M M l e H A N L b i H N n S A g I R n e I U 3 J 3 N e n H f n Y 9 l a c o q Z U / g D 5 / M H Y f i R Q A = = &lt; / l a t e x i t &gt; a p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l y e 4 T i v g G D i A I o Y 6 C Q p I X D J j b t M = " &gt; A A A B 8 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L U g x 6 L X j x W s B / Q h L D Z b t q l m 8 2 y u x F K 6 N / w 4 k E R r / 4 Z b / 4 b t 2 0 O 2 v p g 4 P H e D D P z I s m Z N q 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J V 6 e Z I r R D U p 6 q f o Q 1 5 U z Q j m G G 0 7 5 U F C c R p 7 1 o c j f 3 e 0 9 U a Z a K R z O V N E j w S L C Y E W y s 5 N d x m P v t M Q v l r B 5 W a 2 7 D X Q C t E 6 8 g N S j Q D q t f / j A l W U K F I R x r P f B c a Y I c K 8 M I p 7 O K n 2 k q M Z n g E R 1 Y K n B C d Z A v b p 6 h C 6 s M U Z w q W 8 K g h f p 7 I s e J 1 t M k s p 0 J N m O 9 6 s 3 F / 7 x B Z u K b I G d C Z o Y K s l w U Z x y Z F M 0 D Q E O m K D F 8 a g k m i t l b E R l j h Y m x M V V s C N 7 q y + u k 2 2 x 4 V 4 3 m Q 7 P W u i 3 i K M M Z n M M l e H A N L b i H N n S A g I R n e I U 3 J 3 N e n H f n Y 9 l a c o q Z U / g D 5 / M H Y f i R Q A = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j x t l i Y u 5 Z k j 9 y D f O U e X c L R m b g 1 Q = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 n q Q S 9 i s R e P F e w H p K F s t p t 2 6 W Y T d j d C C f 0 Z X j w o 0 q v / w 7 s X 8 d + 4 a X v Q 1 g c D j / d m m D c T J J w p 7 T j f V m F t f W N z q 7 h t 7 + z u 7 R + U D o 9 a K k 4 l o U 0 S 8 1 h 2 A q w o Z 4 I 2 N d O c d h J J c R R w 2 g 5 G 9 d x v P 1 K p W C w e 9 D i h f o Q H g o W M Y G 0 k r x t h P S S Y Z / V J r 1 R 2 K s 4 M a J W 4 C 1 K + + b C v k + m X 3 e i V P r v 9 m K Q R F Z p w r J T n O o n 2 M y w 1 I 5 x O 7 G 6 q a I L J C A + o Z 6 j A E V V + N o s 8 Q W d G 6 a M w l q a E R j P 1 9 0 S G I 6 X G U W A 6 8 4 h q 2 c v F / z w v 1 e G V n z G R p J o K M l 8 U p h z p G O X 3 o z 6 T l G g + N g Q T y U x W R I Z Y Y q L N l 2 z z B H f 5 5 F X S q l b c i 0 r 1 3 i n X b m G O I p z A K Z y D C 5 d Q g z t o Q B M I x P A E L / B q a e v Z e r O m 8 9 a C t Z g 5 h j + w 3 n 8 A 1 C 2 U m g = = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j x t l i Y u 5 Z k j 9 y D f O U e X c L R m b g 1 Q = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 n q Q S 9 i s R e P F e w H p K F s t p t 2 6 W Y T d j d C C f 0 Z X j w o 0 q v / w 7 s X 8 d + 4 a X v Q 1 g c D j / d m m D c T J J w p 7 T j f V m F t f W N z q 7 h t 7 + z u 7 R + U D o 9 a K k 4 l o U 0 S 8 1 h 2 A q w o Z 4 I 2 N d O c d h J J c R R w 2 g 5 G 9 d x v P 1 K p W C w e 9 D i h f o Q H g o W M Y G 0 k r x t h P S S Y Z / V J r 1 R 2 K s 4 M a J W 4 C 1 K + + b C v k + m X 3 e i V P r v 9 m K Q R F Z p w r J T n O o n 2 M y w 1 I 5 x O 7 G 6 q a I L J C A + o Z 6 j A E V V + N o s 8 Q W d G 6 a M w l q a E R j P 1 9 0 S G I 6 X G U W A 6 8 4 h q 2 c v F / z w v 1 e G V n z G R p J o K M l 8 U p h z p G O X 3 o z 6 T l G g + N g Q T y U x W R I Z Y Y q L N l 2 z z B H f 5 5 F X S q l b c i 0 r 1 3 i n X b m G O I p z A K Z y D C 5 d Q g z t o Q B M I x P A E L / B q a e v Z e r O m 8 9 a C t Z g 5 h j + w 3 n 8 A 1 C 2 U m g = = &lt; / l a t e x i t &gt; R &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O t k L d M O + A / g L 1 v G u s g v + W E s + 9 K A = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 B I v g q S T 1 o B e x 6 M V j F f s B a S i b 7 a Z d u t k N u x u h h P 4 M L x 4 U 6 d X / 4 d 2 L + G / c t D 1 o 6 4 O B x 3 s z z J s J E 0 a V d t 1 v q 7 C y u r a + U d y 0 t 7 Z 3 d v d K + w d N J V K J S Q M L J m Q 7 R I o w y k l D U 8 1 I O 5 E E x S E j r X B 4 k / u t R y I V F f x B j x I S x K j P a U Q x 0 k b y O z H S A 4 x Y d j / u l s p u x Z 3 C W S b e n J S v P u z L Z P J l 1 7 u l z 0 5 P 4 D Q m X G O G l P I 9 N 9 F B h q S m m J G x 3 U k V S R A e o j 7 x D e U o J i r I p p H H z o l R e k 4 k p C m u n a n 6 e y J D s V K j O D S d e U S 1 6 O X i f 5 6 f 6 u g i y C h P U k 0 4 n i 2 K U u Z o 4 e T 3 O z 0 q C d Z s Z A j C k p q s D h 4 g i b A 2 X 7 L N E 7 z F k 5 d J s 1 r x z i r V O 7 d c u 4 Y Z i n A E x 3 A K H p x D D W 6 h D g 3 A I O A J X u D V 0 t a z 9 W Z N Z q 0 F a z 5 z C H 9 g v f 8 A 6 v i U q Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; … … l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 jFigure 2</head><label>12</label><figDesc>z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt;h 1 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 E H s T J u G S B B Z q n p M u r c E C E v p l h k = " &gt; A A A B / X i c b V C 5 T s N A E F 1 z h n C Z o 6 N Z k S B R R X Y o o I y g o Q w S O a T Y W O v N O F l l f W h 3 H S l Y F r 9 C Q w F C t P w H H X / D J n E B C U 8 a 6 e m 9 G c 3 M 8 x P O p L K s b 2 N l d W 1 9 Y 7 O 0 V d 7 e 2 d 3 b N w 8 O 2 z J O B Y U W j X k s u j 6 R w F k E L c U U h 2 4 i g I Q + h 4 4 / u p n 6 n T E I y e L o X k 0 S c E M y i F j A K F F a 8 s z j q j M G m g 1 z j z 1 k T n P I P D u v e m b F q l k z 4 G V i F 6 S C C j Q 9 8 8 v p x z Q N I V K U E y l 7 t p U o N y N C M c o h L z u p h I T Q E R l A T 9 O I h C D d b H Z 9 j s + 0 0 s d B L H R F C s / U 3 x M Z C a W c h L 7 u D I k a y k V v K v 7 n 9 V I V X L k Z i 5 J U Q U T n i 4 K U Y x X j a R S 4 z w R Q x S e a E C q Y v h X T I R G E K h 1 Y W Y d g L 7 6 8 T N r 1 m n 1 R q 9 / V K 4 3 r I o 4 S O k G n 6 B z Z 6 B I 1 0 C 1 q o h a i 6 B E 9 o 1 f 0 Z j w Z L 8 a 7 8 T F v X T G K m S P 0 B 8 b n D / H 1 l O I = &lt; / l a t e x i t &gt; x i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 2 5 6 g Q V B S a u E I X R K o P l m Z d L 8 5 d c = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p v 7 n Q l V m k n x a K Y J D W I 8 F C x i B B s r + d X e h J L s a d Z n 1 X 6 5 4 t b c B d A 6 8 X J S g R z N f v m r N 5 A k j a k w h G O t f c 9 N T J B h Z R j h d F b q p Z o m m I z x k P q W C h x T H W S L k 2 f o w i o D F E l l S x i 0 U H 9 P Z D j W e h q H t j P G Z q R X v b n 4 n + e n J r o J M i a S 1 F B B l o u i l C M j 0 f x / N G C K E s O n l m C i m L 0 V k R F W m B i b U s m G 4 K 2 + v E 7 a 9 Z p 3 V a s / 1 C u N 2 z y O I p z B O V y C B 9 f Q g H t o Q g s I S H i G V 3 h z j P P i v D s f y 9 a C k 8 + c w h 8 4 n z / u a J E E &lt; / l a t e x i t &gt; x i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 2 5 6 g Q V B S a u E I X R K o P l m Z d L 8 5 d c = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p v 7 n Q l V m k n x a K Y J D W I 8 F C x i B B s r + d X e h J L s a d Z n 1 X 6 5 4 t b c B d A 6 8 X J S g R z N f v m r N 5 A k j a k w h G O t f c 9 N T J B h Z R j h d F b q p Z o m m I z x k P q W C h x T H W S L k 2 f o w i o D F E l l S x i 0 U H 9 P Z D j W e h q H t j P G Z q R X v b n 4 n + e n J r o J M i a S 1 F B B l o u i l C M j 0 f x / N G C K E s O n l m C i m L 0 V k R F W m B i b U s m G 4 K 2 + v E 7 a 9 Z p 3 V a s / 1 C u N 2 z y O I p z B O V y C B 9 f Q g H t o Q g s I S H i G V 3 h z j P P i v D s f y 9 a C k 8 + c w h 8 4 n z / u a J E E &lt; / l a t e x i t &gt; x j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 5 / C k n 7 i q gb + 5 R + b y V Q y x L q X g J Y = " &gt; A A A B / H i c b V C 7 T s N A E D z z D O F l S E l j k S B R R X Y o o I y g o Q w S e U i x Z Z 3 P 6 + T I + a G 7 c 4 R l m V + h o Q A h W j 6 E j r / h k r i A h J F W G s 3 s a n f H S x g V 0 j S / t b X 1 j c 2 t 7 c p O d X d v / + B Q P z r u i T j l B L o k Z j E f e F g A o x F 0 J Z U M B g k H H H o M + t 7 k Z u b 3 p 8 A F j a N 7 m S X g h H g U 0 Y A S L J Xk 6 r W G P Q W S 2 5 I y H / L H o n A f G q 5 e N 5 v m H M Y q s U p S R y U 6 r v 5 l + z F J Q 4 g k Y V i I o W U m 0 s k x l 5 Q w K K p 2 K i D B Z I J H M F Q 0 w i E I J 5 8 f X x h n S v G N I O a q I m n M 1 d 8 T O Q 6 F y E J P d Y Z Y j s W y N x P / 8 4 a p D K 6 c n E Z J K i E i i 0 V B y g w Z G 7 M k D J 9 y I J J l i m D C q b r V I G P M M Z E q r 6 o K w V p + e Z X 0 W k 3 r o t m 6 a 9 X b 1 2 U c F X S C T t E 5 s t A l a q N b 1 E F d R F C G n t E r e t O e t B f t X f t Y t K 5 p 5 U w N / Y H 2 + Q P l Q p T s &lt; / l a t e x i t &gt; x j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 5 / C k n 7 i q g b + 5 R + b y V Q y x L q X g J Y = " &gt; A A A B / H i c b V C 7 T s N A E D z z D O F l S E l j k S B R R X Y o o I y g o Q w S e U i x Z Z 3 P 6 + T I + a G 7 c 4 R l m V + h o Q A h W j 6 E j r / h k r i A h J F W G s 3 s a n f H S x g V 0 j S / t b X 1 j c 2 t 7 c p O d X d v / + B Q P z r u i T j l B L o k Z j E f e F g A o x F 0 J Z U M B g k H H H o M + t 7 k Z u b 3 p 8 A F j a N 7 m S X g h H g U 0 Y A S L J X k 6 r W G P Q W S 2 5 I y H / L H o n A f G q 5 e N 5 v m H M Y q s U p S R y U 6 r v 5 l + z F J Q 4 g k Y V i I o W U m 0 s k x l 5 Q w K K p 2 K i D B Z I J H M F Q 0 w i E I J 5 8 f X x h n S v G N I O a q I m n M 1 d 8 T O Q 6 F y E J P d Y Z Y j s W y N x P / 8 4 a p D K 6 c n E Z J K i E i i 0 V B y g w Z G 7 M k D J 9 y I J J l i m D C q b r V I G P M M Z E q r 6 o K w V p + e Z X 0 W k 3 r o t m 6 a 9 X b 1 2U c F X S C T t E 5 s t A l a q N b 1 E F d R F C G n t E r e t O e t B f t X f t Y t K 5 p 5 U w N / Y H 2 + Q P l Q p T s &lt; / l a t e x i t &gt;hp i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 l w B 9 t F b z I g K + V 2 R 9 A r a s v 5 c O Q I = " &gt; A A A B / X i c b V C 5 T s N A E B 1 z h n C Z o 6 N Z k S B R R X Y o o I y g o Q w S O a T Y W O v N O l l l f W h 3 H S l Y F r 9 C Q w F C t P w H H X / D J n E B C U 8 a 6 e m 9 G c 3 M 8 x P O p L K s b 2 N l d W 1 9 Y 7 O 0 V d 7 e 2 d 3 b N w 8 O 2 z J O B a E t E v N Y d H 0 s K W c R b S m m O O 0 m g u L Q 5 7 T j j 2 6 m f m d M h W R x d K 8 m C X V D P I h Y w A h W W v L M 4 6 o z p i Q b 5 h 5 7 y J z m k H l J X v X M i l W z Z k D L x C 5 I B Q o 0 P f P L 6 c c k D W m k C M d S 9 m w r U W 6 G h W K E 0 7 z s p J I m m I z w g P Y 0 j X B I p Z v N r s / R m V b 6 K I i F r k i h m f p 7 I s O h l J P Q 1 5 0 h V k O 5 6 E 3 F / 7 x e q o I r N 2 N R k i o a k f m i I O V I x W g a B e o z Q Y n i E 0 0 w E U z f i s g Q C 0 y U D q y s Q 7 A X X 1 4 m 7 X r N v q j V 7 + q V x n U R R w l O 4 B T O w Y Z L a M A t N K E F B B 7 h G V 7 h z X g y X o x 3 4 2 P e u m I U M 0 f w B 8 b n D 1 H + l S E = &lt; / l a t e x i t &gt; h &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c I M x h z I E B V J e S v 4 z 5 B u v 1 X e E s s c = " &gt; A A A C B X i c b V C 5 T s N A E F 2 H K 4 T L Q A m F R Y J E F d m h g D K C h j J I 5 J B i Y 6 3 X 4 3 j J + t D u O l J k u a H h V 2 g o Q I i W f 6 D j b 9 g c B S Q 8 a a S n 9 2 Y 0 M 8 9 L G R X S N L + 1 0 s r q 2 v p G e b O y t b 2 z u 6 f v H 3 R E k n E C b Z K w h P c 8 L I D R G N q S S g a 9 l A O O P A Z d b 3 g 9 8 b s j 4 I I m 8 Z 0 c p + B E e B D T g B I s l e T q x z V 7 B C S 3 J W U + 5 G F R u A / 3 u d 0 K q W s V N V e v m n V z C m O Z W H N S R X O 0 X P 3 L 9 h O S R R B L w r A Q f c t M p Z N j L i l h U F T s T E C K y R A P o K 9 o j C M Q T j 7 9 o j B O l e I b Q c J V x d K Y q r 8 n c h w J M Y 4 8 1 R l h G Y p F b y L + 5 / U z G V w 6 O Y 3 T T E J M Z o u C j B k y M S a R G D 7 l Q C Q b K 4 I J p + p W g 4 S Y Y y J V c B U V g r X 4 8 j L p N O r W e b 1 x 2 6 g 2 r + Z x l N E R O k F n y E I X q I l u U A u 1 E U G P 6 B m 9 o j f t S X v R 3 r W P W W t J m 8 8 c o j / Q P n 8 A k L W Y m Q = = &lt; / l a t e x i t &gt; h p j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 6 0 / s K F 3 f 3 i O W W 6 1 W i D w n i C N s E g = " &gt; A A A C B X i c b V C 5 T s N A E F 2 H K 4 T L Q A m F R Y J E F d m h g D K C h j J I 5 J B i Y 6 3 X 4 3 j J + t D u O l J k u a H h V 2 g o Q I i W f 6 D j b 9 g c B S Q 8 a a S n 9 2 Y 0 M 8 9 L G R X S N L + 1 0 s r q 2 v p G e b O y t b 2 z u 6 f v H 3 R E k n E C b Z K w h P c 8 L I D R G N q S S g a 9 l A O O P A Z d b 3 g 9 8 b s j 4 I I m 8 Z 0 c p + B E e B D T g B I s l e T q x z V 7 B C S 3 J W U + 5 G F R u A / 3 u d 0 K q Z s W N V e v m n V z C m O Z W H N S R X O 0 X P 3 L 9 h O S R R B L w r A Q f c t M p Z N j L i l h U F T s T E C K y R A P o K 9 o j C M Q T j 7 9 o j B O l e I b Q c J V x d K Y q r 8 n c h w J M Y 4 8 1 R l h G Y p F b y L + 5 / U z G V w 6 O Y 3 T T E J M Z o u C j B k y M S a R G D 7 l Q C Q b K 4 I J p + p W g 4 S Y Y y J V c B U V g r X 4 8 j L p N O r W e b 1 x 2 6 g 2 r + Z x l N E R O k F n y E I X q I l u U A u 1 E U G P 6 B m 9 o j f t S X v R 3 r W P W W t J m 8 8 c o j / Q P n 8 A 8 K + Y 2 A = = &lt; / l a t e x i t &gt;h j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m o 9 O P r 6 3 p V Q b d 6 a 0 V L 0 f l L S t u f E = " &gt; A A A B / H i c b V C 7 T s N A E D y H V w g v Q 0 o a i w S J K r J D A W U E D W W Q y E O K L e t 8 X i d H z g / d n S N Z l v k V G g o Q o u V D 6 P g b L o k L S B h p p d H M r n Z 3 v I R R I U 3 z W 6 t s b G 5 t 7 1 R 3 a 3 v 7 B 4 d H + v F J X 8 Q p J 9 A j M Y v 5 0 M M C G I 2 g J 6 l k M E w 4 4 N B j M P C m t 3 N / M A M u a B w 9 y C w B J 8 T j i A a U Y K k k V 6 8 3 7 R m Q 3 J a U + Z B P i s J 9 b L p 6 w 2 y Z C x j r x C p J A 5 X o u v q X 7 c c k D S G S h G E h R p a Z S C f H X F L C o K j Z q Y A E k y k e w 0 j R C I c g n H x x f G G c K 8 U 3 g p i r i q S x U H 9 P 5 D g U I g s 9 1 R l i O R G r 3 l z 8 z x u l M r h 2 c h o l q Y S I L B c F K T N k b M y T M H z K g U i W K Y I J p + p W g 0 w w x 0 S q v G o q B G v 1 5 X X S b 7 e s y 1 b 7 v t 3 o 3 J R x V N E p O k M X y E J X q I P u U B f 1 E E E Z e k a v 6 E 1 7 0 l 6 0 d + 1 j 2 V r R y p k 6 + g P t 8 w f M s p T c &lt; / l a t e x i t &gt;h i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 4 1 S A i s g S H U s d j q t 4 A b C L N l W x D u 0 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N L m I p h Y k T s s t C T a W G I i S A I X s r f M w Y a 9 3 c v u H g m 5 8 D N s L D T G 1 l 9 j 5 7 9 x g S s U f M k k L + / N Z G Z e m H C m j e d 9 O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S 1 j J V F F t U c q k 6 I d H I m c C W Y Y Z j J 1 F I 4 p D j U z i + m / t P E 1 S a S f F o p g k G M R k K F j F K j J W 6 1 d 4 E a T a a 9 V m 1 X 6 5 4 N W 8 B d 5 3 4 O a l A j m a / / N U b S J r G K A z l R O u u 7 y U m y I g y j H K c l X q p x o T Q M R l i 1 1 J B Y t R B t j h 5 5 l 5 Y Z e B G U t k S x l 2 o v y c y E m s 9 j U P b G R M z 0 q v e X P z P 6 6 Y m u g k y J p L U o K D L R V H K X S P d + f / u g C m k h k 8 t I V Q x e 6 t L R 0 Q R a m x K J R u C v / r y O m n X a / 5 V r f 5 Q r z R u 8 z i K c A b n c A k + X E M D 7 q E J L a A g 4 R l e 4 c 0 x z o v z 7 n w s W w t O P n M K f + B 8 / g D V 6 J D 0 &lt; / l a t e x i t &gt; The high-level structure of HDGI . (a) Local representation encoder is a hierarchical structure: learning node representations in terms of every meta-path based adjacency matrix respectively and then aggregating them through semantic-level attention. (b) Global representation encoder R outputs a graph-level summary vector s. (c) Negative samples generator C is responsible for generating negative nodes. (d) The discriminator D maximizes mutual information between positive nodes and the graph-level summary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>set N eg = [ h m , s] M m=1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>p &lt; l a t e x i t s h a 1 _ 1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 &lt; 1 &lt; 1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 &lt; 1 &lt; 6 &lt; 2 &lt; 7 &lt; 6 &lt; 7 &lt; 4 &lt; 5 &lt; 1 &lt;Figure 3 :</head><label>1123451123451627674513</label><figDesc>b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt; … C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 4 e e B H A v 4 5 k U 4 G G e v 0 A Q O t a R d 0 0 = " &gt; A A A B 9 H i c b V B N T w I x F H y L X 4 h f q E c v j W D i i e z i Q Y 9 E L h 4 x E T C B D e m W L j R 0 2 7 X t k p A N v 8 O L B 4 3 x 6 o / x 5 r + x C 3 t Q c J I m k 5 n 3 8 q Y T x J x p 4 7 r f T m F j c 2 t 7 p 7 h b 2 t s / O D w q H 5 9 0 t E w U o W 0 i u V S P A d a U M 0 H b h h l O H 2 N F c R R w 2 g 0 m z c z v T q n S T I o H M 4 u p H + G R Y C E j 2 F j J r / Y j b M Y E 8 7 Q 5 r w 7 K F b f m L o D W i Z e T C u R o D c p f / a E k S U S F I R x r 3 f P c 2 P g p V o Y R T u e l f q J p j M k E j 2 j P U o E j q v 1 0 E X q O L q w y R K F U 9 g m D F u r v j R R H W s + i w E 5 m G f W q l 4 n / e b 3 E h D d + y k S c G C r I 8 l C Y c G Q k y h p A Q 6 Y o M X x m C S a K 2 a y I j L H C x N i e S r Y E b / X L 6 6 R T r 3 l X t f p 9 v d K 4 z e s o w h m c w y V 4 c A 0 N u I M W t I H A E z z D K 7 w 5 U + f F e X c + l q M F J 9 8 5 h T 9 w P n 8 A M U i R t w = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k v W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1 d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a / a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l 9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 g I O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " E 3 b g o o u r C S z w V A K f l X b i 1 + q f + y s = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s o t C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v E a t f l + v N G / y O I r o D J 2 j S + S h K 9 R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A n F q Q z g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " B q B a a L d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R EA 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k v W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1 d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a / a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l 9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 g I O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " E 3 b g o o u r C S z w V A K f l X b i 1 + q f + y s = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s o t C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v E a t f l + v N G / y O I r o D J 2 j S + S h K 9 R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A n F q Q z g = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " B q B a a L d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R EA 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " h 3 D g w o 2 a z x Q R Z 6 c / 6 N 2 F J o O G n 9 4 = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I i H 8 l x I X v L H m z Y 2 7 3 s z h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w k R w A 6 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J 2 6 h U U 9 a i S i j d D Y l h g k v W A g 6 C d R P N S B w K 1 g n H d 3 O / M 2 H a c C U f Y Z q w I C Z D y S N O C V j J r / Y m j G Z P s 7 5 X 7 Z c r b s 1 d A K 8 T L y c V l K P Z L 3 / 1 B o q m M Z N A B T H G 9 9 w E g o x o 4 F S w W a m X G p Y Q O i Z D 5 l s q S c x M k C 1 O n u E L q w x w p L Q t C X i h / p 7 I S G z M N A 5 t Z 0 x g Z F a 9 u f i f 5 6 c Q 3 Q Q Z l 0 k K T N L l o i g V G B S e / 4 8 H X D M K Y m o J o Z r b W z E d E U 0 o 2 J R K N g R v 9 e V 1 0 q 7 X v K t a / a F e a d z m c R T R G T p H l 8 h D 1 6 i B 7 l E T t R B F C j 2 j V / T m g P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A m V C Q z A = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " W J A o z P M x N s X G t P k p J G X m D z S L g v U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r u Y q E e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D o O m Q 0 Q = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " 7 J q H z x K a P t / A Z k 7 n w h z E X j H Z j + 8 = " &gt; A A A B 8 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k b u z 0 J J o Y 4 m J f C R w I X v L H m z Y 2 7 3 s 7 h H J h Z 9 h Y 6 E x t v 4 a O / + N C 1 y h 4 E s m e X l v J j P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T l p a p I r R J J J e q E 2 J N O R O 0 a Z j h t J M o i u O Q 0 3 Y 4 v p v 7 7 Q l V m k n x a K Y J D W I 8 F C x i B B s r d a u 9 C S X Z 0 6 z v V / v l i l t z F 0 D r x M t J B X I 0 + u W v 3 k C S N K b C E I 6 1 7 n p u Y o I M K 8 M I p 7 N S L 9 U 0 w W S M h 7 R r q c A x 1 U G 2 O H m G L q w y Q J F U t o R B C / X 3 R I Z j r a d x a D t j b E Z 6 1 Z u L / 3 n d 1 E Q 3 Q c Z E k h o q y H J R l H J k J J r / j w Z M U W L 4 1 B J M F L O 3 I j L C C h N j U y r Z E L z V l 9 d J y 6 9 5 V z X / w a / U b / M 4 i n A G 5 3 A J H l x D H e 6 h A U 0 g I O E Z X u H N M c 6 L 8 + 5 8 L F s L T j 5 z C n / g f P 4 A m t W Q z Q = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " M B B 8 x B r S J 7 k A P H 3 C m W 5 + 6 z O A L K Y = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s s C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v K t a / b 5 e a d 7 k c R T R G T p H l 8 h D D d R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A o m 6 Q 0 g = = &lt; / l a t e x i t &gt; p &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C G R 7 w 1 G 2 t H 4 J S i f B y A 2 k Y m P y e 8 = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L l m y t 3 f u 7 g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S A T X x n W / n b X 1 j c 2 t 7 c J O c X d v / + C w d H T c 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U q f Q a I 9 5 P K v 1 S 2 a 2 6 c 5 B V 4 u W k D D k a / d J X b x C z N E J p m K B a d z 0 3 M X 5 G l e F M 4 L T Y S z U m l I 3 p E L u W S h q h 9 r P 5 v V N y b p U B C W N l S x o y V 3 9 P Z D T S e h I F t j O i Z q S X v Z n 4 n 9 d N T X j t Z 1 w m q U H J F o v C V B A T k 9 n z Z M A V M i M m l l C m u L 2 V s B F V l B k b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y C H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D J M W P Y A = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " W J A o z P M x N s X G t P k p J G X m D z S L g v U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r u Y q E e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D o O m Q 0 Q = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " M B B 8 x B r S J 7 k A P H 3 C m W 5 + 6 z O A L K Y = " &gt; A A A B 8 n i c b V A 9 T w J B E N 3 D L 8 Q v 1 N J m I 5 h Y k T s s s C T a W G I i Y H J c y N 4 y B x v 2 d i + 7 e 0 R y 4 W f Y W G i M r b / G z n / j A l c o + J J J X t 6 b y c y 8 M O F M G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k o 2 W q K L S p 5 F I 9 h k Q D Z w L a h h k O j 4 k C E o c c u u H 4 d u 5 3 J 6 A 0 k + L B T B M I Y j I U L G K U G C v 5 1 d 4 E a P Y 0 6 z e q / X L F r b k L 4 H X i 5 a S C c r T 6 5 a / e Q N I 0 B m E o J 1 r 7 n p u Y I C P K M M p h V u q l G h J C x 2 Q I v q W C x K C D b H H y D F 9 Y Z Y A j q W w J g x f q 7 4 m M x F p P 4 9 B 2 x s S M 9 K o 3 F / / z / N R E 1 0 H G R J I a E H S 5 K E o 5 N h L P / 8 c D p o A a P r W E U M X s r Z i O i C L U 2 J R K N g R v 9 e V 1 0 q n X v K t a / b 5 e a d 7 k c R T R G T p H l 8 h D D d R E d 6 i F 2 o g i i Z 7 R K 3 p z j P P i v D s f y 9 a C k 8 + c o j 9 w P n 8 A o m 6 Q 0 g = = &lt; / l a t e x i t &gt; x l a t e x i t s h a 1 _ b a s e 6 4 = " B q B a a L d W m A p r K f v y H M 0 0 x v O O N X Q = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r t o o k e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n d + Q z w = = &lt; / l a t e x i t &gt;x l a t e x i t s h a 1 _ b a s e 6 4 = " d U c n k i 3 Q N e D g L i B z Z M m c 1 3 2 o o L U = " &gt; A A A B 8 n i c b V B N T w I x E O 3 i F + I X 6 t F L I 5 h 4 I r s Y o 0 e i F 4 + Y C J I s G 9 I t X W j o t p t 2 l k g 2 / A w v H j T G q 7 / G m / / G A n t Q 8 C W T v L w 3 k 5 l 5 Y S K 4 A d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o b V S q K W t R J Z T u h M Q w w S V r A Q f B O o l m J A 4 F e w x H t z P / c c y 0 4 U o + w C R h Q U w G k k e c E r C S X + 2 O G c 2 e p r 3 L a q 9 c c W v u H H i V e D m p o B z N X v m r 2 1 c 0 j Z k E K o g x v u c m E G R E A 6 e C T U v d 1 L C E 0 B E Z M N 9 S S W J m g m x + 8 h S f W a W P I 6 V t S c B z 9 f d E R m J j J n F o O 2 M C Q 7 P s z c T / P D + F 6 D r I u E x S Y J I u F k W p w K D w 7 H / c 5 5 p R E B N L C N X c 3 o r p k G h C w a Z U s i F 4 y y + v k n a 9 5 l 3 U 6 v f 1 S u M m j 6 O I T t A p O k c e u k I N d I e a q I U o U u g Z v a I 3 B 5 w X 5 9 3 5 W L Q W n H z m G P 2 B 8 / k D n 2 S Q 0 A = = &lt; / l a t e x i t &gt; … l a t e x i t s h a 1 _ b a s e 6 4 = " W 7 Q k d F k C i V 0 7 P 6 f f A j x 3 9 0 k n C X g = " &gt; A A A B 7 3 i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B m 0 s Y x g P i A 5 w t 5 m L 1 m y t 3 f u z g n h y J + w s V D E 1 r 9 j 5 7 9 x k 1 y h i Q 8 G H u / N M D M v S K Q w 6 L r f z t r 6 x u b W d m G n u L u 3 f 3 B Y O j p u m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T q X X G I m + V + m X y m 7 V n Y O s E i 8 n Z c j R 6 J e + e o O Y p R F X y C Q 1 p u u 5 C f o Z 1 S i Y 5 N N i L z U 8 o W x M h 7 x r q a I R N 3 4 2 v 3 d K z q 0 y I G G s b S k k c / X 3 R E Y j Y y Z R Y D s j i i O z 7 M 3 E / 7 x u i u G 1 n w m V p M g V W y w K U 0 k w J r P n y U B o z l B O L K F M C 3 s r Y S O q K U M b U d G G 4 C 2 / v E p a t a p 3 W a 3 d 1 8 r 1 m z y O A p z C G V y A B 1 d Q h z t o Q B M Y S H i G V 3 h z H p 0 X 5 9 3 5 W L S u O f n M C f y B 8 / k D x P u P I Q = = &lt; / l a t e x i t &gt; The example of generating negative samples mutual information with the binary cross-entropy loss of the discriminator: L(P os, N eg, s) = 1 N + M N n=1 E P os [logD( h n , s)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Symbols and Definitions</figDesc><table><row><cell cols="2">Symbol Interpretation</cell></row><row><cell>Φ</cell><cell>Meta-path</cell></row><row><cell>A Φ</cell><cell>Meta-path based adjacency matrix</cell></row><row><cell>X</cell><cell>The initial node feature matrix</cell></row><row><cell>Vt</cell><cell>The set of nodes with the target type</cell></row><row><cell>N</cell><cell>The number of nodes in Vt</cell></row><row><cell>G</cell><cell>The given heterogeneous graph</cell></row><row><cell>D</cell><cell>Mutual information based discriminator</cell></row><row><cell>C</cell><cell>Negative samples generator</cell></row><row><cell>R</cell><cell>Global representation encoder</cell></row><row><cell>s</cell><cell>The graph-level summary vector</cell></row><row><cell>H Φ</cell><cell>Node-level representations</cell></row><row><cell>q</cell><cell>Semantic-level attention vector</cell></row><row><cell>S Φ</cell><cell>Attention weight of meta-path φ</cell></row><row><cell>H</cell><cell>Final negative nodes representations</cell></row><row><cell>H</cell><cell>Final positive nodes representations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Statistics of experimented datasets.</figDesc><table><row><cell>Dataset</cell><cell>Node-type</cell><cell># Nodes</cell><cell>Edge-type</cell><cell># Edges</cell><cell>feature</cell><cell>Meta-path</cell></row><row><cell>ACM</cell><cell>Paper (P) Author (A) Subject (S)</cell><cell>3025 5835 56</cell><cell>Paper-Author Paper-Subject</cell><cell>9744 3025</cell><cell>1870</cell><cell>PAP PSP</cell></row><row><cell>IMDB</cell><cell>Movie (M) Actor (A) Director (D) Keyword (K)</cell><cell>4275 5431 2082 7313</cell><cell>Movie-Actor Movie-Director Movie-keyword</cell><cell>12838 4280 20529</cell><cell>6344</cell><cell>MAM MDM MKM</cell></row><row><cell>DBLP</cell><cell>Author (A) Paper (P) Conference (C) Term (T)</cell><cell>4057 14328 20 8789</cell><cell>Author-Paper Paper-Conference Paper-Term</cell><cell>19645 14328 88420</cell><cell>334</cell><cell>APA APCPA APTPA</cell></row></table><note>• DBLP [8]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>The results of node classification tasks</figDesc><table><row><cell></cell><cell cols="2">Available data</cell><cell>X</cell><cell>A</cell><cell></cell><cell></cell><cell>X, A, Y</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X, A</cell></row><row><cell>Dataset</cell><cell>Train</cell><cell>Metric</cell><cell>Raw</cell><cell>M2V</cell><cell>DW</cell><cell>GCN</cell><cell>RGCN</cell><cell>GAT</cell><cell>HAN</cell><cell>DW+F</cell><cell>DGI</cell><cell cols="2">HDGI-A HDGI-C</cell></row><row><cell></cell><cell>20%</cell><cell>Micro-F1</cell><cell>0.8590</cell><cell cols="2">0.6125 0.5503</cell><cell>0.9250</cell><cell>0.5766</cell><cell cols="2">0.9178 0.9267</cell><cell>0.8785</cell><cell>0.9104</cell><cell>0.9178</cell><cell>0.9227</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.8585</cell><cell cols="2">0.6158 0.5582</cell><cell>0.9248</cell><cell>0.5801</cell><cell cols="2">0.9172 0.9268</cell><cell>0.8789</cell><cell>0.9104</cell><cell>0.9170</cell><cell>0.9232</cell></row><row><cell>ACM</cell><cell>80%</cell><cell>Micro-F1</cell><cell>0.8820</cell><cell cols="2">0.6378 0.5788</cell><cell>0.9317</cell><cell>0.5939</cell><cell cols="2">0.9250 0.9400</cell><cell>0.8965</cell><cell>0.9175</cell><cell>0.9333</cell><cell>0.9379</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.8802</cell><cell cols="2">0.6390 0.5825</cell><cell>0.9317</cell><cell>0.5918</cell><cell cols="2">0.9248 0.9403</cell><cell>0.8960</cell><cell>0.9155</cell><cell>0.9330</cell><cell>0.9379</cell></row><row><cell></cell><cell>20%</cell><cell>Micro-F1</cell><cell>0.7552</cell><cell cols="2">0.6985 0.2805</cell><cell>0.8192</cell><cell>0.1932</cell><cell cols="2">0.8244 0.8992</cell><cell>0.7163</cell><cell>0.8975</cell><cell>0.9062</cell><cell>0.9175</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.7473</cell><cell cols="2">0.6874 0.2302</cell><cell>0.8128</cell><cell>0.2132</cell><cell cols="2">0.8148 0.8923</cell><cell>0.7063</cell><cell>0.8921</cell><cell>0.8988</cell><cell>0.9094</cell></row><row><cell>DBLP</cell><cell>80%</cell><cell>Micro-F1</cell><cell>0.8325</cell><cell cols="2">0.8211 0.3079</cell><cell>0.8383</cell><cell>0.2175</cell><cell cols="2">0.8540 0.9100</cell><cell>0.7860</cell><cell>0.9150</cell><cell>0.9192</cell><cell>0.9226</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.8152</cell><cell cols="2">0.8014 0.2401</cell><cell>0.8308</cell><cell>0.2212</cell><cell cols="2">0.8476 0.9055</cell><cell>0.7799</cell><cell>0.9052</cell><cell>0.9106</cell><cell>0.9153</cell></row><row><cell></cell><cell>20%</cell><cell>Micro-F1</cell><cell>0.5112</cell><cell cols="2">0.3985 0.3913</cell><cell>0.5931</cell><cell>0.4350</cell><cell cols="2">0.5985 0.6077</cell><cell>0.5262</cell><cell>0.5728</cell><cell>0.5482</cell><cell>0.5893</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.5107</cell><cell cols="2">0.4012 0.3888</cell><cell>0.5869</cell><cell>0.4468</cell><cell cols="2">0.5944 0.6027</cell><cell>0.5293</cell><cell>0.5690</cell><cell>0.5522</cell><cell>0.5914</cell></row><row><cell>IMDB</cell><cell>80%</cell><cell>Micro-F1</cell><cell>0.5900</cell><cell cols="2">0.4203 0.3953</cell><cell>0.6467</cell><cell>0.4476</cell><cell cols="2">0.6540 0.6600</cell><cell>0.6017</cell><cell>0.6003</cell><cell>0.5861</cell><cell>0.6592</cell></row><row><cell></cell><cell></cell><cell>Macro-F1</cell><cell>0.5884</cell><cell cols="2">0.4119 0.4001</cell><cell>0.6457</cell><cell>0.4527</cell><cell cols="2">0.6550 0.6586</cell><cell>0.6049</cell><cell>0.5950</cell><cell>0.5834</cell><cell>0.6646</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table IV :</head><label>IV</label><figDesc>Evaluation results on the node clustering task</figDesc><table><row><cell>Data</cell><cell cols="2">ACM</cell><cell cols="2">DBLP</cell><cell cols="2">IMDB</cell></row><row><cell>Method</cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell><cell>NMI</cell><cell>ARI</cell></row><row><cell>DeepWalk</cell><cell>25.47</cell><cell>18.24</cell><cell>7.40</cell><cell>5.30</cell><cell>1.23</cell><cell>1.22</cell></row><row><cell>Raw Feature</cell><cell>32.62</cell><cell>30.99</cell><cell>11.21</cell><cell>6.98</cell><cell>1.06</cell><cell>1.17</cell></row><row><cell>DeepWalk+F</cell><cell>32.54</cell><cell>31.20</cell><cell>11.98</cell><cell>6.99</cell><cell>1.23</cell><cell>1.22</cell></row><row><cell>Metapath2vec</cell><cell>27.59</cell><cell>24.57</cell><cell>34.30</cell><cell>37.54</cell><cell>1.15</cell><cell>1.51</cell></row><row><cell>DGI</cell><cell>41.09</cell><cell>34.27</cell><cell>59.23</cell><cell>61.85</cell><cell>0.56</cell><cell>2.6</cell></row><row><cell>HDGI-A</cell><cell>57.05</cell><cell>50.86</cell><cell>52.12</cell><cell>49.86</cell><cell>0.8</cell><cell>1.29</cell></row><row><cell>HDGI-C</cell><cell>54.35</cell><cell>49.48</cell><cell>60.76</cell><cell>62.67</cell><cell>1.87</cell><cell>3.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Mine: mutual information neural estimation. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">metap-ath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5119" to="5130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Splinecnn: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="869" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hin2vec: Explore metapaths in heterogeneous information networks for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graphbased consensus maximization among multiple supervised and unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath Kudlur Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymmetric transitivity preserving graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hgat: Hierarchical graph attention network for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2002</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ensemfdet: An ensemble approach to fraud detection based on bipartite graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adagcn: Adaboosting graph convolutional networks into deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05081</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explaining reviews and ratings with paco: Poisson additive co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphrnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Generating realistic graphs with deep autoregressive models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shne: Representation learning for semantic-associated heterogeneous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining, WSDM &apos;19</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="690" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collective classification via discriminative matrix factorization on sparsely labeled networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Social network fusion and mining: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09874</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
