<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Self-Attention Network for Referring Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
							<email>mrochan@cs.umanitoba.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>ywang@cs.umanitoba.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Manitoba</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Modal Self-Attention Network for Referring Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Referring image segmentation is a challenging problem at the intersection of computer vision and natural language processing. Given an image and a natural language expression, the goal is to produce a segmentation mask in the image corresponding to entities referred by the the natural language expression (see <ref type="figure" target="#fig_1">Fig. 4</ref> for some examples). It is worth noting that the referring expression is not limited to specifying object categories (e.g. "person", "cat"). It can take any free form language description which may contain appearance attributes (e.g. "red", "long"), actions (e.g. "standing", "hold") and relative relationships (e.g. "left", "above"), etc. Referring image segmentation can potentially be used in a wide range of applications, such as interactive photo editing and human-robot interaction.</p><p>A popular approach (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>) in this area is to * Zhi Liu and Yang Wang are the corresponding authors <ref type="figure">Figure 1</ref>. (Best viewed in color) Illustration of our cross-modal self-attention mechanism. It is composed of three joint operations: self-attention over language (shown in red), self-attention over image representation (shown in green), and cross-modal attention between language and image (shown in blue). The visualizations of linguistic and spatial feature representations (in bottom row) show that the proposed model can focus on specific key words in the language and spatial regions in the image that are necessary to produce precise referring segmentation masks.</p><p>use convolutional neural network (CNN) and recurrent neural network (RNN) to separately represent the image and the referring expression. The resultant image and language representations are then concatenated to produce the final pixel-wise segmentation result. The limitation of this approach is that the language encoding module may ignore some fine details of some individual words that are important to produce an accurate segmentation mask. Some previous works (e.g. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>) focus on learning multimodal interaction in a sequential manner. The visual feature is sequentially merged with the output of LSTMbased <ref type="bibr" target="#b7">[8]</ref> language model at each step to infer a multimodal representation. However, the multimodal interaction only considers the linguistic and visual information individually within their local contexts. It may not sufficiently capture global interaction information essential for semantic understanding and segmentation.</p><p>In this paper, we address the limitations of aforementioned methods. We propose a cross-modal self-attention (CMSA) module to effectively learn long-range dependencies from multimodal features that represent both visual and linguistic information. Our model can adaptively focus on important regions in the image and informative keywords in the language description. <ref type="figure">Figure 1</ref> shows an example that illustrates the cross-modal self-attention module, where the correlations among words in the language and regions in the image are presented. In addition, we propose a gated multilevel fusion module to further refine the segmentation mask of the referred entity. The gated fusion module is designed to selectively leverage multi-level self-attentive features.</p><p>In summary, this paper makes the following contributions: (1) A cross-modal self-attention method for referring image segmentation. Our model effectively captures the long-range dependencies between linguistic and visual contexts. As a result, it produces a robust multimodal feature representation for the task. (2) A gated multilevel fusion module to selectively integrate multi-level selfattentive features which effectively capture fine details for precise segmentation masks. (3) An extensive empirical study on four benchmark datasets demonstrates that our proposed method achieves superior performance compared with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we review several lines of research related to our work in the following fields. Semantic segmentation: Semantic segmentation has achieved great advances in recent years. Fully convolutional networks (FCN) <ref type="bibr" target="#b17">[18]</ref> take advantage of fully convolutional layers to train a segmentation model in an end-to-end way by replacing fully connected layers in CNN with convolutional layers. In order to alleviate the down-sampling issue and enlarge the semantic context, DeepLab <ref type="bibr" target="#b2">[3]</ref> adopts dilated convolution to enlarge the receptive field and uses atrous spatial pyramid pooling for multi-scale segmentation. An improved pyramid pooling module <ref type="bibr" target="#b29">[30]</ref> further enhances the use of multi-scale structure. Lower level features are explored to bring more detailed information to complement high-level features for generating more accurate segmentation masks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. Referring image localization and segmentation: In referring image localization, the goal is to localize specific objects in an image according to the description of a refer-ring expression. It has been explored in natural language object retrieval <ref type="bibr" target="#b10">[11]</ref> and modelling relationship <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. In order to obtain a more precise result, referring image segmentation is proposed to produce a segmentation mask instead of a bounding box. This problem was first introduced in <ref type="bibr" target="#b9">[10]</ref>, where CNN and LSTM are used to extract visual and linguistic features, respectively. They are then concatenated together for spatial mask prediction. To better achieve word-to-image interaction, <ref type="bibr" target="#b16">[17]</ref> directly combines visual features with each word feature from a language LSTM to recurrently refine segmentation results. Dynamic filter <ref type="bibr" target="#b19">[20]</ref> for each word further enhances this interaction. In <ref type="bibr" target="#b21">[22]</ref>, word attention is incorporated in the image regions to model key-word-aware context. Low-level visual features are also exploited for this task in <ref type="bibr" target="#b14">[15]</ref>, where Convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b24">[25]</ref> progressively refines segmentation masks from high-level to low-level features sequentially. In this paper, we propose to adaptively integrate multi-level self-attentive features by the gated fusion module. Attention: Attention mechanism has been shown to be a powerful technique in deep learning models and has been widely used in various tasks in natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref> to capture keywords for context. In the multimodal tasks, word attention has been used to re-weight the importance of image regions for image caption generation <ref type="bibr" target="#b25">[26]</ref>, image question answering <ref type="bibr" target="#b26">[27]</ref> and referring image segmentation <ref type="bibr" target="#b21">[22]</ref>. In addition, attention is also used for modeling subject, relationship and object <ref type="bibr" target="#b8">[9]</ref> and for referring relationship comprehension <ref type="bibr" target="#b27">[28]</ref>. The diverse attentions of query, image and objects are calculated separately and then accumulated circularly for visual grounding in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Self-attention <ref type="bibr" target="#b22">[23]</ref> is proposed to attend a word to all other words for learning relations in the input sequence. It significantly improves the performance for machine translation. This technique is also introduced in videos to capture long-term dependencies across temporal frames <ref type="bibr" target="#b23">[24]</ref>. Different from these works, we propose a cross-modal selfattention module to bridge attentions across language and vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Model</head><p>The overall architecture of our model is shown in <ref type="figure">Fig. 2</ref>. Given an image and a referring expression as the query, we first use a CNN to extract visual feature maps at different levels from the input image. Each word in the referring expression is represented as a vector of word embedding. Every word vector is then appended to the visual feature map to produce a multimodal feature map. Thus, there is a multimodal feature map for each word in the referring expression. We then introduce self-attention <ref type="bibr" target="#b22">[23]</ref> mechanism to combine the feature maps of different words into a crossmodal self-attentive feature map. The self-attentive feature <ref type="figure">Figure 2</ref>. An overview of our approach. The proposed model consists of three components including multimodal features, cross-modal self-attention (CMSA) and a gated multi-level fusion. Multimodal features are constructed from the image feature, the spatial coordinate feature and the language feature for each word. Then the multimodual feature at each level is fed to a cross-modal self-attention module to build long-range dependencies across individual words and spatial regions. Finally, the gated multi-level fusion module combines the features from different levels to produce the final segmentation mask. map captures rich information and long-range dependencies of both linguistic and visual information of the inputs. In the end, the self-attentive features from multiple levels are combined via a gating mechanism to produce the final features used for generating the segmentation output.</p><p>Our model is motivated by several observations. First of all, in order to solve referring image segmentation, we typically require detailed information of certain individual words (e.g. words like "left", "right"). Previous works (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref>) take word vectors as inputs and use LSTM to produce a vector representation of the entire referring expression. The vector representation of the entire referring expression is then combined with the visual features for referring image segmentation. The potential limitation of this technique is that the vector representation produced by LSTM captures the meaning of the entire referring expression while missing sufficiently detailed information of some individual words needed for the referring image segmentation task. Our model addresses this issue and does not use LSTM to encode the entire referring expression. Therefore, it can better capture more detailed word-level information. Secondly, some previous works (e.g. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>) process each word in the referring expression and concatenate it with visual features to infer the referred object in a sequential order using a recurrent network. The limitation is that these methods only look at local spatial regions and lack the interaction over long-range spatial regions in global context which is essential for semantic understanding and segmentation. In contrast, our model uses a cross-modal selfattention module that can effectively model long-range dependencies between linguistic and visual modalities. Lastly, different from <ref type="bibr" target="#b14">[15]</ref> which adopts ConvLSTM to refine segmentation with multi-scale visual features sequentially, the proposed method employs a novel gated fusion module for combining multi-level self-attentive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multimodal Features</head><p>The input to our model consists of an image I and a referring expression with N words w n , n ∈ 1, 2, ..., N . We first use a backbone CNN network to extract visual features from the input image. The feature map extracted from a specific CNN layer is represented as V ∈ R H×W ×Cv , where H, W and C v are the dimensions of height, width and feature channel, respectively. For ease of presentation, we only use features extracted from one particular CNN layer for now. Later in Sec. 3.3, we present an extension of our method that uses features from multiple CNN layers.</p><p>For the language description with N words, we encode each word w n as a one-hot vector, and project it into a compact word embedding represented as e n ∈ R C l by a lookup table. Different from previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> that apply LSTM to process the word vectors sequentially and encode the entire language description as a sentence vector, we keep the individual word vectors and introduce a crossmodal self-attention module to capture long-range correlations between these words and spatial regions in the image. More details will be presented in Sec. 3.2.</p><p>In addition to visual features and word vectors, spatial coordinate features have also been shown to be useful for referring image segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Following prior works, we define an 8-D spatial coordinate feature at each spatial position using the implementation in <ref type="bibr" target="#b16">[17]</ref>. The first 3-dimensions of the feature map encode the normalized horizontal positions. The next 3-dimensions encode normalized vertical positions. The last 2-dimensions encode the normalized width and height information of the image.</p><p>Finally, we construct a joint multimodal feature representation at each spatial position for each word by concatenating the visual features, word vectors, and spatial coordinate features. Let p be a spatial location in the feature map V , i.e. p ∈ {1, 2, ..., H × W }. We use v p ∈ R Cv to denote the "slice" of the visual feature vector at the spatial location p. The spatial coordinate feature of the location p is denoted as s p ∈ R 8 . Thus we can define the multimodal feature f pn corresponding to the location p and the n-th word as follows:</p><formula xml:id="formula_0">f pn = Concat v p ||v p || 2 , e n ||e n || 2 , s p<label>(1)</label></formula><p>where || · || 2 denotes the L 2 norm of a vector and Concat(·) denotes the concatenation of several input vectors. The feature vector f pn encodes information about the combination of a specific location p in the image and the n-th word w n in the referring expression with a total dimension of (C v + C l + 8). We use F = {f pn : ∀p, ∀n} to represent the collection of features f pn for different spatial locations and</p><formula xml:id="formula_1">words. The dimension of F is N × H × W × (C v + C l + 8).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Modal Self-Attention</head><p>The multimodal feature F is quite large which may contain a lot of redundant information. Additionally, the size of F is variable depending on the number of words in the language description. It is difficult to directly exploit F to produce the segmentation output. In recent years, the attention mechanism <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref> has been shown to be a powerful technique that can capture important information from raw features in either linguistic or visual representation. Different from above works, we propose a cross-modal self-attention module to jointly exploit attentions over multimodal features. In particular, inspired by the success of self-attention <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the designed cross-modal self-attention module can capture long-range dependencies between the words in a referring expression and different spatial locations in the input image. The proposed module takes F as the input and produces a feature map that summarizes F after learning the correlation between the language expression and the visual context. Note that the size of this output feature map does not depend on the number of words present in the language description.</p><p>Given a multimodal feature vector f pn , the cross-modal self-attention module first produces a set of query, key and value pair by linear transformations as q pn = W q f pn , k pn = W k f pn and v pn = W v f pn at each spatial location p and the n-th word, where {W q , W k , W v } are part of the model parameters to be learned. Each query, key and value is reduced from the high dimension of multimodal features to the dimension of 512 in our implementation, i.e. W q , W k , W v ∈ R 512×(Cv+C l +8) , for computation efficiency.</p><p>We compute the cross-modal self-attentive featurev pn as follows:</p><p>v pn = p n a p,n,p ,n v p n , where</p><p>a p,n,p ,n = Softmax(q T p n k pn )</p><p>where a p,n,p ,n is the attention score that takes into account of the correlation between (p, n) and any other combinations of spatial location and word (p , n ).</p><p>Then v pn is transformed back to the same dimension as f pn via a linear layer and is added element-wise with f pn to form a residual connection. This allows the insertion of this module into to the backbone network without breaking its behavior <ref type="bibr" target="#b6">[7]</ref>. The final feature representation is averagepooled over all words in the referring expression. These operations can be summarized as:</p><formula xml:id="formula_4">f p = avg-pool n (W v v pn + f pn ) = N n=1 (W v v pn + f pn ) N (4) where W v ∈ R (Cv+C l +8</formula><p>)×512 and f p ∈ R Cv+C l +8 . We use F = { f p : ∀p} to denote the collection of f p at all spatial locations, i.e. F ∈ R H×W ×(Cv+C l +8) . <ref type="figure">Figure 3</ref> illustrates the process of generating cross-modal self-attentive features.  <ref type="figure">Figure 3</ref>. An illustration of the process of generating the crossmodal self-attentive (CMSA) feature from an image and a language expression ("man in yellow shirt"). We use ⊗ and ⊕ to denote matrix multiplication and element-wise summation, respectively. The softmax operation is performed over each row which indicates the attentions across each visual and language cell in the multimodal feature. We visualize the internal linguistic and spatial representations. Please refer to Sec. 4.2 and Sec. 4.4 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gated Multi-Level Fusion</head><p>The feature representation F obtained from Eq. 4 is specific to a particular layer in CNN. Previous work <ref type="bibr" target="#b14">[15]</ref> has shown that fusing features at multiple scales can improve the performance of referring image segmentation. In this section, we introduce a novel gated fusion technique to integrate multi-level features.</p><p>Let F (i) be the cross-modal self-attentive feature map at the i-th level. Following <ref type="bibr" target="#b14">[15]</ref>, we use ResNet based DeepLab-101 as the backbone CNN and consider feature maps at three levels (i = 1, 2, 3) corresponding to ResNet blocks Res3, Res4 and Res5. Let C vi be the channel dimension of the visual feature map at the i-th level of the network. We use F (i) = { f (i) p : ∀p} to indicate the collection of cross-modal self-attentive features f (i) p ∈ R Cv i +C l +8 for different spatial locations corresponding to the i-th level. Our goal is to fuse the feature maps F (i) (i = 1, 2, 3) to produce a fused feature map for producing the final segmentation output.</p><p>Note that the feature maps F (i) have different channel dimensions at different level i. At each level, we apply a 1 × 1 convolutional layer to make the channel dimensions of different levels consistent and result in an output X (i) .</p><p>For the i-th level, we generate a memory gate m i and a reset gate r i (r i , m i ∈ R Hi×Wi ), respectively. These gates play a similar role to the gates in LSTM. Different from stage-wise memory updates <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref>, the computation of gates at each level is decoupled from other levels. The gates at each level control how much the visual feature at each level contributes to the final fused feature. Each level also has a contextual controller G i which modulates the information flow from other levels to the i-th level. This process can be summarized as:</p><formula xml:id="formula_5">G i = (1 − m i ) X i + j∈{1,2,3}\{i} γ j m j X j F i o = r i tanh(G i ) + (1 − r i ) X i , ∀i ∈ {1, 2, 3}<label>(5)</label></formula><p>where denotes Hadamard product. γ j is a learnable parameter to adjust the relative ratio of the memory gate which controls information flow of features from different levels j combined to the current level i. In order to obtain the segmentation mask, we aggregate the feature maps F i o from the three levels and apply a 3 × 3 convolutional layer followed by the sigmoid function. This sequence of operations outputs a probability map (P ) indicating the likelihood of each pixel being the foreground in the segmentation mask, i.e.:</p><formula xml:id="formula_6">P = σ C 3×3 3 i=1 F i o (6)</formula><p>where σ(·) and C 3×3 denote the sigmoid and 3 × 3 convolution operation, respectively. A binary cross-entropy loss function is defined on the predicted output and the groundtruth segmentation mask Y as follows:</p><formula xml:id="formula_7">L = 1 Ω Ω m=1 (Y (m) log P (m)+(1−Y (m)) log(1−P (m)))<label>(7)</label></formula><p>where Ω is the whole set of pixels in the image and m is mth pixel in it. We use the Adam algorithm <ref type="bibr" target="#b12">[13]</ref> to optimize the loss in Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets and experimental setup in Sec. 4.1. Then we present the main results of our method and compare with other state-of-the-art in Sec. 4.2. Finally, we perform detailed ablation analysis to demonstrate the relative contribution of each component of our proposed method in Sec. 4.3. We also provide visualization and failure cases to help gain insights of our model in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setup</head><p>Implementation details: Following previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>, we keep the maximum length of query expression as 20 and embed each word to a vector of C l = 1000 dimensions. Given an input image, we resize it to 320 × 320 and use the outputs of DeepLab-101 ResNet blocks Res3, Res4, Res5 as the inputs for multimodal features. The dimension used in X (i) for gated fusion is fixed to 500. The network is trained with an initial learning rate of 2.5e −4 and weight decay of 5e −4 . The learning rate is gradually decreased using the polynomial policy with power of 0.9. For fair comparisons, the final segmentation results are refined by DenseCRF <ref type="bibr" target="#b13">[14]</ref>. Datasets: We perform extensive experiments on four referring image segmentation datasets: UNC <ref type="bibr" target="#b28">[29]</ref>, UNC+ <ref type="bibr" target="#b28">[29]</ref>, G-Ref <ref type="bibr" target="#b18">[19]</ref> and ReferIt <ref type="bibr" target="#b11">[12]</ref>.</p><p>The UNC dataset contains 19,994 images with 142,209 referring expressions for 50,000 objects. All images and expressions are collected from the MS COCO <ref type="bibr" target="#b15">[16]</ref> dataset interactively with a two-player game <ref type="bibr" target="#b11">[12]</ref>. Two or more objects of the same object category appear in each image.</p><p>The UNC+ dataset is similar to the UNC dataset. but with a restriction that no location words are allowed in the referring expression. In this case, expressions regarding referred objects totally depend on the appearance and the scene context. It consists of 141,564 expressions for 49,856 objects in 19,992 images.</p><p>The G-Ref dataset is also collected based on MS COCO. It contains of 104,560 expressions referring to 54,822 objects from 26,711 images. Annotations of this dataset come from Amazon Mechanical Turk instead of a two-player game. The average length of expressions is 8.4 words which is longer than that of other datasets (less than 4 words).</p><p>The ReferIt dataset is built upon the IAPR TC-12 <ref type="bibr" target="#b5">[6]</ref> dataset. It has 130,525 expressions referring to 96,654 distinct object masks in 19,894 natural images. In addition to objects, it also contains annotations for stuff classes such as water, sky and ground. Evaluation metrics: Following previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>, we use intersection-over-union (IoU ) and prec@X as the evaluation metrics. The IoU metric is a ratio between intersection and union of the predicted segmentation mask and  <ref type="table">Table 2</ref>. Ablation study of different attention methods for multimodal features on the UNC val set.</p><p>the ground truth. The prec@X metric measures the percentage of test images with an IoU score higher than the threshold X, where X ∈ {0.5, 0.6, 0.7, 0.8, 0.9} in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Evaluation</head><p>Quantitative results: <ref type="table" target="#tab_0">Table 1</ref> presents comparisons of our method with existing state-of-the-art approaches. Our proposed method consistently outperforms all other methods on all four datasets. The improvement is particularly significant on the more challenging datasets, such as UNC+ which has no location words and G-Ref which contains longer and richer query expressions. This demonstrates the advantage of capturing long-range dependencies for cross-modal features and capturing the referred objects based on expressions by our model. Qualitative results: <ref type="figure" target="#fig_1">Figure 4</ref> shows some qualitative examples generated by our network. To better understand the benefit of multi-level self-attentive features, we visualize the linguistic representation to show attention distributions at different levels. For a given level, we get the collection of the attention scores {a p,n,p ,n : ∀p, ∀n, ∀p , ∀n } in Eq. 3 and average over the dimensions p, p and n . Thereby we can get a vector of length N . We repeat this operation for all three levels and finally obtain a matrix of 3 × N . This matrix is shown in <ref type="figure" target="#fig_1">Fig. 4 (2nd column)</ref>. We can see that the attention distribution over words corresponding to a particular feature level is different. Features at higher levels (e.g. l 3 ) tend to focus on words that refer to objects (e.g. "suitcase", "vase"). Features at lower levels (e.g. l 1 , l 2 ) tend to focus on words that refer to attributes (e.g. "black") or relationships (e.g. "bottom", "second").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We perform additional ablation experiments on the UNC dataset to further investigate the relative contribution of each component of our proposed model. Attention methods: We first perform experiments on different attention methods for multimodal features. We alternatively use no attention, word attention, pixel attention and word-pixel pair attention by zeroing out the respective components in Eq. 2. As shown in <ref type="table">Table 2</ref>, the proposed cross-modal self-attention outperforms all other attention methods significantly. This demonstrates the language-tovision correlation can be better learned together within our cross-modal self-attention method. Multimodal feature representation: This experiment evaluates the effectiveness of the multimdoal feature representation. Similar to the baselines, i.e. multimodal LSTM interaction in <ref type="bibr" target="#b16">[17]</ref> and convolution integration in <ref type="bibr" target="#b14">[15]</ref>, we directly take the output of the Res5 of the network to test the performance of multimodal feature representation without the multi-level fusion. We use CMSA-W to denote the proposed method in Sec. 3.2. In addition, a variant method CMSA-S which also uses the same cross-modal self-attentive feature, instead encodes the whole sentence to one single language vector by LSTM.</p><p>As shown in <ref type="table" target="#tab_1">Table 3</ref> (top 4 rows), the proposed crossmodal self-attentive feature based approaches achieve significantly better performance than other baselines. Moreover, the word based method CMSA-W outperforms sentence based method CMSA-S for multimodal feature representation. Multi-level feature fusion: This experiment verifies the relative contribution of the proposed gated multi-level fusion module.</p><p>Here we use our cross-modal selfattentive features as inputs and compare with several wellknown feature fusion techniques, such as Deconv <ref type="bibr" target="#b20">[21]</ref> and PPM <ref type="bibr" target="#b29">[30]</ref> in semantic segmentation and ConvLSTM <ref type="bibr" target="#b14">[15]</ref> in referring image segmentation.</p><p>In order to clearly understand the benefit of our fusion method, we also develop another self-gated method that uses the same gate generation method in Sec. 3.3 to generate memory gates and directly multiply by its own features Method prec@0.5 prec@0.6 prec@0.7 prec@0.8 prec@0.9</p><p>IoU RMI-LSTM <ref type="bibr" target="#b16">[17]</ref> 42  <ref type="bibr" target="#b14">[15]</ref> are slightly higher than original numbers reported in their paper which did not use DenseCRF postprocessing.</p><p>Query: "the bottom two luggage cases being rolled"</p><p>Query: "small black suitcase"</p><p>Query: "second vase from right" without interactions with features from other levels. As presented in the bottom 5 rows in <ref type="table" target="#tab_1">Table 3</ref>, the proposed gated multi-level fusion outperforms these other multi-scale feature fusion methods.</p><formula xml:id="formula_8">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization and Failure Cases</head><p>Visualization: We visualize spatial feature representations with various query expressions for a given image. This helps to gain further insights on the learned model.</p><p>We adopt the same technique in <ref type="bibr" target="#b14">[15]</ref> to generate visualization heatmaps over spatial locations. It is created by normalizing the strongest activated channel of the last feature map, which is upsampled to match with the size of original input image. These generated heatmaps are shown in <ref type="figure">Fig. 5</ref>. It can be observed that our model is able to correctly respond to different query expressions with various categories, locations and relationships. For instance, in the second row, when the query is "woman" and "umbrella", our model highlights every woman and umbrella in the image. Similarly, when the query is "red", it captures both the red clothes and the red part of the umbrella. For a more specific phrase such as "a woman in a green coat", the model accurately identifies the woman being referred to.</p><p>Failure cases: We also visualize some interesting failure "chair" "couch" "partial chair in front" "bottom left portion of couch on left" "woman" "umbrella" "red" "a woman in a green coat" "pot" "pot on left" "two pot on the right" "the pot on the far left" cases in <ref type="figure">Fig. 6</ref>. These failures are caused by the ambiguity of the language (e.g. two boys on right in the 1st example and the feet in the 2nd example), similar object appearance (e.g. car vs cab in the 3rd example), and occlusion (the wheel of the motorbike in the 4th example). Some of these failure cases can potentially be fixed by applying object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed cross-modal self-attention and gated multi-level fusion modules to address two crucial challenges in the referring image segmentation task. Our crossmodal self-attention module captures long-range dependencies between visual and linguistic modalities, which results in a better feature representation to focus on important information for referred entities. In addition, the proposed gated multi-level fusion module adaptively integrates features from different levels via learnable gates for each individual level. The proposed network achieves state-of-theart results on all four benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative examples of referring image segmentation: (a) original image; (b) visualization of the linguistic representation (attentions to word at each of the three feature levels); (c) segmentation result using only features at the 3rd level (i.e. Res5); (d) segmentation result using multi-level features and; (e) ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>(Best viewed in color) Visualization of spatial feature representation. These spatial heatmaps show the responses of the network to different query expressions. Query: "boy on right" Query: " legs on left" Query: "car next to cab" Query: "right bike" Some failure examples of our model: (a) original image; (c) segmentation result; (c) ground truth. The failures are due to factors such as language ambiguity (1st and 2nd rows), similar object appearance (3rd row) and occlusion (4th row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of segmentation performance with the state-of-the-art methods on four evaluation datasets in terms of IoU.</figDesc><table><row><cell></cell><cell></cell><cell>UNC</cell><cell></cell><cell></cell><cell>UNC+</cell><cell></cell><cell cols="2">G-Ref ReferIt</cell></row><row><cell></cell><cell>val</cell><cell cols="2">testA testB</cell><cell>val</cell><cell>testA</cell><cell>testB</cell><cell>val</cell><cell>test</cell></row><row><cell>LSTM-CNN [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>28.14</cell><cell>48.03</cell></row><row><cell>RMI [17]</cell><cell cols="7">45.18 45.69 45.57 29.86 30.48 29.50 34.52</cell><cell>58.73</cell></row><row><cell>DMN [20]</cell><cell cols="7">49.78 54.83 45.13 38.88 44.22 32.29 36.76</cell><cell>52.81</cell></row><row><cell>KWA [22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.92</cell><cell>59.09</cell></row><row><cell>RRN [15]</cell><cell cols="7">55.33 57.26 53.93 39.75 42.15 36.11 36.45</cell><cell>63.63</cell></row><row><cell>Ours</cell><cell cols="7">58.32 60.61 55.09 43.76 47.60 37.89 39.98</cell><cell>63.80</cell></row><row><cell>Method</cell><cell>IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>No attention</cell><cell cols="2">45.63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word attention</cell><cell cols="2">47.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pixel attention</cell><cell cols="2">47.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Word-pixel pair attention 47.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Cross-modal self-attention 50.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on the UNC val set. The top four methods compare results of different methods for multimodal feature representations. The bottom five results show comparisons of multi-level feature fusion methods. CMSA and GF denote the proposed cross-modal self-attention and gated multi-level fusion modules. All methods use the same base model (DeepLab-101) and DenseCRF for postprocessing.</figDesc><table><row><cell></cell><cell>.99</cell><cell>33.24</cell><cell>22.75</cell><cell>12.11</cell><cell>2.23</cell><cell>45.18</cell></row><row><cell>RRN-CNN [15]  *</cell><cell>47.59</cell><cell>38.76</cell><cell>26.53</cell><cell>14.79</cell><cell>3.17</cell><cell>46.95</cell></row><row><cell>CMSA-S</cell><cell>51.19</cell><cell>41.31</cell><cell>29.57</cell><cell>14.99</cell><cell>2.61</cell><cell>48.53</cell></row><row><cell>CMSA-W</cell><cell>51.95</cell><cell>43.11</cell><cell>32.74</cell><cell>19.28</cell><cell>4.11</cell><cell>50.12</cell></row><row><cell>CMSA+PPM</cell><cell>58.25</cell><cell>49.82</cell><cell>39.09</cell><cell>24.76</cell><cell>5.73</cell><cell>53.54</cell></row><row><cell>CMSA+Deconv</cell><cell>58.29</cell><cell>49.94</cell><cell>39.16</cell><cell>25.42</cell><cell>6.75</cell><cell>54.18</cell></row><row><cell>CMSA+ConvLSTM</cell><cell>64.73</cell><cell>56.03</cell><cell>45.23</cell><cell>29.15</cell><cell>7.86</cell><cell>56.56</cell></row><row><cell>CMSA+Gated</cell><cell>65.17</cell><cell>57.25</cell><cell>47.37</cell><cell>33.31</cell><cell>9.66</cell><cell>57.08</cell></row><row><cell>CMSA+GF(Ours)</cell><cell>66.44</cell><cell>59.70</cell><cell>50.77</cell><cell>35.52</cell><cell>10.96</cell><cell>58.32</cell></row></table><note>* The numbers for</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: LY, MR and YW are supported by NSERC. ZL is supported by the National Natural Science Foundation of China under Grant No. 61771301. LY and MR are also supported by the GETS and UMGF programs at the University of Manitoba. Thanks to NVIDIA for donating some of the GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The segmented and annotated iapr tc-12 benchmark. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hugo Jair Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villaseñor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling relationships in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edgar A Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbeláez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-word-aware network for referring expression image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
