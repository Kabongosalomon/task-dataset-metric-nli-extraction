<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TFNet: Multi-Semantic Feature Interaction for CTR Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>SIGIR</publisher>
				<availability status="unknown"><p>Copyright SIGIR</p>
				</availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueli</forename><surname>Yu</surname></persName>
							<email>xueli.yu@cripac.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<email>qiang.liu@realai.ai</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Huang</surname></persName>
							<email>sinohuang@tencent.com</email>
							<affiliation key="aff3">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueli</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Huang</forename></persName>
						</author>
						<title level="a" type="main">TFNet: Multi-Semantic Feature Interaction for CTR Prediction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
						<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval						</meeting>
						<imprint>
							<publisher>SIGIR</publisher>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3397271.3401304</idno>
					<note>ACM Reference Format:20). ACM, New York, NY, USA, 4 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The CTR (Click-Through Rate) prediction plays a central role in the domain of computational advertising and recommender systems. There exists several kinds of methods proposed in this field, such as Logistic Regression (LR), Factorization Machines (FM) and deep learning based methods like Wide&amp;Deep, Neural Factorization Machines (NFM) and DeepFM. However, such approaches generally use the vector-product of each pair of features, which have ignored the different semantic spaces of the feature interactions. In this paper, we propose a novel Tensor-based Feature interaction Network (TFNet) model, which introduces an operating tensor to elaborate feature interactions via multi-slice matrices in multiple semantic spaces. Extensive offline and online experiments show that TFNet: 1) outperforms the competitive compared methods on the typical Criteo and Avazu datasets; 2) achieves large improvement of revenue and click rate in online A/B tests in the largest Chinese App recommender system, Tencent MyApp.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The CTR prediction plays a central role in the domain of computational advertising and recommender systems, where an item could whether be recommended is decided by the probability of whether the user would click on it. Traditional methods to predict the probability of CTR are LR and FM <ref type="bibr" target="#b8">[9]</ref>. However, these kinds of methods can not obtain the higher-order interaction of different features. Several deep learning methods have been proposed to this field in recent years, such as Wide&amp;Deep <ref type="bibr" target="#b1">[2]</ref>, NFM <ref type="bibr" target="#b3">[4]</ref> and DeepFM <ref type="bibr" target="#b2">[3]</ref>. The general architecture of these methods is simply concatenating the first-order features and interactive second-order features, inputting them into the Multilayer Perceptron (MLP) to learn higher-order * The first two authors contributed equally to this work. † This work is supported by National Key Research and Development Program (2018YFB1402600).</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. <ref type="bibr">Request</ref>  However, there exists a potential limitation in such methods, they all have ignored the different semantic spaces among the feature interactions. In other words, the previous works generally use the vector-product of each pair of features, considering them all in one semantic space. Because of the features' semantic diversities, different interactive features may be in different semantic spaces. For instance, in ads recommender systems, it is reasonable that the interactions of feature pair (user, ad) and (banner-position, ad) are in the different semantic spaces, where the former learns the effect of the user's preference on the ad while the latter represents the effect of cost paid by the advertiser on this ad. Therefore, learning such feature interactions via simple vector-product in just one semantic space is obviously insufficient.</p><p>There are several fields of works which have utilized the semantic interactions, such as Natural Language Processing (NLP) <ref type="bibr" target="#b10">[11]</ref> and recommender systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref>. For example, the work in NLP <ref type="bibr" target="#b10">[11]</ref> introduces a tensor-based composition function to learn powerful meaning of pairs of words vectors so as to realize better interactions between each pair of words. Besides, in recommender systems field, an operating tensor is used to explore the semantic effects on the recommendation results <ref type="bibr" target="#b5">[6]</ref>.</p><p>Inspired by the works above, we propose a method named TFNet. We introduce an operating tensor to elaborate feature interactions via multi-slice matrices, by which we can acquire the difference in multiple semantic spaces. The whole procedures of our model are listed following: we firstly input the original features and embed them into dense embedding vectors, then introduce the operating tensor to carry out the tensor-based feature interactions between each pair of field vectors. Further, we input the interactive features and the embedding vectors into DNN separately to model higherorder feature interactions and finally combine the original raw features to make the ultimate prediction. We also demonstrate that TFNet is a more general form of recent models in following sections.</p><p>In summary, our main contributions are as follows: 1) We propose a novel TFNet model, introducing a tensor-based approach to learn feature interactions in multiple semantic spaces, which can capture the interactive mechanism of different features more sufficiently than the current deep learning based models. 2) Extensive offline and online experiments show that TFNet not only outperforms competitive compared methods on typical Criteo and Avazu datasets, but also achieves large improvement of revenue and click rate in online A/B tests in the largest Chinese App recommender systems. Field i Field j <ref type="figure">Figure 1</ref>: The architecture of our proposed TFNet model, which consists of the following parts: sparse input layer, embedding layer, tensor-based feature interaction layer, higher-order feature interaction layer and the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>The whole structure of TFNet model is shown in <ref type="figure">Figure 1</ref>. In this section, we detail the implement and how they work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sparse Input and Embedding Layer</head><p>The sparse input layer imports sparse original features, and low dimensional dense vectors of them are learned via the embedding layer. Supposing there are n feature fields Field 1 , ..., Field n in an ad impression scenario, following previous works <ref type="bibr" target="#b4">[5]</ref>, we input them into the sparse input and embedding layer, embedding each feature</p><formula xml:id="formula_0">Field i as v i ∈ R d , where d is the dimension of Field i 's embedding vectors.</formula><p>Therefore, we can obtain embedding vectors of all the fields as {v 1 , v 2 , ..., v n }. The concatenation of them is defined as x v .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tensor-based Feature Interaction Layer</head><p>In this section, we aim to explore tensor-based feature interactions from the embedding of input features, capturing the feature interactions in different semantic spaces.</p><p>Tensor-based Semantic Interaction. We bring in the operating tensor T 1 to learn how to model the interaction between each pair of features v i and v j , where T 1 ∈ R d ×m×d is a third-order operating tensor and m is the number of slices. Each slice T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[i]</head><p>1 ∈ R d ×d can represent an operation over paired-features in a semantic space. That is to say, the T 1 stores several kinds of semantic operations spaces, for example, the user preference space and the advertiser preference space in the ads impression scenario. Based on the operating tensor T 1 and the embeddings of input instances, we can generate an interactive feature s i j ∈ R m of v i and v j as follows,</p><formula xml:id="formula_1">s i j = v T i T 1 v j .<label>(1)</label></formula><p>Then we can concatenate all the interactive features to construct matrix S ∈ R q×m , where q = n * (n − 1)/2 . Adaptive Gate. Because of the diversity of semantic spaces, modeling all feature interactions in different semantic spaces with the same weight may be not sufficient. For example, when learning the interactions of feature pair (user, ad), the semantic space of user preference tends to be more important than the space of advertiser preference. Therefore, it is necessary to introduce an importance weight to the operating tensor, by which we can discriminate the importance of different semantic spaces during learning the feature interactions. In order to realize it, we introduce an adaptive gate Adaptive Gate with different importance weights to learn the operating tensor T 1 . The architecture of how to learn the weighted T 1 is shown in <ref type="figure" target="#fig_0">Figure  2</ref>. As is illustrated, T 1 can be obtained via attention mechanism on meta-semantic operation tensor T 2 ∈ R d ×m×d as</p><formula xml:id="formula_2">T 1 = g a ⊙ T 2 ,<label>(2)</label></formula><p>where ⊙ means element-wise multiplication of a vector and a tensor, and the adaptive gate g a ∈ R m is the importance weight of metasemantic operations for a specific pair of features interaction (v i , v j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus T [i]</head><p>1 of tensor T 1 can be further interpreted as T</p><formula xml:id="formula_3">[i] 1 = g [i] a T [i]</formula><p>2 . Moreover, to learn the attention score of g a , another operating tensor in the adaptive gate is introduced, which is the T 3 ∈ R d ×m×d , and the g a is computed via g a = so f tmax(v T i T 3 v j ). Control Gate. Among all the generated interactive features above, it is noted that not all of them are useful for the target prediction, therefore, we need to select important features from all generated interactive features. Similar to <ref type="bibr" target="#b11">[12]</ref>, each new feature is associated with a control gate g c ∈ R q to decide whether the feature is useful for prediction. g c should be non-negative and sparse, which is regularized by L1-Norm. Finally, the critical tensorbased interactive features can be obtained, defined as s h = S T g c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Higher-order Feature Interaction Layer</head><p>To model higher-order feature interactions, we further introduce a stack of fully connected layers as hidden layers H l . Formally, the definition of fully connected layers is as follow:</p><formula xml:id="formula_4">z i = σ i (W i z i−1 + b i ) , i = 1, 2, ..., l<label>(3)</label></formula><formula xml:id="formula_5">H l (x) = z l (z l −1 (...z 1 (x))) ,<label>(4)</label></formula><p>where l denotes the number of hidden layers, and the W i , b i and σ i is the weight matrix, bias vector and activation function of the ith layer. As is illustrated in <ref type="figure">Figure 1</ref>, there are two parts of higher-order feature interactions in the TFNet, including the tensor-based interactive features and the embedding vectors. For the tensor-based interactive features, we input the critical features s h into the l 1layer fully connected layers, obtaining the higher-order interactive features t h = H l 1 (s h ). Besides, for the higher-order feature interactions of embedding vectors, the l 2 -layer fully connected layers are constructed and the formulation of it is x h = H l 2 (x v ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Output Layer</head><p>Finally, in the output layer, we concatenate three parts of features to make the ultimate prediction, including the original raw input features and the higher-order interactive features of embedding vectors and tensor-based critical features. The formulation of the final prediction score is as follow,</p><formula xml:id="formula_6">p = σ s (w T concat(c, x h , t h ) + b),<label>(5)</label></formula><p>where p is the final prediction score of the TFNet model, c is the concatenation of original raw features. And the concat function denotes the concatenation of the three parts. w and b are the model weight and the bias vector respectively. Furthermore, we use a non-linear activation function σ s to output the probabilities, which can be modified according to different tasks, for example, we use sigmoid for a classification task here. Given the label y ∈ {0, 1}, the loss function is defined as the cross-entropy of prediction over the labels, formulated as</p><formula xml:id="formula_7">L = − 1 N N i=1 y i log p i + (1 − y i ) log(1 − p i ),<label>(6)</label></formula><p>where N is the total number of training samples and i indexes the training samples. Finally, the model can be trained in an end-to-end way efficiently using stochastic gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed TFNet model in both offline and online environments. We first describe the datasets and settings of the experiments, then report and analyze the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Settings</head><p>Datasets. A common application scenario of user click prediction is the task of click prediction of ad impression. The offline experiments are conducted on two typical and large real-world datasets. Criteo 1 contains 45 million click records of ad impression. There are 13 continuous features and 26 categorical ones. Considering the higher volume and higher unbalance of data, we do negative down-sampling on the Criteo dataset as <ref type="bibr" target="#b7">[8]</ref>. Avazu 2 contains 40 million click records of ad impression with 22 categorical features, such as site category, time, device type, banner position, etc.</p><p>Compared Methods. Several representative methods are used for empirical comparison: (i) FM <ref type="bibr" target="#b8">[9]</ref> , (ii) Wide&amp;Deep <ref type="bibr" target="#b1">[2]</ref> , (iii) DeepFM <ref type="bibr" target="#b2">[3]</ref> , (iv) NFM <ref type="bibr" target="#b3">[4]</ref> and (v) AFM <ref type="bibr" target="#b12">[13]</ref>.</p><p>To make a fair comparison, the number of parameters of the proposed TFNet model is set to be approximately equal to that of most compared models. For the proposed TFNet model, the network structure of two hidden layers H l 1 ,H l 2 are 512-512, d = 45, m = 4, 6 for Criteo dataset and Avazu dataset respectively.</p><p>Evaluation Metrics. To evaluate the performance, we adopt AUC (Area Under ROC), and to estimate the Relative Improvement (RI) of online performance based on offline performance, RI-AUC is proposed to make good comparison between the proposed model and compared models <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_8">RI-AUC = AUC(model) − 0.5 AUC(base) − 0.5 − 1 .<label>(7)</label></formula><p>For an online recommender business, there are two key evaluation metrics, ARPU (Average Revenue Per User) and CTR, which are defined as ARPU = R t /N u , CTR = N s /N i . ARPU is calculated in a standard time period, such as a day or a month. R t is the total revenue generated by all users during a time period. N u is the total number of users during that time period. In a recommender system, an impression means a view of a user to a recommended item. N i is the total number of impressions. N s is the number of impressions that are successfully recommended, which means users either click advertisements or download applications. Similar to <ref type="bibr" target="#b7">[8]</ref>, we can define relative improvement of ARPU and CTR, i.e., RI-ARPU and RI-CTR as follows (X can be either ARPU or CTR),</p><formula xml:id="formula_9">RI-X = X(model) − X(base) X(base) * 100% .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Evaluations</head><p>We conduct offline experiments to obtain thorough comparisons between our model and state-of-the-art models on two typical and large real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>We first make ablation analysis of the higher-order feature interaction part of TFNet, which consists of the higher-order interactions of the embedding vectors and tensor-based interactive features. The analysis verifies that whether the higher-order part can be a complement of tensor-based ones in TFNet. Comparing TFNet-and TFNet in the bottom part of <ref type="table" target="#tab_2">Table 1</ref>, higher-order interactions obtain extra 0.4% relative AUC improvement, which demonstrates the mutual complementation of tensor-based and higher-order feature interaction. <ref type="table" target="#tab_2">Table 1</ref> also illustrates experimental results of other compared models on both Criteo and Avazu datasets. As is shown, the proposed TFNet model can gain prominent relative AUC improvement of around 2% against compared models on both datasets. This verifies the effectiveness of the proposed method. Compared with the baselines, the TFNet utilizes tensor-based interaction method to capture interactive features in different semantic spaces. As is shown in <ref type="table" target="#tab_2">Table 1</ref>, even without the higherorder feature interactions part of the TFNet model, the TFNet-also outperforms the above models in both datasets, which verifies the effectiveness of the tensor-based approach. Hyper-Parameter Study Number of Slice m. As can be depicted in <ref type="table" target="#tab_3">Table 2</ref>, when m gradually increases, the performances of the proposed TFNet model on both datasets reach the peak and then decrease. It is worth mentioned that on the Criteo dataset, the TFNet achieves best AUC when m = 4, while on the Avazu dataset the m = 6. It may be  likely that there are more feature fields in the Avazu dataset, which accordingly needs more slices of semantic spaces. Dimension d of Embeddings. <ref type="table" target="#tab_3">Table 2</ref> illustrates the AUC values of the proposed TFNet model on both datasets with different dimensions d of latent embedding vectors. On both the Criteo and Avazu datasets, the TFNet model achieves the best AUC when dimension d = 45. As the dimension d continues to grow, the performance decreases due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Evaluations</head><p>We perform online A/B tests in Tencent MyApp 3 , the largest Chinese App recommender system. The online baseline model is the Wide&amp;Deep method <ref type="bibr" target="#b1">[2]</ref>. The TFNet model is initialized by the latest 7-day CTR log data. During the following process of online evaluation, these two models are updated every two hours by using the same datasets: the training set includes CTR logs collected from the last hour to last 25 hours, and the validation set includes logs in the last hour.</p><p>For the online advertising system, there are usually around 10% new records of ads in the database each day. Therefore, it is necessary to consider new-comers of ads and users each day, which is the main difference of online and offline evaluations <ref type="bibr" target="#b0">[1]</ref>. Moreover, the unstable online data distribution will lead to fluctuation of prediction performance, which can be illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. During the period of A/B tests, we calculate RI-ARPU and RI-CTR of the TFNet model each day. Great improvements of the TFNet model are observed: an average of 6.22% relative ARPU improvement (max 9.34%, min 1.88%) and an average of 3.81% relative CTR improvement (max 9.77%, min 0.50%). As is illustrated by the Tencent MyAPP, the budget of the latency in online system is 50ms at most, thus our proposed TFNet model's 8∼17ms quite satisfies the common budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION AND FUTURE WORK</head><p>In this work, we propose a tensor-based feature interaction model TFNet, which can learn the feature interactions in different semantic spaces. Extensive offline experiments demonstrate that the TFNet model significantly outperforms existing models and achieves the state-of-the-art results. Moreover, online A/B tests show great revenue and CTR improvements of the TFNet model in the largest Chinese App recommender system.</p><p>In addition, we are trying to utilize this model for ranking on the Chinese mainstream short video platform WeSee 4 , where the scene is a sliding play style instead of a click one, making it more complicated and challenging for samples modeling. It is in the offline verification stage at present, and online evaluation will be accessed later.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of how to learn the weighted operating tensor T 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Online evaluation of the proposed TFNet model against the base model (Wide&amp;Deep).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Offline experimental results of compared methods on the Criteo and Avazu datasets. RI-AUC is the relative AUC improvement of the proposed TFNet model against all other models. TFNet-means TFNet without higher-order feature interactions.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">Avazu AUC RI-AUC</cell><cell cols="2">Criteo AUC RI-AUC</cell></row><row><cell>FM</cell><cell>77.67%</cell><cell>3.22%</cell><cell>78.93%</cell><cell>3.39%</cell></row><row><cell>AFM</cell><cell>77.96%</cell><cell>2.15%</cell><cell>79.00%</cell><cell>3.14%</cell></row><row><cell>DeepFM</cell><cell>77.99%</cell><cell>2.04%</cell><cell>79.34%</cell><cell>1.94%</cell></row><row><cell>NFM</cell><cell>78.00%</cell><cell>2.00%</cell><cell>79.24%</cell><cell>2.29%</cell></row><row><cell>Wide&amp;Deep</cell><cell>78.05%</cell><cell>1.82%</cell><cell>79.37%</cell><cell>1.84%</cell></row><row><cell>TFNet-</cell><cell>78.43%</cell><cell>0.46%</cell><cell>79.79%</cell><cell>0.40%</cell></row><row><cell cols="2">TFNet (Ours) 78.56%</cell><cell>-</cell><cell>79.91%</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The impact of d (dimension of embeddings) and m (the number of slice of operating tensor) of the TFNet model on the Criteo and Avazu datasets.</figDesc><table><row><cell>m</cell><cell>Criteo</cell><cell>Avazu</cell><cell>d</cell><cell>Criteo</cell><cell>Avazu</cell></row><row><cell>1</cell><cell>0.79796</cell><cell>0.78461</cell><cell cols="2">20 0.79840</cell><cell>0.78382</cell></row><row><cell>2</cell><cell>0.79835</cell><cell>0.78522</cell><cell cols="2">25 0.79865</cell><cell>0.78444</cell></row><row><cell>3</cell><cell>0.79866</cell><cell>0.78529</cell><cell cols="2">30 0.79866</cell><cell>0.78499</cell></row><row><cell cols="3">4 0.79910 0.78533</cell><cell cols="2">35 0.79873</cell><cell>0.78505</cell></row><row><cell>5</cell><cell>0.79840</cell><cell>0.78537</cell><cell cols="2">40 0.79897</cell><cell>0.78536</cell></row><row><cell>6</cell><cell cols="2">0.79844 0.78560</cell><cell cols="3">45 0.79910 0.78561</cell></row><row><cell>7</cell><cell>0.79859</cell><cell>0.78544</cell><cell cols="2">50 0.79877</cell><cell>0.78545</cell></row><row><cell>8</cell><cell>0.79886</cell><cell>0.78521</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://s3-eu-west-1.amazonaws.com/criteo-labs/dac.tar.gz 2 https://www.kaggle.com/c/avazu-ctr-prediction/data</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.weishi.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks based Click-Through Rate Prediction with Multiple Feature Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning for Recommender Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepfm: A factorization-machine based neural network for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">COT: Contextual Operating Tensor for Context-Aware Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A convolutional click prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Product-Based Neural Networks for User Response Prediction over Multi-Field Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual Operation for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpret Neural Networks by Identifying Critical Data Routing Paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
