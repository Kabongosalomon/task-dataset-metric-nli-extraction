<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
							<email>sjlee@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
							<email>jskim2@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
							<email>jsyoon@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
							<email>shshin@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
							<email>obailo@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
							<email>nikim@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hee</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Seok</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Hoon</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon † † Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST ‡ Samsung Electronics DMC R&amp;D Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition that is guided by a vanishing point under adverse weather conditions. We tackle rainy and low illumination conditions, which have not been extensively studied until now due to clear challenges. For example, images taken under rainy days are subject to low illumination, while wet roads cause light reflection and distort the appearance of lane and road markings. At night, color distortion occurs under limited illumination. As a result, no benchmark dataset exists and only a few developed algorithms work under poor weather conditions. To address this shortcoming, we build up a lane and road marking benchmark which consists of about 20,000 images with 17 lane and road marking classes under four different scenarios: no rain, rain, heavy rain, and night. We train and evaluate several versions of the proposed multi-task network and validate the importance of each task. The resulting approach, VPGNet, can detect and classify lanes and road markings, and predict a vanishing point with a single forward pass. Experimental results show that our approach achieves high accuracy and robustness under various conditions in realtime (20 fps). The benchmark and the VPGNet model will be publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autonomous driving is a large system that consists of various sensors and control modules. The first key step for robust autonomous driving is to recognize and understand the environment around a subject. However, simple recognition of obstacles and understanding of geometry around a vehicle is insufficient. There are traffic regulations dictated by traffic symbols such as lane and road markings that should be complied with. Moreover, for an algorithm to be applicable to autonomous driving, it should be robust under diverse environments and perform in real-time. However, research on lane and road marking detection thus far has been limited to fine weather conditions. Handcrafted feature based methods exploit edge, color or texture information for detection, which results in a performance drop when the algorithm is tested under challenging weather and illumination conditions. Likewise, methods based on a combination of a Convolutional Neural Network (CNN) and hand-crafted features face the same challenge. Recently, a few CNN based approaches have been developed to tackle the problem in an end-to-end fashion including learning-based algorithms. They demonstrate good performance on benchmarks and in real road scenes, but are still limited to fine weather and simple road conditions. The lack of public lane and road marking datasets is an- With recent advances in deep learning, the key to robust recognition in challenging scenes is a large dataset that incorporates data captured under various circumstances. Since no proper datasets available for lane and road marking recognition, we have collected and annotated lanes and road markings of challenging scenes captured in urban areas. Additionally, a higher network capability with a proper training scheme is required to generate a fine representation to cope with varied data. We propose to train a network that recognizes a global context in a manner similar to humans.</p><p>Interestingly, humans can drive along a lane even when it is hard to spot. Research works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref> have empirically shown that the drivers gaze direction is highly correlated with the road direction. This implies that a geometric context plays a significant role in the lane localization. Inspired by this, we aim to utilize a vanishing point prediction task to embed a geometric context recognition capability to the proposed network. Further, we hope to advance autonomous driving research with the following contributions:</p><p>• We build up a lane and road marking detection and recognition benchmark dataset taken under various weather and illumination conditions. The dataset consists of about 20,000 images with 17 manually annotated lane and road markings classes. Vanishing point annotation is provided as well. • We design a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition that is guided by the vanishing point. We provide an extensive evaluation of our network on the created benchmark. The results show robustness under different weather conditions with realtime performance. Moreover, we suggest that the proposed vanishing point prediction task enables the network to detect lanes that are not explicitly seen. This paper is organized as follows. Section 2 covers recent algorithms developed for lane and road marking detection. A description of the benchmark is given in Section 3. Section 4 explains our network architecture and training scheme. Experimental results are reported in Section 5. Finally, Section 6 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we introduce previous works that aim to resolve the road scene detection challenge. Our setup as well as related works is based on a monocular vision setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lane and Road Marking Detection</head><p>Although lane and road marking detection appears to be a simple problem, the algorithm must be accurate in a variety of environments and have fast computation time. Lane detection methods based on hand-crafted features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> detect generic shapes of markings and try to fit a line or a spline to localize lanes. This group of algorithms performs well for certain situations while showing poor performance in unfamiliar conditions. In the case of road marking detection algorithms, most of the works are based on hand-crafted features. Tao et al. <ref type="bibr" target="#b33">[34]</ref> extract multiple regions of interest as Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b24">[25]</ref>, and rely on FAST <ref type="bibr" target="#b31">[32]</ref> and HOG <ref type="bibr" target="#b6">[7]</ref> features to build templates for each road marking. Similarly, Greenhalgh et al. <ref type="bibr" target="#b12">[13]</ref> utilizes HOG features and a SVM is trained to produce class labels. However, as in the lane detection case, these approaches show a performance drop in unfamiliar conditions.</p><p>Recently, deep learning methods have shown great success in computer vision, including lane detection. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref> proposes a lane detection algorithm based on a CNN. Jun Li et al. <ref type="bibr" target="#b20">[21]</ref> uses both a CNN and a Recurrent Neural Network (RNN) to detect lane boundaries. In this work, the CNN provides geometric information of lane structures, and this information is utilized by the RNN that detects the lane. Bei He et al. <ref type="bibr" target="#b13">[14]</ref> proposes using a Dual-View Convolutional Neutral Network (DVCNN) framework for lane detection. In this approach, the front-view and top-view images are fed as input to the DVCNN. Similar to the lane detection algorithms, several works have examined the application of neural networks as a feature extractor and a classifier to enhance the performance of road marking detection and recognition. Bailo et al. <ref type="bibr" target="#b1">[2]</ref> proposes a method that extracts multiple regions of interest as MSERs <ref type="bibr" target="#b24">[25]</ref>, merges regions that possibly belong to the same class, and finally classifies region proposals by utilizing a PCANet <ref type="bibr" target="#b5">[6]</ref> and a neural network.</p><p>Although the aforementioned approaches provide a promising performance of lane and road marking detection using deep learning, the problem of detection under poor conditions is still not solved. In this paper, we propose a network that performs well in any situation including bad weather and low illumination conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection by CNNs</head><p>With advances of deep learning, recognition tasks such as detection, classification, and segmentation have been solved under a wide set of conditions, yet there is no leading solution. RCNN and its variants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref> provide a breakthrough in detection and classification, outperforming previous approaches. Faster <ref type="bibr">RCNN [27]</ref> replaces handcrafted proposal methods with a convolutional network in a way that the region proposal layer shares extracted features with the classification layer. Overfeat <ref type="bibr" target="#b29">[30]</ref> shows that a con-volutional network with a sliding window approach can be efficiently computed. Its performance in object recognition and localization using multi-scale images is also reported. Some of its variants <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> achieve state of the art performance in detection tasks. Although these approaches show cutting edge results on large-scale benchmarks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>, which contain objects that occupy a significant part of an image, the performance decreases for smaller and thinner objects (e.g. lane or road markings).</p><p>Several deep learning approaches specialize in a lane and small object recognitions. For example, Huval et al. <ref type="bibr" target="#b15">[16]</ref> introduce a method for lanes and vehicles detection based on a fully convolutional architecture. They use the structure of <ref type="bibr" target="#b29">[30]</ref> and extend the method with an integrated regression module composed of seven convolutional layers for feature sharing. The network is divided into two branches which perform binary classification and regression task. They evaluate results under a nice weather on a highway without complex road symbols, but do not perform a multi-label classification. Additionally, Zhu et al. <ref type="bibr" target="#b34">[35]</ref> propose a multitask network for traffic sign (relatively small size) detection and classification. In this work, the classification layer is added in parallel to the <ref type="bibr" target="#b15">[16]</ref> network to perform detection and classification. As a result, this work reports better performance of detecting small objects than Fast RCNN <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection and Annotation</head><p>We have collected the dataset in various circumstances and categorized the images according to the time of the day and weather conditions. The dataset comprises situations during day time with different levels of precipitation: no rain, rainfall, and heavy rainfall. Night time images are not subdivided by weather condition but include general images taken in a challenging situation with low illumination. The number of images for each scenario is shown in <ref type="table" target="#tab_1">Table 1</ref>. Since our dataset is captured under bad weather conditions, we mount a camera inside a vehicle (in the center). In this way, we can avoid damaging the camera sensor while also preventing direct water drops on the camera lens. However, since several videos are recorded in heavy rain, a part of a window wiper is captured occasionally. The camera is directed to the front view of the car. Image resolution is 1288×728. Our data are captured in a downtown area of Seoul, South Korea. The shapes and symbols of the lane and road markings follow the regulations of South Korea.</p><p>We manually annotate corner points of lane and road markings. Corner points are connected to form a polygon which results in a pixel-level mask annotation for each object. In a similar manner, each pixel contains a class label.</p><p>However, if the network is trained with a thin lane annotation, the information tends to vanish through convolution and pooling layers. Further, since most of the neu-  ral networks require a resized image (usually smaller than original size), the thin annotations become barely visible. Therefore, we propose projecting pixel-level annotation to the grid-level mask. The image is divided into a grid 8×8 and the grid cell is filled with a class label if any pixel from the original annotation lies within the grid cell. Considering that the input size of our network is 640×480 and the output size is 80×60, the grid size is set to be proportional to the scale factor (1/8) between the input and output images. Specifically, the grid size is set to be 8×8. <ref type="figure" target="#fig_1">Figure 2</ref> shows an annotation example. The vanishing point annotation is also provided. We localize the vanishing point in a road scene where parallel lanes supposedly meet. The vanishing point is manually annotated by a human. Depending on the scene, a difficulty level (EASY, HARD, NONE) is assigned to every vanishing point. EASY level includes a clear scene (e.g. straight road); HARD level includes a cluttered scene (e.g. traffic jam); NONE is where a vanishing point does not exist (e.g. intersection). It is important to note that both straight and curved lanes are utilized to predict the vanishing point. We describe the definition of our vanishing point in detail in Section 4.2. Furthermore, annotation examples are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Statistics</head><p>Our dataset consists of about 20,000 images taken during three weeks of driving in Seoul. The raw video (30 fps) is sampled at 1Hz intervals to generate image data. Images of the complex urban traffic scenes contain lane and road markings under various weather conditions during different time of the day. In total, 17 classes are annotated covering the most common markings found on the road. Although we recorded the video in various circumstances, a data imbalance between different types of lane and road markings is observed. For example, in the case of lane classes, dashed white and double yellow lines are more common than other lane types. Regarding road marking classes, straight arrows and crosswalks appear most frequently. We also define a "Other markings" class containing road markings that are present only in South Korea, or have an insufficient number of instances to be trained as a separate class. Types of classes and the number of instances are listed in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Neural Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>Our network, VPGNet, is inspired by the work of <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b34">[35]</ref>. The competitive advantage of our network is that it is specialized to detect and recognize lane and road markings as well as to localize vanishing point.</p><p>We propose a data layer to induce grid-level annotation that enables training of both lane and road markings simultaneously. Originally in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref>, the box regression task aims to fit a single box to a particular object. This works well for objects with a blob shape (traffic signs or vehicles), but lane and road markings cannot be represented by a single box. Therefore, we propose an alternative regression that utilizes a grid-level mask. Points on the grid are regressed to the closest grid cell and combined by a multilabel classification task to represent an object. This enables us to integrate two independent targets, lane and road markings, which have different characteristics and shapes. For the post-processing, lane classes only use the output of the multi-label task, and road marking classes utilize both grid box regression and multi-label task (see Section 4.4). Additionally, we add a vanishing point detection task to infer a global geometric context during training of patterns of lane and road markings (explained in Section 4.2).</p><p>The overall architecture is described in <ref type="table" target="#tab_3">Table 3 and Figure 3</ref>. The network has four task modules and each task performs complementary cooperation: grid box regression, object detection, multi-label classification, and prediction of the vanishing point. This structure allows us to detect and classify the lane and road markings, and predict the vanishing region simultaneously in a single forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Vanishing Point Prediction Task</head><p>Due to poor weather environments, illumination conditions, and occlusion, the visibility of lanes decreases. However, in such situations, humans intuitively can predict the locations of the lanes from global information such as nearby structures of roads or the flow of traffic <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. Inspired by this, we have designed a Vanishing Point Prediction (VPP) task that guides robust lane and road marking detection similar to human vision. A vanishing point is a point where parallel lines in a three-dimensional space converge to a two-dimensional plane by a graphical perspective. In most cases of driving, lane and road markings converge to a single point regardless of whether the roads are curved or straight. In this paper, "Vanishing Point (VP)" is defined as the nearest point on the horizon where lanes converge and disappear predictively around the farthest point of the visible lane. This VP can be used to provide a global geometric context of a scene, which is important to infer the location of lanes and road markings. We integrate the VPP module with the multi-task network to train the geometric patterns of lane convergence to one point.</p><p>Borji <ref type="bibr" target="#b3">[4]</ref> has shown that a CNN can localize the VP. The author vectorizes the spatial output of the network to predict the exact location of a VP by using a softmax classifier. However, selecting exactly one point over the whole network's output size results in imprecise localization. In order to provide more robust localization, we perform several experiments to guide the VP.</p><p>First, for the VPP task, we tried regression losses (i.e. L1, L2, hinge losses) that directly calculate pixel distances from a VP. Unfortunately, the results are not favorable since it is difficult to balance the losses with other tasks (object detection/multi-label classification) due to the difference in the loss scale. Therefore, we adopt a cross entropy loss to balance the gradients propagated from each of the detection tasks. By using cross entropy loss, first we apply a binary classification method that directly classifies background and foreground ( i.e. vanishing area, see <ref type="figure" target="#fig_5">Figure 4a</ref>), as in the object detection task. The binary mask is generated in the data layer by drawing a fixed size circle centered at the VP we annotated. However, using this method on the VPP task results in extremely fast convergence of the training loss. This is caused by the imbalance of the number of background and foreground pixels. Since the vanishing area is drastically smaller than the background, the network is initialized to infer every pixel as background class. This phenomenon contradicts our original intention of training the VPP to learn the global context of a scene.</p><p>Considering the challenge imposed by the aforementioned imbalance during the binary VPP method, we have newly designed the VPP module. As stated before, the purpose of attaching the VPP task is to improve a scene representation that implies a global context to predict invisible lanes due to occlusions or extreme illumination condition. The whole scene should be taken into account to efficiently reflect global information inferring lane locations. We use a quadrant mask that divides the whole image into four sections. The intersection of these four sections is a VP. In this way, we can infer the VP using four quadrant sections which cover the structures of a global scene. To implement this, we define five channels for the output of the VPP task: one absence channel and four quadrant channels. Every pixel   in the output image chooses to belong to one of the five channels. The absence channel is used to represent a pixel with no VP, while the four quadrant channels stand for one of the quadrant sections on the image. For example, if the VP is present in the image, every pixel should be assigned to one of the quadrant channels, while the absence channel cannot be chosen. Specifically, the third channel would be guided by the upper right diagonal edges from the road scene, and the fourth channel would extract the upper left diagonal edges from the road scene. On the other hand, if the VP is hard to be identified (e.g. intersection roads, occlusions), every pixel will tend to be classified as the absence channel. In this case, the average confidence of the absence channel would be high. Unlike the binary classification approach, our quadrant method enriches the gradient information that contains a global structure of a scene. The loss comparison in <ref type="figure" target="#fig_5">Figure 4b</ref> indirectly shows that the network is trained without overfitting compared to the binary case. Note that we only use the quadrant VPP method for the evaluation. The binary VPP method is introduced only to show readers that a naive VPP training scheme does not yield satisfactory results. The whole multi-task network allows us to detect and recognize the lane and road marking, as well as to predict the VP simultaneously in a single forward pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>Our network includes four tasks which cover different contexts. The detection task recognizes objects and covers a local context, while the VPP task covers a global context. If those tasks are trained altogether at the same training phase, the network can be highly influenced by a certain dominant task. We noticed that during the training stage the VPP task became dependent on the lane detection task. The dependency between lanes and the VP implies a strong information correlation. In this case, the VP provides redun-  dant information to the network, leading to marginal lane detection improvement. In order to prevent this side effect, we train the network in two phases to tolerate the balance between the tasks.</p><p>In the first phase, we train only the VPP task. We fix the learning rates to zero for every task except the VPP module. In this way, we can train the kernels to learn a global context of the image. The training of this phase stops upon reaching convergence of the VP detection task. Although we train only the VPP task, due to the weight update of the mutually shared layers, losses of the other detection tasks are also decreased by about 20%. This shows that lane and road marking detection and VPP tasks share some common characteristics in the feature representation layers.</p><p>In the second phase, we further train all the tasks using the initialized kernels from the first phase. Since all tasks are trained together at this point, it is important to balance their learning rates. If a certain task loss weight is small, it becomes dependent on other tasks and vice versa. Equation <ref type="bibr" target="#b0">(1)</ref> shows the summation of four losses from each task:</p><formula xml:id="formula_0">Loss = w1Lreg + w2Lom + w3L ml + w4Lvp<label>(1)</label></formula><p>where L reg is a grid regression L1 loss, L om and L ml and L vp are cross entropy losses in each branch of the network. We balance the tasks by weight terms w 1 ∼w 4 in the following way. First, w 1 ∼w 4 are set to be equal to 1, and the starting losses are observed. Then, we set the reciprocal of these initial loss values to the loss weight so that the losses are uniform. In the middle of the training, if the scale difference between losses becomes large, this process is repeated to balance the loss values. The second phase stops when the validation accuracy is converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Post-Processing</head><p>Each lane and road marking class and VPs are required to be represented suitably for real world application. Therefore, we implement post-processing techniques to generate visually satisfying results.</p><p>Lane In the case of the lane classes, we use the following techniques: point sampling, clustering, and lane regression. First, we subsample local peaks from the region where the probability of lane channels from the multi-label task is high. The sampled points are potential candidates to become the lane segments. Further, selected points are projected to the birds-eye view by inverse perspective mapping (IPM) <ref type="bibr" target="#b2">[3]</ref>. IPM is used to separate the sampled points near the VP. This is useful not only for the case of straight roads but also curved ones. We then cluster the points by our modified density-based clustering method. We sequentially decide the cluster by the pixel distance. After sorting the points by the vertical index, we stack the point in a bin if there is a close point among the top of the existing bins. Otherwise, we create a new bin for a new cluster. By doing this, we can reduce the time complexity of the clustering. The last step is quadratic regressions of the lines from the obtained clusters utilizing the location of the VP. If the farthest sample point of each lane cluster is close to the VP, we include it in the cluster to estimate a polynomial model. This makes the lane results stable near the VP. The class type is assigned to each line segment from the multi-labeled output of the network.</p><p>Road marking For the road marking classes, grid sampling and box clustering are applied. First, we extract grid cells from the grid regression task with high confidence for each class from the multi-label output. We then select corner points of each grid and merge them with the nearby grid cells iteratively. If no more neighboring grid cells belong to the same class, the merging is terminated. Some road markings such as crosswalks or safety zones that are difficult to define by a single box are localized by grid sampling without subsequent merging.</p><p>Vanishing point Our VPP module outputs five channels of the confidence map: four quadrant channels and one absence channel. Through these quadrants, we generate the location of a VP. The VP is where all four quadrants intersect. That is, we need to find a point where four confidences from each quadrant channel become close. Equation <ref type="formula">(2)</ref>  </p><p>where P avg is the probability that a VP exists in the image, p n (x, y) is the confidence of (x, y) on n th channel (n = 0: absence channel), m×n is the confidence map size, and loc vp is the location of the VP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Our experiments consist of six parts. First, we show the experimental settings such as dataset splits and training parameters. Secondly, we provide an analysis of our network. We explore how multiple tasks jointly cooperate and affect the performance of each other. Third, our evaluation metric for each target is introduced. Lastly, we show lanes, road markings, and VPs detection and classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>A summary of the datasets is provided in <ref type="table" target="#tab_1">Table 1</ref>. During the training, we double the number of images by flipping the original ones. This, in turn, doubles the training set and also prevents positional bias that comes from the lane positions. More specifically, the dataset is obtained in a right-sided driving country, and by flipping the dataset we can simulate a left-sided environment.</p><p>At the first training phase, we initialize the network only by the VPP task. After the initialization, all four tasks are trained simultaneously. For every task, we use Stochastic Gradient Descent optimization with a momentum of 0.9 and a mini-batch size of 20. Since multiple tasks must converge proportionally, we tune the learning rate of each task.</p><p>We train three models of the network divided by task: 2-Task (revised <ref type="bibr" target="#b15">[16]</ref>), 3-Task (revised <ref type="bibr" target="#b34">[35]</ref>), and 4-Task (VPGNet). 2-Task network includes regression and binary classification tasks. 3-Task network includes 2-Task and a multi-label classification task. 4-Task network includes 3-Task and a VPP task, which is the VPGNet. Since the lane detection in <ref type="bibr" target="#b15">[16]</ref> is not fully reproducible, we modify the data layer to handle the grid mask and move one convolutional layer from shared layers to branch layers, as in the 3and 4-Task networks. The 3-Task network is similar to <ref type="bibr" target="#b34">[35]</ref>, but we modify the data layer to handle the grid mask.</p><p>We test our models on NVIDIA GTX Titan X and achieve a speed of 20 Hz by using only a single forward pass. Specifically, the single forward pass takes about 30 ms and the post-processing takes about 20 ms or less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of Multi Task Learning</head><p>In this section, we validate whether our multi-task modules contribute to improvement of the network training. We observe the activated neurons in the feature sharing network. From the lower to higher layer, the abstraction level is accelerated. <ref type="figure" target="#fig_7">Figure 5</ref> shows the activated neurons after each convolutional layer before the branch. We average over all channel values. For a fair comparison, we equalize the intensity scale in each layer activation. As the results show, if we use more tasks, more neurons respond, especially around the boundaries of roadways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Metrics</head><p>In this section, we show the newly proposed evaluation metrics for our benchmark evaluation. First, we introduce our evaluation metric for the lane detection. Since the ground truth of our benchmark is annotated with grid cells we compute the minimum distance from the center of each cell to the sampled lane points for every cell. If the minimum distance is within the boundary R, we mark these sampled points as true positive and the corresponding grid cell as detected. By measuring every grid cell on the lane, we can strictly evaluate the location of lane segments. Additionally, we measure F1 score for the comparison.</p><p>In the case of road markings, we use mitigated evaluation measurement. Since the only information we need while driving is the road marking in front of us rather than the exact boundary of the road markings, we measure the precision of predicted blobs. Specifically, we count all predicted cells overlapped with the ground truth grid cells. The overlapped cells are marked as true positive cells. If the number of true positive cells is greater than half of the number of all predicted cells over a clustered blob, the overlaid ground truth target is defined as detected. Additionally, we measure the recall score for comparison.</p><p>For evaluation of the VP, we measure the Euclidean distance between a ground truth point and a predicted VP. The recall score is evaluated by varying the threshold distance R from the ground truth VP. <ref type="figure">Figure 6</ref> shows a summary of how we measure all three targets of our network.  <ref type="figure">Figure 6</ref>. Graphical explanation of the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Lane Detection and Recognition</head><p>For lane classes, we measure detection, as well as simultaneous detection and classification performance. First, we compare our multi-task networks with the baseline methods in the Caltech Lanes Dataset <ref type="bibr" target="#b0">[1]</ref> (see <ref type="figure">Figure 7</ref>). We set R to equal to the average half value of the lane thickness (20 pixels). Due to perspective effect, the double lane in front of the camera is about 70 to 80 pixels thick, and it is as small as 8 pixels (a single grid size) near the VP. Since this dataset contains relatively easy scenes during daytime, the overall performance of 2-, 3-, and 4-Task networks is very similar. Nevertheless, our network achieves the best F1 score.</p><p>Further, we provide a comparison of the proposed three versions of multi-task networks and the FCN-8s <ref type="bibr" target="#b23">[24]</ref> segmentation method on our benchmark dataset. It is important to note that our networks utilize grid-level annotation, while FCN-8s is trained independently with both pixel-and gridlevel annotations. For testing purposes, four scenarios have been selected as in Section 5.1, and the F1 score is compared in each scenario. <ref type="figure">Figure 8</ref> shows the experimental results. Noticeably, our method shows significantly better lane detection performance in each bad weather condition scenario. Moreover, the forward pass time of the VPGNet is 30 ms, while FCN-8s <ref type="bibr" target="#b23">[24]</ref> takes 130 ms.</p><p>Interestingly, FCN-8s shows better performance with the proposed grid-level annotation scheme compared to pixellevel annotation. This proves that the grid-level annotation is more suitable for lane detection and recognition. The reason is that grid-level annotation generates stronger gradients from the edge information around the thinly annotated area (i.e. lane or road markings), which, in turn, results in en-  <ref type="figure">Figure 8</ref>. Lane detection score on our benchmark.</p><p>riched training and leads to better performance. In order to see what happens if the VP does not exist, we conducted an additional test on images without the VP (e.g. intersection roads or occlusions). <ref type="table">Table 4</ref> shows the results of the experiment, demonstrating that the enhancement of feature representation through the VPP task helps to find lanes even when there is no VP. Selected results are shown in the supplementary material.</p><p>For the simultaneous detection and classification of lane classes, due to the class imbalance, we measure the F1 score of the top four lane classes by the number of instances. The selected classes are: single white, dashed white, single yellow, and double yellow lines. <ref type="table">Table 5</ref> shows the performance of the 3-and 4-Task networks. Except for "no rain, daytime condition", recognition of the single white line is highly improved. This shows that using the VPP task on rainy and night conditions improves the activation of roadway boundaries which are usually marked with single white lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Road Marking Detection and Recognition</head><p>In the case of road marking classes, we evaluate the simultaneous detection and classification performance. Due to the dataset imbalance of road marking classes, we measure the recall score of the top four road marking classes by the number of instances. The selected classes are as follows: stop line, straight arrow, crosswalk, and safety zone. <ref type="table" target="#tab_6">Table 6</ref> shows the performance of 3-and 4-Task networks. Except for the stop line class in "no rain, daytime condition", the evaluation results are highly improved. This makes sense because the stop line has horizontal edges which are not  The proportion of true frames  <ref type="figure">Figure 9</ref>. Evaluation on the VPP task.</p><p>closely related to the VPP task. Other road markings have shapes that give directions to VP from a geometric perspective. Consequently, responses to those classes become highly activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Vanishing Point Prediction</head><p>In the case of a VP, we compare the VPP-only and 4-Task networks. In this manner, we can observe how the VPP is influenced by the lane and road marking detection. Moreover, we compare the performances of each scenario. <ref type="figure">Figure 9</ref> shows the experimental results. The left graph shows a comparison between two outputs: a prediction after the first phase and a prediction after the second phase. The prediction after the second phase is highly improved meaning that the VPP task gets help from lane and road marking detection tasks. The right graph shows the results of the prediction after the second phase for each scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we introduced lane and road marking benchmark that covers four scenarios: daytime (no rain, rain, heavy rain) and night conditions. We have also proposed a multi-task network for simultaneous detection and classification of lane and road markings, guided by a VP. The evaluation shows that the VPGNet model is robust under different weather conditions and performs in realtime. Furthermore, we have concluded that the VPP task enhances both lane and road marking detection and classification by enhancing activation of lane and road markings and the boundary of the roadway.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>1 https://github.com/SeokjuLee/VPGNet Examples of our lane and road markings detection results in: (a) complex city scene; (b) multiple road markings; (c) night scene; (d) rainy condition. Yellow region is the vanishing area. Each class label is annotated in white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Pixel-and grid-level annotations of the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>VPGNet performs four tasks: grid regression, object detection, multi-label classification, and vanishing point prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>loss (w/ binary VPP) task loss (w/ quadrant VPP) Object mask Object mask task loss (w/ binary VPP) task loss (w/ quadrant VPP) Multi-label task loss (w/ binary VPP) Multi-label task loss (w/ quadrant VPP) (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>(a) Output visualization of binary and quadrant VPP methods. For the prediction of the quadrant method, only four quadrant channels are visualized except for an absence channel. (b) The loss comparison of two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>and (3) describe the boundary intersection of each quadrant: Pavg = 1 − ( p0(x, y))/(m × n) (x, y)| 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Activated neurons in the feature sharing network. Intensity scale in each layer activation is equalized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1710.06288v1 [cs.CV] 17 Oct 2017 other challenge for the advancement of autonomous driving. Available datasets are often limited and insufficient for deep learning methods. For example, Caltech Lanes Dataset [1] contains 1,225 images taken from four different places. Further, Road Marking Dataset [34] contains 1,443 images manually labeled into 11 classes of road markings. Existing datasets are all taken under sunny days with a clear scene and adverse weather scenarios are not considered.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Number of frames for each scenario in the dataset.</figDesc><table><row><cell cols="2">Scenario (Scn.)</cell><cell cols="3">Total frames Training set Test set</cell></row><row><cell></cell><cell>No rain (Scn. 1)</cell><cell>13,925</cell><cell>9,184</cell><cell>4,741</cell></row><row><cell>Daytime</cell><cell>Rain (Scn. 2)</cell><cell>4,059</cell><cell>3,322</cell><cell>737</cell></row><row><cell></cell><cell>Heavy rain (Scn. 3)</cell><cell>825</cell><cell>462</cell><cell>363</cell></row><row><cell cols="2">Night (Scn. 4)</cell><cell>2,288</cell><cell>1,815</cell><cell>473</cell></row><row><cell></cell><cell>Total</cell><cell>21,097</cell><cell>14,783</cell><cell>6,314</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Single yellow</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dashed white</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Double yellow</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Straight arrow</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Number of instances for each class in the dataset.</figDesc><table><row><cell>Lane</cell><cell></cell><cell cols="2">Road marking</cell><cell cols="2">Vanishing point</cell></row><row><cell>Single white</cell><cell>25,354</cell><cell>Stop line</cell><cell>7,298</cell><cell cols="2">EASY 19,302</cell></row><row><cell cols="2">Dashed white 74,733</cell><cell>Left arrow</cell><cell cols="2">1,186 HARD</cell><cell>262</cell></row><row><cell>Double white</cell><cell>206</cell><cell>Right arrow</cell><cell>537</cell><cell cols="2">NONE 1,533</cell></row><row><cell cols="3">Single yellow 28,054 Straight arrow</cell><cell>6,968</cell><cell></cell></row><row><cell cols="2">Dashed yellow 5,734</cell><cell>U-turn arrow</cell><cell>127</cell><cell></cell></row><row><cell cols="2">Double yellow 8,998</cell><cell>Speed bump</cell><cell>1,523</cell><cell></cell></row><row><cell>Dashed blue</cell><cell>1,306</cell><cell>Crosswalk</cell><cell>13,632</cell><cell></cell></row><row><cell>Zigzag</cell><cell>1,417</cell><cell>Safety zone</cell><cell>6,031</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Other markings 52,975</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Proposed network structure.</figDesc><table><row><cell></cell><cell cols="2">Layer</cell><cell cols="6">Conv 1 Conv 2 Conv 3 Conv 4 Conv 5 Conv 6</cell><cell>Conv 7</cell><cell>Conv 8</cell></row><row><cell></cell><cell cols="4">Kernel size, stride, pad 11, 4, 0 5, 1, 2</cell><cell>3, 1, 1</cell><cell>3, 1, 1</cell><cell>3, 1, 1</cell><cell>6, 1, 3</cell><cell>1, 1, 0</cell><cell>1, 1, 0</cell></row><row><cell></cell><cell cols="2">Pooling size, stride</cell><cell>3, 2</cell><cell>3, 2</cell><cell></cell><cell></cell><cell>3, 2</cell><cell></cell></row><row><cell></cell><cell cols="2">Addition</cell><cell>LRN</cell><cell>LRN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Dropout Dropout, branched Branched</cell></row><row><cell></cell><cell cols="2">Receptive field</cell><cell>11</cell><cell>51</cell><cell>99</cell><cell>131</cell><cell>163</cell><cell>355</cell><cell>355</cell><cell>355</cell></row><row><cell>3</cell><cell>96</cell><cell>256</cell><cell>384</cell><cell></cell><cell>384</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv1+Pool</cell><cell>Conv2+Pool</cell><cell>Conv3</cell><cell></cell><cell>Conv4</cell><cell></cell><cell>Conv5+Pool</cell><cell></cell><cell>Conv6</cell></row><row><cell>(11×11)</cell><cell>(5×5)</cell><cell>(3×3)</cell><cell></cell><cell>(3×3)</cell><cell></cell><cell>(3×3)</cell><cell></cell><cell>(6×6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 7. Lane detection score on Caltech lanes dataset.</figDesc><table><row><cell></cell><cell>0.9</cell><cell>0.85</cell><cell>0.87</cell><cell>0.865</cell><cell></cell><cell></cell><cell></cell><cell>2-Task (Revised [16]) 3-Task (Revised [35]) 4-Task (VPGNet)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCN-8s (Pixel-annotated [24])</cell></row><row><cell>F1 Score</cell><cell>0.8</cell><cell>0.733</cell><cell cols="2">0.788</cell><cell>0.704 0.788 0.778 0.718 0.777</cell><cell>0.746 0.752 0.768</cell><cell>0.744</cell><cell>0.711 0.731 0.743 FCN-8s (Grid-annotated [24]) 0.702</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.634</cell><cell>0.633</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Scenario 1</cell><cell></cell><cell>Scenario 2</cell><cell>Scenario 3</cell><cell></cell><cell>Scenario 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell>0.866</cell><cell>0.869</cell><cell>0.884</cell><cell>0.861</cell><cell>0.848</cell><cell>0.869</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F1 Score</cell><cell>0.8</cell><cell>0.723</cell><cell>0.759</cell><cell>Caltech [1] 2-Task (Revised [16]) 3-Task (Revised [35])</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell>4-Task (VPGNet)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Caltech Set 1 (Cordova1)</cell><cell>Caltech Set 2 (Washington1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Lane detection score on No-VP set (Red: Best). Simultaneous detection and classification F1 score for lane classes (Red: Best).</figDesc><table><row><cell></cell><cell></cell><cell>FCN-8s</cell><cell>FCN-8s</cell><cell>3-Task</cell><cell></cell><cell>4-Task</cell></row><row><cell></cell><cell></cell><cell>(pixel)</cell><cell>(grid)</cell><cell>(revised [35])</cell><cell cols="2">(VPGNet)</cell></row><row><cell cols="2">No-VP set</cell><cell>0.3310</cell><cell>0.4496</cell><cell>0.4535</cell><cell></cell><cell>0.5234</cell></row><row><cell cols="2">Lane class</cell><cell cols="4">Single white Dashed white Single yellow</cell><cell>Double yellow</cell></row><row><cell>Scenario 1</cell><cell>3-Task 4-Task</cell><cell>0.55 0.49</cell><cell>0.77 0.76</cell><cell>0.57 0.58</cell><cell></cell><cell>0.32 0.36</cell></row><row><cell>Scenario 2</cell><cell>3-Task 4-Task</cell><cell>0.45 0.52</cell><cell>0.67 0.66</cell><cell>0.64 0.65</cell><cell></cell><cell>0.62 0.61</cell></row><row><cell>Scenario 3</cell><cell>3-Task 4-Task</cell><cell>0.31 0.42</cell><cell>0.72 0.73</cell><cell>0.70 0.71</cell><cell></cell><cell>0.37 0.40</cell></row><row><cell>Scenario 4</cell><cell>3-Task 4-Task</cell><cell>0.27 0.42</cell><cell>0.68 0.69</cell><cell>0.48 0.42</cell><cell></cell><cell>0.36 0.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Simultaneous detection and classification recall score for road marking classes (Red: Best).</figDesc><table><row><cell cols="2">Road marking class</cell><cell>Stop line</cell><cell cols="4">Straight arrow Crosswalk Safety zone</cell></row><row><cell>Scenario 1</cell><cell>3-Task 4-Task</cell><cell>0.83 0.78</cell><cell>0.46 0.80</cell><cell></cell><cell>0.88 0.94</cell><cell>0.59 0.80</cell></row><row><cell>Scenario 2</cell><cell>3-Task 4-Task</cell><cell>0.60 0.73</cell><cell>0.41 0.65</cell><cell></cell><cell>0.81 0.85</cell><cell>0.47 0.65</cell></row><row><cell>Scenario 3</cell><cell>3-Task 4-Task</cell><cell>0.33 0.56</cell><cell>0.39 0.63</cell><cell></cell><cell>0.84 0.93</cell><cell>0.47 0.61</cell></row><row><cell>Scenario 4</cell><cell>3-Task 4-Task</cell><cell>0.60 0.80</cell><cell>0.48 0.68</cell><cell></cell><cell>0.82 0.89</cell><cell>0.37 0.38</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pixel distance</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by DMC R&amp;D Center of Samsung Electronics Co.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust road marking detection and recognition using densitybased grouping and machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Real-time lane and obstacle detection on the system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>IV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vanishing point detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00967</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel lane detection system with efficient ground truth generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems (TITS)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="374" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A random finite set approach to multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szczot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic detection and recognition of symbols and text on the road surface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPRAM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Accurate and robust lane detection based on dual-view convolutional neutral network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lang</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-lane detection in urban driving environments using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Seo</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An efficient lane detection algorithm for lane departure detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust lane detection based on convolutional neural network and random sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Which parts of the road guide steering?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Horwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">377</biblScope>
			<biblScope unit="issue">6547</biblScope>
			<biblScope unit="page" from="339" to="340" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Where do we look when we steer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="issue">6483</biblScope>
			<biblScope unit="page" from="742" to="744" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural network for structural prediction and lane detection in traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust widebaseline stereo from maximally stable extremal regions. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="761" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-point visual control model of steering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Salvucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1233" to="1248" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vision-based lane analysis: Exploration of issues and approaches for embedded realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel curve lane detection based on improved river flow and ransa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITSC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Features from accelerated segment test (fast)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Viswanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lane-mark extraction for automobiles under complex conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2756" to="2767" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A practical system for road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic-sign detection and classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
