<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mechanical and Mechatronics Engineering Department University Of Waterloo Waterloo</orgName>
								<address>
									<addrLine>Canada apon@uwaterloo.ca, oandrien@uwaterloo.ca, www.aharakeh.com</addrLine>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oles</forename><surname>Andrienko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mechanical and Mechatronics Engineering Department University Of Waterloo Waterloo</orgName>
								<address>
									<addrLine>Canada apon@uwaterloo.ca, oandrien@uwaterloo.ca, www.aharakeh.com</addrLine>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mechanical and Mechatronics Engineering Department University Of Waterloo Waterloo</orgName>
								<address>
									<addrLine>Canada apon@uwaterloo.ca, oandrien@uwaterloo.ca, www.aharakeh.com</addrLine>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
							<email>stevenw@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Mechanical and Mechatronics Engineering Department University Of Waterloo Waterloo</orgName>
								<address>
									<addrLine>Canada apon@uwaterloo.ca, oandrien@uwaterloo.ca, www.aharakeh.com</addrLine>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Deep Architecture and Mini-Batch Selection Method For Joint Traffic Sign and Light Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>object detection</term>
					<term>autonomous driving</term>
					<term>traffic light</term>
					<term>traffic sign</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic light and sign detectors on autonomous cars are integral for road scene perception. The literature is abundant with deep learning networks that detect either lights or signs, not both, which makes them unsuitable for reallife deployment due to the limited graphics processing unit (GPU) memory and power available on embedded systems. The root cause of this issue is that no public dataset contains both traffic light and sign labels, which leads to difficulties in developing a joint detection framework. We present a deep hierarchical architecture in conjunction with a mini-batch proposal selection mechanism that allows a network to detect both traffic lights and signs from training on separate traffic light and sign datasets. Our method solves the overlapping issue where instances from one dataset are not labelled in the other dataset. We are the first to present a network that performs joint detection on traffic lights and signs. We measure our network on the Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small Traffic Lights benchmark for traffic light detection and show it outperforms the existing Bosch Small Traffic light state-of-the-art method. We focus on autonomous car deployment and show our network is more suitable than others because of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/ YmogPzBXOw.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recent progress in deep learning has led to a proliferation of deep learning networks on object detection dataset leaderboards. These leaderboards, however, often do not account for the graphics processing unit (GPU) memory and power required, nor the run-time speed of these networks. These considerations are important in autonomous driving; cost and power limitations make it unsustainable to have multiple onboard GPUs to support deep neural networks, and real-time detection speeds are essential for safety. Ideally, a single network would perform multiple tasks to preserve GPU memory and power.</p><p>Unfortunately, in the domain of traffic light and sign detection, networks often only detect traffic lights or signs because no public dataset includes both traffic light and sign labels. The COCO dataset <ref type="bibr" target="#b0">[1]</ref> comes close with labelled traffic lights and signs, but it does not differentiate traffic light states and only labels stop signs. As such, to have stateof-the-art traffic light and sign detection on an autonomous car, currently two separate networks are needed. This work takes a step towards joint traffic light and sign detection with the motivation of minimizing GPU memory and power required for both tasks.</p><p>Since no public dataset contains the labels for both traffic lights and signs, it is logical to try to train on a combination of datasets. One difficulty in combining datasets is that the network is required to approximate a more complex function, so lower performance is expected than if individual networks were used. To minimize this loss of performance we propose an implicit, hierarchical neural network architecture that exploits the characteristic that traffic lights only differ by their state and that traffic signs share similar features. The network determines the global class of a proposal, which we define as either traffic light, sign or background. The network also determines the subclass of each proposal, but only proposals that correctly identify the global class are considered in the loss function.</p><p>Another difficulty in combining datasets is overlapping; both datasets contain unlabeled instances of the object of interest from the other dataset. Overlapping will confuse the network during training when it correctly classifies the unlabeled instances. For instance, a network could identify a traffic light in the traffic sign dataset, but since it is unlabelled it would be penalized for the correct detection. To overcome the described issue we propose a mechanism referred to as the background threshold. The background threshold alters the standard region proposal network (RPN) <ref type="bibr" target="#b1">[2]</ref> mini-batch process by requiring proposals labeled as the background class to have a minuscule overlap with a ground truth box. This method reduces the likelihood of an unlabeled object of interest considered as the background because it exploits the characteristic that traffic signs and lights are naturally separated in physical space.</p><p>The contributions of this paper are as follows:</p><p>• We present an implicit, hierarchical approach to traffic light and sign detection that classifies objects according to their global categories first and subclasses second. • We present a novel approach to train on two overlapping datasets. Our approach does not add any new parameters and reduces the probability of confusing the background class with unlabeled objects of interest from the datasets. <ref type="bibr">•</ref> We are the first to present a network that performs joint traffic light and sign detection. Our architecture is suitable for autonomous car deployment because it saves GPU memory by solving two tasks and has realtime detection speeds. We test our single network on the Bosch Small Traffic Light dataset <ref type="bibr" target="#b2">[3]</ref> and the Tsinghua-Tencent 100K Traffic Sign dataset <ref type="bibr" target="#b3">[4]</ref> and show it outperforms the existing Bosch dataset state-of-the-art. The rest of the paper is structured as follows. Section II provides an overview of the state-of-the-art in traffic light and sign detection. Section III provides the problem formulation and our proposed solutions. Section IV discusses the results on the two test datasets. Lastly, we conclude the paper with Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Traffic Sign and Traffic Light Datasets</head><p>The existing public traffic light datasets include the LaRA dataset <ref type="bibr" target="#b4">[5]</ref>, LISA dataset <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and the Bosch dataset. The LaRA dataset has low resolution images and a temporal matching evaluation procedure that makes it inadequate for training our deep architecture. There are also several flaws in the LISA dataset. Lee et al. <ref type="bibr" target="#b7">[8]</ref> noted annotations for small traffic lights are missing. Furthermore, there is inconsistency in which images and classes are used for evaluation, which makes it difficult to have comparisons; <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> only evaluate using green and red lights and <ref type="bibr" target="#b7">[8]</ref> uses part of the designated training set as their test set. The recently proposed Bosch dataset was chosen for training and evaluation because the small traffic lights it includes makes it challenging and the evaluation procedure is clear because of their available test set.</p><p>There are many European traffic sign datasets. The most common is the German Traffic Sign Detection Benchmark (GTSDB) <ref type="bibr" target="#b9">[10]</ref> and the German Traffic Sign Recognition Benchmark (GTSRB) <ref type="bibr" target="#b10">[11]</ref>, which test object detection and classification, respectively. GTSRB is not representative of a real driving scenario as it contains crops of traffic signs, whereas real traffic signs are only a small component of the image in road scenes. Moreover, the GTSDB only contains 4 classes allowing multiple unpublished methods to achieve 100% accuracy and precision. We have chosen to use the Tsinghua-Tencent 100K dataset because it is significantly more challenging; there are 45 classes of signs, and the images are not cropped to include only the traffic sign extent.</p><p>Moreover, the recent attempts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> on this dataset and standard evaluation procedure allows our work to be compared to the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Traffic Light Detectors</head><p>There are two main categories for traffic light detection algorithms: image processing based models and learning based models. Driven by intuition, initial traffic light detection approaches often used image processing based models, which rely on traffic light shape and color as cues for detection. Franke et al. <ref type="bibr" target="#b13">[14]</ref> and Lindner et al. <ref type="bibr" target="#b14">[15]</ref> classify the color of each pixel and then use connected component analysis for segmentation to determine regions of interest. Other image processing approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> often normalize the color space of the image and then identify and group pixels that exceed specified thresholds. These groups are then further processed by imposing shape constraints to find the traffic lights. In addition, in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, prior maps were used to minimize false positives by providing traffic light location and state information.</p><p>Image processing based methods have several advantages. These methods do not require training data or suffer from overfitting, and they tend to work well when tailored for specific scenarios. Due to the rigidness of these deterministic methods, however, they are prone to break under slight variability. As an example, algorithms tailored to handle round traffic lights struggle when the traffic light states include arrows. Moreover, methods that rely on prior maps are limited in areas without mapping information.</p><p>Learning based models can overcome many of the limitations incurred by image processing based models as they can be trained on a broad set of traffic lights that include arrows and lights in various environments. Learning based models have also been proven to outperform image processing based models in traffic light detection. In <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b5">[6]</ref>, ACF detectors outperformed several image processing based detectors on the LISA dataset. Despite the advantages of learning based traffic light detectors, their usage is relatively new so only a few other methods have been implemented. In <ref type="bibr" target="#b2">[3]</ref>, YOLO <ref type="bibr" target="#b19">[20]</ref> is used to detect traffic lights and a separate convolutional neural network (CNN) classifies the traffic light states. In <ref type="bibr" target="#b20">[21]</ref>, YOLO2 <ref type="bibr" target="#b21">[22]</ref> is used on the LISA dataset. We introduce a modified Faster R-CNN <ref type="bibr" target="#b1">[2]</ref> architecture that outperforms the current state-of-the-art on the Bosch dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Traffic Sign Detectors</head><p>Traffic sign and traffic light detection research have followed a similar trend. Classical traffic sign detection methods rely on color and shape information <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Learning based methods became prevalent after the emergence of AlexNet <ref type="bibr" target="#b25">[26]</ref> and other deep learning networks. Using learning based methods is logical for traffic sign detection because signs have variability in color and shape, making it difficult to create hand-crafted features that generalize well for all signs. In addition, similarly in traffic light detection, learning based methods provide an opportunity to solve illumination changes and varying viewpoints. Examples of deep learning traffic sign detection approaches include multi-scale CNN <ref type="bibr" target="#b26">[27]</ref>, Support Vector Machines (SVMs) to classify into global classes coupled with a CNN to further classify to finer categorization <ref type="bibr" target="#b27">[28]</ref>, a CNN with diluted convolutions <ref type="bibr" target="#b28">[29]</ref>, and a CNN with a Generative Adversarial Network (GAN) that enhances small images <ref type="bibr" target="#b11">[12]</ref>. Our approach is the first to merge traffic sign and traffic light detection without a large loss in performance in either one of the global class categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hierarchical Object Detection</head><p>Using a hierarchical scheme to classify an object into a generic category then a specific category has been studied in works including <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. We extend these works by applying a hierarchy to joint traffic light and sign classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION AND PROPOSED SOLUTIONS</head><p>This section describes the problem formulation of 2D object detection, the difficulties induced by combining two datasets, and our proposed approaches to solve both of these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Object Detection</head><p>Problem Formulation: Let f be a function that maps a M × N image I to a set O, which represents all objects of interest in a dataset. Each object o i is indexed by i ∈ O and is parameterized by {x 1 , y 1 , x 2 , y 2 } ∈ R 4 , the coordinates of the top left and bottom right corners of the tightest bounding box around the object, and c ∈ K = {1, . . . , K} ⊆ Z a discrete integer variable that defines the class of an object from a choice of K classes. The problem is thus defined as:</p><formula xml:id="formula_0">Find f : R M ×N → (R 4 × K) I f (I) = O O : {o i = {x 1,i , x 2,i , y 1,i , y 2,i , c i }| ∀i ∈ O : 0 ≤ x 1 , x 2 ≤ M 0 ≤ y 1 , y 2 ≤ N x 1 &lt; x 2 y 1 &lt; y 2 c ∈ K}.<label>(1)</label></formula><p>Solving the generalized 2D object detection problem is out of the scope of this paper, so we restrict ourselves to K = 50 classes that describe traffic lights and signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Solution:</head><p>We tackle the problem of 2D object detection with a hierarchical deep architecture that jointly regresses the bounding box coordinates {x 1 , y 1 , x 2 , y 2 } and determines the class c. Our hierarchical approach, shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, is built upon the ResNet-50 <ref type="bibr" target="#b31">[32]</ref> version of Faster R-CNN. We use Faster R-CNN's RPN to generate j amodal region proposals. Each region proposal is parameterized by a feature vector h extracted from the average pool feature map via a crop and re-size operation. We model the function in Eq. 1 as the following:</p><formula xml:id="formula_1">f (I) = g(h) k(h) ,<label>(2)</label></formula><p>where g : R 2048 → R 4 and k : R 2048 → Z. The function g is modeled by the regression layer of Faster R-CNN and estimates the extent of a bounding box regardless of its class. The function k, on the other hand, is different from the classification layer used by Faster R-CNN and is modeled as a hierarchical classifier with two stages. The first stage classifies the global category of the region proposal as either traffic light, sign, or background. The second stage then classifies the subclass of the region proposal only if the first stage predicts a traffic light or sign. Traffic light subclasses include red, green, yellow, etc., while traffic sign subclasses include stop, yield, no parking, etc. We use a hierarchical architecture to exploit that similar features are shared between the subclasses of traffic lights and between the subclasses of traffic signs; traffic lights only differ by their state and traffic signs are generally constrained to specific shapes and colors.</p><p>Implementation Details: Instead of explicitly forcing hierarchy in the neural network layers, we realize the proposed hierarchical classifier by adding a global category classification layer parallel to the second stage classifier as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> and modify the classification network loss function. The original classification loss function of Faster R-CNN is as follows:</p><formula xml:id="formula_2">L(p i , t i ) = 1 N tot i L cls (p i , u i )+ λ 1 N + i p * i L loc (t i , v i ),<label>(3)</label></formula><p>where p i and t i are the classification and regression outputs, and u i and v i are the classification and regression ground truth, respectively. The variable p * i is an indicator of whether the region proposal has an intersection-over-union (IoU) greater than 0.5 with any ground truth bounding box. Proposals that satisfy this requirement are referred to as positive. The cross-entropy loss L cls is normalized by the total number of proposals N tot , while L loc is the smooth L1 loss normalized by the number of positive proposals N + . A factor λ is used to balance the two losses. We modify the original loss function to account for our hierarchical <ref type="figure">Figure 2</ref>: An image from the Bosch dataset <ref type="bibr" target="#b2">[3]</ref>. Ground truth is shown in green, while the positive and negative members of the mini-batch are shown in blue and red respectively. Left: The standard mini-batch selection process causes some negative proposals to contain the unlabeled object of interest. Right: Our mini-batch selection method minimizes negative selection of non-labeled objects by sampling near the labeled ground truth box. architecture as follows:</p><formula xml:id="formula_3">L(p i , t i ) = 1 N tot i L cls (p i , u i )+ 1 N † i p † i L cls (p i ,û i )+ λ 1 N + i p * i L loc (t i , v i ).<label>(4)</label></formula><p>wherep i andû i are subclass predictions and ground truth, respectively. In the first term, the classification cross-entropy loss is computed for all proposals based on the predicted global classes. Proposals are given a p † i value of 1 if their global class was classified correctly, or 0 if it was incorrect; therefore, proposals with an incorrect global class do not contribute to the subclass classification loss. The subclass classification loss is normalized by the total number of correctly identified global class proposals N † . The regression loss function remains the same. To compensate for the changed classification-to-regression loss ratio, λ is doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training On Two Overlapping Sets With Missing Labels</head><p>Problem Formulation: Like before, let O be the set of labels for all traffic lights and signs. Let L be the set of frames with labels for traffic lights only -a traffic light data set. Also, let S be the set of frames with labels for traffic signs only -a traffic sign dataset. If we take L ∪ S, we result in a dataset with unlabeled traffic lights in S and unlabeled traffic signs in L; in other words, L ∪ S O. <ref type="figure">Fig. 2</ref> shows how these missing labels creates issues for the mini-batch selection process for any two-stage object detector. The problem is thus the following: train a function approximator that estimates f (I) such that non-labeled objects of interest in the joint dataset do not degrade the estimation quality.</p><p>Proposed Solution: Our proposed solution leverages Faster R-CNN's mini-batch selection mechanism. Let B be the set of all proposals and G be the set of ground truth bounding boxes in a frame I. In the original Faster R-CNN mini-batch selection, proposals are separated into a positive set B + and a negative set B − for loss computation according to:</p><formula xml:id="formula_4">B + = {b i |0.7 ≤ IoU (b i , g i ) ≤ 1}, B − = {b i |IoU (b i , g i ) ≤ 0.3}, ∀b i ∈ B, g i ∈ G,<label>(5)</label></formula><p>where IoU is the intersection over union between b i , an element of B, and g i , an element of G.</p><p>To solve the overlapping issue we enforce a lower IoU bound of 0.01 for negative proposals, that we refer to as the background threshold. By requiring this minuscule overlap, we reduce the likelihood of an unlabelled object of interest considered as the background because it exploits </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Bosch  the characteristic that traffic signs and lights are naturally separated in physical space. A negative proposal is less likely to contain an unlabeled object if we sample close to the ground truth box of the labeled object. Although the validity of this prior is dependent on the dataset, we verified that only ∼7% of images in the Tsinghua-Tencent dataset violate this assumption, and we expect this result to be similar for other traffic sign and light datasets. Our mini-batches are selected according to:</p><formula xml:id="formula_5">B + = {b i |0.7 ≤ IoU (b i , g i ) ≤ 1}, B − = {b i |0.01 ≤ IoU (b i , g i ) ≤ 0.3}, ∀b i ∈ B, g i ∈ G,<label>(6)</label></formula><p>which introduces no additional computation overhead and allows training on two overlapping training sets with missing target class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluate our proposed solutions described in Section III by performing experiments on the Bosch and Tsinghua-Tencent datasets. For training, we reduce the Tsinghua-Tencent dataset to 45 classes as done in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. We similarly limit the Bosch training dataset to 5 classes to avoid training on sparse classes. All of our novel models were trained on both the Bosch and Tsinghua-Tencent dataset and their results are shown in <ref type="table" target="#tab_1">Table I</ref>, and sample detections are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation and Evaluation Procedure</head><p>Implementation: Baseline networks used the Tensorflow <ref type="bibr" target="#b34">[35]</ref> implementation of Faster R-CNN with ResNet-50, and all models were trained with stochastic gradient descent with momentum of 0.9 on a NVIDIA GeForce GTX 1080 Ti GPU. Following <ref type="bibr" target="#b35">[36]</ref>, the maximum number of proposals generated by the RPN was set to 50. Other hyperparameters were unmodified from their default value. On average all models process one image in 0.015 seconds.</p><p>Bosch Evaluation Procedure: Following the procedure of <ref type="bibr" target="#b2">[3]</ref>, precision-recall curves were used. We required our detections to have an IoU ≥ 0.5 with the ground truth boxes. We also compute mean average precision (mAP) which will allow for better future comparisons; <ref type="bibr" target="#b2">[3]</ref> only provides precision-recall curves which makes it difficult to  <ref type="bibr" target="#b32">[33]</ref>. This model was trained on images from San Diego, United States (LISA Sign <ref type="bibr" target="#b33">[34]</ref> and LISA Light datasets <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>). This shows the model was able to generalize well to images from another data source and city. make quantitative comparisons.</p><p>Tsinghua-Tencent Evaluation Procedure: Following <ref type="bibr" target="#b3">[4]</ref>, we evaluate our approaches on accuracy and recall metrics on small (area &lt; 32 2 pixels), medium (32 2 &lt; area &lt; 96 2 ) and large (area &gt; 96 2 ) objects used in the Microsoft COCO benchmark. Although better suited metrics exist for object detection such as mAP, average recall is sufficient for comparison due to the respective correlation with detection performance <ref type="bibr" target="#b36">[37]</ref>. With a minimum IoU threshold of ≥ 0.5, we evaluate over all classes to determine the final model performances with respect to the Tsinghua-Tencent test set. <ref type="table" target="#tab_1">Table I</ref> shows that the baseline network trained on the Bosch dataset outperformed the model in <ref type="bibr" target="#b2">[3]</ref>. Behrendt et al. <ref type="bibr" target="#b2">[3]</ref> provide a precision-recall curve created from their network that detects a generic traffic light class, and they provide a classification accuracy from their network that classifies the generic traffic light detections. We generously estimate their mAP as 0.40 by measuring the area under their precision-recall curve and assuming a classification accuracy of 100%. Our baseline model's mAP is 0.54. We mostly attribute the difference with YOLO's localization weakness; YOLO "uses relatively coarse features for predicting bounding boxes" <ref type="bibr" target="#b19">[20]</ref>, which could affect its ability in detecting small objects. In addition, all of our novel models outperformed <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison to Bosch Dataset State-of-the-Art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to Tsinghua-Tencent Dataset State-of-the-Art</head><p>In <ref type="table" target="#tab_1">Table II</ref>, we provide a comparison with our best model trained on the Tsinghua-Tencent dataset and the state-of-theart. Our model's low accuracy and recall can be attributed to the architecture's design for real-time performance; the motivation for our research is to implement a detection network on an autonomous vehicle, so we use a lightweight feature extractor and low region proposal count. The methods we compare to do not make the same prioritization and thus do not achieve real-time performance. Li et al. <ref type="bibr" target="#b11">[12]</ref>    <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>In this section we validate the usage of the hierarchical architecture and background threshold. As shown in <ref type="table" target="#tab_1">Table I</ref>, training a network on both the Bosch and Tsinghua-Tencent dataset resulted in an average performance loss of 27% compared to training a network on only one of the datasets. This result confirms our expectation in Section I that combining datasets affects performance because it requires the network to approximate a more complex function. We show that this loss is reduced to only 18% when using the hierarchical architecture and background threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Hierarchical Architecture:</head><p>We hypothesize the increase in performance from the hierarchical architecture is due to the new loss function being more effective. It is easier to detect the global class of traffic light or traffic sign than to detect the subclasses because there are more training samples of the global class. With a substantial number of global class examples, the network can be trained more efficiently by first detecting the global class then categorizing the detection into local classes.</p><p>Effectiveness of Background Threshold: The background threshold also increases the network performance compared to naively training on the combined dataset. This result is expected because the threshold reduces the probability of the RPN selecting non-labelled target objects as the background. There was a concern that requiring the background threshold on a negative image would result in a poor representation of the background, but the increase in performance diminishes this concern.</p><p>Effectiveness of Hierarchical Architecture and Background Threshold: The best configuration was the network with both the hierarchical architecture and background threshold. The background threshold and hierarchical architecture benefits are additive as they affect separate parts of the network. We also use this model on a video <ref type="bibr" target="#b32">[33]</ref> found online and show our model is able to generalize well to traffic lights and signs from a different data source here. Additional qualitative results are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>V. CONCLUSIONS It is not common for object detection systems to detect both traffic lights and signs as there are no datasets that contain both traffic light and sign labels, and combining overlapping datasets leads to unlabeled objects of interest. In this paper we bridge this gap by proposing a novel deep hierarchical architecture with a mini-batch selection mechanism that allows our network to train on a combined traffic light and sign dataset. While our network does not achieve the accuracy and recall of certain networks in the literature, it is more suitable for autonomous car deployment. We achieve real-time performance and can perform traffic light and traffic sign detection with one network, thus lowering GPU memory requirements. Further performance could be achieved with more training examples, which could be done through data augmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The hierarchical architecture used to jointly detect traffic signs and lights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Results from the Hierarchical + Threshold model on the Bosch dataset (left) and on the Tsinghua-Tencent dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results of the Hierarchical + Threshold model on images from Los Angeles, United States</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I :</head><label>I</label><figDesc>Comparison of the performance of our baselines and novel architectures on the Bosch and Tsinghua-Tencent datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>has a slow detection time of 0.6 seconds per frame excluding proposal</figDesc><table><row><cell></cell><cell cols="2">Small Medium</cell><cell>Large</cell><cell>Overall</cell></row><row><cell>Zhu [4] (A)</cell><cell>82</cell><cell>91</cell><cell>91</cell><cell>88</cell></row><row><cell>Zhu [4] (R)</cell><cell>87</cell><cell>94</cell><cell>88</cell><cell>90</cell></row><row><cell>Li [12] (A)</cell><cell>84</cell><cell>91</cell><cell>91</cell><cell>89</cell></row><row><cell>Li [12] (R)</cell><cell>89</cell><cell>96</cell><cell>89</cell><cell>91</cell></row><row><cell>Meng [13] (A)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90</cell></row><row><cell>Meng [13] (R)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93</cell></row><row><cell>Ours (A)</cell><cell>65</cell><cell>67</cell><cell>75</cell><cell>68</cell></row><row><cell>Ours (R)</cell><cell>24</cell><cell>54</cell><cell>70</cell><cell>44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II</head><label>II</label><figDesc>] uses the computationally intensive OverFeat<ref type="bibr" target="#b37">[38]</ref> framework. Our model is able to perform inference at 0.015 seconds per image, a 40X speedup over</figDesc><table><row><cell>: Accuracy (A) and recall (R) comparison with the</cell></row><row><cell>Tsinghua-Tencent state-of-the-art detectors. [13] only cites</cell></row><row><cell>an overall accuracy and recall.</cell></row><row><cell>time because of their generative model, and [13] and [4]</cell></row><row><cell>do not provide inference time, but both state speed as a</cell></row><row><cell>needed improvement for their models. Meng et al. [13] uses</cell></row><row><cell>an expensive image pyramid and sliding window approach</cell></row><row><cell>and [4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Deep Learning Approach to Traffic Lights: Detection, Tracking, and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libor</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Botros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Traffic-sign detection and classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robotics Centre of Mines ParisTech</title>
	</analytic>
	<monogr>
		<title level="m">Traffic Lights Recognition (TLR) public benchmarks</title>
		<imprint>
			<date type="published" when="2015-12-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Traffic light detection: A learning algorithm and evaluations on challenging dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mgelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 18th International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision for Looking at Traffic Lights: Issues, Survey, and Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Borno Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Philip Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mogelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Baltzer Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Traffic light recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Consumer Electronics (ICCE)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="277" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traffic Light Recognition for Complex Scene With Fusion Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Supplement C. Selected Papers from IJCNN 2011. 2</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Perceptual generative adversarial networks for small object detection. CoRR, abs/1706.05274</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Detecting small signs from large images. CoRR, abs/1706.08574</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Autonomous Driving approaches Downtown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Görzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wöhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust recognition of traffic signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaelberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic light detection with color and edge information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masako</forename><surname>Omachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichiro</forename><surname>Omachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd IEEE International Conference on Computer Science and Information Technology</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="284" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A robust video based traffic light detection algorithm for intelligent vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umit</forename><surname>Ozguner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Redmill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Traffic light mapping and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Fairfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Urmson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5421" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Traffic Light Detection at Night: Comparison of a Learning-Based Detector and Three Model-Based Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bahnsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Møgelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="774" to="783" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating state-of-the-art object detector on challenging traffic light data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="882" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Road traffic sign detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De La Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Salichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="848" to="859" />
			<date type="published" when="1997-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time color segmentation of road signs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Benallal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCECE 2003 -Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1823" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time traffic sign recognition from video by class-specific discriminative features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Neural Information Processing Systems</title>
		<meeting>the 25th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>NIPS&apos;12</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards real-time traffic sign detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2022" to="2031" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical approach for detection and classification of traffic signs using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elnaz</forename><forename type="middle">Jahani</forename><surname>Hamed Habibi Aghdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenec</forename><surname>Heravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fine-grained categorization for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojan</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="36" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative sub-categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1666" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Driving downtown -la&apos;s santa monica street 4k -los angeles usa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Utah</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=DcOj1FogG1U&amp;t=1294s" />
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visionbased traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mogelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1484" to="1497" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<idno>abs/1603.04467</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/1611.10012</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1502.05082</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
	</analytic>
	<monogr>
		<title level="m">Michaël Mathieu, Rob Fergus, and Yann LeCun</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
