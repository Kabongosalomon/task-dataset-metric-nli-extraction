<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Siam R-CNN: Visual Tracking by Re-Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.dephst@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Siam R-CNN: Visual Tracking by Re-Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www. vision.rwth-aachen.de/page/siamrcnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We approach Visual Object Tracking using the paradigm of Tracking by Re-Detection. We present a powerful novel re-detector, Siam R-CNN, an adaptation of Faster R-CNN <ref type="bibr" target="#b72">[73]</ref> with a Siamese architecture, which re-detects a template object anywhere in an image by determining if a region proposal is the same object as a template region, and regressing the bounding box for this object. Siam R-CNN is robust against changes in object size and aspect ratio as the proposals are aligned to the same size, which is in contrast to the popular cross-correlation-based methods <ref type="bibr" target="#b48">[49]</ref>.</p><p>Tracking by re-detection has a long history, reaching back to the seminal work of Avidan <ref type="bibr" target="#b0">[1]</ref> and Grabner et al. <ref type="bibr" target="#b27">[28]</ref>. Re-detection is challenging due to the existence of distractor objects that are very similar to the template object. In the past, the problem of distractors has mainly been approached by strong spatial priors from previous predictions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>, or by online adaptation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b41">42]</ref>. Both of these strategies are prone to drift.</p><p>We instead approach the problem of distractors by making two novel contributions beyond our Siam R-CNN re- † Work performed both while at the RWTH Aachen University and on a research visit at the University of Oxford. detector design. Firstly we introduce a novel hard example mining procedure which trains our re-detector specifically for difficult distractors. Secondly we propose a novel Tracklet Dynamic Programming Algorithm (TDPA) which simultaneously tracks all potential objects, including distractor objects, by re-detecting all object candidate boxes from the previous frame, and grouping boxes over time into tracklets (short object tracks). It then uses dynamic programming to select the best object in the current timestep based on the complete history of all target object and distractor object tracklets. By explicitly modeling the motion and interaction of all potential objects and pooling similarity information from detections grouped into tracklets, Siam R-CNN is able to effectively perform long-term tracking, while being resistant to tracker drift, and being able to immediately re-detect objects after disappearance. Our TDPA requires only a small set of new re-detections in each timestep, updating its tracking history iteratively online. This allows Siam R-CNN to run at 4.7 frames per second (FPS) and its speed-optimized variant to run at more than 15 FPS.</p><p>We present evaluation results on a large number of datasets. Siam R-CNN outperforms all previous methods on six short-term tracking benchmarks as well as on four long-term tracking benchmarks, where it achieves especially strong results, up to 10 percentage points higher than previous methods. By obtaining segmentation masks using an off-the-shelf box-to-segmentation network, Siam R-CNN also outperforms all previous Video Object Segmentation methods that only use the first-frame bounding box (without the mask) on four recent VOS benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Object Tracking (VOT). VOT is the task of tracking an object through a video given the first-frame bounding box of the object. VOT is commonly evaluated on benchmarks such as OTB <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98]</ref>, VOT <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b44">45]</ref>, and many more <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b42">43]</ref>. Recently a number of long-term tracking benchmarks have been proposed <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b22">23]</ref> which extend VOT to a more difficult and realistic setting, where objects must be tracked over many frames, with objects disappearing and reappearing.</p><p>Many classical methods use an online learned classifier to re-detect the object of interest over the full image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b41">42]</ref>. In contrast, Siam R-CNN learns the expected appearance variations by offline training instead of learning a classifier online.</p><p>Like our Siam R-CNN, many recent methods approach VOT using Siamese architectures. Siamese region proposal networks (SiamRPN <ref type="bibr" target="#b48">[49]</ref>) use a single-stage RPN <ref type="bibr" target="#b72">[73]</ref> detector adapted to re-detect a template by cross-correlating the deep template features with the deep features of the current frame. Here, single-stage means directly classifying anchor boxes <ref type="bibr" target="#b56">[57]</ref> which is in contrast to two-stage architectures <ref type="bibr" target="#b72">[73]</ref> which first generate proposals, and then align their features and classify them in the second stage.</p><p>Recent tracking approaches improve upon SiamRPN, making it distractor aware (DaSiamRPN <ref type="bibr" target="#b116">[117]</ref>), adding a cascade (C-RPN <ref type="bibr" target="#b24">[25]</ref>), producing masks (SiamMask <ref type="bibr" target="#b92">[93]</ref>), using deeper architectures (SiamRPN+ <ref type="bibr" target="#b112">[113]</ref> and SiamRPN++ <ref type="bibr" target="#b47">[48]</ref>) and maintaining a set of diverse templates (THOR <ref type="bibr" target="#b76">[77]</ref>). These (and many more <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b61">62]</ref>) only search for the object within a small window of the previous prediction. DiMP <ref type="bibr" target="#b4">[5]</ref> follows this paradigm while meta-learning a robust target and background appearance model.</p><p>Other recent developments in VOT include using domain specific layers with online learning <ref type="bibr" target="#b65">[66]</ref>, learning an adaptive spatial filter regularizer <ref type="bibr" target="#b16">[17]</ref>, exploiting categoryspecific semantic information <ref type="bibr" target="#b83">[84]</ref>, using continuous <ref type="bibr" target="#b19">[20]</ref> or factorized <ref type="bibr" target="#b17">[18]</ref> convolutions, and achieving accurate bounding box predictions using an overlap prediction network <ref type="bibr" target="#b18">[19]</ref>. Huang et al. <ref type="bibr" target="#b38">[39]</ref> propose a framework to convert any detector into a tracker. Like Siam R-CNN, they also apply two-stage architectures, but their method relies on metalearning and it achieves a much lower accuracy.</p><p>Long-term tracking is mainly addressed by enlarging the search window of these Siamese trackers when the detection confidence is low <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b47">48]</ref>. In contrast, we use a two-stage Siamese re-detector which searches over the whole image, producing stronger results across many benchmarks. Video Object Segmentation (VOS). VOS is an extension of VOT where a set of template segmentation masks are given, and segmentation masks need to be produced in each frame. Many methods perform fine-tuning on the template masks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b59">60]</ref>, which leads to strong results but is slow. Recently, several methods have used the first-frame masks without fine-tuning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b67">68]</ref>, running faster but often not performing as well.</p><p>Very few methods <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b106">107]</ref> tackle the harder problem of producing mask tracking results while only using the given template bounding box and not the mask. We adapt our method to perform VOS in this setting by using a second network to produce masks for our box tracking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Inspired by the success of Siamese trackers <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b46">47]</ref>, we use a Siamese architecture for our re-detector. Many recent trackers <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b4">5]</ref> adopt a single-stage detector architecture. For the task of single-image object detection, two-stage detector networks such as Faster R-CNN <ref type="bibr" target="#b72">[73]</ref> have been shown to outperform single-stage detectors. Inspired by this, we design our tracker as a Siamese twostage detection network. The second stage can directly compare a proposed Region of Interest (RoI) to a template region by concatenating their RoI aligned features. By aligning proposals and reference to the same size, Siam R-CNN achieves robustness against changes in object size and aspect ratio, which is hard to achieve when using the popular cross-correlation operation <ref type="bibr" target="#b48">[49]</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview of Siam R-CNN including the Tracklet Dynamic Programming Algorithm (TDPA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Siam R-CNN</head><p>Siam R-CNN is a Siamese re-detector based on a twostage detection architecture. Specifically, we take a Faster R-CNN network that has been pre-trained on the COCO <ref type="bibr" target="#b55">[56]</ref> dataset for detecting 80 object classes. This network consists of a backbone feature extractor followed by two detection stages; first a category-agnostic RPN, followed by a category-specific detection head. We fix the weights of the backbone and the RPN and replace the category-specific detection head with our re-detection head.</p><p>We create input features for the re-detection head for each region proposed by the RPN by performing RoI Align <ref type="bibr" target="#b32">[33]</ref> to extract deep features from this proposed region. We  also take the RoI Aligned deep features of the initialization bounding box in the first frame, and then concatenate these together and feed the combined features into a 1 × 1 convolution which reduces the number of features channels back down by half. These joined features are then fed into the re-detection head with two output classes; the proposed region is either the reference object or it is not. Our redetection head uses a three-stage cascade <ref type="bibr" target="#b8">[9]</ref> without shared weights. The structure of the re-detection head is the same as the structure of the detection head of Faster R-CNN, except for using only two classes and for the way the input features for the re-detection head are created by concatenation. The backbone and RPN are frozen and only the redetection head (after concatenation) is trained for tracking, using pairs of frames from video datasets. Here, an object in one frame is used as reference and the network is trained to re-detect the same object in another frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Hard Example Mining</head><p>During conventional Faster R-CNN training, the negative examples for the second stage are sampled from the regions proposed by the RPN in the target image. However, in many images there are only few relevant negative examples. In order to maximize the discriminative power of the re-detection head, we need to train it on hard negative examples. Mining hard examples for detection has been explored in previous works (e.g. <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b77">78]</ref>). However, rather than finding general hard examples for detection, we find hard examples for re-detection conditioned on the reference object by retrieving objects from other videos. Embedding Network. A straightforward approach to selecting relevant videos from which to get hard negative examples for the current video, is taking videos in which an object has the same class as the current object <ref type="bibr" target="#b116">[117]</ref>. However, object class labels are not always available, and some objects of the same class could be easy to distinguish, while some objects of different classes could also be potentially hard negatives. Hence, we propose to use an embedding network, inspired by person re-identification, which extracts an embedding vector for every ground truth bounding box which represents the appearance of that object. We use the network from PReMVOS <ref type="bibr" target="#b59">[60]</ref>, which is trained with batch-hard triplet loss <ref type="bibr" target="#b35">[36]</ref> to separate classes on COCO before being trained on YouTube-VOS to disambiguate between individual object instances. E.g., two distinct persons should be far away in the embedding space, while two crops of the same person in different frames should be close.</p><p>Index Structure. We next create an efficient indexing structure for approximate nearest neighbor queries (see supplemental material) and use it to find nearest neighbors of the tracked object in the embedding space. Training Procedure. Evaluating the backbone on-the-fly on other videos to retrieve hard negative examples for the current video frame would be very costly. Instead, we precompute the RoI-aligned features for every ground truth box of the training data. For each training step, as usual, a random video and object in this video is selected and then a random reference and a random target frame. Afterwards, we use the indexing structure to retrieve for the reference box the 10,000 nearest neighbor bounding boxes from other videos and sample 100 of them as additional negative training examples. More details about video hard example mining can be found in the supplemental material.</p><p>Algorithm 1 Update tracklets for one time-step t 1: Inputs ff gt feats, tracklets, imaget, detst−1 2: backbone feats ← backbone(image t ) 3: RoIs ← RPN(backbone feats) ∪ detst−1 4: detst ← redetection head(RoIs, ff gt feats) <ref type="bibr">5:</ref> scores are set to −∞ if spatial distance is &gt; γ 6: scores ← score pairwise redetection(detst, detst−1, γ) 7: for dt ∈ detst do 8:</p><formula xml:id="formula_0">s1 ← max d t−1 ∈dets t−1 scores[dt, dt−1] 9:dt−1 ← argmax d t−1 ∈dets t−1 scores[dt, dt−1] 10:</formula><p>Max score of all other current detections <ref type="bibr">11:</ref> s2 ← maxd t ∈dets t \{d t } scores[dt,dt−1]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Max score of all other previous detections <ref type="bibr" target="#b12">13</ref>: </p><formula xml:id="formula_1">s3 ← max d t−1 ∈dets t−1 \{d t−1 } scores[dt, dt−1] 14: if s1 &gt; α ∧ s2 ≤ s1 − β ∧ s3 ≤ s1 − β</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Tracklet Dynamic Programming Algorithm</head><p>Our Tracklet Dynamic Programming Algorithm (TDPA) implicitly tracks both the object of interest and potential similar-looking distractors using spatio-temporal cues. In this way, distractors can be consistently suppressed, which would not be possible using only visual similarity. To this end, TDPA maintains a set of tracklets, i.e., short sequences of detections which almost certainly belong to the same object. It then uses a dynamic programming based scoring algorithm to select the most likely sequence of tracklets for the template object between the first and the current frame.</p><p>Each detection is part of exactly one tracklet and it is defined by a bounding box, a re-detection score, and its RoIaligned features. A tracklet is defined by a set of detections, exactly one for each time step from its start to its end time. Tracklet Building. We extract the RoI aligned features for the first-frame ground truth bounding box (ff gt feats) and initialize a tracklet consisting of just this box. For each new frame, we update the set of tracklets as follows (c.f . Algorithm 1): We extract backbone features of the current frame and evaluate the region proposal network (RPN) to get regions of interest (RoIs, lines 2-3). To compensate for potential RPN false negatives, the set of RoIs is extended by the bounding box outputs from the previous frame. We run the re-detection head (including bounding box regression) on these RoIs to produce a set of re-detections of the first-frame template (line 4). Afterwards, we re-run the classification part of the re-detection head (line 6) on the current detections dets t , but this time with the detections dets t−1 from the previous frame as reference instead of the first-frame ground truth box, to calculate similarity scores (scores) between each pair of detections.</p><p>To measure the spatial distance of two detections, we represent their bounding boxes by their center coordinates x and y, and their width w and height h, of which x and w are normalized with the image width, and y and h are normalized with the image height, so that all values are between 0 and 1. The spatial distance between two bounding boxes (x 1 , y 1 , w 1 , h 1 ) and (x 2 , y 2 , w 2 , h 2 ) is then given by the L ∞ norm, i.e., max(|x 1 − x 2 |, |y 1 − y 2 |, |w 1 − w 2 |, |h 1 − h 2 |). In order to save computation and to avoid false matches, we calculate the pairwise similarity scores only for pairs of detections where this spatial distance is less than γ and set the similarity score to −∞ otherwise. We extend the tracklets from the previous frame by the current frame detections (lines 7-19) when the similarity score to a new detection is high (&gt;α) and there is no ambiguity, i.e., there is no other detection which has an almost as high similarity (less than β margin) with that tracklet, and there is no other tracklet which has an almost as high similarity (less than β margin) with that detection. Whenever there is any ambiguity, we start a new tracklet which initially consists of a single detection. The ambiguities will then be resolved in the tracklet scoring step. Scoring. A track A = (a 1 , . . . , a N ) is a sequence of N non-overlapping tracklets, i.e., end(a i ) &lt; start(a i+1 ) ∀i ∈ {1, . . . , N − 1}, where start and end denote the start and end times of a tracklet, respectively. The total score of a track consists of a unary score measuring the quality of the individual tracklets, and of a location score which penalizes spatial jumps between tracklets, i.e.</p><formula xml:id="formula_2">score(A) = N i=1 unary(a i ) + N −1 i=1</formula><p>w loc loc score(a i , a i+1 ). </p><formula xml:id="formula_3">+ (1 − w ff )ff tracklet score(a i,t ),<label>(2)</label></formula><p>where ff score denotes the re-detection confidence for the detection a i,t of tracklet a i at time t from the re-detection head using the first-frame ground truth bounding box as reference. There is always one tracklet which contains the first-frame ground truth bounding box, which we denote as the first-frame tracklet a ff . All detections in a tracklet have a very high chance of being a correct continuation of the initial detection of this tracklet, because in cases of ambiguities tracklets are terminated. Hence, the most recent detection of the first-frame tracklet is also the most recent observation that is almost certain to be the correct object. Thus, we use this detection as an additional reference for re-detection producing a score denoted by ff tracklet score which is linearly combined with the ff score.</p><p>The location score between two tracklets a i and a j is given by the negative L 1 norm of the difference between   <ref type="figure">Figure 4</ref>: Results on OTB2015 <ref type="bibr" target="#b97">[98]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success plots of OPE</head><p>the bounding box (x, y, w, h) of the last detection of a i and the bounding box of the first detection of a j , i.e. loc score(a i , a j ) = −|end bbox(a i ) − start bbox(a j )| 1 .</p><p>Online Dynamic Programming. We efficiently find the sequence of tracklets with the maximum total score (Eq. 1) by maintaining an array θ which for each tracklet a stores the total score θ[a] of the optimal sequence of tracklets which starts with the first-frame tracklet and ends with a. Once a tracklet is not extended, it is terminated. Thus, for each new frame only the scores for tracklets which have been extended or newly created need to be newly computed. For a new time-step, first we set θ[a ff ] = 0 for the firstframe tracklet a ff , since all tracks have to start with that tracklet. Afterwards, for every tracklet a which has been updated or newly created, θ[a] is calculated as</p><formula xml:id="formula_4">θ[a] = unary(a) + max a:end(ã)&lt;start(a) θ[ã] + w loc loc score(ã, a).</formula><p>To retain efficiency for very long sequences, we allow a maximum temporal gap between two tracklets of 1500 frames, which is long enough for most applications. After updating θ for the current frame, we select the trackletâ with the highest dynamic programming score, i.e. a = arg max a θ[a]. If the selected tracklet does not contain a detection in the current frame, then our algorithm has indicated that the object is not present. For benchmarks that require a prediction in every frame we use the most recent box from the selected tracklet, and assign it a score of 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Box2Seg</head><p>To produce segmentation masks for the VOS task, we use an off-the-shelf bounding-box-to-segmentation (Box2Seg) network from PReMVOS <ref type="bibr" target="#b59">[60]</ref>. Box2Seg is a fully convolutional DeepLabV3+ <ref type="bibr" target="#b10">[11]</ref> network with an Xception-65 <ref type="bibr" target="#b15">[16]</ref> backbone. It has been trained on Mapillary <ref type="bibr" target="#b66">[67]</ref> and COCO <ref type="bibr" target="#b55">[56]</ref> to output the mask for a bounding box crop. Box2Seg is fast, running it after tracking only requires 0.025 seconds per object per frame. We combine overlapping masks such that masks with less pixels are on top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Details</head><p>Siam R-CNN is built upon the Faster R-CNN <ref type="bibr" target="#b72">[73]</ref> implementation from <ref type="bibr" target="#b94">[95]</ref>, with a ResNet-101-FPN backbone <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b54">55]</ref>, group normalization <ref type="bibr" target="#b95">[96]</ref> and cascade <ref type="bibr" target="#b8">[9]</ref>. It has   <ref type="figure">Figure 5</ref>: Results on UAV123 <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success rate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Success plots of OPE</head><p>been pre-trained from scratch <ref type="bibr" target="#b31">[32]</ref> on COCO <ref type="bibr" target="#b55">[56]</ref>. Except where specified otherwise, we train Siam R-CNN on the training sets of multiple tracking datasets simultaneously: ImageNet VID <ref type="bibr" target="#b73">[74]</ref> (4000 videos), YouTube-VOS 2018 <ref type="bibr" target="#b99">[100]</ref> (3471 videos), GOT-10k <ref type="bibr" target="#b37">[38]</ref> (9335 videos) and</p><p>LaSOT <ref type="bibr" target="#b22">[23]</ref> (1120 videos). In total, we use 18k videos and 119k static images from COCO, which is a significant amount of data, but it is actually less than what previous methods used, e.g. SiamRPN++ uses 384k videos and 1867k static images. More details about the amount of training data are in the supplemental material. During training, we use motion blur <ref type="bibr" target="#b116">[117]</ref>, grayscale, gamma, flip, and scale augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate Siam R-CNN for standard visual object tracking, for long-term tracking, and on VOS benchmarks. We tune a single set of hyper-parameters for our Tracklet Dynamic Programming Algorithm (c.f . Section 3.3) on the DAVIS 2017 training set, as this is a training set that we did not use to train our re-detector. We present results using these hyper-parameters on all benchmarks, rather than tuning the parameters separately for each one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Short-Term Visual Object Tracking Evaluation</head><p>We evaluate short-term VOT on six benchmarks, and on five further benchmarks in the supplemental material. OTB2015. We evaluate on OTB2015 <ref type="bibr" target="#b97">[98]</ref> (100 videos, 590 frames average length), calculating the success and precision over varying overlap thresholds. Methods are ranked by the area under the success curve (AUC). <ref type="figure">Fig. 4</ref> compares our results to eight state-of-the-art (SOTA) trackers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b116">117]</ref>. Siam R-CNN achieves 70.1% AUC, which equals the previous best result by UPDT <ref type="bibr" target="#b5">[6]</ref>. UAV123. <ref type="figure">Fig. 5</ref> shows our results on UAV123 <ref type="bibr" target="#b63">[64]</ref> (123 videos, 915 frames average length) on the same metrics as OTB2015 compared to six SOTA approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18]</ref>. We achieve an AUC of 64.9%, which is close to the previous best result of DiMP-50 <ref type="bibr" target="#b4">[5]</ref> with 65.4%.</p><p>NfS. Tab. 1 shows our results on the NfS dataset <ref type="bibr" target="#b42">[43]</ref> (30FPS, 100 videos, 479 frames average length) compared to five SOTA approaches. Siam R-CNN achieves a success score of 63.9%, which is 1.9 percentage points higher than <ref type="bibr">Huang</ref>     <ref type="table">Table 3</ref>: Results on VOT2018 <ref type="bibr" target="#b44">[45]</ref>.</p><p>the previous best result by DiMP-50 <ref type="bibr" target="#b4">[5]</ref>.</p><p>TrackingNet. Tab. 2 shows our results on the TrackingNet test set <ref type="bibr" target="#b64">[65]</ref> (511 videos, 442 frames average length), compared to five SOTA approaches. Siam R-CNN achieves a success score of 81.2%, i.e., 7.2 percentage points higher than the previous best result of DiMP-50 <ref type="bibr" target="#b4">[5]</ref>. In terms of precision the gap is more than 10 percentage points. GOT-10k. <ref type="figure">Fig. 6</ref> shows our results on the GOT-10k <ref type="bibr" target="#b37">[38]</ref> test set (180 videos, 127 frames average length) compared to six SOTA approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b17">18]</ref>. On this benchmark, methods are only allowed to use the GOT-10k training set as video data for training. Therefore we train a new model starting from COCO pre-training, and train only on GOT-10k. We achieve a success rate of 64.9% which is 3.8 percentage points higher than the previous best result from DiMP-50 <ref type="bibr" target="#b4">[5]</ref>.  <ref type="figure">Figure 6</ref>: Results on GOT-10k <ref type="bibr" target="#b37">[38]</ref>.</p><p>the highest accuracy scores. The last row shows the average overlap (AO), when using the normal (non-reset) evaluation. When estimating rotated bounding boxes from segmentation masks produced by Box2Seg, Siam R-CNN's EAO increases to 0.423 and the accuracy greatly improves to 0.684. More details on rotated boxes and on the shortterm tracking algorithm are in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Long-Term Visual Object Tracking Evaluation</head><p>We evaluate Siam R-CNN's ability to perform long-term tracking on three benchmarks, LTB35 <ref type="bibr" target="#b60">[61]</ref>, LaSOT <ref type="bibr" target="#b22">[23]</ref> and OxUvA <ref type="bibr" target="#b84">[85]</ref>. In the supplemental material we also evaluate on UAV20L <ref type="bibr" target="#b63">[64]</ref>. In long-term tracking, sequences are much longer, and objects may disappear and reappear again (LTB35 has on average 12.4 disappearances per video, each one on average 40.6 frames long). Siam R-CNN significantly outperforms all previous methods on all of these benchmarks, indicating the strength of our tracking by redetection approach. By searching globally over the whole image rather than within a local window of a previous prediction, our method is more resistant to drift, and can easily re-detect a target after disappearance. LTB35. <ref type="figure" target="#fig_7">Fig. 7</ref> shows the results of our method on the LTB35 benchmark (also known as VOT18-LT) <ref type="bibr" target="#b60">[61]</ref> (35 videos, 4200 frames average length) compared to eight SOTA approaches. Trackers are required to output a confidence of the target being present for the prediction in each frame. Precision (Pr) and Recall (Re) are evaluated for a range of confidence thresholds, and the F -score is calculated as F = 2P rRe P r+Re . Trackers are ranked by the maximum F -score over all thresholds. We compare to the 6 bestperforming methods in the 2018 VOT-LT challenge <ref type="bibr" target="#b44">[45]</ref> and to SiamRPN++ <ref type="bibr" target="#b47">[48]</ref> and SPLT <ref type="bibr" target="#b102">[103]</ref>. Siam R-CNN outperforms all previous methods with an F -score of 66.8%, i.e., 3.9 percentage points higher than the previous best result.</p><p>LaSOT. <ref type="figure">Fig. 8</ref> shows results on the LaSOT test set <ref type="bibr" target="#b22">[23]</ref> (280 videos, 2448 frames average length) compared to nine SOTA methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>. Siam R-CNN achieves an unprecedented result with a success rate of 64.8% and 72.2% normalized precision. This is 8 percentage points higher in success and 7.4 points higher in normalized precision than the previous best method.</p><p>OxUvA. Tab. 4 shows results on the OxUvA test set <ref type="bibr" target="#b84">[85]</ref>    Ours (Fine-tuned) SiamMask <ref type="bibr" target="#b92">[93]</ref> PReMVOS [60] STM-VOS <ref type="bibr" target="#b67">[68]</ref> DyeNet <ref type="bibr" target="#b51">[52]</ref> FEELVOS <ref type="bibr" target="#b85">[86]</ref> OSVOS-S [63] RGMP <ref type="bibr" target="#b98">[99]</ref> CINM <ref type="bibr" target="#b2">[3]</ref> VideoMatch <ref type="bibr" target="#b36">[37]</ref> OnAVOS <ref type="bibr" target="#b87">[88]</ref> FAVOS <ref type="bibr" target="#b12">[13]</ref> OSVOS <ref type="bibr" target="#b7">[8]</ref>  <ref type="figure">Figure 9</ref>: Quality versus timing on DAVIS 2017 (validation set). Only SiamMask <ref type="bibr" target="#b92">[93]</ref> and our method (red) can work without the first-frame ground truth mask and require just the bounding box. Methods shown in blue fine-tune on the first-frame mask. Ours (fastest) denotes Siam R-CNN with ResNet-50, half resolution, and 100 RoIs, see Section 4.5.</p><p>(166 videos, 3293 frames average length) compared to five SOTA methods. Trackers must make a hard decision each frame whether the object is present. We do this by comparing the detector confidence to a threshold tuned on the dev set. Methods are ranked by the maximum geometric mean (MaxGM) of the true positive rate (TPR) and the true negative rate (TNR). Siam R-CNN achieves a MaxGM more than 10 percentage points higher than all previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Video Object Segmentation (VOS) Evaluation</head><p>We further evaluate the ability to track multiple objects and to segment them on VOS datasets using the J metric (mask intersection over union (IoU)), the F metric (mask boundary similarity), and the bounding box IoU J box .   <ref type="table">Table 6</ref>: Results on the YouTube-VOS 2018 <ref type="bibr" target="#b99">[100]</ref> validation set. The notation is explained in the caption of Tab. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS 2017.</head><p>Tab. 5 and <ref type="figure">Fig. 9</ref> show results on the DAVIS 2017 validation set (30 videos, 2.03 objects and 67.4 frames average length per video). Methods are ranked by the mean of J and F. Siam R-CNN significantly outperforms the previous best method that only uses the firstframe bounding boxes, SiamMask <ref type="bibr" target="#b92">[93]</ref>, by 14.8 percentage points. To evaluate how much of this improvement comes from Box2Seg and how much from our tracking, we applied Box2Seg to the output of SiamMask. This does improve the results while still being 7.3 percentage points worse than our method. We also run SiamRPN++ <ref type="bibr" target="#b47">[48]</ref> and DiMP-50 <ref type="bibr" target="#b4">[5]</ref> with Box2Seg for comparison. As a reference for the achievable performance for our tracker, we ran Box2Seg on the ground truth boxes which resulted in a score of 82.6%.</p><p>Even without using the first-frame mask, Siam R-CNN outperforms many methods that use the mask such as RGMP <ref type="bibr" target="#b98">[99]</ref> and VideoMatch <ref type="bibr" target="#b36">[37]</ref>, and even some methods like OSVOS-S [63] that perform slow first-frame finetuning. Our method is also more practical, as it is far more tedious to create a perfect first-frame segmentation mask by hand than a bounding box initialization. If the first-frame mask is available, then we are able to fine-tune Box2Seg on  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation and Timing Analysis</head><p>Tab. 7 shows a number of ablations of Siam R-CNN on three datasets together with their speed (using a V100 GPU). Siam R-CNN runs at 4.7 frames per second (FPS) using a ResNet-101 backbone, 1000 RPN proposals per frame, and TDPA. The row "No hard ex. mining" shows the results without hard example mining (c.f . Sec. 3.2). Hard example mining improves results on all datasets, by up to 1.7 percentage points. We compare TDPA to using just the highest scoring re-detection in each frame ("Argmax") and the short-term algorithm we used for the reset-based VOT18 evaluation ("Short-term"). TDPA outperforms both of these on all datasets. A per-attribute analysis of the influence of TDPA can be found in the supplemental material. For the long-term datasets, Argmax significantly outperforms both the short-term variant and even all previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Making Siam R-CNN Even Faster</head><p>Tab. 7 also shows the result of three changes aimed at increasing the speed of Siam R-CNN (smaller backbone, smaller input resolution, and less RoI proposals). More details and analyses are in the supplemental material.</p><p>When evaluating with a ResNet-50 backbone, Siam R-CNN performs slightly faster and still achieves SOTA results (62.3 on LaSOT, compared to 56.8 for DiMP-50 with the same backbone). This shows that the results are not only due to a larger backbone. When using half input resolution and only 100 RoIs from the RPN, the speed increases from 4.7 FPS to 13.6 FPS, or even 15.2 FPS in the case of ResNet-50. These setups still show very strong re- sults, especially for long-term tracking. Note that even the fastest variant is not real-time and our work focuses on accuracy achieving much better results, especially for longterm tracking, while still running at a reasonable speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Generic Object Tracking Analysis</head><p>Siam R-CNN should be able to track any generic object. However, its backbone and RPN have been trained only on 80 object classes in COCO and have then been frozen. In <ref type="figure" target="#fig_0">Fig. 10</ref>, we investigate the recall of our RPN on the 44% of OTB2015 sequences that contain objects not in COCO, compared to the rest. With the default of 1000 proposals, the RPN achieves only 69.1% recall for unknown objects, compared to 98.2% for known ones. One solution is to increase the number of proposals used. When using 10, 000 proposals the RPN achieves 98.7% recall for unknown objects but causes Siam R-CNN to run much slower (around 1 FPS). Our solution is to instead include the previous-frame re-detections (up to 100) as additional proposals. This increases the recall to 95.5% for unknown objects when using 1000 RPN proposals. This shows why Siam R-CNN is able to outperform all previous methods on OTB2015, even though almost half of the objects are not from COCO classes. We also run a recall analysis on the DAVIS 2017 and LTB35 datasets where most objects belong to COCO classes and we achieve excellent recall (see <ref type="figure" target="#fig_0">Fig. 10</ref> right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce Siam R-CNN as a Siamese two-stage fullimage re-detection architecture with a Tracklet Dynamic Programming Algorithm. Siam R-CNN outperforms all previous methods on ten tracking benchmarks, with especially strong results for long-term tracking. We hope that our work will inspire future work on using two-stage architectures and full-image re-detection for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material for Siam R-CNN: Visual Tracking by Re-Detection Abstract</head><p>We provide more details of Siam RCNN's training procedure, a more detailed description of the video hard example mining procedure, of the short-term tracking algorithm, and of the estimation of rotated bounding boxes for VOT2018. Additionally, we explain and analyze different methods to speed up Siam R-CNN in more detail and we analyze the amount of training data used by previous methods compared to our method. Moreover, we conduct a perattribute analysis of variants of Siam R-CNN on OTB2015 and also analyze the effect of fine-tuning Box2Seg. Finally, we present additional quantitative and qualitative results for short-term tracking, long-term tracking and video object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further Method Details</head><p>In the following, we provide further details on the training procedure, video hard example mining, the short-termtracking algorithm, and on rotated bounding box estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training</head><p>We train Siam R-CNN with random image scale sampling by scaling the small edge of the image to a random value between 640 and 800 pixels, while keeping the aspect ratio. In cases where this resizing would result in a larger edge size of more than 1333 pixels, it is resized to a larger edge size of 1333 pixels instead. During test time we resize the image to a smaller edge length of 800 pixels, again keeping the longer edge size no larger than 1333 pixels. Note that these settings are the default of the Mask R-CNN implementation we used.</p><p>We train our network with two NVIDIA GTX 1080 Ti GPUs for 1 million steps (5.8 days) with a learning rate of 0.01, and afterwards for 120,000 steps and 80,000 steps with learning rates of 0.001 and 0.0001, respectively. The hard example training is done afterwards with a single GPU for 160,000 steps and a learning rate of 0.001. A batch size of one pair of images (reference and target) per GPU is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Video Hard Example Mining</head><p>Index Structure. For the indexing structure, we use the "Approximate Nearest Neighbors Oh Yeah" library for approximate nearest neighbor queries 1 .</p><p>Feature Pre-computation. During normal training without hard negative examples, the RoIs given to the second stage are generated automatically by the RPN and are thus not always perfectly aligned to the object boundaries. In the three cascade stages, the RoI will then be successively refined by bounding box regression. However, when naively pre-computing the features for the ground truth bounding boxes, the network might overfit to these perfect boxes.</p><p>In order for the network to learn to handle imperfect bounding boxes, we add random Gaussian noise to the ground truth bounding boxes before pre-computing the features, and afterwards run these jittered RoIs through the cascade stages and also pre-compute the re-aligned features after every cascade stage.</p><p>In particular, we take the ground truth bounding box (x 0 , y 0 , x 1 , y 1 ) and independently add to each component noise sampled from a clipped Gaussian with mean 0 and standard deviation 0.25, i.e., for i ∈ {0, 1, 2, 3}, we havẽ</p><formula xml:id="formula_5">r i ∼ N (0, 0.25),<label>(3)</label></formula><formula xml:id="formula_6">r i = clip(r i , −0.25, 0.25).<label>(4)</label></formula><p>The jittered box is then given by (x 0 + r 0 , y 0 + r 1 , x 1 + r 2 , y 1 + r 3 ).</p><p>Training Procedure.</p><p>Having pre-computed the RoIaligned features for each object and each cascade stage, the training procedure now works as follows. For each training step, as usual, a random video and object in this video is selected and then a random reference and a random target frame. Afterwards, we use the indexing structure to retrieve the 10,000 nearest neighbor bounding boxes from other videos (50,000 boxes for LaSOT because of the long sequences). Note that the nearest neighbors are searched for over all training datasets, regardless where the reference comes from. Since the nearest neighbors are found per frame, often a few videos will dominate the set of nearest neighbors. To get more diverse negative examples, we create a list of all videos (excluding the reference) in which nearest neighbor boxes were found and randomly select 100 of these videos. For each of the 100 videos, we then randomly select 1 of the boxes which were retrieved as nearest neighbors from this video and add them as negative examples for the re-detection head.</p><p>Adding only additional negative examples creates an imbalance in the training data. Hence, we also retrieve the features of the ground truth bounding boxes of 30 randomly selected frames of the current reference video as additional positive examples. RoIs ← RoIs ∪ {shift(dett−1, shiftx, shifty)} 8: end for 9: end for 10: detst, det scorest ← redetection head(RoIs, ff gt feats) 11: prev scores t ← score redetection(detst, dett−1) 12: loc scorest ← |bbox(detst) − bbox(dett−1)|1 <ref type="bibr">13:</ref> scorest ← det scorest + prev scores t + δ · loc scorest <ref type="bibr">14:</ref> Filter out detections with too large spatial distance <ref type="bibr">15:</ref> for dett ∈ detst do <ref type="bibr">16:</ref> if dett − dett−1 ∞ &gt; ξ then <ref type="bibr" target="#b16">17</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Short-term Tracking Algorithm</head><p>For the VOT2018 dataset <ref type="bibr" target="#b44">[45]</ref>, it is standard to use a reset-based evaluation, where once the object is lost (0 IoU between predicted and ground truth bounding box), the tracker is restarted with the ground truth box five frames later and receives a penalty. This extreme short-term tracking scenario is not what Siam R-CNN with the Tracklet Dynamic Programming Algorithm (TDPA) was designed for. It often triggers resets, which normally (without reset-based evaluation) Siam R-CNN could have automatically recovered from.</p><p>Since VOT2018 is an important tracking benchmark, we created a short-term version of the Siam R-CNN tracking algorithm (see Alg. 2). Given the RoI Aligned features ff gt feats of the first-frame bounding box and the previous-frame tracking result det t−1 , we first extract the backbone features of the current image and afterwards generate regions of interest (RoIs) using the region proposal network <ref type="figure" target="#fig_0">(RPN, lines 1-3)</ref>.</p><p>Note, that we know for sure that the previous-frame predicted box det t−1 has a positive IoU with the ground truth box, as otherwise a reset would have been triggered. Hence, the object to be tracked should be located close to the previous-frame predicted box. In order to exploit this, and to compensate for potential false negatives of the RPN, we add shifted versions of the previous-frame prediction as additional RoIs (lines 5-9). Here, the function shift(·) shifts the previous-frame box det t−1 by factors of its width and height, e.g., if shift x = 0.5 and shift y = 1.0, the box is shifted by half its width in x-direction and by its full height in y-direction.</p><p>Afterwards, we use the redetection head to produce  detections dets t with detection scores det scores t for the current frame t (line 10). We then additionally compute previous-frame scores prev scores t by using the previousframe box as a reference to score the current detections (line 11). To exploit spatial consistency, we also compute location scores (loc scores t , line 12), given by the L 1norm of the pairwise differences between the current detection boxes and the previous-frame predicted box. All three scores are then combined (line 13) by a linear combination, where the current-frame and previous-frame scores have equal weight.</p><p>Even when using a location score, it can happen, that a distractor object appears far away from the object to be tracked and gets a high combined score because it looks very similar to that object. However, since we know that the previous-frame box has positive overlap with the ground truth box, a far-away detection cannot be the object to be tracked. Hence, we explicitly filter out detections which have a large spatial distance (measured by the L ∞ -norm) to the previous-frame predicted box (lines <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>.</p><p>Finally, we report the detection with the highest combined score as the result for the current frame (line 20), or in case there is no valid detection, we repeat the previousframe result as result for the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Rotated Bounding Box Estimation</head><p>For VOT2018, the ground truth is given as rotated bounding boxes which were automatically estimated by an optimization procedure based on hand-annotated segmentation masks <ref type="bibr" target="#b43">[44]</ref>. Nevertheless, most methods produce axisaligned bounding boxes and then evaluate against the rotated bounding box ground truth.</p><p>As an extension of Siam R-CNN, we used Box2Seg to produce segmentation masks and then also ran the optimization procedure which was used to create the ground truth to generate rotated bounding boxes. Note that this optimization procedure (implemented in MATLAB) is very slow, slowing down our whole method to around 0.23 FPS. Note that the speed of the optimization might strongly depend on the hardware/software setup. However, here we do not aim for a good run-time but instead want to analyze the achiev-  <ref type="table">Table 9</ref>: Extended timing analysis of Siam R-CNN using a V100 GPU.</p><p>able performance using rotated bounding boxes. <ref type="table" target="#tab_16">Table 8</ref> shows our results on VOT2018 with rotated bounding boxes. When creating a rotated bounding box for each frame, the overall EAO stays almost the same and only increases from 0.408 to 0.409. However, the accuracy which measures the average intersection-over-union of the bounding box with the ground truth while disregarding resets, increases strongly from 0.609 to 0.686. At the same time, the number of resets strongly increases which can be seen by the robustness degrading from 0.220 to 0.272.</p><p>A manual inspection of the results revealed that in some cases the estimated segmentation mask from Box2Seg was almost empty and the resulting rotated bounding box is hence of very poor quality and can easily trigger a reset.</p><p>To avoid these cases, we apply a mask density filter, which means that in cases where the estimated segmentation mask fills less than 10% of the bounding box which was used to generate it, we stick to the original axis-aligned bounding box in this frame instead of reporting the rotated bounding box. In this setup, the EAO significantly increases to 0.423, while keeping a high accuracy of 0.684. With the mask density filter, Siam R-CNN achieves a robustness of 0.248 which is still worse than the robustness of the axisaligned version, but significantly better than the version without the filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Further Analyses</head><p>In the following, we conduct further analyses of the speed-accuracy trade-off of Siam R-CNN and of the dependence of Siam R-CNN and other methods on the used training data. Additionally, we conduct a per-attribute analysis on OTB2015, and an analysis of the fine-tuning of Box2Seg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Speed-Accuracy Trade-off</head><p>Tab. 9 extends the timing analysis of the main paper. Here, we evaluate three changes aimed at increasing the speed of Siam R-CNN (smaller backbone, smaller input resolution, and fewer RoI proposals) in more detail.</p><p>When evaluating with a ResNet-50 backbone, Siam R-CNN performs slightly faster and still achieves state-of-the-art (SOTA) results (62.3 on LaSOT, compared to 56.8 for DiMP-50 with the same backbone). This shows that the strong results are not only due to a larger backbone, but due to our tracking by re-detection approach.</p><p>We also evaluate reducing the image input size to a smaller image edge length of 400 pixels instead of 800 (row " 1 2 res"). This also results in only a slight decrease in performance in two benchmarks, and a slight increase in performance on OTB2015.</p><p>The row "100 RoIs" shows the results of using 100 RoIs from the RPN, instead of 1000. This almost doubles the speed as most compute occurs in the re-detection head. This results in only a small score decrease on two benchmarks, while improving results on LTB35. This shows that Siam R-CNN can run very quickly, even though it is based on a twostage detection architecture, as very few RoIs are required.</p><p>The fastest setup with all three of these speed improvements (ResNet-50 + 100 RoIs + 1 2 res.) achieves 15.2 frames per second with a V100 GPU, but still achieves strong results, especially for long-term tracking. The same setup with a ResNet-101 backbone instead of ResNet-50 (100 RoIs + 1 2 res.) runs at 13.6 frames per second and achieves excellent results and loses at most 1.6 percentage points over these three datasets compared to the standard Siam R-CNN, while running almost three times as fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training Data Dependence</head><p>We show how our training data compares to the data used by some important recent methods in <ref type="table" target="#tab_5">Table 10</ref>. We use more video 'datasets', but actually use far less data. DiMP-50 <ref type="bibr" target="#b4">[5]</ref> and ATOM <ref type="bibr" target="#b18">[19]</ref> both use 2.28 times more videos. SiamRPN++ <ref type="bibr" target="#b47">[48]</ref> uses 21.3 times more. All three use Ima-geNet and train on COCO by creating artificial videos using augmentations (we use COCO without this complex kind of augmentation). The only dataset we use which is not used by any of the other considered methods is YouTube-VOS <ref type="bibr" target="#b99">[100]</ref>, as we also evaluate on VOS benchmarks. For GOT-10k <ref type="bibr" target="#b37">[38]</ref> evaluation, where only GOT-10k video training data is allowed, all other methods also use static images from ImageNet. For SiamRPN++, we use COCO but not ImageNet. This shows that our strong results are not due to the amount of training data. <ref type="table" target="#tab_5">Table 11</ref> shows a per-attribute ablation on OTB2015. Hard example mining improves results over all attributes and is particularly helpful for low resolution (LR) and background clutter (BC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Per-Attribute Analysis</head><p>Our Tracklet Dynamic Programming Algorithm (TDPA) models spatio-temporal consistency cues only where it is likely that there are consistent predictions, by building up tracklets. It corrects itself immediately after disappearance by tracking all objects simultaneously, and determining the Eval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Videos</head><p>Additional    most likely set of previous tracklets for the object online using dynamic programming. For the Out-of-View (OV) attribute (the target disappears in the video), TDPA significantly outperforms Short-term, which is unable to rely on spatio-temporal consistency cues during disappearance and thus often fails and performs worse than Argmax. TDPA tackles this problem of object disappearance, by using a dynamic and robust type of spatio-temporal consistency cues and also increases robustness against distractors, improving results across all attributes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Fine-tuning Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Experimental Results</head><p>In addition to the 11 benchmarks that we presented in the main paper, we present here results on eight further benchmarks, of which five are short-term tracking benchmarks, one is a long-term tracking benchmark and two are video object segmentation benchmarks. For all benchmarks (except for the three VOT benchmarks) we use exactly the same tracking parameters. This is in contrast to many other   <ref type="table" target="#tab_5">Table 13</ref>: Results on OTB2013 <ref type="bibr" target="#b96">[97]</ref>.</p><p>methods which have parameters explicitly tuned for each dataset. This shows the generalization ability of our tracker to many different scenarios. For the three VOT datasets we use the short-term variant of the tracking parameters (with the same parameters across these three benchmarks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Further Short-Term Tracking Evaluation</head><p>In the main paper we presented short-term tracking results on OTB2015 <ref type="bibr" target="#b97">[98]</ref>, UAV123 <ref type="bibr" target="#b63">[64]</ref>, NfS <ref type="bibr" target="#b42">[43]</ref>, Track-ingNet <ref type="bibr" target="#b64">[65]</ref>, VOT2018 (the same as VOT2017) <ref type="bibr" target="#b44">[45]</ref>, and GOT-10k <ref type="bibr" target="#b37">[38]</ref>. Here in the supplemental material we present results on five further short-term tracking benchmarks. These further benchmarks are OTB-50 <ref type="bibr" target="#b97">[98]</ref>, OTB2013 <ref type="bibr" target="#b96">[97]</ref>, VOT2015 <ref type="bibr" target="#b45">[46]</ref>, VOT2016 <ref type="bibr" target="#b43">[44]</ref>, and Tem-pleColor128 (TC128) <ref type="bibr" target="#b53">[54]</ref>. OTB-50. We evaluate on the OTB-50 benchmark <ref type="bibr">[</ref>     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Further Long-Term Tracking Evaluation</head><p>We evaluate on one further long-term tracking dataset, in addition to the three benchmarks presented in the main paper. UAV20L. We evaluate on the UAV20L benchmark <ref type="bibr" target="#b63">[64]</ref> (20 videos, 2934 frames average length). This dataset contains 20 of the 123 sequences of UAV123, however each of these sequences extends for many more frames than the equivalent sequence in the UAV123 version. It is also evaluated with the same evaluation measures as OTB2015. Tab. 17 compares our results to five state-of-the-art trackers. Siam R-CNN achieves 67.2 AUC, which outperforms the previous best published results by DaSiamRPN <ref type="bibr" target="#b116">[117]</ref> by 5.5 percentage points, which further highlights the ability of Siam R-CNN to perform long-term tracking. <ref type="table" target="#tab_5">Table 18</ref> is an extended version of the results on the DAVIS 2017 <ref type="bibr" target="#b70">[71]</ref> validation set shown in the main paper. <ref type="table" target="#tab_5">Table 19</ref> shows results on the DAVIS 2016 validation set <ref type="bibr" target="#b69">[70]</ref> (20 videos, 68.8 frames average length, 1 object per video) compared to 14 state-of-the-art methods. Methods are ranked by the mean of J and F. Among methods which only use the first-frame bounding box (without the mask), Siam R-CNN achieves the strongest result with 78.6% J &amp;F, which is 8.8 percentage points higher than SiamMask <ref type="bibr" target="#b92">[93]</ref>. When fine-tuning Box2Seg, our method achieves 87.1% J &amp;F, which is close to the best result on DAVIS 2016 by STM-VOS <ref type="bibr" target="#b67">[68]</ref> with 89.3%. <ref type="table" target="#tab_6">Table 20</ref> shows results on the DAVIS 2017 <ref type="bibr" target="#b70">[71]</ref> test-dev set (30 videos, 67.9 frames average length, 2.97 objects per video on average) compared to six state-of-the-art methods. Siam R-CNN achieves 53.3% J &amp;F, which is more than 10 percentage points higher than the result of SiamMask <ref type="bibr" target="#b92">[93]</ref>. STM-VOS <ref type="bibr" target="#b67">[68]</ref> performs significantly better with 72.3%, however it relies on the first-frame mask which makes it less usable in practice. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the speed-accuracy tradeoff of different methods for the YouTube-VOS 2018 validation set. Again, Siam R-CNN achieves a good speed/accuracy trade-off which is only beaten by STM-VOS <ref type="bibr" target="#b67">[68]</ref> (which relies on the first-frame mask).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Further Video Object Segmentation Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Further Qualitative Evaluation</head><p>In <ref type="figure" target="#fig_0">Figure 13</ref> we present further qualitative results of our method on the OTB2015, LTB35 and DAVIS 2017 benchmarks. We present results of our method compared to the best competing method. We show sequences for which Siam R-CNN has the best and worst relative performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Ours (fastest) SiamMask <ref type="bibr" target="#b92">[93]</ref> STM-VOS <ref type="bibr" target="#b67">[68]</ref> RGMP <ref type="bibr" target="#b98">[99]</ref> Ours (Fine-tuned) PReMVOS <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b58">59]</ref> OnAVOS <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b86">87]</ref> OSVOS <ref type="bibr" target="#b7">[8]</ref>  <ref type="figure" target="#fig_0">Figure 12</ref>: Quality versus timing on the YouTube-VOS 2018 <ref type="bibr" target="#b99">[100]</ref> validation set. Only SiamMask <ref type="bibr" target="#b92">[93]</ref> and our method (red) are able to work without the ground truth mask of the first frame and require just the bounding box. Methods shown in blue fine-tune on the first-frame mask. Ours (fastest) denotes Siam R-CNN with ResNet-50 backbone, half input resolution, and 100 RoIs from the RPN. compared to the competing method, as well as the sequence with the median relative performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Thorough Comparison to Previous Methods</head><p>Throughout the main paper and supplemental material we presented results on 11 short-term tracking benchmarks and four long-term tracking benchmarks, however these comparisons are spread throughout a number of tables and figures. We provide a unified and thorough comparison of our results to previous methods across all of these benchmarks in <ref type="table" target="#tab_5">Table 21</ref>. We compare to the results of every paper that presents comparable tracking results from major vision conferences in 2018 and 2019. As well as including all results from all of these papers we also present additional results from some methods that were either taken from later papers or that we obtained by evaluating open-source code. Sometimes these additional results were different to those presented in the original papers, in which case both results are shown. By evaluating on all of these datasets, and comparing to all methods from these two years, we are able to present a complete and holistic evaluation of our method compared to previous works.</p><p>Our method outperforms all previous methods on six out of the 11 evaluated short-term tracking benchmarks, sometimes by up to 7.2 percentage points. On the remaining five benchmarks we achieve close to the best results, with only a few previous methods obtaining better results, and by not too large a margin.</p><p>For long-term tracking, Siam R-CNN performs extremely well. Siam R-CNN outperforms all previous methods over all four benchmarks by between 3.9 and 10.1 percentage points.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>T 0.2T 0.4T 0.6T 0.8T <ref type="figure" target="#fig_0">Figure 13</ref>: Qualitative results on OTB2015 <ref type="bibr" target="#b97">[98]</ref>, LTB35 <ref type="bibr" target="#b60">[61]</ref>, and DAVIS 2017 <ref type="bibr" target="#b70">[71]</ref> (validation set). We compare the results of Siam R-CNN to the best competing methods, which are UPDT <ref type="bibr" target="#b5">[6]</ref> for OTB 2015, SiamRPN++ <ref type="bibr" target="#b47">[48]</ref> for LTB35, and SiamMask <ref type="bibr" target="#b92">[93]</ref> for DAVIS 2017. Siam R-CNN's result is shown in red (magenta for DAVIS 2017), the competing methods' results are shown in blue, and the ground truth in yellow. For each benchmark, the sequences with the best, worst and median relative performance (∆) between Siam R-CNN and the competing method are shown. Six frames spaced equally throughout each video are shown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example results of Siam R-CNN on 3 different tracking tasks where it obtains new state-of-the-art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of Siam R-CNN. A Siamese R-CNN provides re-detections of the object given in the first-frame bounding box, which are used by our Tracklet Dynamic Programming Algorithm along with re-detections from the previous frame. The results are bounding box level tracks which can be converted to segmentation masks by the Box2Seg network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Hard negative mining examples retrieved from other videos for the reference objects shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>shows examples of the retrieved hard negative examples. As can be seen, most of the negative examples are very relevant and hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>unary(a i ) = end(ai) t=start(ai) w ff ff score(a i,t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Results on LTB35 [61] (VOT18-LongTerm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>+ prev. boxes) In COCO (RPN) In COCO (RPN + prev. boxes) Not in COCO (RPN) Not in COCO (RPN + prev. boxes) RPN recall with varying number of proposals. Dotted lines have up to 100 re-detections from the previous frame added. Left: comparison on COCO/non-COCO classes of OTB2015. Right: comparison over three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 2 1 :</head><label>21</label><figDesc>Perform Short-term Tracking for time-step t Inputs ff gt feats, imaget, dett−1 2: backbone feats ← backbone(image t ) 3: RoIs ← RPN(backbone feats) 4: Add shifted versions of dett−1 to RoIs 5: for shiftx ∈ {−1.5, −1.0, −0.5, 0.0, 0.5, 1.0, 1.5} do6:    for shifty ∈ {−1.5, −1.0, −0.5, 0.0, 0.5, 1.0, 1.5} do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>11 :</head><label>11</label><figDesc>Per attribute ablation for Success (AUC) on OTB2015. The first row shows the performance of the full model, and all other rows show the absolute difference to the full model. All: All Videos, BC: Background Clutters, DEF: Deformation, FM: Fast Motion, IPR: In-Plane Rotation, IV: Illumination Variation, LR: Low Resolution, MB: Motion Blur, OCC: Occlusion, OPR: Out-of-Plane Rotation, OV: Out-of-View, SV: Scale Variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 showsFigure 11 :</head><label>1111</label><figDesc>the result of Siam R-CNN with a different number of fine-tuning steps for Box2Seg. For the finetuned Box2Seg variant in the main paper, we used 300 steps which yields a speed-accuracy trade-off of 74.8 J &amp;F with a run-time of 1 FPS (compared to 70.6 J &amp;F and 3.1 FPS without fine-tuning). Note that here the timing is per frame, and not per object, so that the run-time without fine-tuning is longer than for single-object tracking scenarios. When increasing the number of fine-tuning steps to 1000, Siam R-CNN achieves a J &amp;F score of 75.4 with a run-time of 0.38 FPS. Segmentation quality on the DAVIS 2017 validation set depending on the number of fine-tuning steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>et al. UPDT ATOM Tripathi et al. DiMP-50 Siam</figDesc><table><row><cell></cell><cell>[39]</cell><cell>[6]</cell><cell>[19]</cell><cell>[84]</cell><cell>[5]</cell><cell>R-CNN</cell></row><row><cell>Success</cell><cell>51.5</cell><cell cols="2">53.7 58.4</cell><cell>60.5</cell><cell>62.0</cell><cell>63.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Results on NfS<ref type="bibr" target="#b4">[5]</ref>.</figDesc><table><row><cell></cell><cell cols="6">DaSiamRPN UPDT ATOM SiamRPN++ DiMP-50 Siam</cell></row><row><cell></cell><cell>[117]</cell><cell>[6]</cell><cell>[19]</cell><cell>[48]</cell><cell>[5]</cell><cell>R-CNN</cell></row><row><cell>Precision</cell><cell>59.1</cell><cell cols="2">55.7 64.8</cell><cell>69.4</cell><cell>68.7</cell><cell>80.0</cell></row><row><cell>Norm. Prec.</cell><cell>73.3</cell><cell cols="2">70.2 77.1</cell><cell>80.0</cell><cell>80.1</cell><cell>85.4</cell></row><row><cell>Success</cell><cell>63.8</cell><cell cols="2">61.1 70.3</cell><cell>73.3</cell><cell>74.0</cell><cell>81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Results on TrackingNet<ref type="bibr" target="#b64">[65]</ref>.</figDesc><table><row><cell></cell><cell cols="6">LADCF ATOM SiamRPN++ THOR DiMP-50 Ours Ours</cell></row><row><cell></cell><cell>[102]</cell><cell>[19]</cell><cell>[48]</cell><cell>[5]</cell><cell>[77]</cell><cell>(short-t.)</cell></row><row><cell>EAO</cell><cell cols="2">0.389 0.401</cell><cell>0.414</cell><cell cols="3">0.416 0.440 0.140 0.408</cell></row><row><cell cols="3">Accuracy 0.503 0.590</cell><cell>0.600</cell><cell cols="3">0.582 0.597 0.624 0.609</cell></row><row><cell cols="3">Robustn. 0.159 0.204</cell><cell>0.234</cell><cell cols="3">0.234 0.153 1.039 0.220</cell></row><row><cell>AO</cell><cell>0.421</cell><cell>-</cell><cell>0.498</cell><cell>-</cell><cell>-</cell><cell>0.476 0.462</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This shows that Siam R-CNN's advantage over all previous methods is not just due to different training data, but from the tracking approach itself. VOT2018. Tab. 3 shows our results on VOT2018<ref type="bibr" target="#b44">[45]</ref> (60 videos, 356 frames average length), where a reset-based evaluation is used. Once the object is lost, the tracker is restarted with the ground truth box five frames later and receives a penalty. The main evaluation criterion is the Expected Average Overlap (EAO)<ref type="bibr" target="#b45">[46]</ref>. This extreme shortterm tracking scenario is not what Siam R-CNN with the TDPA was designed for. It often triggers resets, which without reset-based evaluation Siam R-CNN could automatically recover from, resulting in an EAO of 0.140. For this setup, we created a simple short-term version of Siam R-CNN which averages the predictions of re-detecting the first-frame reference and re-detecting the previous prediction and combines them with a strong spatial prior. With 0.408 EAO this variant is competitive with many SOTA approaches. Notably, both versions of Siam R-CNN achieve</figDesc><table><row><cell></cell><cell>1.0</cell><cell>Success plots on GOT-10k</cell></row><row><cell></cell><cell>0.8</cell></row><row><cell>Success rate</cell><cell cols="2">Overlap threshold 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 Siam R CNN: [0.649] DiMP-50: [0.611] DCFST: [0.610] ATOM2: [0.602] SPM: [0.513] SiamRPN++: [0.454] ECO: [0.316]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results on OxUvA<ref type="bibr" target="#b84">[85]</ref>.</figDesc><table><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell>Ours Ours (fastest)</cell><cell>OSMN [104]</cell></row><row><cell>&amp;F</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell>J</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Time per frame (seconds)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>CNN (ours) 70.6 66.1 75.0 78.3 0.32 Siam R-CNN (fastest) 70.5 66.4 74.6 76.9 0.12 SiamMask [93] 55.8 54.3 58.5 64.3 0.06 † SiamMask [93] (Box2Seg) 63.3 59.5 67.3 64.3 0.11 SiamRPN++ [48] (Box2Seg) 61.6 56.8 66.3 64.0 0.11</figDesc><table><row><cell>Init</cell><cell>Method</cell><cell>FT M J &amp;F J</cell><cell>F J box t(s)</cell></row><row><cell>bbox</cell><cell>Siam R-DiMP-50 [5] (Box2Seg)</cell><cell cols="2">63.7 60.1 67.3 65.6 0.10</cell></row><row><cell>mask</cell><cell>STM-VOS [68] FEELVOS [86] RGMP [99]</cell><cell cols="2">81.8 79.2 84.3 − 0.32  † 71.5 69.1 74.0 71.4 0.51 66.7 64.8 68.6 66.5 0.28  †</cell></row><row><cell>mask+ft</cell><cell cols="3">PReMVOS [60] Ours (Fine-tun. Box2Seg) 74.8 69.3 80.2 78.3 1.0 77.8 73.9 81.7 81.4 37.6 DyeNet [52] 74.1 − − − 9.32  †</cell></row><row><cell></cell><cell>GT boxes (Box2Seg)</cell><cell cols="2">82.6 79.3 85.8 100.0 −</cell></row><row><cell cols="4">GT boxes (Fine-t. Box2Seg) 86.2 81.8 90.5 100.0 −</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Results on the DAVIS 2017 validation set. FT: fine-tuning, M: using the first-frame masks, t(s): time per frame in seconds. †: timing extrapolated from DAVIS 2016. An extended table is in the supplemental material. Siam R-CNN (fastest) denotes Siam R-CNN with ResNet-50 backbone, half input resolution, and 100 RoIs, see Section 4.5.</figDesc><table><row><cell>Init</cell><cell>Method</cell><cell cols="3">FT M O Jseen Junseen t(s)</cell></row><row><cell>bbox</cell><cell cols="2">Siam R-CNN (ours) Siam R-CNN (fastest) 66.2 69.2 68.3 69.9 SiamMask [93] 52.8 60.2</cell><cell>61.4 57.7 45.1</cell><cell>0.32 0.12 0.06</cell></row><row><cell>mask</cell><cell>STM-VOS [68] RGMP [99]</cell><cell cols="3">79.4 79.7 53.8 59.5 0.26  2 73.5 72.8 0.30  † 45.2 66.2 0.65</cell></row><row><cell></cell><cell>PReMVOS [60, 59]</cell><cell>66.9 71.4</cell><cell>56.5</cell><cell>6</cell></row><row><cell></cell><cell>OnAVOS [88]</cell><cell>55.2 60.1</cell><cell>46.6</cell><cell>24.5</cell></row><row><cell></cell><cell>OSVOS [8]</cell><cell>58.8 59.8</cell><cell>54.2</cell><cell>17  †</cell></row></table><note>† mask+ft Ours (Fi.-tu. Box2Seg) 73.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Ablation and timing analysis of Siam R-CNN.this, improving results by 4.2 percentage points at the cost of speed. We evaluate on the DAVIS 2017 test-dev benchmark and on DAVIS 2016<ref type="bibr" target="#b69">[70]</ref> in the supplemental material.</figDesc><table /><note>YouTube-VOS. Tab. 6 shows results on YouTube-VOS 2018 [100] (474 videos, 1.89 objects and 26.6 frames av- erage length per video). Methods are ranked by the mean O of the J and F metrics over classes in the training set (seen) and unseen classes. Siam R-CNN again outperforms all methods which do not use the first-frame mask (by 15.5 per- centage points), and also outperforms PReMVOS [60, 59] and all other previous methods except for STM-VOS [68].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 :</head><label>8</label><figDesc>Results using rotated bounding boxes on VOT2018. Mask Dens. Filt. denotes using a mask density filter.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Images Total GOT-10k ImageNet-Vid LaSOT YT-VOS TrackingNet YT-BB COCO ImageNet ImageNet-Det Videos + Add.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Imgs</cell></row><row><cell></cell><cell>9k</cell><cell>4k</cell><cell>1k</cell><cell>3k</cell><cell>30k</cell><cell>380k</cell><cell>119k</cell><cell>1281k</cell><cell>457k</cell></row><row><cell>ALL</cell><cell>Siam R-CNN DiMP &amp; ATOM SiamRPN++</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>18k+119k 41k+1400k 384k+1867k</cell></row><row><cell>GOT</cell><cell>Siam R-CNN DiMP &amp; ATOM SiamRPN++</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9k+119k 9k+1281k 9k+1400k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Training data used compared to some important recent methods<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48]</ref>. Videos + Add. Imgs: Number of videos plus number of additional images not in videos. Eval.: Evaluation benchmark setup. ALL: All benchmarks except GOT-10k.</figDesc><table><row><cell cols="13">GOT: Evaluate on GOT-10k, use only GOT-10k training data in addition to static images. ImageNet-Vid: ImageNet Video,</cell></row><row><cell cols="6">YT-VOS: YouTube-VOS, YT-BB: YouTube BoundingBoxes.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ALL</cell><cell>BC</cell><cell>DEF</cell><cell>FM</cell><cell>IPR</cell><cell>IV</cell><cell>LR</cell><cell>MB</cell><cell>OCC</cell><cell>OPR</cell><cell>OV</cell><cell>SV</cell></row><row><cell>Siam R-CNN (ours)</cell><cell>70.1</cell><cell>69.1</cell><cell>64.5</cell><cell>71.0</cell><cell>69.9</cell><cell>71.6</cell><cell>71.1</cell><cell>74.2</cell><cell>66.6</cell><cell>68.6</cell><cell>67.9</cell><cell>72.1</cell></row><row><cell>No hard ex. min.</cell><cell>-1.7</cell><cell>-3.0</cell><cell>-1.3</cell><cell>-2.2</cell><cell>-2.2</cell><cell>-2.3</cell><cell>-5.3</cell><cell>-2.1</cell><cell>-1.7</cell><cell>-2.1</cell><cell>-1.2</cell><cell>-1.7</cell></row><row><cell>No TDPA (Argmax)</cell><cell>-6.3</cell><cell>-7.9</cell><cell>-1.5</cell><cell>-5.6</cell><cell>-9.5</cell><cell>-3.3</cell><cell>-14.0</cell><cell>-5.8</cell><cell>-4.6</cell><cell>-8.4</cell><cell>-6.8</cell><cell>-7.2</cell></row><row><cell>No TDPA (Short-term)</cell><cell>-2.8</cell><cell>-6.5</cell><cell>-2.8</cell><cell>-3.5</cell><cell>-1.6</cell><cell>-2.5</cell><cell>-6.2</cell><cell>-8.4</cell><cell>-4.8</cell><cell>-3.7</cell><cell>-14.5</cell><cell>-5.3</cell></row><row><cell>0 RoIs</cell><cell cols="12">-12.9 -14.0 -10.5 -20.5 -10.5 -12.2 -20.3 -25.8 -14.9 -12.1 -21.8 -13.8</cell></row><row><cell>100 RoIs</cell><cell>-1.4</cell><cell>-1.8</cell><cell>+1.2</cell><cell>-3.6</cell><cell>-3.1</cell><cell>-0.8</cell><cell>-7.8</cell><cell>-4.5</cell><cell>-1.6</cell><cell>-2.2</cell><cell>-4.2</cell><cell>-2.1</cell></row><row><cell>10000 RoIs</cell><cell>-0.5</cell><cell>-0.8</cell><cell>0.0</cell><cell>-0.1</cell><cell>-0.6</cell><cell>+0.4</cell><cell>+0.8</cell><cell>0.0</cell><cell>-0.7</cell><cell>-0.7</cell><cell>-0.3</cell><cell>-0.6</cell></row><row><cell>DiMP-50 [5]</cell><cell>-1.3</cell><cell>-5.3</cell><cell>+3.7</cell><cell>-2.6</cell><cell>-1.4</cell><cell>-2.2</cell><cell>-8.1</cell><cell>-4.3</cell><cell>-0.2</cell><cell>-0.8</cell><cell>-5.4</cell><cell>-2.8</cell></row><row><cell>SiamRPN++ [48]</cell><cell>-0.4</cell><cell>-0.0</cell><cell>+1.7</cell><cell>-2.0</cell><cell>-0.5</cell><cell>-0.3</cell><cell>-5.1</cell><cell>-3.0</cell><cell>-0.3</cell><cell>-0.3</cell><cell>-3.1</cell><cell>-2.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>Results on OTB-50<ref type="bibr" target="#b97">[98]</ref>.</figDesc><table><row><cell cols="3">RPCF SACF MCCT DRT GFS-DCF Siam</cell></row><row><cell>[82] [109] [91] [80]</cell><cell cols="2">[101] R-CNN</cell></row><row><cell>Success AUC 71.3 71.3 71.4 72.0</cell><cell>72.2</cell><cell>70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>98] (50 videos, 539 frames average length). This dataset is a subset of OTB2015 using exactly half of the sequences. It is evaluated with the same evaluation measures as OTB2015. Tab. 12 compares our results to five state-of-the-art trackers. Siam R-CNN achieves 66.3 AUC, which outperforms the previous best published results by ACT<ref type="bibr" target="#b9">[10]</ref> by 0.6 percentage points. OTB2013. We evaluate on the OTB2013 benchmark<ref type="bibr" target="#b96">[97]</ref> (51 videos, 578 frames average length). This dataset is a predecessor to OTB2015 and is evaluated with the same evaluation measures. Tab. 13 compares our results to five state-of-the-art trackers. Siam R-CNN achieves 70.4 AUC, which is comparable to state-of-the-art trackers while being 1.8 percentage points behind the best published results by GFS-DCF<ref type="bibr" target="#b100">[101]</ref>. TC128. We evaluate on the TempleColor128 (TC128) benchmark<ref type="bibr" target="#b53">[54]</ref> (128 videos, 429 frames average length). This dataset is also evaluated using the OTB2015 evaluation measures. Tab. 14 compares our results to five state-ofthe-art trackers. Siam R-CNN achieves 60.1 AUC, which is comparable to state-of-the-art trackers while being slightly inferior to the best published results by UPDT [6] by 2.1 percentage points. VOT2015. We evaluate on the VOT2015 benchmark<ref type="bibr" target="#b45">[46]</ref> (60 videos, 358 frames average length). This is evaluated with the same evaluation measures as VOT2018. Tab. 15 compares our results to five state-of-the-art trackers. The</figDesc><table><row><cell cols="6">MCCT STRCF RTINet ASRCF UPDT Siam</cell></row><row><cell>[91]</cell><cell>[50]</cell><cell>[106]</cell><cell>[17]</cell><cell cols="2">[6] R-CNN</cell></row><row><cell>Success AUC 59.6</cell><cell>60.1</cell><cell>60.2</cell><cell>60.3</cell><cell>62.2</cell><cell>60.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>Results on TC128<ref type="bibr" target="#b53">[54]</ref>.</figDesc><table><row><cell></cell><cell cols="6">FlowTrack SACF SiamRPN SiamDW DaSiamRPN Ours</cell></row><row><cell></cell><cell>[118]</cell><cell>[109]</cell><cell>[49]</cell><cell>[113]</cell><cell>[117]</cell><cell>(short-t.)</cell></row><row><cell>EAO</cell><cell>34.1</cell><cell>34.3</cell><cell>35.8</cell><cell>38.0</cell><cell>44.6</cell><cell>45.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>Results on VOT2015<ref type="bibr" target="#b45">[46]</ref>.</figDesc><table><row><cell></cell><cell cols="5">DaSiamRPN SPM DRT SiamMask UpdateNet Ours</cell></row><row><cell></cell><cell>[117]</cell><cell>[89] [80]</cell><cell>[93]</cell><cell>[108]</cell><cell>(short-t.)</cell></row><row><cell>EAO</cell><cell>41.1</cell><cell>43.4 44.2</cell><cell>44.2</cell><cell>48.1</cell><cell>46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 16 :</head><label>16</label><figDesc>Results on VOT2016<ref type="bibr" target="#b43">[44]</ref>.</figDesc><table><row><cell></cell><cell cols="5">SiamFC PTAV ECO SiamRPN DaSiamRPN Siam</cell></row><row><cell></cell><cell>[4]</cell><cell>[24] [18]</cell><cell>[49]</cell><cell>[117]</cell><cell>R-CNN</cell></row><row><cell>Success AUC</cell><cell>39.9</cell><cell>42.3 43.5</cell><cell>45.4</cell><cell>61.7</cell><cell>67.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 :</head><label>17</label><figDesc>Results on UAV20L [64]. short-term version of Siam R-CNN achieves 45.4 EAO, which outperforms the previous best published results by DaSiamRPN [117] by 0.8 percentage points. VOT2016.We evaluate on the VOT2016 benchmark<ref type="bibr" target="#b43">[44]</ref> (60 videos, 358 frames average length). This is also evaluated with the same evaluation measures as VOT2018. Tab. 16 compares our results to five state-of-the-art trackers. The short-term version of Siam R-CNN achieves 46.5 EAO, which outperforms all previous published results except those of UpdateNet<ref type="bibr" target="#b107">[108]</ref> which outperforms our results by 1.6 percentage points.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 18 :</head><label>18</label><figDesc>Results on the DAVIS 2017 validation set. FT: fine-tuning, M: using the first-frame masks, t(s): time per frame in seconds. †: timing extrapolated from DAVIS 2016 assuming linear scaling in the number of objects. Siam R-CNN (fastest) denotes Siam R-CNN with ResNet-50 backbone, half input resolution, and 100 RoIs from the RPN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>CNN (ours) 78.6 76.8 80.4 86.6 0.24 Siam R-CNN (fastest) 79.0 77.4 80.6 85.0 0.08 SiamMask [93] 69.8 71.7 67.8 73.3 0.03 SiamMask [93] (Box2Seg) 75.9 75.6 76.3 73.3 0.06</figDesc><table><row><cell>Init</cell><cell>Method</cell><cell>FT M J &amp;F J</cell><cell cols="2">F J box t(s)</cell></row><row><cell cols="2">bbox Siam R-mask STM-VOS [68] RGMP [99] FEELVOS [86] FAVOS [13] VideoMatch [37]</cell><cell cols="3">89.3 88.7 89.9 − 81.8 81.5 82.0 79.3 0.14 0.16 81.7 81.1 82.2 80.2 0.45 81.0 82.4 79.5 83.1 0.6 80.9 81.0 80.8 − 0.32</cell></row><row><cell></cell><cell>PML [12]</cell><cell cols="3">77.4 75.5 79.3 75.9 0.28</cell></row><row><cell></cell><cell>OSMN [104]</cell><cell cols="3">73.5 74.0 72.9 71.8 0.14</cell></row><row><cell></cell><cell cols="4">Ours (Fine-t. Box2Seg) 87.1 85.3 88.8 86.6 0.56</cell></row><row><cell></cell><cell>PReMVOS [60]</cell><cell cols="3">86.8 84.9 88.6 89.9 32.8</cell></row><row><cell>mask+ft</cell><cell>DyeNet [52] OSVOS-S [63] OnAVOS [88]</cell><cell cols="3">− 86.2 − 86.5 85.6 87.5 84.4 4.5 − 4.66 85.0 85.7 84.2 84.1 13</cell></row><row><cell></cell><cell>CINM [3]</cell><cell cols="3">84.2 83.4 85.0 83.6 &gt; 120</cell></row><row><cell></cell><cell>OSVOS [8]</cell><cell cols="2">80.2 79.8 80.6 76.0</cell><cell>9</cell></row><row><cell></cell><cell>GT boxes (Box2Seg)</cell><cell cols="3">80.5 79.1 81.9 100.0 −</cell></row><row><cell cols="5">GT boxes (Fine-t. Box2Seg) 89.0 87.6 90.5 100.0 −</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 19 :</head><label>19</label><figDesc>Quantitative results on the DAVIS 2016 validation set. FT denotes fine-tuning, M denotes using the first-frame mask, and t(s) denotes time per frame in seconds. Siam R-CNN (fastest) denotes Siam R-CNN with ResNet-50 backbone, half input resolution, and 100 RoIs from the RPN. Ours (Fine-t. Box2Seg) 62.1 57.3 66.9 1.48</figDesc><table><row><cell>Init</cell><cell>Method</cell><cell>FT M J &amp;F J</cell><cell>F</cell><cell>t(s)</cell></row><row><cell>bbox</cell><cell cols="4">Siam R-CNN (ours) Siam R-CNN (fastest) 51.6 46.3 56.8 0.16 53.3 48.1 58.6 0.44 SiamMask [93] 43.2 40.6 45.8 0.09  †</cell></row><row><cell>mask</cell><cell>STM-VOS [68] RGMP [99] FEELVOS [86]</cell><cell cols="3">72.3 69.3 75.2 0.48  † 52.9 51.4 54.4 0.42  † 57.8 55.2 60.5 0.54</cell></row><row><cell>mask+ft</cell><cell>PReMVOS [60] OnAVOS [88]</cell><cell cols="3">71.6 67.5 75.7 41.3 56.5 53.4 59.6 39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 20 :</head><label>20</label><figDesc>Quantitative results on the DAVIS 2017 test-dev set. FT denotes fine-tuning, M denotes using the first-frame masks, and t(s) denotes time per frame in seconds. †: timing extrapolated from DAVIS 2016 assuming linear scaling in the number of objects. Ours (fastest) denotes Siam R-CNN with ResNet-50 backbone, half input resolution, and 100 RoIs from the RPN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 21 :</head><label>21</label><figDesc>Comparison to all trackers published in CVPR, ICCV and ECCV in 2018 and 2019. Results from original papers, except when marked with § which are from later papers, or from running open-source code. Results in {Red, Green, Blue} are the {Best, Second, Third}, respectively. Benchmarks are ordered by performance relative to the best method other than ours (∆ to SOTA). Methods are ordered first by conference date, then by most 'bests', most 'seconds', most 'thirds' and finally by approximate 'head-to-head' performance. On all benchmarks Siam R-CNN uses exactly the same network weights and tracking hyper-parameters, except for those marked with † which use the 'short-term' tracking parameters, and those marked with * which use weights trained only on GOT-10k.</figDesc><table><row><cell>Δ=+66.4</cell></row><row><cell>Δ=-36.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/spotify/annoy</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: For partial funding of this project, PV, JL and BL would like to acknowledge the ERC Consolidator Grant DeeViSe (ERC-2017-COG-773161) and a Google Faculty Research Award. PHST would like to acknowledge CCAV project Streetwise and EPSRC/MURI grant EP/N019474/1. The authors would like to thank Sourabh Swain, Yuxin Wu, Goutam Bhat, and Bo Li for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<title level="m">Support vector tracking. PAMI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust object tracking with online multiple instance learning. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in a cnn-based higher-order spatiotemporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time &apos;actor-critic&apos; tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-aware deep feature compression for high-speed visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep meta learning for real-time target-aware visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive spatially-regularized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ATOM: accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization for tracking with continuous deep q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time tracking via on-line boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Struck: Structured output tracking with kernels. PAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">Defense of the Triplet Loss for Person Re-Identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Got-10k: A large highdiversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11981</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bridging the gap between detection and tracking: A unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning aberrance repressed correlation filters for real-time uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-time mdnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Č</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for singletarget trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gradnet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Target-aware deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Proc</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep regression tracking with shrinkage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">PReMVOS: Proposal-generation, refinement and merging for the YouTube-VOS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCVW</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Now you see me: evaluating performance in long-term visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Č</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07056</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">TrackingNet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with iterative shift for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Online multi-class lpboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On-line random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Tracking holistic object representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haddadin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">VITAL: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Correlation tracking via joint discrimination and reliability learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning spatialaware regressions for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Roi pooled correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">High-speed tracking with multi-kernel correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Tracking the known and the unknown by leveraging semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">FEELVOS: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for the 2017 DAVIS challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Spm-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Multi-cue correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Sint++: Robust visual tracking via adversarial positive instance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequenceto-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Joint group feature selection and discriminative filter learning for robust visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Proc</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Skimming-Perusal&apos; Tracking: A framework for real-time and robust long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Katsaggelos. Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Joint representation and truncated inference learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Superpixel-based tracking-by-segmentation using markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Visual tracking via spatially aligned correlation filters network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Learning regression and verification networks for long-term visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04320</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Fastdeepkcf without boundary effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Learning features with differentiable closed-form solver for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10414</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Haibin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07437</idno>
		<title level="m">Vision meets drones: A challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Init</forename><surname>Method</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ft M J &amp;amp;f J F J</forename><surname>Box T</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">s) bbox Siam R-CNN</title>
		<imprint/>
	</monogr>
	<note>ours</note>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Siam</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>fastest</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
