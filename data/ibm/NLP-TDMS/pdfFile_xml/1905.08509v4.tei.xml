<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Information Maximization in Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-24">24 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengqian</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Mutual Information Maximization in Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-24">24 Mar 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-machine learning</term>
					<term>neural networks</term>
					<term>mutual information</term>
					<term>graph theory</term>
					<term>convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A variety of graph neural networks (GNNs) frameworks for representation learning on graphs have been recently developed. These frameworks rely on aggregation and iteration scheme to learn the representation of nodes. However, information between nodes is inevitably lost in the scheme during learning. In order to reduce the loss, we extend the GNNs frameworks by exploring the aggregation and iteration scheme in the methodology of mutual information. We propose a new approach of enlarging the normal neighborhood in the aggregation of GNNs, which aims at maximizing mutual information. Based on a series of experiments conducted on several benchmark datasets, we show that the proposed approach improves the state-of-the-art performance for four types of graph tasks, including supervised and semi-supervised graph classification, graph link prediction and graph edge generation and classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Learning with graph structure data requires effective representation of graph structure. Many approaches have been developed recently for this representation learning on graphs, such as graph convolutional networks (GCN) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>, advanced pooling operation on graphs <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, random walkbased methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, mutual information neural estimation-based graph learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, path-searching method <ref type="bibr" target="#b11">[12]</ref>, tree-exploration strategy <ref type="bibr" target="#b12">[13]</ref>, and graph kernel framework <ref type="bibr" target="#b13">[14]</ref>. Learned representation of the graph structure is then applied to graph tasks, including semi-supervised and supervised graph classification, graph link prediction, and graph embedding estimation.</p><p>Different neighborhood aggregation, graph-level pooling schemes, mutual information estimations, walk strategies and graph kernels are applied to GNNs <ref type="bibr" target="#b14">[15]</ref> variants. These models have achieved state-of-the-art performance in a variety of tasks such as node classification, link prediction, and graph classification. However, the design of these new models has little analysis of the transformation of the adjacency matrix of the graph data. Besides, to the best of our knowledge, while complex approaches such as mutual information theory and expectation-maximization estimation yield good results, the current transformation of the adjacency matrix achieves little improvement on graph tasks.</p><p>To further improve the performance of GNNs in the above tasks, we explore the scheme of aggregation and iteration of GNNs in the methodology of mutual information. In particular, we first introduce an equation of mutual information for the aggregation and iteration. We then update the equation of aggregation and iteration for the growth of mutual informa-tion. We further explore one of simple approaches aiming at the growth of mutual information. We implement this approach on several state-of-the-art graph models. According to the experimental results, performance improvements are achieved compared with a list of state-of-the-art graph models. Specifically, we obtain better performances for graph tasks such as supervised graph classification, semi-supervised graph classification, graph classification with missing edges and link prediction. Our implementation codes are available online at https://github.com/CODE-SUBMIT/Graph Neighborhood 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There has been a rich line of research on graph learning models in recent years. Inspired by the first order graph Laplacian models <ref type="bibr" target="#b1">[2]</ref>, GCN is introduced to achieve promising performance on graph representation tasks, and its variants are proposed. For example, relational graph convolutional networks are developed for link prediction and entity classification <ref type="bibr" target="#b15">[16]</ref>; linear mapping is applied to GCN in the concatenation module <ref type="bibr" target="#b14">[15]</ref>; the sum module is developed in GCN for the aggregate representation of neighbors in the neighborhood <ref type="bibr" target="#b16">[17]</ref>; the capsule module is applied to the graph models <ref type="bibr" target="#b17">[18]</ref> to solve especially graph classification problem; the knowledge network is used for the representation learning on graphs <ref type="bibr" target="#b3">[4]</ref>, and hierarchical representation of graph is applied to different pooling strategies <ref type="bibr" target="#b6">[7]</ref>. However, these graph model variants rely on designs of complex convolution modules, and mutual information aspect of the aggregation and iteration scheme is not fully investigated. To fill in this research gap, we explore this aspect and propose an easily implementable approach for graph models in this paper.</p><p>Recent developed graph models have been combined with a variety of technologies in different fields. For instance, while the graph Markov neural network (GMNN) <ref type="bibr" target="#b18">[19]</ref> is developed to combine Markov networks and graph convolutional networks (GMNN), the VAE module is parameterized with graph models based on an iterative graph refinement strategy (Graphite) <ref type="bibr" target="#b19">[20]</ref>. Inspired by the encoder-decoder architectures like U-Nets, both gPool and gunPool are developed to build the encoder-decoder model on graph (Graph U-Nets) <ref type="bibr" target="#b20">[21]</ref>. There is also maximization of mutual information being applied to patch representation and corresponding high-level summaries of graphs <ref type="bibr" target="#b10">[11]</ref>, and a classical discrete probability distribution on the edges of the graph being learned for real problem while the given graph is incomplete or corrupted (LDS-GNN) <ref type="bibr" target="#b21">[22]</ref>. However, it is unknown whether the information entropy plays an important role in the GNNs.</p><p>Many research efforts have been made on mutual information for graph neural networks <ref type="bibr" target="#b16">[17]</ref>. Information maximization is developed between edges states and transform parameters for the molecule property prediction tasks <ref type="bibr" target="#b22">[23]</ref>. The mutual information is applied to maximize the mutual information between node representation and the pooled global graph representation <ref type="bibr" target="#b2">[3]</ref>. The student model is learned from the teacher model through the maximization of mutual information between intermediate representations learned by two models <ref type="bibr" target="#b23">[24]</ref>. However, to the best of our knowledge, the methodology of mutual information has not yet used for the analysis of the scheme of aggregation and iteration for GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>Graph neural networks generate node representations through aggregations over local node neighborhoods. GNN is a flexible class of embedding architectures and the representation of nodes are learned by aggregations of the neighborhood nodes. The READOUT function is used to summarize all the representations into a graph-level representation. The k-th layer of a GNN can be represented by</p><formula xml:id="formula_0">h k v = COMBINE k (h k−1 v , AGGREGATE k ({(h k−1 v , h k−1 µ , e µv ) : µ ∈ N (v)})).</formula><p>Here node v and nodes µ ∈ N (v), h k v and h k µ are the corresponding feature vector at the k-th layer. N (v) is the neighborhoods to node v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Improvement based on mutual Information</head><p>During the learning of GNNs, we let the variable V k , M k denote h k v , M k v respectively. The above aggregation and iteration which depend on the variables such as V k−1 and M k−1 could be written as</p><formula xml:id="formula_1">I(V k ) ≈ I(V k−1 ) + N i=1 I(M k−1 i ),<label>(1)</label></formula><p>where M k−1 i denotes the variable of h k−1 µi , ∀, µ i ∈ N ( v), i ∈ {1, ..., N }. The mutual information of I(V k ) depends on the sum of the above mutual information I(V k−1 ) and</p><formula xml:id="formula_2">I(M k−1 i ) where i ∈ {1, ..., N }.</formula><p>The GNN is trained to learn the representation of each node (variable) by observing other nodes (variables). The mutual information is a measure of the amount of information about one variable through observing the other variables. In the framework of GNNs, the difficulty of learning the representation of nodes grows when observing more variables. In other words, the difficulty grows as the corresponding mutual information reduces. Therefore, we update the above mutual information equation about the aggregation and iteration as follows</p><formula xml:id="formula_3">I(V k ) ≈ I(V k−1 ) + N i=1 I(M k−1 i ) + N j=1 I(N k−1 j ), (2) where N k−1 i denotes the variable of h k−1 νi , ∀, ν j / ∈ N ( v), j ∈ {1, ..., N }, ν = v.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neighborhood enlargement for mutual information growth</head><p>One simple way to implement the updated equation is to observe more nodes (variables). We propose an approach to enlarge the normal neighborhood in the aggregation of GNNs. The enlargement aims at a growth of mutual information for the scheme as analyzed above. The details are illustrated in the following.</p><p>Let graph G = (V, E) where each node v ∈ V and each edge e ∈ E. Let A denote the adjacency matrix of G, where a non-zero entry A ij indicates an edge between nodes i and j. This adjacency matrix is a binary matrix (A ∈ {0, 1} n×n ). Let the length of all e ∈ E be one. For v ∈ V , the first neighborhood of v consists of all vertices of distance one from v, denoted by N 1 (v). The complementary neighborhood of v consists of all vertices of distance two from v, denoted by N 2 (v). Let d(u, v) denote the distance between two vertices u and v where u, v ∈ E. The d(u, v) is defined as the length of the path from the node u to the node v.</p><p>We let A 1 denote the adjacency matrix for the first neigh-</p><formula xml:id="formula_4">borhood of the node v ∈ V . That is, A 1ij = 1, for v i , v j ∈ V, d(i, j) = 1</formula><p>and A 1ij = 0 if node i and node j are not adjacent. Let A 2 denote the adjacency matrix for the complementary neighborhood of the node v ∈ V , i.e.,</p><formula xml:id="formula_5">A 2ij = 1, for v i , v j ∈ V if d(i, j) = 2, and A 1ij = 0 if d(i, j) = 2.</formula><p>Here d(i, j) denotes the distance of the node v i and the node v j . Note that A 1ij = A 2ij = 0 for i = j. Besides, both A 1ij and A 2ij are the binary matrix.</p><p>In the literature, the adjacency matrix A is equal to A 1 . In contrast, we propose a transformation of the adjacency matrix as a combination of the above two neighborhoods A = A 1 + A 2 . This transformation is simple to implement and applicable to many state-of-art methods as we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation of the neighborhood enlargement</head><p>We first refer to the graph convolutional network (GCN) which is proposed by <ref type="bibr" target="#b0">[1]</ref>. This GCN graph convolutional layer is defined as:</p><formula xml:id="formula_6">H i+1 = σ(A ∧ H i W i )<label>(3)</label></formula><p>where H i ∈ R n×si and H i+1 ∈ R n×si+1 are the input and output activations for layer i, W i ∈ R si×si+1 is a trainable weight matrix, σ is an element-wise activation function, A ∧ is a symmetrically normalized adjacency matrix with selfconnections, and A ∧ = D −1/2 (A + I n )D −1/2 . Here I n is an n × n identity matrix. In this case, our proposed method has the following form</p><formula xml:id="formula_7">H i+1 = σ(A ∧ s H i W i ), A ∧ s = D −1/2 (A 1 + A 2 + I n )D −1/2 .<label>(4)</label></formula><p>Next, we refer to a discrete structure learning framework for graph neural network <ref type="bibr" target="#b21">[22]</ref> where the graph structure and the parameters of graph convolutional networks are jointly learned. This jointly learning is based on a bilevel program given two objective functions F and L: F is the outer objectives for the learning of the outer function</p><formula xml:id="formula_8">f w θ = X N × H N → Y N<label>(5)</label></formula><p>and L is the inner objectives for the learning of the inner function</p><formula xml:id="formula_9">L(w θ , A) = v∈Vtrain l(f w θ (X, A) v , y v ),<label>(6)</label></formula><p>where f w (X, A) v is the output of f w for node v and l : Y × Y → R + is a point-wise loss function. Here X ∈ X N is the feature matrix of the graph, A ∈ H N is the adjacency matrix of G, y ∈ Y is the labels of the their true class,w ∈ R d and θ ∈ R m are the parameters of f w θ . The bilevel program is then given by min</p><formula xml:id="formula_10">θ,w θ F (w θ , θ) such that w θ ∈ arg min w L(w, θ).</formula><p>To apply our proposed transformation, we first replace</p><formula xml:id="formula_11">H N by H sN where H sN = {A s |A s = A + A 2 , A ∈ H N }. For A s ∈ H sN ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the outer and inner functions then become</head><formula xml:id="formula_12">f w θ = X N × H sN → Y N , L(w θ , A s ) = v∈Vtrain l(f w θ (X, A s ) v , y v ).<label>(7)</label></formula><p>Other state-of-the-art graph models such as GIN <ref type="bibr" target="#b16">[17]</ref>, GMNN <ref type="bibr" target="#b18">[19]</ref>, Graph-Unets <ref type="bibr" target="#b20">[21]</ref> and LDS-GNN <ref type="bibr" target="#b21">[22]</ref> share a similar way of transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct a bunch of experiments on various datasets and compare our proposed approach with the state-of-the-art graph models. In particular, our proposed method is applied to several graph learning tasks including supervised graph classification, semi-supervised graph classification, graph link prediction, and graph generation and classification with missing edges. The purpose is to show that our proposed approach outperforms the state-of-the-art methods for the listed datasets and is general to different GNN tasks. We also compare with other similar methods for adjacency matrix, and show that our proposed approach both works well with other methods and improves the performance of graph tasks. For a fair comparison, we keep all parameters the same as those in the baseline models GCN <ref type="bibr" target="#b1">[2]</ref>, GIN <ref type="bibr" target="#b16">[17]</ref>, KNN-LDS <ref type="bibr" target="#b21">[22]</ref>, graph Markov neural network (GMNN) <ref type="bibr" target="#b18">[19]</ref>, Graphite <ref type="bibr" target="#b19">[20]</ref>, Graph-Unet <ref type="bibr" target="#b20">[21]</ref> and Mixhop <ref type="bibr" target="#b24">[25]</ref>. In the following, (s)Model denotes our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supervised graph classification</head><p>We first evaluate our approach on two state-of-the-art graph models GCN <ref type="bibr" target="#b1">[2]</ref> and GIN <ref type="bibr" target="#b16">[17]</ref>, which have achieved good results on both social and bi-logical graph datasets. Seven graph datasets are evaluated with our approach in comparison with these two baselines, including MUTAG (188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels), PROTEINS (nodes for secondary structure elements and edge for neighbors in the amino-acid sequence or in 3D space), PTC (344 chemical compounds with 19 discrete labels), NCI1 (a dataset of chemical compounds with 37 discrete labels), IMDB-BINARY and IMDB-MULTI (movie collaboration datasets correspond to an ego-network for each actor/actress), and COLLAB (a scientific collaboration dataset derived from 3 public datasets). For a fair comparison, we keep all the parameters the same as the baseline models (GCN <ref type="bibr" target="#b1">[2]</ref> and GIN <ref type="bibr" target="#b16">[17]</ref>).</p><p>All experiments are conducted with 10-cross validation. <ref type="table" target="#tab_0">Table I</ref> shows both average accuracy and standard deviation. Compared with GAN, our approach improves the average accuracy from 85.6% to 86.23% on MUTAG, from 76.0% to 77.99% on PROTEINS, from 64.2% to 69.23% on PTC, from 80.2% to 80.3% on NCI1, from 74.0% to 76.1% on IMDB-B, from 51.9% to 52.4% on IMDB-M and from 79.0% to 80.89% on COLLAB. Compared with GIN, our approach improves the average accuracy from 89.4% to 94.14% on MUTAG, from 76.2% to 78.97% on PROTEINS, from 64.6% to 73.56% on PTC, from 82.7% to 83.85% on NCI1, from 75.1% to 77.94% on IMDB-B, from 52.3% to 54.52% on IMDB-M and from 80.2% to 80.71% on COLLAB. It is clear that our proposed approach achieves better performance in comparison with the baselines.</p><p>We then evaluate our approach on another state-of-the-art graph model KNN-LDS <ref type="bibr" target="#b21">[22]</ref>. In this case, six graph datasets are evaluated with our approach and the baseline <ref type="bibr" target="#b21">[22]</ref>. These datasets include WINE (a dataset of 178 samples with 3 discrete labels, each sample has 13 features), CANCER (a dataset of 569 samples with 2 discrete labels, each sample has 30 features), DIGITS (a dataset of 1797 samples with 10 discrete labels, each sample has 64 features), CITESEER (a data set of 3327 samples with 6 discrete labels, each sample has 3703 features), CORA (a dataset of 2708 samples with 7 discrete labels, each sample has 1433 features), 20NEWS (a dataset of 9607 samples with 10 discrete labels, each sample has 236 features). For a fair comparison, all parameters remain the same as the baseline <ref type="bibr" target="#b21">[22]</ref>.</p><p>Similarly, all experiments are conducted with 10-cross validation. Both the average accuracy and standard deviation are presented in <ref type="table" target="#tab_0">Table II</ref>. Our approach improves the average accuracy from 97.5% to 98.0% on WINE, from 94.9% to 95.7% on CANCER, from 71.5% to 73.7% on CITESEER, from 71.5% to 72.3% on CORA, and from 46.4% to 47.9% on 20NEWS. Our proposed approach achieves improvement in performance compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semi-supervised graph classification</head><p>In order to evaluate the effectiveness of our approach on semi-supervised graph classification, we apply the proposed transformation on the graph Markov neural network (GMNN) <ref type="bibr" target="#b18">[19]</ref> which combines both statistical relational learning and graph neural networks. If the nodes have no labels, the neighbors of each node are predicted and the predicted neighbors are treated as pseudo labels. We apply our approach with neighborhood combination to evaluate the performance of the combined neighbors for the neighbor prediction and generation of pseudo labels. Three graph datasets are evaluated with our approach and GMNN including CORA, CITESEER, and PUBMED. In each object, we use the same data partition method as in the baseline <ref type="bibr" target="#b18">[19]</ref>. Both our approach and GMNN are trained without the usage of object attributes since     All experiments are conducted with 10-cross validation, and both average accuracy and standard deviation are summarized in <ref type="table" target="#tab_0">Table III</ref>. Our approach improves the average accuracy from 83.5% to 83.4% on CORA, from 73.0% to 73.4% on CITESEER, and from 81.3% to 81.6% on PUBMED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph link prediction</head><p>The task of link prediction is to predict whether an edge exists between a pair of nodes. In order to evaluate how effective our proposed approach is for the link prediction, we apply the approach to the state-of-the-art graph models VGAE <ref type="bibr" target="#b25">[26]</ref> and Graphite <ref type="bibr" target="#b19">[20]</ref>. A balanced set of positive and negative (false) edges to the original graph are added to the original graph, with 5% edges used for validation, and 10% edges used for testing. Both the area under the ROC curve (AUC) and average precision (AP) metrics are assessed. Three datasets including CORA, CITESEER, and PUNMED are used for the evaluation.</p><p>All experiments are conducted with 10-cross validation. AUC is shown in Table IV which shows the comparison between our approach and two baseline models: VGAE and Graphite-VAE. Compared with VGAE, our approach improves the AUC from 90.1% to 93.4% on CORA, from 92.0% to 92.6% on CITESEER, and from 92.3% to 92.7% on PUBMED. Our approach improves the AUC from 91.5% to 93.7% on CORA, from 93.5% to 94.1% on CITESEER, and from 94.6% to 94.8% on PUBMED when the base model is Graphite-VAE. The performance comparison can be found in <ref type="table" target="#tab_4">Table V</ref>. Compared with VAE, our approach improves AP from 92.3% to 93.0% on CORA, from 94.2% to 94.3% on CITESEER, and from 94.2% to 94.5% on PUBMED. Our approach improves the AP from 93.2% to 93.5% on CORA, from 95.0% to 95.4% on CITESEER, and from 96.0% to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Graph classification with missing edges</head><p>In practice, real-world graphs are often noisy and incomplete. As a consequence, the edges are usually missing in these graphs. In order to evaluate our approach for real-world graph tasks, we apply our approach to graph classification tasks with missing edges in the graphs. These graphs are obtained by randomly sampling 25%, 50% and 75% of the edges. Two datasets including CORA and CITESEER are evaluated.</p><p>All experiments are conducted with 10-cross validation, and both average accuracy and standard deviation are presented in <ref type="table" target="#tab_0">Table VI</ref>. Compared with LDS <ref type="bibr" target="#b21">[22]</ref> for the dataset CITESEER, our approach improves the average accuracy from 71.92% to 74.60% when 75% of the edges are missing, from 73.26% to 74.90% when 50% of the edges are missing, from 74.58% to 75.70% when 25% of the edges are missing, and from 75.54% to 76.11% when no edges are missing. Compared with LDS <ref type="bibr" target="#b21">[22]</ref> for the dataset CORA, our approach improves the average accuracy from 74.18% to 78.12% when 75% of the edges are missing, from 78.98% to 80.41% when 50% of the edges are missing, from 81.54% to 83.61% when 25% of the edges are missing, and from 84.08% to 84.81% when no edges are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Other transformations for adjacency matrix</head><p>We also compare our approach with other state-of-the-art methods for the transformation of adjacency matrix.</p><p>1) Node attribute and batch normalization.: Firstly, we compare our approach with three baseline graph models: GCN, MGCN, and MGCNK <ref type="bibr" target="#b26">[27]</ref> by applying continuous node attribute and batch-normalization. We also evaluate our approach by applying the spectral multigraph module. We compare our approach with the three graph models on dataset ENZYMES with continuous node attributes.</p><p>All experiments are conducted with 10-cross validation, and both average accuracy and standard deviation are shown in <ref type="table" target="#tab_0">Table VII</ref>. Applying our approach to GCN [2] on ENZYMES leads to an increase in the average performance from 32.33% to 40.11%, and from 51.17% to 75.33% with the usage of continuous node attribute and batch normalization. Compared with MGCN <ref type="bibr" target="#b26">[27]</ref> on ENZYMES, our approach improves the average performance from 40.50% to 51.83%, from 59.83% to 72.16% with the usage of continuous node attribute and batch normalization. Compared with MGCNK <ref type="bibr" target="#b26">[27]</ref> on ENZYMES, our approach improves the average performance from 61.04% to 64.57%, and from 66.67% to 71.50% with the usage of continuous node attribute and batch normalization.</p><p>2) Multi-graph models: Secondly, we evaluate our approach by applying two multi-graph modules and show its  3) Powers of adjacency matrix and self-connectivity: Thirdly, we evaluate our approach by applying the power of adjacency matrix or the addition of self-loops for adjacency matrix. Our approach is tested on the dataset PROTEINS. Its performance is compared against two baseline models with the application of the above transformation.</p><p>All experiments are conducted with 10-cross validation, and both the average accuracy and the standard deviation are shown in <ref type="table" target="#tab_0">Table IX</ref>. With the usage of A 2 , the average accuracy increases from 74.37% to 74.56% on GCN <ref type="bibr" target="#b0">[1]</ref>, and from 72.45% to 72.28% on Graph-Unet <ref type="bibr" target="#b20">[21]</ref>. With the usage of A 2 +2I, the average accuracy increases to 74.23% and 73.18%, respectively. Besides, our approach improves the accuracy from 74.37% to 78.49% on GCN and from 72.45% to 74.12% on Graph-Unet <ref type="bibr" target="#b20">[21]</ref>. 4) Convolutional transformation: Finally, we test our approach with the transformation of adjacency matrix based on deep networks. The deep networks work on the calculation of the power of adjacency matrix. We compare our approach with the state-of-the-art Mixhop graph module <ref type="bibr" target="#b24">[25]</ref>. Three datasets are used to evaluate the performances of different models.</p><p>All experiments are conducted with 10-cross validation, and both average accuracy and standard deviation are shown in <ref type="table" target="#tab_7">Table X</ref>. Our approach, which applies to GMNN <ref type="bibr" target="#b18">[19]</ref>, achieves better performance on all three datasets: it achieves 73.4% on CORA while Mixhop obtains 71.4%; it achieves 83.5% on CITESEER while Mixhop achieves 81.9%, and it achieves 81.6% on PUBMED while Mixhop <ref type="bibr" target="#b24">[25]</ref> only reaches 80.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>In this paper, we analyze the scheme of the aggregation and iteration in the graph neural networks in the methodology of mutual information. We propose an approach to enlarge the normal neighborhood in the aggregation of GNNs. We apply the proposed method on several of the state-of-the-art graph neural network models and conduct a series of experiments on the graph representation tasks including supervised graph classification, semi-supervised graph classification, and graph edge generation and classification. The numerical results on various datasets show that our approach improves the performance of the listed state-of-the-art models. For the future work, it is worthwhile to study new network architecture, message transmission and graph kernels in the proposed approach, and test on large-scale graph tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I GCN</head><label>I</label><figDesc>AND GIN SUPERVISED GRAPH CLASSIFICATION TASK ON SEVEN DATASETS.</figDesc><table><row><cell>Graph models</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>PTC</cell><cell>NCL1</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>COLLAB</cell></row><row><cell>GCN</cell><cell>85.6 ± 5.8</cell><cell>76.0 ± 3.2</cell><cell>64.2 ± 4.3</cell><cell>80.2 ± 2.0</cell><cell>74.0 ± 3.4</cell><cell>51.9 ± 3.8</cell><cell>79.0 ± 1.8</cell></row><row><cell>(s)GCN</cell><cell>86.23 ± 6.71</cell><cell>77.99 ± 3.72</cell><cell>69.23 ± 5.59</cell><cell>80.3 ± 2.1</cell><cell>76.1 ± 2.86</cell><cell>52.4 ± 2.56</cell><cell>80.89 ± 2.3</cell></row><row><cell>GIN-0</cell><cell>89.4 ± 5.6</cell><cell>76.2 ± 2.8</cell><cell>64.6 ± 7.0</cell><cell>82.7 ± 1.7</cell><cell>75.1 ± 5.1</cell><cell>52.3 ± 2.8</cell><cell>80.2 ± 1.9</cell></row><row><cell>(s)GIN-0</cell><cell>94.14 ± 2.74</cell><cell>78.97 ± 3.17</cell><cell>73.56 ± 4.27</cell><cell>83.85 ± 1.05</cell><cell>77.94 ± 4.31</cell><cell>54.52 ± 0.39</cell><cell>80.71 ± 1.48</cell></row><row><cell>GIN-ǫ</cell><cell>89.0 ± 6.0</cell><cell>75.9 ± 3.8</cell><cell>63.7 ± 8.2</cell><cell>82.7 ± 1.6</cell><cell>74.3 ± 5.1</cell><cell>52.1 ± 3.6</cell><cell>80.1 ± 1.9</cell></row><row><cell>(s)GIN-ǫ</cell><cell>93.47 ± 1.64</cell><cell>77.61 ± 3.05</cell><cell>72.16 ± 2.17</cell><cell>82.92 ± 1.69</cell><cell>75.19 ± 5.1</cell><cell>53.62 ± 0.61</cell><cell>80.51 ± 1.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II KNN</head><label>II</label><figDesc>-LDS SUPERVISED GRAPH CLASSIFICATION TASK ON SIX DATASETS.</figDesc><table><row><cell>Graph models</cell><cell>WINE</cell><cell>CANCER</cell><cell>DIGITS</cell><cell>CITESEER</cell><cell>CORA</cell><cell>20NEWS</cell></row><row><cell>KNN-LDS</cell><cell cols="2">97.5 ± 1.2 94.9 ± 0.5</cell><cell>92.5 ± 0.7</cell><cell>71.5 ± 1.1</cell><cell>71.5 ± 0.8</cell><cell>46.4 ± 1.6</cell></row><row><cell>(s)KNN-LDS</cell><cell cols="2">98.0 ± 1.1 95.7 ± 0.6</cell><cell>92.5 ± 0.6</cell><cell>73.7 ± 0.9</cell><cell>72.3 ± 0.6</cell><cell>47.9 ± 1.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III GMNN</head><label>III</label><figDesc>SEMI-SUPERVISED GRAPH CLASSIFICATION TASK ON THREE DATASETS.</figDesc><table><row><cell>Graph models</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell></row><row><cell>GMNN</cell><cell>83.4 ± 0.8</cell><cell>73.0 ± 0.8</cell><cell>81.3 ± 0.5</cell></row><row><cell>(s)GMNN</cell><cell>83.5 ± 0.2</cell><cell>73.4 ± 0.1</cell><cell>81.6 ± 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV AREA</head><label>IV</label><figDesc>UNDER THE ROC CURVE (AUC) FOR TWO BASELINES ON LINK PREDICTION TASK.</figDesc><table><row><cell>Graph models</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell></row><row><cell>VGAE</cell><cell>90.1 ± 0.15</cell><cell>92.0 ± 0.17</cell><cell>92.3 ± 0.06</cell></row><row><cell>(s)VGAE</cell><cell>93.4 ± 0.12</cell><cell>92.6 ± 0.12</cell><cell>92.7 ± 0.05</cell></row><row><cell>Graphite-VAE</cell><cell>91.5 ± 0.15</cell><cell>93.5.0 ± 0.13</cell><cell>94.6 ± 0.04</cell></row><row><cell>(s)Graphite-VAE</cell><cell>93.7 ± 0.13</cell><cell>94.1 ± 0.10</cell><cell>94.8 ± 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V AVERAGE</head><label>V</label><figDesc>PRECISION (AP) SCORES FOR TWO BASELINES ON LINK PREDICTION TASK.</figDesc><table><row><cell>Graph models</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell></row><row><cell>VGAE</cell><cell>92.3 ± 0.12</cell><cell>94.2 ± 0.12</cell><cell>94.2 ± 0.04</cell></row><row><cell>(s)VGAE</cell><cell>93.0 ± 0.10</cell><cell>94.3 ± 0.08</cell><cell>94.5 ± 0.02</cell></row><row><cell>Graphite-VAE</cell><cell>93.2 ± 0.13</cell><cell>95.0 ± 0.10</cell><cell>96.0 ± 0.03</cell></row><row><cell>(s)Graphite-VAE</cell><cell>93.5 ± 0.11</cell><cell>95.4 ± 0.09</cell><cell>96.3 ± 0.02</cell></row><row><cell cols="2">the difficulty of semi-classification drops with known object</cell><cell></cell><cell></cell></row><row><cell>attributes.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI LDS</head><label>VI</label><figDesc>ON CITESEER AND CORA WITH VARIOUS PERCENTAGE RETAINED.TABLE VII GCN, MGCN, AND MGCNK ON ENZYMES WITH CONTINUOUS NODE ATTRIBUTE AND BATCH NORMALIZATION.</figDesc><table><row><cell cols="2">Models (data)</cell><cell>25%</cell><cell></cell><cell>50%</cell><cell>75%</cell><cell>100% (full graph)</cell></row><row><cell cols="2">LDS (CITESEER)</cell><cell cols="2">71.92 ± 1.0</cell><cell>73.26 ± 0.6</cell><cell>74.58 ± 0.9</cell><cell>75.54 ± 0.4</cell></row><row><cell cols="4">(s)LDS (CITESEER) 74.60 ± 0.9</cell><cell>74.90 ± 0.7</cell><cell>75.50 ± 0.7</cell><cell>76.11 ± 0.6</cell></row><row><cell cols="2">LDS (CORA)</cell><cell cols="2">74.18 ± 1.0</cell><cell>78.98 ± 0.6</cell><cell>81.54 ± 0.9</cell><cell>84.08 ± 0.4</cell></row><row><cell cols="2">(s)LDS (CORA)</cell><cell cols="2">78.12 ± 0.7</cell><cell>80.41 ± 0.7</cell><cell>83.61 ± 0.8</cell><cell>84.81 ± 0.7</cell></row><row><cell></cell><cell></cell><cell cols="3">ENZYMES dataset</cell></row><row><cell>Models</cell><cell></cell><cell>-</cell><cell cols="3">Batch normalization</cell><cell>Continuous node attribute</cell></row><row><cell>GCN</cell><cell cols="2">32.33±5.07</cell><cell></cell><cell>−</cell><cell>51.17 ± 5.63</cell></row><row><cell>(s)GCN</cell><cell cols="2">40.11 ± 3.27</cell><cell></cell><cell>56.67 ± 4.47</cell><cell>75.33 ± 3.93</cell></row><row><cell>MGCN</cell><cell cols="2">40.50 ± 5.58</cell><cell></cell><cell>−</cell><cell>59.83 ± 6.56</cell></row><row><cell>(s)MGCN</cell><cell cols="2">51.83 ± 5.84</cell><cell></cell><cell>59.23 ± 4.12</cell><cell>72.16 ± 4.39</cell></row><row><cell>MGCNK</cell><cell cols="2">61.04 ± 4.78</cell><cell></cell><cell>−</cell><cell>66.67 ± 6.83</cell></row><row><cell>(s)MGCNK</cell><cell cols="2">64.57 ± 5.63</cell><cell></cell><cell>65.12 ± 4.28</cell><cell>71.50 ± 6.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE VIII</cell></row><row><cell cols="6">GCN, MGCN, AND MHCNK SUPERVISED GRAPH CLASSIFICATION TASK ON TWO DATASETS WITHOUT CONTINUOUS NODE ATTRIBUTE AND BATCH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NORMALIZATION.</cell></row><row><cell></cell><cell></cell><cell>Graph models</cell><cell></cell><cell>MUTAG</cell><cell>PROTEINS</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell>76.5 ± 1.4</cell><cell>74.45 ± 4.91</cell></row><row><cell></cell><cell></cell><cell>(s)GCN</cell><cell></cell><cell>91.39 ± 6.56</cell><cell>78.49 ± 3.19</cell></row><row><cell></cell><cell></cell><cell>MGCN</cell><cell></cell><cell>84.4 ± 1.6</cell><cell>74.62 ± 2.56</cell></row><row><cell></cell><cell></cell><cell>(s)MGCN</cell><cell></cell><cell>87.25 ± 4.16</cell><cell>78.23 ± 3.23</cell></row><row><cell></cell><cell></cell><cell>MGCNK</cell><cell></cell><cell>89.1 ± 1.4</cell><cell>76.27 ± 2.82</cell></row><row><cell></cell><cell></cell><cell>(s)MGCNK</cell><cell></cell><cell>89.42 ± 5.97</cell><cell>78.04 ± 2.97</cell></row><row><cell cols="4">96.3% on PUBMED when the base model is Graphite.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IX SUPERVISED</head><label>IX</label><figDesc>GRAPH CLASSIFICATION IN COMPARISON WITH TWO TRANSFORMATION FORMS AND TWO BASELINE MODELS.</figDesc><table><row><cell>GCN</cell><cell>GCN+A 2</cell><cell>GCN+A 2 +2I</cell><cell>(s)GCN</cell></row><row><cell>74.37 ± 0.31</cell><cell>74.56 ± 0.26</cell><cell>74.23 ± 0.37</cell><cell>78.49 ± 3.19</cell></row><row><cell>Graph-Unet</cell><cell>Graph-Unet+A 2</cell><cell>Graph-Unet+A 2 +2I</cell><cell>(s)Graph-Unet</cell></row><row><cell>72.45 ± 0.88</cell><cell>72.87 ± 0.52</cell><cell>73.18 ± 0.50</cell><cell>74.12 ± 4.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X SUPERVISEDTable</head><label>X</label><figDesc>GRAPH CLASSIFICATION FOR THREE DATASETS COMPARING MIXHOP AND (S)GMNN. VIII. When the base model is GCN, our approach improves the average accuracy from 76.5% to 91.39% on MUTAG and from 74.45% to 78.49% on PROTEINS. Compared with MGCN [27], our approach improves the average accuracy from 84.4% to 87.25% on MUTAG and from 74.62% to 78.23% on PROTEINS. Compared with MGCNK [27], our approach improves the average accuracy from 89.1% to 89.42% on MUTAG and from 76.27% to 78.04% on PROTEINS.</figDesc><table><row><cell>Graph models</cell><cell>CORA</cell><cell>CITESEER</cell><cell>PUBMED</cell></row><row><cell>Mixhop</cell><cell>71.4 ± 0.81</cell><cell>81.9 ± 0.40</cell><cell>80.8 ± 0.58</cell></row><row><cell>(s)GMNN</cell><cell>73.4 ± 0.76</cell><cell>83.5 ± 0.57</cell><cell>81.6 ± 0.64</cell></row><row><cell cols="2">usefulness for multigraph models. Two datasets including</cell><cell></cell><cell></cell></row><row><cell>MUTAG and PROTEINS are used.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">All experiments are conducted with 10-cross validation, and</cell><cell></cell><cell></cell></row><row><cell cols="2">both average accuracy and standard deviation are reported</cell><cell></cell><cell></cell></row><row><cell>in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mine: mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep graph infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Expressivity versus efficiency of graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international workshop on mining graphs, trees and sequences</title>
		<meeting>the first international workshop on mining graphs, trees and sequences</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A persistent weisfeiler-lehman procedure for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5448" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gmnn: Graph markov neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06214</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10459</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05178</idno>
		<title level="m">Graph u-nets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11960</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph networks as a universal machine learning framework for molecules and crystals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mixhop: Higher-order graph convolution architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Spectral multigraph networks for discovering and fusing relationships in molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09595</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
