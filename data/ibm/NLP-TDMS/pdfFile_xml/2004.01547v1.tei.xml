<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context Prior for Scene Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
							<email>changqianyu@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong 4 Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
							<email>cgao@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
							<email>nsang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Image Processing and Intelligent Control</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence and Automation</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context Prior for Scene Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works have widely explored the contextual dependencies to achieve more accurate segmentation results. However, most approaches rarely distinguish different types of contextual dependencies, which may pollute the scene understanding. In this work, we directly supervise the feature aggregation to distinguish the intra-class and interclass context clearly. Specifically, we develop a Context Prior with the supervision of the Affinity Loss. Given an input image and corresponding ground truth, Affinity Loss constructs an ideal affinity map to supervise the learning of Context Prior. The learned Context Prior extracts the pixels belonging to the same category, while the reversed prior focuses on the pixels of different classes. Embedded into a conventional deep CNN, the proposed Context Prior Layer can selectively capture the intra-class and inter-class contextual dependencies, leading to robust feature representation. To validate the effectiveness, we design an effective Context Prior Network (CPNet). Extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against state-of-the-art semantic segmentation approaches. More specifically, our algorithm achieves 46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on Cityscapes. Code is available at https://git.io/ContextPrior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene segmentation is a long-standing and challenging problem in computer vision with many downstream applications e.g., augmented reality, autonomous driving <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>, human-machine interaction, and video content analysis. The goal is to assign each pixel with a category label, which provides comprehensive scene understanding.</p><p>Benefiting from the effective feature representation of (a) Input Image (b) Pyramid Method (c) CPNet (d) Input Image (e) Attention Method (f) CPNet <ref type="figure">Figure 1</ref>. Hard examples in scene segmentation. In the first row, the central part of the sand in the red box is misclassified as the sea, because the shadow part has a similar appearance with the sea. With the pyramid-based aggregation method <ref type="bibr" target="#b2">[3]</ref>, aggregation of the confused spatial information may lead to undesirable prediction as visualized in (b). In the second row, the table in the green box has a similar appearance to the bottom part of the bed. The attention-based method <ref type="bibr" target="#b50">[50]</ref> fails to effectively distinguish the confused spatial information without prior knowledge, leading to less correct prediction as shown in (e). In the proposed CPNet, we aggregate the contextual dependencies with clear distinguishment. Notably, the Context Prior models the intra-class and inter-class relationships as a context prior knowledge to capture the intra-class and inter-class contextual dependencies.</p><p>the Fully Convolutional Network (FCN), a few approaches have obtained promising performance. However, limited by the structure of convolutional layers, the FCN provides insufficient contextual information, leaving room for improvement. Therefore, various methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b18">19]</ref> explore the contextual dependencies to obtain more accurate segmentation results. There are mainly two paths to aggregate the contextual information: 1) Pyramidbased aggregation method. Several methods <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> adopt pyramid-based modules or global pooling to aggre-gate regional or global contextual details regularly. However, they capture the homogeneous contextual relationship, ignoring the contextual dependencies of different categories, as shown in <ref type="figure">Figure 1</ref> <ref type="bibr">(b)</ref>. When there are confused categories in the scene, these methods may result in a less reliable context. 2) Attention-based aggregation method. Recent attention-based methods learn channel attention <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b43">43]</ref>, spatial attention <ref type="bibr" target="#b22">[23]</ref>, or point-wise attention <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">44]</ref> to aggregate the heterogeneous contextual information selectively. Nevertheless, due to the lack of explicit regularization, the relationship description of the attention mechanism is less clear. Therefore, it may select undesirable contextual dependencies, as visualized in <ref type="figure">Figure 1(e)</ref>. Overall, both paths aggregate contextual information without explicit distinction, causing a mixture of different contextual relationships. We notice that the identified contextual dependencies help the network understand the scene. The correlation of the same category (intra-class context) and the difference between the different classes (inter-class context) make the feature representation more robust and reduce the search space of possible categories. Therefore, we model the contextual relationships among categories as prior knowledge to obtain more accurate prediction, which is of great importance to the scene segmentation.</p><p>In this paper, we construct a Context Prior to model the intra-class and inter-class dependencies as the prior knowledge. We formulate the context prior as a binary classifier to distinguish which pixels belong to the same category for the current pixel, while the reversed prior can focus on the pixels of different classes. Specifically, we first use a fully convolutional network to generate the feature map and the corresponding prior map. For each pixel in the feature map, the prior map can selectively highlight other pixels belonging to the same category to aggregate the intra-class context, while the reversed prior can aggregate the inter-class context. To embed the prior into the network, we develop a Context Prior Layer incorporating an Affinity Loss, which directly supervises the learning of the prior. Meanwhile, Context Prior also requires spatial information to reason the relationships. To this end, we design an Aggregation Module, which adopts the fully separable convolution (separate on both the spatial and depth dimensions) <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b29">29</ref>] to efficiently aggregate spatial information.</p><p>To demonstrate the effectiveness of the proposed Context Prior, we design a simple fully convolutional network called Context Prior Network (CPNet). Based on the output features of the backbone network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">36]</ref>, the Context Prior Layer uses the Aggregation Module to aggregate the spatial information to generate a Context Prior Map. With the supervision of Affinity Loss, the Context Prior Map can capture intra-class context and inter-class context to refine the prediction. Extensive evaluations demonstrate that the proposed method performs favorably against several recent state-of-the-art semantic segmentation approaches.</p><p>The main contributions of this work are summarized as follows.</p><p>• We construct a Context Prior with supervision of an Affinity Loss embedded in a Context Prior Layer to capture the intra-class and inter-class contextual dependencies explicitly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Context Aggregation. In recent years, various methods have explored contextual information, which is crucial to scene understanding <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>There are mainly two paths to capture contextual dependencies. 1) PSPNet <ref type="bibr" target="#b49">[49]</ref> adopts the pyramid pooling module to partition the feature map into different scale regions. It averages the pixels of each area as the local context of each pixel in this region. Meanwhile, Deeplab <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> methods employ atrous spatial pyramid pooling to sample the different range of pixels as the local context. 2) DANet <ref type="bibr" target="#b10">[11]</ref>, OCNet <ref type="bibr" target="#b44">[44]</ref>, and CCNet <ref type="bibr" target="#b17">[18]</ref> take advantage of the selfsimilarity manner <ref type="bibr" target="#b37">[37]</ref> to aggregate long-range spatial information. Besides, EncNet <ref type="bibr" target="#b45">[45]</ref>, DFN <ref type="bibr" target="#b43">[43]</ref>, and ParseNet <ref type="bibr" target="#b26">[27]</ref> use global pooling to harvest the global context. Despite the success of these attention mechanisms, they maybe capture undesirable contextual dependencies without explicitly distinguishing the difference of different contextual relationships. Therefore, in the proposed approach, we explicitly regularize the model to obtain the intra-class and inter-class contextual dependencies.</p><p>Attention Mechanism. Recent years have witnessed the broad application of the attention mechanism. It can be used for various tasks such as machine translation <ref type="bibr" target="#b34">[34]</ref>, image/action recognition <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref>, object detection <ref type="bibr" target="#b14">[15]</ref> and semantic segmentation <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">44]</ref>. For the semantic segmentation task, <ref type="bibr" target="#b3">[4]</ref> learns an attention mechanism to weight the multi-scale features softly. Inspired by SENet <ref type="bibr" target="#b15">[16]</ref>, some methods such as EncNet <ref type="bibr" target="#b45">[45]</ref>, DFN <ref type="bibr" target="#b43">[43]</ref>, and BiSeNet <ref type="bibr" target="#b42">[42]</ref> adopt the channel attention to select the desired feature map. Following <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b37">37]</ref>, DANet <ref type="bibr" target="#b10">[11]</ref> and OCNet <ref type="bibr" target="#b44">[44]</ref> use the self-attention to capture the long-range dependency, while PSANet <ref type="bibr" target="#b50">[50]</ref>   tively learns point-wise attention to harvest the long-range information. However, these effective methods lack the explicit regularization, maybe leading to an undesirable context aggregation. Therefore, in our work, we propose a Context Prior embedded in the Context Prior Layer with an explicit Affinity Loss to supervise the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Context Prior</head><p>Contextual dependencies play a crucial role in scene understanding, which is widely explored in various methods <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">43]</ref>. However, these methods aggregate different contextual dependencies as a mixture. As discussed in Section 1, the clear distinguished contextual relationships are desirable to the scene understanding.</p><p>In our study, we propose a Context Prior to model the relationships between pixels of the same category (intra-context) and pixels of the different categories (intercontext). Based on the Context Prior, we propose a Context Prior Network, incorporating a Context Prior Layer with the supervision of an Affinity Loss, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this section, we first introduce the Affinity Loss, which supervises the layer to learn a Context Prior Map. Next, we demonstrate the Context Prior Layer, which uses the learned Context Prior Map to aggregate the intra-context and intercontext for each pixel. The Aggregation Module is designed to aggregate the spatial information for reasoning. Finally, we elaborate on our complete network structure. The downsampled ground truth L is first encoded with the one-hot encoding. The size of the ground truth L becomes H × W × C, where C is the number of the classes. Each vector in L is composed of a single high value (1) and all the others low (0). We conduct A = L L to generate the Ideal Affinity Map. In this map, the green box and blue box represent 1 and 0, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Affinity Loss</head><p>In the scene segmentation task, for each image, we have one ground truth, which assigns a semantic category for each pixel. It is hard for the network to model the contextual information from isolated pixels. To explicitly regularize the network to model the relationship between categories, we introduce an Affinity Loss. For each pixel in the image, this loss forces the network to consider the pixels of the same category (intra-context) and the pixels among the different categories (inter-context).</p><p>Given a ground truth for an input, we can know the "context prior" of each pixel (i.e., which pixels belong to the same category and which pixels do not). Therefore, we can learn a Context Prior to guiding the network according to the ground truth. To this end, we first construct an Ideal Affinity Map from the ground truth as the supervision. Given an input image I and the ground truth L, we feed the input image I to the network, obtaining a feature map X of size H × W . As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we first down-sample the ground truth L into the same size of the feature map X, yielding a smaller ground truth L. We use a one-of-K scheme (one-hot encoding) to encode each categorical integer label in the ground truth L, leading to a matrix L of H ×W ×C size, where C is the number of classes. Next, we reshape the encoded ground truth to N × C size, in which N = H ×W . Finally, we conduct the matrix multiplication: A = L L . A is our desired Ideal Affinity Map with size N ×N , which encodes which pixels belong to the same category. We employ the Ideal Affinity Map to supervise the learning of Context Prior Map.</p><p>For each pixel in the prior map, it is a binary classification problem. A conventional method for addressing this problem is to use the binary cross entropy loss. Given the predicted Prior Map P of size N × N , where {p n ∈ P , n ∈ [1, N 2 ]} and the reference Ideal Affinity Map A, where {a n ∈ A, n ∈ [1, N 2 ]}, the binary cross entropy loss can be denoted as:</p><formula xml:id="formula_0">L u = − 1 N 2 N 2 n=1</formula><p>(a n log p n + (1 − a n ) log (1 − p n )). <ref type="formula">(1)</ref> However, such a unary loss only considers the isolated pixel in the prior map ignoring the semantic correlation with other pixels. The pixels of each row of the Prior Map P is corresponding to the pixels of the feature map X. We can divide them into intra-class pixels and inter-class pixels, the relationships of which are helpful to reason the semantic correlation and scene structure. Therefore, we can consider the intra-class pixels and inter-class pixels as two wholes to encode the relationships respectively. To this end, we devise the global term based on the binary cross entropy loss:</p><formula xml:id="formula_1">T p j = log N i=1 a ij p ij N i=1 p ij ,<label>(2)</label></formula><formula xml:id="formula_2">T r j = log N i=1 a ij p ij N i=1 a ij ,<label>(3)</label></formula><formula xml:id="formula_3">T s j = log N i=1 (1 − a ij )(1 − p ij ) N i=1 (1 − a ij ) ,<label>(4)</label></formula><formula xml:id="formula_4">L g = − 1 N N j=1 (T p j + T r j + T s j ),<label>(5)</label></formula><p>where T p j , T r j , and T s j represent the intra-class predictive value (precision), true intra-class rate (recall), and true interclass rate (specificity) at j th row of P , respectively. Finally,  based on both the unary term and global term, the complete Affinity Loss can be denoted as follows:</p><formula xml:id="formula_5">3x3 Conv DWConv DWConv DWConv DWConv BN ReLU BN ReLU k × 1 1 × k 1 × k k × 1 FSConv FSConv 1 × k k × 1 k × 1 1 × k (a)</formula><formula xml:id="formula_6">L p = λ u L u + λ g L g ,<label>(6)</label></formula><p>where L p , L u , and L g represent the affinity loss, unary loss (binary cross entropy loss), and global loss functions, respectively. In addition, λ u and λ g are the balance weights for the unary loss and global loss, respectively. We empirically set the weights as: λ u = 1 and λ g = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Prior Layer</head><p>Context Prior Layer considers an input feature X with the shape of H × W × C 0 , as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We adopt an aggregation module to adapt X to X with the shape of H × W × C 1 . Given X, one 1 × 1 convolution layer followed by a BN layer <ref type="bibr" target="#b19">[20]</ref> and a Sigmoid function is applied to learn a prior map P with the size H × W × N (N = H × W ). With the explicit supervision of the Affinity Loss, Context Prior Map P can encode the relationship between intra-class pixels and interclass pixels. The intra-class is given by Y = P X, where X is reshaped into N × C 1 size. In this operator, the prior map can adaptively select the intra-class pixels as the intra-class context for each pixel in the feature map. On the other hand, the reversed prior map is applied to selectively highlight the inter-class pixels as the inter-class context: Y = (1 − P ) X, where 1 is an all-ones matrix with the same size of P . Finally, we concatenate the original feature and both kinds of context to output the final prediction: F = Concat(X, Y , Y ). With both context, we can reason the semantic correlation and scene structure for each pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Aggregation Module</head><p>As discussed in Section 1, the Context Prior Map requires some local spatial information to reason the semantic correlation. Therefore, we devise an efficient Aggregation Module with the fully separable convolution (separate on both the spatial and depth dimensions) to aggregate the spatial information. The convolution layer can inherently aggregate nearby spatial information. A natural method to aggregate more spatial information is to use the a large filter size convolutions. However, convolutions with large filter size are computationally expensive. Therefore, similar to <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b32">32]</ref>, we factorize the standard convolution into two asymmetric convolutions spatially. For a k × k convolution, we can use a k ×1 convolution followed by a 1×k convolution as the alternative, termed spatial separable convolution. It can decrease k 2 computation and keep the equal size of receptive filed in comparison to the standard convolution. Meanwhile, each spatial separable convolution adopts the depth-wise convolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b13">14]</ref>, further leading to the computation decrease. We call this separable convolution as Fully Separable Convolution with consideration both the spatial and depth dimensions. <ref type="figure" target="#fig_3">Figure 4</ref> demonstrates the complete structure of the Aggregation Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>The Context Prior Network (CPNet) is a fully convolutional network composed of a backbone network and a Context Prior Layer, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The backbone network is an off-the-shelf convolutional network <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b35">35]</ref>, e.g., ResNet <ref type="bibr" target="#b12">[13]</ref>, with the dilation strategy <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45]</ref>. In the Context Prior Layer, the Aggregation Module first aggregates some spatial information efficiently. Based on the aggregated spatial information, the Context Prior Layer learns a context prior map to capture intra-class context and inter-class context. Meanwhile, the Affinity Loss regularizes the learning of Context Prior, while the cross-entropy loss function is the segmentation supervision. Following the pioneering work <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45]</ref>, we employ the auxiliary loss on stage 4 of the backbone network, which is also a cross-entropy loss. The final loss function is as follows:</p><formula xml:id="formula_7">L = λ s L s + λ a L a + λ p L p ,<label>(7)</label></formula><p>where L s , L a , and L p represent the main segmentation loss, auxiliary loss, and affinity loss functions, respectively. In addition, λ s , λ a , and λ p are the weights to balance the segmentation loss, auxiliary loss, and affinity loss, respectively. We empirically set the weights as: λ s = 1 and λ p = 1. Similar to <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45]</ref>, we set the weight: λ a = 0.4,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first introduce the implementation and training details of the proposed network. Next, we eval-uate the proposed method and compare it with state-ofthe-art approaches on three challenging scene segmentation datasets, including ADE20K <ref type="bibr" target="#b52">[52]</ref>, PASCAL-Context <ref type="bibr" target="#b30">[30]</ref>, and Cityscapes <ref type="bibr" target="#b7">[8]</ref>. We implement the proposed model using PyTorch <ref type="bibr" target="#b31">[31]</ref> toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network. We adopt the ResNet <ref type="bibr" target="#b12">[13]</ref> as our pre-trained model with dilation strategy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. Then we adopt the bilinear interpolation to up-sample the prediction eight times to compute the segmentation loss. Following <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b45">45]</ref>, we integrate the auxiliary loss on stage 4 of the backbone network. We set the filter size of the fully separable convolution in the Aggregation Module as 11.</p><p>Data Augmentation. In the training phase, we apply the mean subtraction, random horizontal flip and random scale, which contains {0.5, 0.75, 1.0, 1.5, 1.75, 2.0}, on the input images in avoiding of overfitting. Finally, we randomly crop the large image or pad the small image into a fix size for training (480 × 480 for ADE20K, 512 × 512 for PASCAL-Context and 768 × 768 for Cityscapes).</p><p>Optimization. We fine-tune the CPNet model using the stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b21">[22]</ref> with 0.9 momentum, 10 −4 weight decay and 16 batch size. Notably, we set the weight decay as 5 × 10 −4 when training on the Cityscapes dataset. Following the pioneering work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b42">42]</ref>, we adopt the "poly" learning rate strategy γ = γ 0 × (1 − Niter N total ) p , where N iter and N total represent the current iteration number and total iteration number, and p = 0.9. We set the base learning rate γ 0 as 2×10 −2 for the experiments on ADE20K, while 1 × 10 −2 for the experiments on PASCAL-Context and Cityscapes. Meanwhile, we train the model for 80K iterations on ADE20K, 25K for PASCAL-Context and 60K for Cityscapes. We use the standard cross entropy loss when training on the ADE20K and PASCAL-Context dataset. While training on Cityscapes, similar to <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44]</ref>, we adopt the bootstrapped crossentropy loss <ref type="bibr" target="#b38">[38]</ref> to mitigate the class imbalance problem in this dataset.</p><p>Inference. In the inference phase, following <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45]</ref>, we average the predictions of multiple scaled and flipped inputs to further improve the performance. We use the scales including {0.5, 0.75, 1.0, 1.5, 1.75} for the ADE20K and PASCAL-Context datasets, while {0.5, 0.75, 1, 1.5} for the Cityscapes dataset. In addition, we adopt the pixel accuracy (pixAcc) and mean intersection of union (mIoU) as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluations on the ADE20K Dataset</head><p>Dataset description. ADE20K is a challenging scene parsing benchmark due to its complex scene and up to 150 category labels. This dataset can be divided into 20K/2K/3K for training, validation and testing respectively. We report the results on the validation set using pixAcc and mIoU.</p><p>Ablation studies. To demonstrate the effectiveness of our Context Prior and CPNet, we conduct the experiments with different settings and compared with other spatial information aggregation module, as shown in <ref type="table">Table 1</ref>. First, we introduce our baseline model. We evaluate the FCN <ref type="bibr" target="#b27">[28]</ref> model with dilated convolution <ref type="bibr" target="#b0">[1]</ref> based on ResNet-50 <ref type="bibr" target="#b12">[13]</ref> on the validation set. Following <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50]</ref>, we add the auxiliary loss on stage 4 of the ResNet backbone. This can improve mIoU by 1.86% (34.38% → 36.24%) and pixAcc by 0.86% (76.51% → 77.37%). We adopt this model as our baseline.</p><p>Based on the features extracted by FCN, various methods aggregate contextual information to improve the performance. The pyramid-based methods (e.g., PSP and ASPP) adopts pyramid pooling or pyramid dilation rates to aggregate multi-range spatial information. Recent approaches <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b10">11]</ref> apply the self-attention <ref type="bibr" target="#b37">[37]</ref> method to aggregate the long-range spatial information, while the PSA module <ref type="bibr" target="#b50">[50]</ref> learns over-parametric point-wise attention. <ref type="table">Table 1</ref> lists our reimplement results with different spatial information aggregation modules. While these methods can improve the performance over the baseline, they aggregate the spatial information as a mixture of the intra-class and inter-class context, maybe making the network confused, as discussed in Section 1. Therefore, different from these methods, the proposed CPNet considers the contextual dependencies as a Context Prior to encoding the identified contextual relationship. Specifically, for each pixel, we capture the intra-class context and inter-class context with the Context Prior Layer. With the same backbone ResNet-50 and without other testing tricks, our method performs favorably against these methods.  We also investigate the effectiveness of the Aggregation Module, IntraPrior branch, InterPrior branch and Affinity Loss in our CPNet model. We use the Aggregation Module with filter size 11 to aggregate the local spatial information. Similar to <ref type="bibr" target="#b50">[50]</ref>, the Aggregation Module generates an attention mask with the resolution of N × N (N = H × W ) to refine the prediction. As shown in <ref type="table">Table 1</ref>, the Aggregation Module improves the mIoU and pixAcc by 5.27% / 2.56% over the baseline model. With the IntraPrior branch based on the binary cross entropy loss, our single scale testing results obtain 42.34% / 80.15% in terms of mIoU and pixAcc, surpassing the baseline by 6.1% / 2.78%. On the other hand, the InterPrior branch achieves 42.88% / 79.96% with the same setting. Both of the significant improvements demonstrate the effectiveness of the proposed Context Prior.</p><p>To further improve the quality of the context prior map, we devise an Affinity Loss. <ref type="table">Table 1</ref> indicates that the Affinity Loss can improve the mIoU and pixAcc by 0.4% / 0.15% based on IntraPrior branch, while boosting 0.55% / 0.25% based on InterPrior branch. We integrate both IntraPrior branch and InterPrior branch with the Affinity Loss to achieve 43.92% mIoU and 80.77% pixAcc, which demonstrates that both priors can be complementary. To further improve the performance, we apply the multi-scale and flipped testing strategy to achieve 44.46% mIoU and 81.38% pixAcc. Deeper network leading to better feature representation, our CPNet obtains 45.39% mIoU and 81.04% pixAcc with the ResNet-101. With the testing strategy, our model based on ResNet-101 achieves 46.27% mIoU and 81.85% pixAcc. <ref type="figure" target="#fig_4">Figure 5</ref> provides some visualization examples.</p><p>Analysis and discussion. In <ref type="table">Table 1</ref>, the proposed CPNet achieves considerable improvement on the ADE20K benchmark. Someone may argue that the large filter size of the Aggregation Module leads to the performance gain. Or one may question whether the Context Prior can generalize to other algorithms. We thus provide more evidence to thoroughly understand the Context Prior. We conduct the discussion experiments on the ADE20K validation set with ResNet-50 backbone. The results reported in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref> are the single scale testing results. <ref type="bibr" target="#b0">(1)</ref> The influence between the spatial information and Context Prior. As discussed in Section 3, the distinguished contextual dependencies are helpful to scene understanding. Therefore, we propose a Context Prior to model the intracontext and inter-context. Meanwhile, the Context Prior requires some spatial information to reason the relationship. To this end, we integrate an Aggregation Module in the Context Prior Layer. <ref type="table" target="#tab_3">Table 2</ref> indicates that with the increasing filter size, the models without Context Prior obtain the close results. However, with Context Prior, each model achieves improvements steadily. Meanwhile, the improvements gradually increase with the increasing filter size. When the filter size is 11, the performance (43.92% mIoU) and the relative gain (2.41%) reach the peak. If we continue to increase the filter size, the performance and the corresponding improvement both drop. In other words, Context Prior requries appropriate local spatial information to reason the relationships.</p><p>(2) Generalization to other spatial information aggregation module. To validate the generalization ability of the proposed Context Prior, we further replace the Aggregation Module with PPM or ASPP module to generate Context Prior Map with the supervision of Affinity Loss. As shown in <ref type="table" target="#tab_4">Table 3</ref>, Context Prior can further improve the mIoU by 1.06% over the PPM without Context Prior, 2.3% over Visualization of prior maps. To get a deeper understanding of our Context Prior, we randomly choose some examples from the ADE20K validation set and visualize the learned Context Prior Maps in <ref type="figure" target="#fig_5">Figure 6</ref>. We use the Aggregation Module to generate the attention map without the guidance of the Affinity Loss. Compared with the Ideal Affinity Map, we observe this attention map actually has a rough trend to learn this relationship. With the Affinity Loss, our Context Prior Layer can learn a prior map with more explicit structure information, which helps to refine the prediction.</p><p>Comparison with state-of-the-art. We conduct the comparison experiments with other state-of-the-art algorithms on  as the backbone. This significant improvement manifests the effectiveness of our Context Prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations on PASCAL-Context</head><p>Dataset description. PASCAL-Context <ref type="bibr" target="#b30">[30]</ref> is a scene understanding dataset which contains 10, 103 images from PASCAL VOC 2010. These images are re-annotated as pixel-wise segmentation maps with consideration of both the stuff and thing categories. This dataset can be divided into 4, 998 images for training and 5, 105 images for testing. The most common 59 categories are used for evaluation.</p><p>Comparison with state-of-the-art. <ref type="table" target="#tab_6">Table 5</ref> shows the performance comparison with other state-of-the-art approaches. Our algorithm achieves 53.9% mIoU on validation set and outperforms state-of-the-art EncNet by over 1.0 point. Similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b10">11]</ref>, we evaluate the model with the multi-scale and flipped testing strategy. The scales contain {0.5, 0.75, 1, 1  <ref type="table">Table 6</ref>. Quantitative evaluations on the Cityscapes test set. The proposed CPNet performs favorably against state-of-the-art segmentation methods. We only list the methods training with merely the fine dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluations on Cityscapes</head><p>Dataset description. Cityscapes <ref type="bibr" target="#b7">[8]</ref> is a large urban street scene parsing benchmark. It contains 2, 975 fine annotation images for training, 500 images for validation, 1, 525 images for testing and extra 20, 000 coarsely annotated images for training. We only use the fine annotation set in our experiments. It includes 19 categories for evaluation.</p><p>Comparison with state-of-the-art. <ref type="table">Table 6</ref> lists the performance results of other state-of-the-art methods and our CPNet. We adopt the multi-scale and flipped testing strategy on our experiments. Following the pioneering work <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b42">42]</ref>, we train our model with both the train-fine set and val-fine set to improve the performance on the test set. Our CPNet achieves 81.3% mIoU on the Cityscapes test set only with the fine dataset, which outperforms the DenseASPP based on DenseNet-161 <ref type="bibr" target="#b16">[17]</ref> by 0.9 point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>In this work, we construct an effective Context Prior for scene segmentation. It distinguishes the different contextual dependencies with the supervision of the proposed Affinity Loss. To embed the Context Prior into the network, we present a Context Prior Network, composed of a backbone network and a Context Prior Layer. The Aggregation Module is applied to aggregate spatial information for reasoning the contextual relationship and embedded into the Context Prior Layer. Extensive quantitative and qualitative comparison shows that the proposed CPNet performs favorably against recent state-of-the-art scene segmentation approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed Context Prior Layer. The Context Prior Layer contains an Aggregation Module and a Context Prior Map supervised by Affinity Loss. With the extracted input features, the Aggregation Module aggregates the spatial information to reason the contextual relationship. We generate a point-wise Context Prior Map with the supervision of an Affinity Loss. The Affinity Loss constructs an Ideal Affinity Map which indicates the pixels of the same category to supervise the learning of the Context Prior Map. Based on the Context Prior Map, we can obtain the intra-prior (P ) and inter-prior (1 − P ). The original feature map is reshaped to N × C1 size, where N = H × W . We conduct matrix multiplication on the reshaped feature map with P and (1 − P ) to capture the intra-class and inter-class context. Finally, we feed the representation of the Context Prior Layer into the last convolutional layer to generate a perpixel prediction. (Notation: Aggregation Aggregation Module, Conv convolutional layer, matrix multiplication, P Context Prior Map, Concat concatenate operation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F igure 3 .</head><label>3</label><figDesc>Illustration of the construction of the Ideal Affinity Map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Aggregation Module (b) Receptive filed of Aggregation Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the Aggregation Module and its receptive field. (a) We use two asymmetric fully separable convolutions to aggregate the spatial information, the output of which has the same channels with the input features. (b) The Aggregation Module has the same size of receptive field with the standard convolution. However, our Aggregation Module leads to less computation. (Notation: Conv standard convolution, DWConv depthwise convolution FSConv fully separable convolution, k the filter size of the fully separable convolution, BN batch normalization, ReLU relu non-linear activation function.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) Input Image (b) Ground Truth (c) FCN (d) CPNet (ours) Visual improvement on validation set of ADE20K. Harvesting the intra-class context and inter-class context is helpful to the scene understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>(a) Attention Map (b) Learned Prior Map (c) Ideal Affinity Map Visualization of the Prior Map predicted by our CP-Net. (a) We only use the Aggregation Module to generate an attention map without the supervision of the Affinity Loss. (b) With the guidance of the Affinity Loss, the Context Prior Layer can capture the intra-class context and inter-class context. (c) The Ideal Affinity Map is constructed from the ground truth. Deeper color denotes higher response. the ASPP module and 2.41% over our Aggregation Module. This improvement demonstrates the effectiveness and generalization ability of our Context Prior. Besides, without Context Prior, our Aggregation Module also achieves the highest performance comparing to the PPM and ASPP module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Aux auxiliary loss, BCE binary cross entropy loss, AL Affinity Loss, MS multi-scale and flip testing strategy.)</figDesc><table><row><cell>model</cell><cell>mIoU</cell><cell>pixAcc</cell></row><row><cell>ResNet-50 (Dilation)</cell><cell>34.38</cell><cell>76.51</cell></row><row><cell>ResNet-50 + Aux (Baseline)</cell><cell>36.24</cell><cell>77.37</cell></row><row><cell>ResNet-50 + ASPP</cell><cell>40.39</cell><cell>79.71</cell></row><row><cell>ResNet-50 + PSP</cell><cell>41.49</cell><cell>79.61</cell></row><row><cell>ResNet-50 + NonLocal</cell><cell>40.96</cell><cell>79.98</cell></row><row><cell>ResNet-50 + PSA</cell><cell>41.92</cell><cell>80.17</cell></row><row><cell>ResNet-50 + Aggregation Module</cell><cell>41.51</cell><cell>79.93</cell></row><row><cell>ResNet-50 + IntraPrior (BCE)</cell><cell>42.34</cell><cell>80.15</cell></row><row><cell>ResNet-50 + InterPrior (BCE)</cell><cell>41.88</cell><cell>79.96</cell></row><row><cell>ResNet-50 + IntraPrior (AL)</cell><cell>42.74</cell><cell>80.30</cell></row><row><cell>ResNet-50 + InterPrior (AL)</cell><cell>42.43</cell><cell>80.21</cell></row><row><cell>ResNet-50 + ContextPriorLayer</cell><cell>43.92</cell><cell>80.77</cell></row><row><cell>ResNet-50 + ContextPriorLayer MS</cell><cell>44.46</cell><cell>81.38</cell></row><row><cell>ResNet-101 + ContextPriorLayer</cell><cell>45.39</cell><cell>81.04</cell></row><row><cell>ResNet-101 + ContextPriorLayer MS</cell><cell>46.27</cell><cell>81.85</cell></row><row><cell cols="3">Table 1. Ablative studies on the ADE20K [52] validation set</cell></row><row><cell cols="3">in comparison to other contextual information aggregation ap-</cell></row><row><cell>proaches. (Notation:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>CP 42.06 41.86 41.87 42.32 41.51 42.34 42.23 w/ CP 42.26 42.81 43.38 43.14 43.92 42.54 42.59 Experimental results (mIoU) w/ or w/o Context Prior based on different kernel sizes. (Notation: k the kernel size of the fully separable convolution, ∆ the improvement of introducing the Context Prior, CP Context Prior.)</figDesc><table><row><cell>k</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>11</cell><cell>13</cell><cell>15</cell></row><row><cell>w/o ∆</cell><cell>0.2</cell><cell>0.95</cell><cell>1.51</cell><cell>0.82</cell><cell>2.41</cell><cell>0.2</cell><cell>0.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PPM</cell><cell>ASPP</cell><cell>AM</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">w/o CP</cell><cell>41.49</cell><cell>40.39</cell><cell>41.51</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">w/ CP</cell><cell>42.55</cell><cell>42.69</cell><cell>43.92</cell><cell></cell><cell></cell></row><row><cell></cell><cell>∆</cell><cell></cell><cell>↑1.06</cell><cell>↑2.3</cell><cell>↑2.41</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Generalization to the PPM and ASPP module. The evaluation metric is mIoU (%). (Notation: PPM pyramid pooling module, ASPP atrous spatial pyramid pooling, CP Context Prior, AM: Aggregation Module.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The proposed CPNet achieves 46.27% mIoU and 81.85% pixAcc, which performs favorably against previous state-of-the-art methods, even exceeds the winner entry of the COCO-Place Challenge 2017 based on ResNet-269. Our CPNet50 (with ResNet-50 as the backbone) achieves 44.46% mIoU and 81.38% pixAcc, even outper-</figDesc><table><row><cell>forms PSPNet [49], PSANet [50] and SAC [47] with deeper</cell></row><row><cell>ResNet-101 and RefineNet with much deeper ResNet-152</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Quantitative evaluations on the PASCAL-Context validation set. The proposed CPNet performs favorably against stateof-the-art segmentation methods. † means the method uses extra dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.5, 1.75}.</figDesc><table><row><cell>model</cell><cell>reference</cell><cell>backbone</cell><cell>mIoU</cell></row><row><cell>RefineNet [25]</cell><cell>CVPR2017</cell><cell>ResNet-101</cell><cell>73.6</cell></row><row><cell>GCN [32]</cell><cell>CVPR2017</cell><cell>ResNet-101</cell><cell>76.9</cell></row><row><cell>DUC [36]</cell><cell>WACV2018</cell><cell>ResNet-101</cell><cell>77.6</cell></row><row><cell>DSSPN [24]</cell><cell>CVPR2018</cell><cell>ResNet-101</cell><cell>77.8</cell></row><row><cell>SAC [47]</cell><cell>ICCV2017</cell><cell>ResNet-101</cell><cell>78.1</cell></row><row><cell>PSPNet [49]</cell><cell>CVPR2017</cell><cell>ResNet-101</cell><cell>78.4</cell></row><row><cell>BiSeNet [42]</cell><cell>ECCV2018</cell><cell>ResNet-101</cell><cell>78.9</cell></row><row><cell>AAF [21]</cell><cell>ECCV2018</cell><cell>ResNet-101</cell><cell>79.1</cell></row><row><cell>DFN [43]</cell><cell>CVPR2018</cell><cell>ResNet-101</cell><cell>79.3</cell></row><row><cell>PSANet [50]</cell><cell>ECCV2018</cell><cell>ResNet-101</cell><cell>80.1</cell></row><row><cell>DenseASPP [40]</cell><cell>CVPR2018</cell><cell>DenseNet-161</cell><cell>80.6</cell></row><row><cell>ANL [53]</cell><cell>ICCV2019</cell><cell>ResNet-101</cell><cell>81.3</cell></row><row><cell>CPNet101</cell><cell>-</cell><cell>ResNet-101</cell><cell>81.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="0">2 1 1 0 0 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Natural Science Foundation of China (No. 61433007 and 61876210).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aˆ2-nets: Double attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene parsing with global context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive affinity fields for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Tsung-Wei Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamicstructured semantic propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6165" to="6174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Parsenet: Looking wider to see better. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Gyu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5683" to="5692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PSANet: Pointwise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
