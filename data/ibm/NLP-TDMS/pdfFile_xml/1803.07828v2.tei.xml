<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Expeditious Generation of Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-09">9 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Soru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ruberto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Valdestilhas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bigerl</surname></persName>
							<email>alexander.bigerl@uni-paderborn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgard</forename><surname>Marx</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Esteves</surname></persName>
							<email>esteves@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Soru</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Edgard</roleName><forename type="first">André</forename><surname>Valdestilhas</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marx</forename><surname>Aksw</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ruberto</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DICE</orgName>
								<orgName type="institution">Paderborn University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bigerl</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">SDA</orgName>
								<orgName type="institution" key="instit2">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Esteves</surname></persName>
							<affiliation key="aff4">
								<address>
									<addrLine>K I T S C I E N T I F I C P U B L I S H I N G</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Expeditious Generation of Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-09">9 Nov 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.5445/KSP/XXXXXXXX/XX</idno>
					<note>A R C H I V E S O F D ATA S C I E N C E , S E R I E S A ( O N L I N E F I R S T )</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was accepted for presentation at the 5th European Conference on Data Analysis (ECDA 2018) under the title "A Simple and Fast Approach to Knowledge Graph Embedding".</p><p>Abstract Knowledge Graph Embedding methods aim at representing entities and relations in a knowledge base as points or vectors in a continuous vector space. Several approaches using embeddings have shown promising results on tasks such as link prediction, entity recommendation, question answering, and triplet classification. However, only a few methods can compute low-dimensional embeddings of very large knowledge bases without needing</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>state-of-the-art computational resources. In this paper, we propose KG2Vec, a simple and fast approach to Knowledge Graph Embedding based on the skipgram model. Instead of using a predefined scoring function, we learn it relying on Long Short-Term Memories. We show that our embeddings achieve results comparable with the most scalable approaches on knowledge graph completion as well as on a new metric. Yet, KG2Vec can embed large graphs in lesser time by processing more than 250 million triples in less than 7 hours on common hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, the number of public datasets in the Linked Data cloud has significantly grown to almost 10 thousands. At the time of writing, at least four of these datasets contain more than one billion triples each. 1 This huge amount of available data has become a fertile ground for Machine Learning and Data Mining algorithms. Today, applications of machine-learning techniques comprise a broad variety of research areas related to Linked Data, such as Link Discovery, Named Entity Recognition, and Structured Question Answering. The field of Knowledge Graph Embedding (KGE) has emerged in the Machine Learning community during the last five years. The underlying concept of KGE is that in a knowledge base, each entity and relation can be regarded as a vector in a continuous space. The generated vector representations can be used by algorithms employing machine learning, deep learning, or statistical relational learning to accomplish a given task. Several KGE approaches have already shown promising results on tasks such as link prediction, entity recommendation, question answering, and triplet classification <ref type="bibr" target="#b25">(Xiao et al, 2015;</ref><ref type="bibr">Lin et al, 2015a,b;</ref><ref type="bibr" target="#b17">Nickel et al, 2016)</ref>. Moreover, Distributional Semantics techniques (e.g., Word2Vec or Doc2Vec) are relatively new in the Semantic Web community. The RDF2Vec approaches <ref type="bibr" target="#b19">(Ristoski and Paulheim, 2016;</ref><ref type="bibr" target="#b3">Cochez et al, 2017a)</ref> are examples of pioneering research and to date, they represent the only option for learning embeddings on a large knowledge graph without the need for state-of-the-art hardware. To this end, we devise the KG2Vec approach, which comprises skip-gram techniques for creating embeddings on large knowledge graphs in a feasible time but still maintaining the quality of state-of-the-art embeddings. Our evaluation shows that KG2Vec achieves a vector quality com-parable to the most scalable approaches and can process more than 250 million triples in less than 7 hours on a machine with suboptimal performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>An early effort to automatically generate features from structured knowledge was proposed in <ref type="bibr" target="#b2">(Cheng et al, 2011)</ref>. RESCAL <ref type="bibr" target="#b14">(Nickel et al, 2011</ref>) is a relational-learning algorithm based on Tensor Factorization using Alternating Least-Squares which has showed to scale to large RDF datasets such as YAGO <ref type="bibr" target="#b15">(Nickel et al, 2012)</ref> and reach good results in the tasks of link prediction, entity resolution, or collective classification <ref type="bibr" target="#b16">(Nickel et al, 2014)</ref>. Manifold approaches which rely on translations have been implemented so far <ref type="bibr" target="#b1">(Bordes et al, 2013;</ref><ref type="bibr" target="#b24">Wang et al, 2014b;</ref><ref type="bibr" target="#b5">Jia et al, 2015;</ref><ref type="bibr" target="#b11">Lin et al, 2015b;</ref><ref type="bibr" target="#b22">Wang et al, 2015;</ref><ref type="bibr" target="#b25">Xiao et al, 2015)</ref>. TransE is the first method where relationships are interpreted as translations operating on the low-dimensional embeddings of the entities <ref type="bibr" target="#b1">(Bordes et al, 2013)</ref>. On the other hand, TransH models a relation as a hyperplane together with a translation operation on it <ref type="bibr" target="#b24">(Wang et al, 2014b)</ref>. TransA explores embedding methods for entities and relations belonging to two different knowledge graphs finding the optimal loss function <ref type="bibr" target="#b5">(Jia et al, 2015)</ref>, whilst PTransE relies on paths to build the final vectors <ref type="bibr" target="#b10">(Lin et al, 2015a)</ref>. The algorithms TransR and CTransR proposed in <ref type="bibr" target="#b11">Lin et al (2015b)</ref> aim at building entity and relation embeddings in separate entity space and relation spaces, so as to learn embeddings through projected translations in the relation space; an extension of this algorithm makes use of rules to learn embeddings . An effort to jointly embed structured and unstructured data (such as text) was proposed in <ref type="bibr" target="#b23">Wang et al (2014a)</ref>. The idea behind the DistMult approach is to consider entities as low-dimensional vectors learned from a neural network and relations as bilinear and/or linear mapping functions <ref type="bibr" target="#b26">Yang et al (2014)</ref>. TransG, a generative model address the issue of multiple relation semantics of a relation, has showed to go beyond state-of-the-art results <ref type="bibr" target="#b25">(Xiao et al, 2015)</ref>. ComplEx is based on latent factorization and, with the use of complex-valued embeddings, it facilitates composition and handles a large variety of binary relations <ref type="bibr" target="#b21">Trouillon et al (2016)</ref>. The fastText algorithm was meant for word embeddings, however <ref type="bibr" target="#b6">Joulin et al (2017)</ref> showed that a simple bag-of-words can generate surprisingly good KGEs.</p><p>The field of KGE has considerably grown during the last two years, earning a spot also in the Semantic Web community. In 2016, <ref type="bibr" target="#b17">Nickel et al (2016)</ref> proposed HolE, which relies on holographic models of associative memory by employing circular correlation to create compositional representations. HolE can capture rich interactions by using correlation as the compositional operator but it simultaneously remains efficient to compute, easy to train, and scalable to large datasets. In the same year, <ref type="bibr" target="#b19">Ristoski and Paulheim (2016)</ref> presented RDF2Vec which uses language modeling approaches for unsupervised feature extraction from sequences of words and adapts them to RDF graphs. After generating sequences by leveraging local information from graph substructures by random walks, RDF2Vec learns latent numerical representations of entities in RDF graphs. The algorithm has been extended in order to reduce the computational time and the biased regarded the random walking <ref type="bibr" target="#b3">(Cochez et al, 2017a)</ref>. More recently, <ref type="bibr" target="#b4">Cochez et al (2017b)</ref> exploited the Global Vectors algorithm to compute embeddings from the co-occurrence matrix of entities and relations without generating the random walks. In following research, the authors refer to their algorithm as KGloVe. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KG2Vec</head><p>This study addresses the following research questions:</p><p>1. Can we generate embeddings at a high rate while preserving accuracy? 2. How can we test the distributional hypothesis of KGEs? 3. Can we learn a scoring function for knowledge base completion which performs better than the standard one?</p><p>Formally, let t = (s, p, o) be a triple containing a subject, a predicate, and an object in a knowledge base K. For any triple,</p><formula xml:id="formula_0">(s, p, o) ⊆ E × R × (E ∩ L),</formula><p>where E is the set of all entities, R is the set of all relations, and L is the set of all literals (i.e., string or numerical values). A representation function F defined as</p><formula xml:id="formula_1">F : (E ∩ R ∩ L) → R d<label>(1)</label></formula><p>assigns a vector of dimensionality d to an entity, a relation, or a literal. However, some approaches consider only the vector representations of entities or subjects (i.e, {s ∈ E : ∃(s, p, o) ∈ K}). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity <ref type="bibr" target="#b16">(Nickel et al, 2014;</ref><ref type="bibr" target="#b5">Jia et al, 2015;</ref><ref type="bibr" target="#b11">Lin et al, 2015b)</ref>. The reduction is performed through an embedding map Φ : R d → R k , which is a homomorphism that maps the initial vector space into a smaller, reduced space. The positive value k &lt; d is called the rank of the embedding. Note that each dimension of the reduced common space does not necessarily have an explicit connection with a particular relation. Dimensionality reduction methods include Principal Component Analysis techniques <ref type="bibr" target="#b16">(Nickel et al, 2014)</ref> and generative statistical models such as Latent Dirichlet Allocation <ref type="bibr" target="#b7">(Jurgens and Stevens, 2010;</ref><ref type="bibr" target="#b18">Rehůřek and Sojka, 2010)</ref>. Existing KGE approaches based on the skip-gram model such as RDF2Vec (Ristoski and Paulheim, 2016) submit paths built using random walks to a Word2Vec algorithm. Instead, we preprocess the input knowledge base by converting each triple into a small sentence of three words. Our method is faster as it allows us to avoid the path generation step. The generated text corpus is thus processed by the skip-gram model as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adapting the skip-gram model</head><p>We adapt the skip-gram model <ref type="bibr" target="#b12">(Mikolov et al, 2013a)</ref> to deal with our small sequences of length three. In this work, we only consider URIs and discard literals, therefore we compute a vector for each element u ∈ E ∩ R. Considering a triple as a sequence of three URIs T = {u s , u p , u o }, the aim is to maximize the average log probability</p><formula xml:id="formula_2">1 3 ∑ u∈T ∑ u ∈T \u log p(u|u )<label>(2)</label></formula><p>which means, in other words, to adopt a context window of 2, since the sequence size is always |T | = 3. The probability above is theoretically defined as:</p><formula xml:id="formula_3">p(u|u ) = exp(v O u v I u ) ∑ x∈E∩R exp(v O x v I u )<label>(3)</label></formula><p>where v I x and v O x are respectively the input and output vector representations of a URI x. We imply a negative sampling of 5, i.e. 5 words are randomly selected to have an output of 0 and consequently update the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Scoring by analogy</head><p>Several methods have been proposed to evaluate word embeddings. The most common ones are based on analogies <ref type="bibr" target="#b13">(Mikolov et al, 2013b;</ref><ref type="bibr" target="#b9">Levy and Goldberg, 2014)</ref>, where word vectors are summed up together, e.g.:</p><formula xml:id="formula_4">v["queen"] ≈ v["king"] + v["woman"] − v["man"]<label>(4)</label></formula><p>An analogy where the approximation above is satisfied within a certain threshold can thus predict hidden relationships among words, which in our environment means to predict new links among entities <ref type="bibr" target="#b19">Ristoski and Paulheim (2016)</ref>. The analogy-based score function for a given triple (s,p,ō) is defined as follows.</p><p>score(s,p,ō) = 1</p><formula xml:id="formula_5">|{(s,p, o) ∈ K}| ∑ (s,p,o)∈K 1 if vs + v o − v s − vō ≤ ε 0 otherwise (5)</formula><p>where ε is an arbitrarily small positive value. In words, given a predicatep, we select all triples where it occurs. For each triple, we compute the relation vector as the difference between the object and the subject vectors. We then count a match whenever the vector sum of subjects and relation is close to objectō within a radius ε. The score is equal to the rate of matches over the number of selected triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Scoring by neural networks</head><p>We evaluate the scoring function above against a neural network based on Long Short-Term Memories (LSTM). The neural network takes a sequence of embeddings as input, namely v s , v p , v o for a triple (s, p, o) ∈ K. A dense hidden layer of the same size of the embeddings is connected to a single output neuron with sigmoid activation, which returns a value between 0 and 1. The negative triples are generated using two strategies, i.e. for each triple in the training set (1) randomly extract a relation and its two nodes or (2) corrupt the subject or the object. We use the Adam optimizer and 100 epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metrics</head><p>As recently highlighted by several members of the ML and NLP communities, KGEs are rarely evaluated on downstream tasks different from link prediction (also known as knowledge base completion). Achieving high performances on link prediction does not necessarily mean that the generated embeddings are good, since the inference task is often carried out in combination with an external algorithm such as a neural network or a scoring function. The complexity is thus approach-dependent and distributed between the latent structure in the vector model and the parameters (if any) of the inference algorithm. For instance, a translational model such as <ref type="bibr">TransE Bordes et al (2013)</ref> would likely feature very complex embeddings, since in most approaches the inference function is a simple addition. On the other hand, we may find less structure in a tensor factorization model such as <ref type="bibr">RESCAL Nickel et al (2011)</ref>, as the inference is performed by a feed-forward neural network which extrapolates the hidden semantics layer by layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Neighbour Similarity Test</head><p>In this paper, we introduce two metrics inspired by The Identity of Indiscernibles <ref type="bibr" target="#b0">(Black, 1952)</ref> to gain insights over the distributional quality of the learned embeddings.</p><p>The more characteristics two entities share, the more similar they are and so should be their vector representations.</p><p>Considering the set of characteristics C K (s) = {(p 1 , o 1 ), . . . , (p m , o m )} of a subject s in a triple, we can define a metric that expresses the similarity among two entities e 1 , e 2 as the Jaccard index between their sets of characteristics C K (e 1 ) and C K (e 2 ). Given a set of entitiesẼ and their N nearest neighbours in the vector space, the overall Neighbour Similarity Test (NST) metric is defined as:</p><formula xml:id="formula_6">NST (Ẽ, N, K) = 1 N|Ẽ| ∑ e∈Ẽ N ∑ j=1 |C K (e) ∩C K (n (e) j )| |C K (e) ∪C K (n (e) j )| (6)</formula><p>where n (e) j is the jth nearest neighbour of e in the vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Type and Category Test</head><p>The second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 6 except for sets C K (e), which are replaced by sets of types and categories TC K (e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We implemented KG2Vec in Python 2.7 using the Gensim and Keras libraries with Theano environment. Source code, datasets, and vectors obtained are avail-able online. <ref type="bibr">3</ref> All experiments were carried out on an Ubuntu 16.04 server with 128 GB RAM and 40 CPUs. The dataset used in the experiments are described in <ref type="table" target="#tab_0">Table 1</ref>. The AKSW-bib dataset -employed for the link prediction evaluation -was created using information from people and projects on the AKSW.org website and bibliographical data from Bibsonomy. We built a model on top of the English 2015-10 version of the DBpedia knowledge graph <ref type="bibr" target="#b8">(Lehmann et al, 2009)</ref>; <ref type="figure" target="#fig_0">Figure 1</ref> shows a 3dimensional plot of selected entities. For the English DBpedia 2016-04 dataset, we built two models. In the first, we set a threshold to embed only the entities occurring at least 5 times in the dataset; we chose this setting to be aligned to the related works' models. In the second model, all 36 million entities in DBpedia are associated a vector. More insights about the first model can be found in the next two subsections, while the resource consumption for creating the second model can be seen in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Runtime</head><p>In this study, we aim at generating embeddings at a high rate while preserving accuracy. In <ref type="table" target="#tab_0">Table 1</ref>, we already showed that our simple pipeline can achieve a rate of almost 11, 000 triples per second on a large dataset such as DBpedia 2016-04. In <ref type="table" target="#tab_1">Table 2</ref>, we compare KG2Vec with three other scalable approaches for embedding knowledge bases. We selected the best settings of RDF2Vec and KGloVe according to their respective articles, since both algorithms had already been successfully evaluated on DBpedia <ref type="bibr" target="#b19">(Ristoski and Paulheim, 2016;</ref><ref type="bibr" target="#b4">Cochez et al, 2017b)</ref>. We also tried to compute fastText embeddings on our machine, however we had to halt the process after three days. As the goal of our investigation is efficiency, we discarded any other KGE approach that would have needed more than three days of computation to deliver the final model <ref type="bibr" target="#b4">(Cochez et al, 2017b)</ref>.</p><p>RDF2Vec has shown to be the most expensive in terms of disk space consumed, as the created random walks amounted to ∼300 GB of text. Moreover, we could not measure the runtime for the first phase of KGloVe, i.e. the calculation of the Personalized PageRank values of DBpedia entities. In fact, the authors used pre-computed entity ranks from <ref type="bibr" target="#b20">Thalhammer and Rettinger (2016)</ref> and the KGloVe source code does not feature a PageRank algorithm. We estimated the runtime comparing their hardware specs with ours. Despite being unable to reproduce any experiments from the other three approaches, we managed to evaluate their embeddings by downloading the pretrained models 4 and creating a KG2Vec embedding model of the same DBpedia dataset there employed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preliminary results on link prediction</head><p>For the link prediction task, we partition the dataset into training and test set with a ratio of 9:1. In <ref type="table" target="#tab_2">Table 3</ref>, we show preliminary results between the different strategies on the AKSW-bib dataset using KG2Vec embeddings. As can be seen, our LSTM-based scoring function significantly outperforms the analogybased one in both settings. According to the Hits@10 accuracy we obtained, corrupting triples to generate negative examples is the better strategy. This first insight can foster new research on optimizing a scoring function for KGE approaches based on distributional semantics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distributional quality</head><p>Computing the NST and TCT distributional quality metrics on the entire DBpedia dataset is time-demanding, since for each entity, the model and the graph need to be queried for the N nearest neighbours and their respective sets. However, we approximate the final value by tracing the partial values of NST and TCT over time. In other words, at each iteration i, we compute the metrics over E i = {e 1 , . . . , e i }. <ref type="figure">Figure 2</ref> shows the partial TCT value on the most important 10,000 entities for N = {1, 10} according to the ranks computed by <ref type="bibr" target="#b20">Thalhammer and Rettinger (2016)</ref>. Here, KG2Vec maintains a higher index than the other two approaches, despite these are steadily increasing after the ∼ 2, 000th entity. We interpret the lower TCT for the top 2, 000 entities as noise produced by the fact that these nodes are hyperconnected to the rest of the graph, therefore it is hard for them to remain close to their type peers. In <ref type="figure">Figures 3 and 4</ref>, the TCT and NST metrics respectively are computed on 10,000 random entities. In both cases, the values for the two settings of all approaches stabilize after around 1, 000 entities, however we clearly see that RDF2Vec embeddings achieve the highest distributional quality by type and category. The higher number of occurrences per entity in the huge corpus of random walks in RDF2Vec might be the reason of this result for rarer entities.</p><p>In <ref type="figure">Figure 5</ref>, we show the CPU, Memory, and disk consumption for KG2Vec on the larger model of DBpedia 2016-04. All three subphases of the algorithm are visible in the plot. For 2.7 hours, tokens are counted; then, the learning proceeds for 7.7 hours; finally in the last 2.3 hours, the model is saved.   <ref type="figure">Fig. 6</ref> We show the comparison of the run-times for all four approaches. Note that since we do not know how long the PageRank computation takes, we reported the estimated runtime for the plain version of KGloVe. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We presented a fast approach for generating KGEs dubbed KG2Vec. We conclude that the skip-gram model, if trained directly on triples as small sentences of length three, significantly gains in runtime while preserving a decent vector quality. Moreover, the KG2Vec embeddings have shown higher distributional quality for the most important entities in the graph according to PageRank. As a future work, we plan to extend the link prediction evaluation to other benchmarks by using analogies and our LSTM-based scoring function over the embedding models of the approaches here compared.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>A selection of DBpedia resources along with their vectors in 3 dimensions obtained using Principal Component Analysis. Blue points are resources, whilst red points are classes. As can be seen, resources follow the distributional hypothesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 Fig. 5</head><label>45</label><figDesc>Partial NST value on DBpedia 2016-04 for 10,000 random entities. CPU, Memory, and disk consumption forKG2Vec on the larger model of DBpedia 2016-04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Details and runtimes for the generation of KG2Vec embeddings on two datasets.</figDesc><table><row><cell>Dataset</cell><cell>AKSW-bib</cell><cell>DBpedia 2015-10</cell><cell cols="2">DBpedia 2016-04</cell></row><row><cell>Number of triples</cell><cell>3922</cell><cell>164,369,887</cell><cell>276,316,003</cell><cell>276,316,003</cell></row><row><cell>Number of vectors</cell><cell>954</cell><cell>14,921,691</cell><cell>23,816,469</cell><cell>36,596,967</cell></row><row><cell>Dimensionality</cell><cell>10</cell><cell>300</cell><cell>200</cell><cell>200</cell></row><row><cell>Runtime (s)</cell><cell>2.2</cell><cell>18,332</cell><cell>25,380</cell><cell>46,099</cell></row><row><cell>Rate (triples/s)</cell><cell>1,604</cell><cell>8,966</cell><cell>10,887</cell><cell>5,994</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Runtime comparison of the single phases. Those with (*) are estimated runtimes.</figDesc><table><row><cell cols="2">Approach Steps</cell><cell>Time</cell></row><row><cell>RDF2Vec</cell><cell cols="2">Random walks generation 123 minutes Word2Vec training &gt;96 hours (*)</cell></row><row><cell></cell><cell>Personalized PageRank</cell><cell>N/A</cell></row><row><cell>KGloVe</cell><cell cols="2">Co-occurrence count matrix 12 hours (*) GloVe training</cell></row><row><cell>KG2Vec</cell><cell>Conversion to text Word2Vec training</cell><cell>5 minutes 6 hours 58 minutes</cell></row><row><cell>fastText</cell><cell>Conversion to text fastText training</cell><cell>5 minutes &gt;72 hours (*)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Filtered Hits@10 values on link prediction on AKSW-bib using different strategies.</figDesc><table><row><cell></cell><cell>Hits@1</cell><cell>Hits@3</cell><cell>Hits@10</cell></row><row><cell>LSTM + corrupted</cell><cell>3.84%</cell><cell>9.79%</cell><cell>19.23%</cell></row><row><cell>LSTM + random</cell><cell>1.39%</cell><cell>4.89%</cell><cell>10.49%</cell></row><row><cell>Analogy</cell><cell>0.00%</cell><cell>0.51%</cell><cell>3.82%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://lodstats.aksw.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://datalab.rwth-aachen.de/embedding/KGloVe/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://github.com/AKSW/KG2Vec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://data.dws.informatik.uni-mannheim.de/rdf2vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The identity of indiscernibles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">242</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated feature generation from structured knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM, ACM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1395" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biased graph walks for rdf graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics</title>
		<meeting>the 7th International Conference on Web Intelligence, Mining and Semantics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Global rdf vector space embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="190" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno>arXiv:151201370</idno>
		<title level="m">Locally adaptive translation for knowledge graph embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast linear model for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>arXiv:171010881</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The s-space package: an open source package for word space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DBpedia -a crystallization point for the web of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth conference on computational natural language learning</title>
		<meeting>the eighteenth conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1506.00379</idno>
		<ptr target="http://arxiv.org/abs/1506.00379" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Burges C, Bottou L, Welling M, Ghahramani Z, Weinberger K</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>WWW, ACM</publisher>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>ELRA, Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rdf2vec: Rdf graph embeddings for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thalhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rettinger</surname></persName>
		</author>
		<title level="m">PageRank on Wikipedia: Towards General Importance Scores for Entities</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="227" to="240" />
		</imprint>
	</monogr>
	<note>The Semantic Web: ESWC 2016 Satellite Events</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1859" to="1865" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP, Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>AAAI, Citeseer</publisher>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>arXiv:150905488</idno>
		<title level="m">Transg: A generative mixture model for knowledge graph embedding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
