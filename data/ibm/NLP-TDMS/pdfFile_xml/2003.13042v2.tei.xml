<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omni-sourced Webly-supervised Learning for Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Omni-sourced Webly-supervised Learning for Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K minutes videos crawled from the internet without human labeling (less than 2% of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D-and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6% Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Following the great success of representation learning in image recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, recent years have witnessed great progress in video classification thanks to the development of stronger models <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">44]</ref> as well as the collection of largerscale datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b32">32]</ref>. However, labelling large-scale image datasets <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b61">61]</ref> is well known to be costly and time-consuming. It is even more difficult to do so for trimmed video recognition. The reason is that most online videos are untrimmed, i.e. containing numerous shots with multiple concepts, making it unavoidable to first go through the entire video and then manually cut it into informative video clips based on a specific query. Such procedure requires far more efforts than image annotation where a simple glance and click is needed. As a result, while the quantity of web videos grows exponentially over the past 3 years, the Kinetics dataset merely grows from 300K videos in 400 classes <ref type="bibr" target="#b19">[20]</ref> to 650K in 700 classes <ref type="bibr" target="#b1">[2]</ref>, partially limiting the scaling-up of video architectures <ref type="bibr" target="#b2">[3]</ref>.  <ref type="figure" target="#fig_0">Fig. 1</ref>: OmniSource Framework. We first train a teacher network on the target dataset. Then, we use the teacher network to filter collected web data of different formats, to reduce noise and improve data quality. Specific transformations are conducted on the filtered out data corresponding to their formats. The target dataset and auxiliary web datasets are used for joint training of the student network Instead of confining ourselves to the well-annotated trimmed videos, we move beyond by exploring the abundant visual data that are publicly available on the Internet in a more labor-saving way. These visual data are in various formats, including images, short video clips, and long videos. They capture the same visual world while exhibiting different advantages: e.g. images may be of higher quality and focus on distinctive moments; short videos may be edited by the user, therefore contain denser information; long videos may depict an event in multiple views. We transform the data in different formats into a unified form so that a single model can combine the best of both worlds.</p><p>Recent works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b12">13]</ref> explore the possibility of pre-training from massive unlabeled web images or videos only with hashtags. However, they restrict the scope to the data of a single format. Also, these methods usually require billions of images to obtain a pre-trained 2D CNN model that is resilient to noise, which poses great costs and restricts its practicability. Besides, to take advantage of representation learned from large-scale images for videos, we have to take extra steps to transfer the 2D ConvNets to the 3D counterparts, either by inflating <ref type="bibr" target="#b2">[3]</ref> or distillation <ref type="bibr" target="#b13">[14]</ref>, and then perform fine-tuning on the target dataset, which is tedious and may be suboptimal.</p><p>In this work, we propose a simple and unified framework for video classification while utilizing multiple sources of web data in different formats simultaneously. To enhance data efficiency, we propose task-specific data collection, i.e. obtaining topmost results using class labels as keywords on search engines, making the supervision most informative. Our framework consists of three steps: <ref type="bibr" target="#b0">(1)</ref> We train one (or more) teacher network on the labeled dataset; <ref type="bibr" target="#b1">(2)</ref> For each source of data collected, we apply the corresponding teacher network to obtain pseudo-labels and filter out irrelevant samples with low confidence; <ref type="bibr" target="#b2">(3)</ref> We apply different transforms to convert each type of web data (e.g. images) to the target input format (e.g. video clips) and train the student network.</p><p>There are two main obstacles during joint training with the labeled dataset and unlabeled web datasets. First, possible domain gaps occur. For example, web images may focus more on objects and contain less motion blur than videos. Second, teacher filtering may lead to unbalanced data distribution across different classes. To mitigate the domain gap, we propose to balance the size of training batches between the labeled dataset and unlabeled web datasets and apply cross-dataset mixup. To cope with data imbalance, we try several resampling strategies. All these techniques contribute to the success of our approach.</p><p>Compared to the previous methods, our method excels at the following aspects: (1) It leverages a mixture of web data forms, including images, trimmed videos and untrimmed videos into one student network, aiming at an omnisourced fashion. (2) It is data-efficient. Empirical results show that only 2M images, a significantly smaller amount compared to the total frame number of Kinetics (240K videos, ∼ 70M frames), are needed to produce notable improvements (about 1%). For trimmed videos, the required amount is around 0.5M. In stark contrast, 65M videos are collected to obtain a noise-resilient pre-trained model in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">50]</ref>. It is also noteworthy that our framework can also benefit from the massively weakly-supervised pre-training from billions of images or videos.</p><p>To sum up, our contributions are as follows:</p><p>(1) We propose OmniSource, a simple and efficient framework for weblysupervised video classification, which can leverage web data in different formats.</p><p>(2) We propose good practices for problems during joint training with omnisourced data, include source-target balancing, resampling and cross-dataset mixup.</p><p>(3) In experiments, our models trained by OmniSource achieve state-of-theart performance on the Kinetics-400, for all pre-training strategies we tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Webly-supervised learning Leveraging information from the Internet, termed webly-supervised learning, has been extensively explored <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12]</ref>. Divvala et al in <ref type="bibr" target="#b7">[8]</ref> proposes to automatically learn models from online resources for visual concept discovery and image annotation. Chen et al reveals that images crawled from the Internet can yield superior results over the fully-supervised method <ref type="bibr" target="#b4">[5]</ref>. For video classification, Ma et al proposes to use web images to boost action recognition models in <ref type="bibr" target="#b29">[29]</ref> at the cost of manually filtering web action images. To free from additional human labor, efforts have been made to learn video concept detectors <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b27">27]</ref> or to select relevant frames from videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref>. These methods are based on frames thus fail to consider the rich temporal dynamics of videos. Recent works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b12">13]</ref> show that webly-supervised learning can produce better pre-training models with very large scale noisy data (∼ 10 9 images and ∼ 10 7 videos). Being orthogonal to the pre-training stage, our framework works in a joint-training paradigm and is complementary to large-scale pre-training. Semi-supervised learning Our framework works under the semi-supervised setting where labeled and unlabeled(web) data co-exist. Representative classical approaches include label propagation <ref type="bibr" target="#b63">[63]</ref>, self-training <ref type="bibr" target="#b37">[37]</ref>, co-training <ref type="bibr" target="#b0">[1]</ref>, and graph networks <ref type="bibr" target="#b21">[22]</ref>. Deep models make it possible to learn directly from unlabeled data via generative models <ref type="bibr" target="#b20">[21]</ref>, self-supervised learning <ref type="bibr" target="#b55">[55]</ref>, or consensus of multiple experts <ref type="bibr" target="#b56">[56]</ref>. However, most existing methods are validated only on small scale datasets. One concurrent work <ref type="bibr" target="#b50">[50]</ref> proposes to first train a student network with unlabeled data with pseudo-labels and then fine-tune it on the labeled dataset. Our framework, however, works on the two sources simultaneously, free from the pretrain-finetune paradigm and is more data-efficient. Distillation According to the setting of knowledge distillation <ref type="bibr" target="#b16">[17]</ref> and data distillation <ref type="bibr" target="#b36">[36]</ref>, given a set of manually labeled data, we can train a base model in the manner of supervised learning. The model is then applied to the unlabeled data or its transforms. Most of the previous efforts <ref type="bibr" target="#b36">[36]</ref> are confined to the domain of images. In <ref type="bibr" target="#b13">[14]</ref>, Rohit et al proposes to distill spatial-temporal features from unlabeled videos with image-based teacher networks. Our framework is capable of distilling knowledge from multiple sources and formats within a single network. Domain Adaptation Since web data from multiple sources are taken as input, domain gaps inevitably exist. Previous efforts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b3">4]</ref> in domain adaptation focus on mitigating the data shift <ref type="bibr" target="#b35">[35]</ref> in terms of data distributions. On the contrary, our framework focuses on adapting visual information in different formats (e.g. still images, long videos) into the same format (i.e. trimmed video clips). Video classification Video analysis has long been tackled using hand-crafted feature <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b47">47]</ref>. Following the success of deep learning for images, video classification architectures have been dominated by two families of models, i.e. twostream <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b48">48]</ref> and 3D ConvNets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">45]</ref>. The former uses 2D networks to extract image-level feature and performs temporal aggregation <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b18">19]</ref> on top while the latter learns spatial-temporal features directly from video clips <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">44</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We propose a unified framework for omni-sourced webly-supervised video recognition, formulated in Sec. 3.2. The framework exploits web data of various forms (images, trimmed videos, untrimmed videos) from various sources (search engine, social media, video sharing platform) in an integrated way. Since web data can be very noisy, we use a teacher network to filter out samples with low confidence scores and obtain pseudo labels for the remaining ones (Sec. 3.4). We devise transformations for each form of data to make them applicable for the target task in Sec. 3.5. In addition, we explore several techniques to improve the robustness of joint training with web data in Sec. 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework formulation</head><p>Given a target task (trimmed video recognition, e.g.) and its corresponding target dataset D T = {(x i , y i )}, we aim to harness information from unlabeled web resources U = U 1 ∪ · · · ∪ U n , where U i refers to unlabeled data in a specific source or format. First, we construct the pseudo-labeled dataset D i from U i . Samples with low confidence are dropped using a teacher model M trained on D T , and the remaining data are assigned with pseudo-labels y = PseudoLabel(M(x)). Second, we devise appropriate transforms T i (x) : D i → D A,i to process data in a specific format (e.g. still images or long videos) into the data format (trimmed videos in our case) in the target task. We denote the union of D A,i to be the auxiliary dataset D A . Finally, a model M (not necessarily the original M), can be jointly trained on D T and D A . In each iteration, we sample two mini-batches of data B T , B A from D T , D A respectively. The loss is a sum of cross entropy loss on both B T and B A , indicated by Eq 1.</p><formula xml:id="formula_0">L = x,y∈B T L(F(x; M ), y) + x, y∈B A L(F(x; M ), y)<label>(1)</label></formula><p>For clarification, we compare our framework with some recent works on billion-scale webly-supervised learning in <ref type="table" target="#tab_1">Table 1</ref>. OmniSource is capable of dealing with web data from multiple sources. It is designed to help a specific task, treats webly-supervision as co-training across multiple data sources instead of pre-training, thus is much more data-efficient. It is also noteworthy that our framework is orthogonal to webly-supervised pre-training <ref type="bibr" target="#b12">[13]</ref>. Webly-supervised pretrain <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b12">13]</ref> Web-scale semi-supervised <ref type="bibr" target="#b50">[50]</ref> OmniSource <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-specific data collection</head><p>We use class names as keywords for data crawling, with no extra query expansion. For tag-based system like Instagram, we use automatic permutation and stemming 4 to generate tags. We crawl web data from various sources, including search engine, social media and video sharing platform. Because Google restricts the number of results for each query, we conduct multiple queries, each of which is restricted by a specific period of time. Comparing with previous works <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b12">13]</ref> which rely on large-scale web data with hashtags, our task-specific collection uses keywords highly correlated with labels, making the supervision stronger. Moreover, it reduces the required amount of web data by 2 orders of magnitude (e.g. from 65M to 0.5M videos on Instagram).</p><p>After data collection, we first remove invalid or corrupted data. Since web data may contain samples very similar to validation data, data de-duplication is essential for a fair comparison. We perform content-based data de-duplication based on feature similarity. First, we extract frame-level features using an ImageNetpretrained ResNet50. Then, we calculate the cosine similarity of features between the web data and target dataset and perform pairwise comparison after whitening. The average similarity among different crops of the same frame is used as the threshold. Similarity above it indicates suspicious duplicates. For Kinetics-400, we filter out 4,000 web images (out of 3.5M, 0.1%) and 400 web videos (out of 0.5M, 0.1%). We manually inspect a subset of them and find that less than 10% are real duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Teacher filtering</head><p>Data crawled from the web are inevitably noisy. Directly using collected web data for joint training leads to a significant performance drop (over 3%). To prevent irrelevant data from polluting the training set, we first train a teacher network M on the target dataset and discard those web data with low confidence scores. For web images, we observe performance deterioration when deflating 3D teachers to 2D and therefore only use 2D teachers. For web videos, we find both applicable and 3D teachers outperform 2D counterparts consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naï ve Inflating</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inflating with Perspective Warping Snippets Clip</head><p>Untrimmed videos to snippets or clips Images to pseudo videos </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Transforming to the target domain</head><p>Web Images. To prepare web images for video recognition training, we devise several ways to transform images into pseudo videos. The first naïve way is to replicate the image n times to form an n-frame clip. However, such clips may not be optimal since there is a visible gap between static clips and natural videos which visually change over time. Therefore, we propose to generate video clips from static images by viewing them with a moving camera. Given an image I, under the standard perspective projection model <ref type="bibr" target="#b9">[10]</ref>, an image with another perspective I can be generated by a homographic transform H which is induced by a homographic matrix H ∈ R 3×3 , i.e., I = H(I) = F(I; H). To generate a clip J = {J 1 , · · · , J N } from I, starting from J 1 = I, we have</p><formula xml:id="formula_1">J i = H i (J i−1 ) = (H i • H i−1 • · · · • H 1 )(I)<label>(2)</label></formula><p>Each matrix H i is randomly sampled from a multivariate Gaussian distribution N (µ, Σ), while the parameters µ and Σ are estimated using maximum likelihood estimation on the original video source. Once we get pseudo videos, we can leverage web images for joint training with trimmed video datasets. Untrimmed Videos. Untrimmed videos form an important part of web data.</p><p>To exploit web untrimmed videos for video recognition, we adopt different transformations respectively for 2D and 3D architectures. For 2D TSN, snippets sparsely sampled from the entire video are used as input. We first extract frames from the entire video at a low frame rate (1 FPS). A 2D teacher is used to get the confidence score of each frame, which also divides frames into positive ones and negative ones. In practice, we find that only using positive frames to construct snippets is a sub-optimal choice. Instead, combining negative frames and positive frames can form harder examples, results in better recognition performance. In our experiments, we use 1 positive frame and 2 negative frames to construct a 3-snippet input.</p><p>For 3D ConvNets, video clips (densely sampled continuous frames) are used as input. We first cut untrimmed videos into 10-second clips, then use a 3D teacher to obtain confidence scores. Only positive clips are used for joint training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Joint training</head><p>Once web data are filtered and transformed into the same format of that in the target dataset D T , we construct an auxiliary dataset D A . A network can then be trained with both D T and D A using sum of cross-entropy loss in Eq. 1. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, web data across classes are extremely unbalanced, especially after teacher filtering. Also there exists potential domain gap between D T and D A . To mitigate these issues, we enumerate several good practices as follows.</p><p>Balance between target and auxiliary mini-batches. Since the auxiliary dataset may be much larger than the target dataset and the domain gap may occur, the data ratio between target and auxiliary mini-batches is crucial for the final performance. Empirically, |B T | : |B A | = 2 : 1 ∼ 1 : 1 works reasonably well. Resampling strategy. Web data are extremely unbalanced, especially after teacher filtering (see <ref type="figure" target="#fig_3">Fig 3)</ref>. To alleviate this, we explore several sampling policies: (1) sampling from a clipped distribution: classes whose samples exceeds threshold N c are clipped; (2) sampling from distribution modified by a power law: the probability of choosing class with N samples is proportional to N p (p ∈ (0, 1)). We find that (2) parameterized by p = 0.2 is generally a better practice. Cross-dataset mixup. Mixup <ref type="bibr" target="#b57">[57]</ref> is a widely used strategy in image recognition. It uses convex combinations of pairs of examples and their labels for training, thus improving the generalization of deep neural networks. We find that technique also works for video recognition. When training teacher networks on D T only, we use the linear combination of two clip-label pairs as training data, termed as intra-dataset mixup. When both target and auxiliary datasets are used, the two pairs are samples randomly chosen from both datasets, termed as cross-dataset mixup. Mixup works fairly well when networks are trained from scratch. For fine-tuning, the performance gain is less noticeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>In this section, we introduce the datasets on which experiments will be conducted. Then we go through different sources from which web data are collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Target datasets</head><p>Kinetics-400 The Kinetics dataset <ref type="bibr" target="#b2">[3]</ref> is one of the largest video datasets. We use the version released in 2017 which contains 400 classes and each category has more than 400 videos. In total, it has around 240K, 19K, and 38K videos for training, validation and testing subset respectively. In each video, a 10-second clip is annotated and assigned a label. These 10-second clips constitute the data source for the default supervised learning setting, which we refer to K400-tr. The rest part of training videos is used to mimic untrimmed videos sourced from the Internet which we refer to K400-untr. Youtube-car Youtube-car <ref type="bibr" target="#b62">[62]</ref> is a fine-grained video dataset with 10K training and 5K testing videos of 196 types of cars. The videos are untrimmed, last several minutes. Following <ref type="bibr" target="#b62">[62]</ref>, the frames are extracted from videos at 4 FPS. UCF101 UCF101 <ref type="bibr" target="#b41">[41]</ref> is a small scale video recognition dataset, which has 101 classes and each class has around 100 videos. We use the official split-1 in our experiments, which has about 10K and 3.6K videos for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web sources</head><p>We collect web images and videos from various sources including search engines, social medias and video sharing platforms.</p><p>GoogleImage GoogleImage is a search engine based web data source for Kinetics-400, Youtube-car and UCF101. We query each class name in the target dataset on Google to get related web images. We crawl 6M, 70K, 200K URLs for Kinetics-400, Youtube-car and UCF101 respectively. After data cleaning and teacher filtering, about 2M, 50K, 100K images are used for training on these three datasets. We denote the three datasets as GG-k400, GG-car, and GG-UCF respectively. Instagram Instagram is a social media based web data source for Kinetics-400. It consists of InstagramImage and InstagramVideo. We generate several tags for each class in Kinetics-400, resulting in 1,479 tags and 8.7M URLs. After removing corrupted data and teacher filtering, about 1.5M images and 500K videos are used for joint training, denoted as IG-img and IG-vid. As shown in <ref type="figure" target="#fig_3">Fig 3,</ref> IG-img is significantly unbalanced after teacher filtering. Therefore, in the coming experiments, IG-img is used in combination with GG-k400. YoutubeVideo YoutubeVideo is a video sharing platform based web data source for Youtube-car. We crawl 28K videos from youtube by querying class names. After de-duplicating (remove videos in the original Youtube-car dataset) and teacher filtering, 17K videos remain, which we denote as YT-car-17k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Video architectures</head><p>We mainly study two families of video classification architectures, namely Temporal Segment Networks <ref type="bibr" target="#b48">[48]</ref> and 3D ConvNets <ref type="bibr" target="#b2">[3]</ref>, to verify the effectiveness of our design. Unless specified, we use ImageNet-pretrained models for initialization. We conduct all experiments using MMAction <ref type="bibr" target="#b59">[59]</ref>. 2D TSN Different from the original setting in <ref type="bibr" target="#b48">[48]</ref>, we choose ResNet-50 <ref type="bibr" target="#b15">[16]</ref> to be the backbone, unless otherwise specified. The number of segments is set to be 3 for Kinetics/UCF-101 and 4 for Youtube-car, respectively. 3D ConvNets For 3D ConvNet, we use the SlowOnly architecture proposed in <ref type="bibr" target="#b8">[9]</ref> in most of our experiments. It takes 64 consecutive frames as a video clip and sparsely samples 4/8 frames to form the network input. Different initialization strategies are explored, including training from scratch and fine-tuning from a pre-trained model. Besides, more advanced architecture like Channel Separable Network <ref type="bibr" target="#b44">[44]</ref> and more powerful pre-training (IG-65M <ref type="bibr" target="#b12">[13]</ref>) is also explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Verifying the efficacy of OmniSource</head><p>We verify our framework's efficacy by examining several questions. Why do we need teacher filtering and are search results good enough? Some may question the necessity of a teacher network for filtering under the impression that a modern search engine might have internally utilized a visual recognition model, possibly trained on massively annotated data, to help generate the search results. However, we argue that web data are inherently noisy and we observe nearly half of the returned results are irrelevant. More quantitatively, 70% -80% of the web data are rejected by the teacher. On the other hand, we conduct an experiment without teacher filtering. Directly using collected web data for joint training leads to a significant (over 3%) performance drop on TSN. This reveals that teacher filtering is necessary to help retain the useful information from the crawled web data while eliminating the useless.</p><p>Does every data source contribute? We explore the contribution of different source types: images, trimmed videos and untrimmed videos. For each data source, we construct auxiliary dataset and use it for joint training with K400-tr. Results in <ref type="table" target="#tab_1">Table 13</ref> reveal that every source contributes to improving accuracy on the target task. When combined, the performance is further improved. For images, when the combination of GG-k400 and IG-img is used, the Top-1 accuracy increases around 1.4%. For trimmed videos, we focus on IG-vid. Although being extremely unbalanced, IG-vid still improves Top-1 accuracy by over 1.0% in all settings. For untrimmed videos, we use the untrimmed version of Kinetics-400 (K400-untr) as the video source and find it also works well.</p><p>Do multiple sources outperform a single source? Seeing that web data from multiple sources can jointly contribute to the target dataset, we wonder if multiple sources are still better than a single source with the same budget. To verify this, we consider the case of training TSN on both K400-tr and D A = GG-k400 + IG-img. We fix the scale of auxilary dataset to be that of GG-k400 and vary the ratio between GG-k400 and IG-img by replacing images from GG-k400 with those in IG-img. From <ref type="figure">Fig. 4</ref>, we observe an improvement of 0.3% without increasing |D A |, indicating that multiple sources provide complementary information by introducing diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does OmniSource work with different architectures?</head><p>We further conduct experiments on a wide range of architectures and obtain the results in <ref type="table" target="#tab_4">Table 3</ref>. For TSN, we use EfficientNet-B4 <ref type="bibr" target="#b43">[43]</ref> instead as the backbone, on which Om-niSource improves Top-1 accuracy by 1.9%. For 3D-ConvNets, we conduct experiments on the SlowOnly-8x8-ResNet101 baseline, which takes longer input and has a larger backbone. Our framework also works well in this case, improving the Top-1 accuracy from 76.3% to 80.4% when training from scratch, from 76.8% to 80.5% with ImageNet pretraining. The improvement on larger networks is higher, suggesting that deeper networks are more prone to suffering from the scarcity of video data and OmniSource can alleviate this.  Is OmniSource compatible with different pre-training strategies? As discussed, OmniSource alleviates the data-hungry issue by utilizing auxiliary data. One natural question is: how does it perform when training 3D networks from scratch? Can we simply drop ImageNet pretraining in pursuit of a more straightforward training policy? Indeed, we find that OmniSource works fairly well under this setting and interestingly the performance gain is more significant than fine-tuning. For example, SlowOnly-(4x16, R50) increases the Top-1 accuracy by 3.9% when training from scratch while fine-tuning only increases by 2.8%. The model trained from scratch beats the fine-tuned counterpart by 0.2% with OmniSource though being 0.9% lower with only K400-tr. Similar results can be observed for SlowOnly-(8x8, R101). With large-scale webly supervised pretraining, OmniSource still leads to significant performance improvement (+2.6% Top-1 for TSN-3seg-R50, +1.0% Top-1 for irCSN-32x2).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do features learned by OmniSource transfer to other tasks? Although</head><p>OmniSource is designed for a target video recognition task, the learned features also transfer well to other video recognition tasks. To evaluate the transfer capability, we finetune the learned model on two relatively smaller datasets: UCF101 <ref type="bibr" target="#b41">[41]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_1">Table 15</ref> indicates that on both benchmarks, pretraining with OmniSource leads to significant performance improvements.</p><p>Following standard evaluation protocol, SlowOnly-8x8-R101 achieves 97.3% Top-1 accuracy on UCF101, 79.0% Top-1 accuracy on HMDB51 with RGB input. When combined with optical flow, it achieves 98.6% and 83.8% Top-1 accuracy on UCF101 and HMDB51, which is the new state-of-the-art. More results on transfer learning are provided in the supplementary material. Does OmniSource work in different target domains? Our framework is also effective and efficient in various domains. For a fine-grained recognition benchmark called Youtube-car, we collect 50K web images (GG-car) and 17K web videos (YT-car-17k) for training. <ref type="table" target="#tab_7">Table 5</ref> shows that the performance gain is significant: 5% in both Top-1 accuracy and mAP. On UCF-101, we train a two-stream TSN network with BNInception as the backbone. The RGB stream is trained either with or without GG-UCF. The results are listed in <ref type="table" target="#tab_8">Table 6</ref>. The Top-1 accuracy of the RGB stream improves by 2.7%. When fused with the flow stream, there is still an improvement of 1.1%.</p><p>Where does the performance gain come from? To find out why web data help, we delve deeper into the collected web dataset and analyze the improvement on individual classes. We choose TSN-3seg-R50 trained either with or without GG-k400, where the improvement is 0.9% on average. We mainly focus on the confusion pairs that web images can improve. We define the confusion score of a class pair as s ij = (n ij + n ji )/(n ij + n ji + n ii + n jj ), where n ij denotes the number of images whose ground-truth are class i while being recognized as class j. Lower confusion score denotes better discriminating power between the two classes. We visualize some confusing pairs in <ref type="figure" target="#fig_4">Fig 5.</ref> We find the improvement can be mainly attributed to two reasons: (1) Web data usually focus on key objects of action. For example, we find that in those pairs with the largest confusion score reduction, there exist pairs like "drinking beer" vs."drinking shots", and "eating hotdog" vs."eating chips". Training with web data leads to better object recognition ability in some confusing cases. (2) Web data usually include discriminative poses, especially for those actions which last for a short time. For example, "rock scissors paper" vs."shaking hands" has the second-largest confusion score reduction. Other examples including "sniffing"-"headbutting", "break dancing"-"robot dancing", etc.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons with state-of-the-art</head><p>In <ref type="table" target="#tab_9">Table 7</ref>, we compare OmniSource with current state-of-the-art on Kinetics-400. For 2D ConvNets, we obtain competitive performance with fewer segments and lighter backbones. For 3D ConvNets, considerable improvement is achieved for all pre-training settings with OmniSource applied. With IG-65M pre-trained irCSN-152, OmniSource achieves 83.6% Top-1 accuracy, an absolute improvement of 1.0% with only 1.2% relatively more data, establishing a new record.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Validating the good practices in OmniSource</head><p>We conduct several ablation experiments on techniques we introduced. The target dataset is K400-tr and the auxiliary dataset is GG-k400 unless specified. Transforming images to video clips. We compare different ways to transform web images into clips in <ref type="table" target="#tab_10">Table 8</ref>. Naïvely replicating still image brings limited improvement (0.3%). We then apply translation with randomized or constant speed to form pseudo clips. However, the performance deteriorates slightly, suggesting that translation cannot mimic the camera motion well. Finally, we resort to perspective warping to hallucinate camera motion. Estimating class-agnostic distribution parameters is slightly better, suggesting that all videos might share similar camera motion statistics. Cross-Dataset mixup In <ref type="table" target="#tab_11">Table 9</ref>, we find that mixup is effective for video recognition in both intra-and cross-dataset cases when the model is trained from scratch. The effect is unclear for fine-tuning. In particular, mixup can lead to 0.4% and 0.3% Top-1 accuracy improvement for intra-and inter-dataset cases. Impact of teacher choice. Since both teacher and student networks can be 2D or 3D ConvNets, there are 4 possible combinations for teacher network choosing. For images, deflating 3D ConvNets to 2D yields a dramatic performance drop. Therefore, we do not use 3D ConvNet teachers for web images. For videos, however, 3D ConvNets lead to better filtering results comparing to its 2D counterpart. To examine the effect of different teachers, we fix the student model to be a ResNet-50 and vary the choices of teacher models (ResNet-50, EfficientNet-b4, and the ensemble of ResNet-152 and EfficientNet-b4). Consistent improvement is observed against the baseline (70.6%). The student accuracy increases when a better teacher network is used. It also holds for 3D ConvNets on web videos. Effectiveness when labels are limited. To validate the effectiveness with limited labeled data, we construct 3 subsets of K400-tr with a proportion of 3%, 10%, and 30% respectively. We rerun the entire framework including data filtering with a weaker teacher. The final results on the validation set of K400-tr is shown in <ref type="figure">Fig 7.</ref> Our framework consistently improves the performance as the percentage of labeled videos varies. Particularly, the gain is more significant when data are scarce, e.g. a relative increase of over 30% with 3% labeled data.   Balancing between the target and auxiliary dataset. We tune the ratio between the batch size of the target dataset |B T | and the auxiliary dataset |B A | and obtain the accuracy on <ref type="figure">Fig 8.</ref> We test 3 scenarios: (1) the original GG-k400, clarified in Sec 4.2; (2) [GG+IG]-k400, the union of GG-k400 and IG-img; <ref type="figure" target="#fig_3">(3)</ref> [GG+IG]-k400-half which is the half of (2). We observe that the performance gain is robust to the choice of |B T |/|B A | in most cases. However, with less auxiliary data, the ratio has to be treated more carefully. For example, smaller |D A | but larger |B A | may cause overfitting auxiliary samples and hurt the overall result. Resampling strategies. The target dataset is usually balanced across classes. The nice property doesn't necessarily hold for the auxiliary dataset. Thus we propose several resampling strategies. From <ref type="table" target="#tab_1">Table 10</ref>, we see that simple techniques to tailor the distribution into a more balanced one yield nontrivial improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose OmniSource, a simple yet effective framework for webly-supervised video recognition. Our method can utilize web data from multiple sources and formats by transforming them into a same format. In addition, our task-specific data collection is more data-efficient. The framework is applicable to various video tasks. Under all settings of pretraining strategies, we obtain state-of-the-art performance on multiple benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Datasets</head><p>In the main paper, we conduct experiments on three benchmarks, namely Kinetics-400, Youtube-car and UCF101. The detailed statistics of the target and auxiliary datasets are listed in <ref type="table" target="#tab_1">Table 11</ref>. Our framework is very data efficient, comparing to approaches which use billion of images, dozens of millions of videos for pretraining. All the web data we collected are only several Tera-Bytes. After filtering, remaining web data only takes around 3TB in space, which can easily fit into one hard drive. In stark comparison, the space required by <ref type="bibr" target="#b12">[13]</ref> is estimated to be at least 100TB. In this section, we visualize videos in these three datasets, and data in the auxiliary datasets we construct, to show why OmniSource benefits these tasks in different levels. <ref type="table" target="#tab_1">Table 11</ref>: Dataset Statistics. Here we show the statistics of dataset we use in our experiments. We report storage amount of lowest cost format for videos (videos when using high fps for training, and frames when using low fps for training). Our framework is data efficient, the amount of data we used is two orders less than web data pretraining approach. Tri-vid denotes trimmed videos and Unt-vid denotes untrimmed videos.  <ref type="figure">Fig. 9</ref>: UCF101. Visualization of data in UCF101 and its auxiliary dataset. For some classes, web data are more diversified and contain more discriminative poses.</p><p>Youtube-Car Youtube-Car is the benchmark on which our framework benefits most. It mainly has two reasons: <ref type="bibr" target="#b0">(1)</ref> The web data are much cleaner: when searching with the name of a car, it is easy to get a bunch of images with little noise, since nothing is ambiguous. (2) The source for both target and auxiliary dataset is YouTube, which mean the domain gap is much smaller. Some samples from Youtube-Car and its auxiliary datasets are visualized in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p><p>UCF101 Our framework also works on UCF101, which is a small-scale video recognition dataset. UCF101 has much less data diversity and lower visual quality, while auxiliary web data can be complementary in these two aspects. For example, from <ref type="figure">Fig. 9</ref>, one can hardly tell the difference between BreastStroke and FrontCrawl videos in UCF101. The difference is much more significant in web data. Using our framework, models can learn those discriminative features from web data, and can better recognize videos in the target dataset.</p><p>Kinetics-400 We visualize some images in GG-k400 and some videos in k400-tr, IG-vid in <ref type="figure" target="#fig_0">Fig. 11</ref>. The observations are summarized below: (1) Web data have much more diverse appearance comparing to the target dataset. <ref type="bibr" target="#b1">(2)</ref> Web data are very noisy. The teacher network filtering results reveal that around 60% -70% data in the web data is irrelevant to the task we are interested in. <ref type="formula" target="#formula_2">(3)</ref> We can eliminate noise in web data at the minimal cost of dropping some false negative samples, resulting in a much cleaner auxiliary dataset.  Since one can easily get images of centain types of cars by querying its name, the quality of the auxiliary dataset is much better. The high quality web data leads to considerable gain in model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Implementation Details</head><p>Here, we report the implementation details for all our experiments for Kinetics-400 and transfer learning in UCF101 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Kinetics-400</head><p>For all experiments on Kinetics-400, we use an SGD with momentum of 0.9, and weight decay of 10 −4 . The initial learning rate (LR) we use linearly scales with the number of samples and is decreased to its 10 −1 . For TSN-2D experiments, we use 4 × 10 −5 /sample as the starting LR. The training process lasts 100 epoches and LR decays at 40 and 80 epochs. For 3D-ConvNet experiments, we use 1.6 × 10 −4 /sample as the starting LR for experiments with ImageNetpretrain, 1.6 × 10 −3 /sample as the starting LR for train-from-scratch experiments. For ImageNet-pretrain experiments, training lasts 150 epochs and LR decays at 90 and 130 epochs. For train-from-scratch experiments, we use CosineLR schedule instead of StepLR schedule, and training lasts for 256 epoches and 196 epoches respectively for SlowOnly-4x16 and SlowOnly-8x8, same as training schedules used in <ref type="bibr" target="#b8">[9]</ref>. For IG-65M pretrained irCSN-152, we use 5 × as the starting LR. The training process lasts 58 epochs and LR decays at 32 and 48 epochs, which is consistent with <ref type="bibr" target="#b12">[13]</ref>. Warmup is also used in our experiments, which lasts 34 epochs for the train-from-scratch SlowOnly approach, 16 epochs for irCSN-152. During warmup, learning rate grows linearly from 0 to the starting LR. The warmup schedules follows <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments for Transfer Learning on UCF-101 and HMDB-51</head><p>We use one simple schedule for all transfer learning experiments. We we use an SGD with momentum of 0.9, and weight decay of 10 −4 . The starting LR is set to 5 × 10 −6 /sample. We train 90 epoches on UCF101 and HMDB51 and the first 20 epoches are used for warmup, during which learning rate grows linearly from 0 to the starting LR. No LR decay is performed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: More Detailed Experimental Resutls</head><p>Due to space limitation, some experiment results are not described in detail in the main paper. In this part, we discuss these experiments at length.</p><p>Verifying the efficacy of OmniSource.</p><p>Why do we need teacher filtering and are search results good enough? In the main text, we argue that directly using collected web data for joint training leads to a significant performance drop (Top-1 Accuracy: 70.6% to 67.4%) on TSN, which proves the necessity of having a teacher network. However, since we crawl Top 1000 images for each class name from search engines, one may argue that too many queries lead to bad data quality. In response to this question, we construct two subset of GG-k400-Raw, which include Top 1 ⁄4 (GG-k400-Raw-1 ⁄4) and Top 1 ⁄2 (GG-k400-Raw-1 ⁄2) results in GG-k400-Raw respectively. To make sure web images are much more than trimmed videos in the target dataset, we construct a subset of k400-tr, named k400-tr-half, which includes half classes and half videos per class. We jointly train k400-tr-half with different auxiliary datasets. From <ref type="table" target="#tab_1">Table 12</ref>, we see that raw web data are of low quality, even for top search results. Thus teacher filtering is an essential step in OmniSource. Does every data source contribute? In the main text, we use two groups of experiments which use ImageNet pretrained TSN-3seg-R50 and SlowOnly-4x16-R50 as baselines, to prove that every source contributes. Besides that, the conclusion also holds for SlowOnly-4x16-R50 trained from scratch. From <ref type="table" target="#tab_1">Table 13</ref>, we see that for the train-from-scratch setting, each data source not only contributes to the target task, but the improvement is much larger than the ImageNet-pretrain setting.  <ref type="table" target="#tab_1">Table 16</ref> compares the transfer learning performance of OmniSource trained models with other state-of-the-art approaches. We see that OmniSource outperforms other methods by a large margin. Untrimmed videos to snippets. In the main paper, we mention that combining negative frames and positive frames is a good practice to construct harder snippets, which leads to better recognition performance. We provide detailed results in <ref type="table" target="#tab_1">Table 14</ref>, in which we explore each possible combinations during joint training k400-tr and k400-untr with TSN-3seg-R50 baseline. We find that combining one positive frame and two negative frames to form a 3-frame snippet leads to best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validating the good practices in OmniSource</head><p>Impact of teacher choice. In the main paper, we mention that for web video data, 3D teachers always outperform 2D ones. Besides that, the conclusion that the accuracy of the student network increases when a better teacher network is used also holds for web video data. Here, we provide some quantitative results to prove those conclusions in <ref type="table" target="#tab_1">Table 17</ref>. SlowOnly-4x16-R50 with ImageNet-pretrain is used as the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Improvement Analysis</head><p>We further study the improvement of our framework, when using the full auxiliary set for training. Recall that our framework can improve 3.0% and 3.9% respectively on 2D and 3D baseline with all auxiliary data we collected, We analyze the improvement on confusing pairs over these two cases. We use delta of confusion score (∆ ij ) to denote the improvement:</p><formula xml:id="formula_2">∆ ij = Oscore ij − Bscore ij ,<label>(3)</label></formula><p>where Oscore ij denotes the confusion score of pair &lt; i, j &gt; when trained with OmniSource, and Bscore ij denotes the confusion score of pair &lt; i, j &gt; of baseline model.</p><p>We show success and failure cases of 2D model in <ref type="table" target="#tab_1">Table 18</ref>. The contribution of our framework mainly attributes to the better object recognition ability. Besides that, it also improves when discriminative element can be found in web data, like two hands touched in handshaking, two head touched in headbutting, etc.. There are also failure cases when motion is needed for action recognition or when the taxonomy is not reasonable.</p><p>We show success and failure cases of 3D model in <ref type="table" target="#tab_1">Table 19</ref>. Thanks to the capability of using motion cues for action recognition, the pair 'rock scissors <ref type="table" target="#tab_1">Table 16</ref>: We compare transfer learning results with state-of-the-art approaches. We report mean Top-1 accuracies on three splits of UCF101 and HMDB51. We see that OmniSource framework not only outperforms RGB-Only methods. When fused with the flow stream, it surpasses all methods by a large margin, even for those which ensemble results of RGB, Flow and other modalities (*We reimplement Flow-I3D as our flow stream)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Pretrain UCF101 HMDB51 Two-Stream <ref type="bibr">[</ref>  paper' and 'slapping' is no longer a failure case (∆ from +0.176 to -0.059). However, when appearance and motion are all similar, our framework might fail due to the introduced noises. Due to the improved ability of object recognition, the accuracy improvement on actions of eating something is much more significant. On average, the accuracy for eating something improved 5.8%, 8.3% for 2D and 3D models respectively, while the average improvement for all classes are 3.0% and 3.9%. We visualize the improvement on this subset in <ref type="figure" target="#fig_0">Fig. 12</ref>    <ref type="figure" target="#fig_0">Fig. 12</ref>: Improvement on eating something. Rows denote groundtruth and columns denote predictions. Blockij represents the difference in numbers of samples which belongs to class i but recognized as class j between the baseline and our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Ours) Procedure 1 .</head><label>1</label><figDesc>Train a model M on U . 1. Train a model M on D T . 1. Train one (or more) model M on D T . 2. Fine-tune M on D T . 2. Run M on U to pseudo-labeled D. 2. Run M on i U i to pseudo-labeled i D i . 3. Train a student model M on D. (Samples under certain threshold are dropped.) 4. Fine-tune M on D T . 3. Apply transforms T i : D i → D A,i . 4. Train model M (or M) on D T ∪ D A . |U | 3.5B images or 65M videos 1B images or 65M videos |U |: 13M images and 1.4M videos (0.4%∼2%) |D A |: 3.5M images and 0.8M videos (0.1%∼1%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Transformations. Left: Inflating images to clips, by replicating or inflating with perspective warping; Right: Extracting segments or clips from untrimmed videos, guided by confidence scores</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Web Data Distribution. The inter-class distribution of three web datasets is visualized in (a,b,c), both before and after filtering. (d) gives out samples of filtered out images (cyan) and remained images (blue) for GG-K400. Teacher filtering successfully filters out lots of negative examples while making inter-class distribution more uneven</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Confusing pairs improved by OmniSource. The original accuracy and change are denoted in black and in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :Fig. 8 :</head><label>68</label><figDesc>Better teachers leads to better students GG-k400 + IG-img (½ data) w/o. Distill w/. GG-k400 + IG-img Accuracy with different ratios |BT | : |BA|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>= 5000) 71.9/90.0 Power (∼ N p , p = 0.5)71.8/89.7 Power (∼ N p , p = 0.2)72.0/90.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Acknowledgment This work is partially supported by the SenseTime Collaborative Grant on Large-scale Multi-modality Analysis (CUHK Agreement No. TS1610626 &amp; No. TS1712093), the General Research Fund (GRF) of Hong Kong (No. 14203518 &amp; No. 14205719), and Innovation and Technology Support Program (ITSP) Tier 2, ITS/431/18F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>YT-Car-17k GG-car Aston Martin Virage Coupe 2012 Audi S6 Sedan 2011 Youtube-car YT-car-17k GG-car</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Youtube-car. Visualization of data in Youtube-car and its auxiliary dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Kinetics. Data from Kinetics and data from auxiliary datasets are visualized, both raw and clean. Red boxes denote that the image is identified as negative by teacher. There might be some false-negative during teacher filtering, but the data filtered out by teacher are almost clean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2003.13042v2 [cs.CV] 25 Aug 2020</figDesc><table><row><cell>Raw Web Data</cell><cell></cell><cell cols="2">Filtered Web Data</cell><cell cols="2">Transformed Web Data</cell><cell></cell></row><row><cell>Images</cell><cell></cell><cell>Images</cell><cell>T A</cell><cell></cell><cell>Trimmed Videos</cell><cell cols="2">Trimmed</cell></row><row><cell>Untrimmed Videos …</cell><cell>Teacher Network (Base Recognizer)</cell><cell>Untrimmed Videos Trimmed Videos …</cell><cell>T B …</cell><cell></cell><cell>Trimmed Videos Trimmed Videos …</cell><cell>Joint Training</cell><cell>Videos Student Network (Better Recognizer)</cell></row><row><cell>Trimmed Videos</cell><cell></cell><cell cols="3">Target Dataset</cell><cell>Trimmed Videos</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Difference to previous works. The notions follow Sec. 3.2: U is the unlabeled web data, DT is the target dataset. |U|, |DA| denotes the scale of web data and filtered auxiliary dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Every source contributes. We find that each source contributes to the target task. With all sources combined(we intuitively set the ratio as: K400-tr : Web-img : IG-vid : K400-untr = 2: 1: 1: 1), the improvement can be more considerable. The conclusion holds for both 2D TSN and 3D ConvNets (Format: Top-1 Acc/ Top-5 Acc)</figDesc><table><row><cell cols="2">Arch/Dataset K400-tr +GG-k400 +GG&amp;IG-img +IG-vid +K400-untr</cell><cell>+ All</cell></row><row><cell>TSN-3seg R50</cell><cell>70.6/89.4 71.5/89.5 72.0/90.0 72.0/90.3 71.7/89.6</cell><cell>73.6/91.0</cell></row><row><cell>SlowOnly 4x16,R50</cell><cell>73.8/90.9 74.5/91.4 75.2/91.6 75.2/91.7 74.5/91.1</cell><cell>76.6/92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>71.8%</cell><cell></cell></row><row><cell>71.0%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Baseline</cell></row><row><cell></cell><cell></cell><cell>w/o. Omni</cell></row><row><cell>Top-1 70.2%</cell><cell cols="2">0% 20% 50% 100%</cell></row><row><cell>Acc</cell><cell cols="2">|IG-img|/ |IG-img + GG-k400|</cell></row><row><cell cols="2">Fig. 4:</cell><cell>Multi-</cell></row><row><cell cols="3">source is better.</cell></row><row><cell cols="2">Mixed</cell><cell>sources</cell></row><row><cell cols="3">lead to better per-</cell></row><row><cell cols="3">formance with a</cell></row><row><cell cols="3">constrained number</cell></row><row><cell cols="3">of web images</cell></row></table><note>Improvement under various experiment con- figurations. OmniSource is extensively tested on various archi- tectures with various pretraining strategies. The improvement is significant in ALL tested choices. Even for the SOTA set- ting, which uses 65M web videos for pretraining, OmniSource still improves the Top-1 accuracy by 1.0% (Format: Top-1 / Top-5 Acc)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>OmniSource features transfer well.</figDesc><table><row><cell>Drinking shots</cell><cell>Drinking beer</cell><cell cols="2">Rock scissors paper Shake hands</cell></row><row><cell>16.3% + 18.4%</cell><cell>38.0% -2.0%</cell><cell>10.0% + 12.0%</cell><cell>29.2% -2.1%</cell></row><row><cell>Mopping floor</cell><cell>Sweeping floor</cell><cell cols="2">Eating doughnuts Eating burger</cell></row><row><cell>36.0% + 2.0%</cell><cell>54.0% + 10.0%</cell><cell>18.4% + 8.2%</cell><cell>67.3% + 4.1%</cell></row><row><cell>We finetune on UCF101 and HMDB51 with</cell><cell></cell><cell></cell><cell></cell></row><row><cell>K400-tr pretrained weight. Pretraining with Om-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>niSource improves the performance significantly</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Youtube-car</figDesc><table><row><cell>Setting</cell><cell>Top-1 mAP</cell></row><row><cell>Baseline</cell><cell>77.05 71.95</cell></row><row><cell>+GG-car</cell><cell>80.96 77.05</cell></row><row><cell cols="2">+YT-car-17k 81.68 78.61</cell></row><row><cell cols="2">+[GG-]+[YT-] 81.95 78.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>UCF-101</figDesc><table><row><cell cols="2">Setting + Flow Top-1</cell></row><row><cell>Baseline</cell><cell>86.04</cell></row><row><cell>+ GG-UCF</cell><cell>88.74</cell></row><row><cell>Baseline</cell><cell>93.47</cell></row><row><cell>+ GG-UCF</cell><cell>94.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparisons with Kinetics-400 state-of-the-art</figDesc><table><row><cell>Method</cell><cell>backbone</cell><cell cols="2">pretrain Top-1 Top-5</cell></row><row><cell>TSN-7seg [48]</cell><cell cols="3">Inception-v3 ImageNet 73.9 91.1</cell></row><row><cell>TSM-8seg [28]</cell><cell cols="3">ResNet50 ImageNet 72.8 N/A</cell></row><row><cell>TSN-3seg (Ours)</cell><cell cols="3">ResNet50 ImageNet 73.6 91.0</cell></row><row><cell>TSN-3seg (Ours)</cell><cell cols="3">Efficient-b4 ImageNet 75.2 92.0</cell></row><row><cell>SlowOnly-8x8 [9]</cell><cell>ResNet101</cell><cell>-</cell><cell>75.9 N/A</cell></row><row><cell>SlowFast-8x8 [9]</cell><cell>ResNet101</cell><cell>-</cell><cell>77.9 93.2</cell></row><row><cell cols="2">SlowOnly-8x8 (Ours) ResNet101</cell><cell>-</cell><cell>80.4 94.4</cell></row><row><cell>I3D-64x1 [3]</cell><cell cols="3">Inception-V1 ImageNet 72.1 90.3</cell></row><row><cell>NL-128x1 [49]</cell><cell cols="3">ResNet101 ImageNet 77.7 93.3</cell></row><row><cell>SlowFast-8x8 [9]</cell><cell cols="3">ResNet101 ImageNet 77.9 93.2</cell></row><row><cell cols="4">LGD-3D (RGB) [34] ResNet101 ImageNet 79.4 94.4</cell></row><row><cell>STDFB [31]</cell><cell cols="3">ResNet152 ImageNet 78.8 93.6</cell></row><row><cell cols="4">SlowOnly-8x8 (Ours) ResNet101 ImageNet 80.5 94.4</cell></row><row><cell>irCSN-32x2 [13]</cell><cell>irCSN-152</cell><cell cols="2">IG-65M 82.6 95.3</cell></row><row><cell>irCSN-32x2 (Ours)</cell><cell>irCSN-152</cell><cell cols="2">IG-65M 83.6 96.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Different ways to transform images into video clips. Still inflation is a strong baseline, while agnostic perspective warping performs best</figDesc><table><row><cell>Inflation</cell><cell>Top-1 Top-5</cell></row><row><cell>N/A</cell><cell>73.8 90.9</cell></row><row><cell>replication (still)</cell><cell>74.1 91.2</cell></row><row><cell>translation (random)</cell><cell>73.7 90.9</cell></row><row><cell cols="2">translation (constant) 73.8 90.8</cell></row><row><cell cols="2">perspective warp [spec] 74.4 91.3</cell></row><row><cell cols="2">perspective warp [agno] 74.5 91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Mixup technique can be beneficial to the model performance, both for intra-and cross-dataset cases. However, it works only when the model is trained from scratch</figDesc><table><row><cell cols="2">Pretraining w. mixup w.GG-img Top-1 Top-5</cell></row><row><cell>ImageNet</cell><cell>73.8 90.9</cell></row><row><cell>ImageNet</cell><cell>73.6 91.1</cell></row><row><cell>None</cell><cell>72.9 90.9</cell></row><row><cell>None</cell><cell>73.3 90.9</cell></row><row><cell>None</cell><cell>74.1 91.0</cell></row><row><cell>None</cell><cell>74.4 91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Resampling</cell></row><row><cell>strategies. Simple resam-</cell></row><row><cell>pling strategies lead to non-</cell></row><row><cell>trivial improvement</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Joint training k400-tr-half with different raw web datasets. We see that even top search results are of bad quality, lead to inferior performance.</figDesc><table><row><cell>Thus teacher</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>For the train-from-scratch setting, every data source also contributes to the target task. The improvement is much larger compared to the ImageNet-pretrain setting. (FT: ImageNet-pretrain; SC: train-from-scratch) Do features learned by OmniSource transfer to other tasks? In this section, we provide extensive experiment results on transfer learning, much more than results presented in the main text.Table 15lists transfer learning results on UCF101-split1 and HMDB-split1. Those results further support 2 points proposed in the main text: (1) OmniSource framework can learn better representation, which leads to significant performance improvement on downstream tasks. (2) ImageNet-pretraining is not indispensable for OmniSource to learn good representation. When combined with flow stream, state-of-the-art results on UCF101 and HMDB51 can be achieved by finetuning models jointly trained on Kinetics and auxiliary datasets.</figDesc><table><row><cell cols="2">Arch/Dataset K400-tr +GG-k400 +GG&amp;IG-img +IG-vid +K400-untr</cell><cell>+ All</cell></row><row><cell>SlowOnly 4x16, R50 [FT]</cell><cell>73.8/90.9 74.5/91.4 75.2/91.6 75.2/91.7 74.5/91.1</cell><cell>76.6/92.5</cell></row><row><cell>SlowOnly 4x16, R50 [SC]</cell><cell>72.9/90.9 74.1/91.0 74.8/91.4 75.8/92.0 74.8/91.2</cell><cell>76.8/92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>We explore different combinations to build a 3-frame snippet, and find that 1 Pos. + 2 Neg. is the best choice. Neg. 71.44 89.<ref type="bibr" target="#b57">57</ref> 1 Pos. + 2 Neg. 71.66 89.63</figDesc><table><row><cell cols="2">Configuration Top-1 Top-5</cell></row><row><cell>3 Rand.</cell><cell>71.42 89.34</cell></row><row><cell>3 Pos.</cell><cell>71.22 89.54</cell></row><row><cell>2 Pos. + 1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Detailed results of transfer learning. We report Top-1 accuracies on the official split-1 of UCF101 and HMDB51. We see that OmniSource framework can learn better representation which transfers to other recognition tasks well, even without ImageNet pretraining.</figDesc><table><row><cell cols="3">Architecture w/. ImageNet-pretrain w/. OmniSource UCF101-Top1 HMDB51-Top1</cell></row><row><cell>TSN-3seg</cell><cell>91.51</cell><cell>63.53</cell></row><row><cell>ResNet50</cell><cell>93.29</cell><cell>65.88</cell></row><row><cell>TSN-3seg</cell><cell>92.52</cell><cell>66.27</cell></row><row><cell>Efficient-b4</cell><cell>93.05</cell><cell>66.54</cell></row><row><cell></cell><cell>94.69</cell><cell>69.35</cell></row><row><cell>SlowOnly-4x16</cell><cell>95.98</cell><cell>70.71</cell></row><row><cell>ResNet50</cell><cell>94.05</cell><cell>65.82</cell></row><row><cell></cell><cell>96.01</cell><cell>70.98</cell></row><row><cell></cell><cell>96.40</cell><cell>76.41</cell></row><row><cell>SlowOnly-8x8</cell><cell>97.38</cell><cell>78.95</cell></row><row><cell>ResNet101</cell><cell>96.61</cell><cell>75.82</cell></row><row><cell></cell><cell>97.52</cell><cell>79.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>More results on the impact of teacher choice. 3D teachers always outperform 2D ones. The accuracy of the student network increases when a better teacher network is used.</figDesc><table><row><cell>Aux. Dataset</cell><cell>Teacher</cell><cell cols="3">Teacher Top-1 2D / 3D ? Top-1 Top-5</cell></row><row><cell></cell><cell>TSN-3seg-R50</cell><cell>70.6</cell><cell>2D</cell><cell>73.2 90.8</cell></row><row><cell>IG-vid</cell><cell>SlowOnly-4x16-R50</cell><cell>73.8</cell><cell>3D</cell><cell>75.2 91.7</cell></row><row><cell></cell><cell>IRCSN-152</cell><cell>82.6</cell><cell>3D</cell><cell>75.4 91.9</cell></row><row><cell></cell><cell>TSN-3seg-R50</cell><cell>70.6</cell><cell>2D</cell><cell>74.1 91.0</cell></row><row><cell>K400-untr</cell><cell>SlowOnly-4x16-R50</cell><cell>73.8</cell><cell>3D</cell><cell>74.5 91.1</cell></row><row><cell></cell><cell>IRCSN-152</cell><cell>82.6</cell><cell>3D</cell><cell>75.0 91.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Case</cell><cell>Action 1</cell><cell>Action 2</cell><cell>∆ ij ↓</cell></row><row><cell></cell><cell cols="2">rock scissors paper shaking hands</cell><cell>-0.160</cell></row><row><cell></cell><cell>headbutting</cell><cell>sniffing</cell><cell>-0.159</cell></row><row><cell>Success</cell><cell>sweeping floor</cell><cell>mopping floor</cell><cell>-0.113</cell></row><row><cell></cell><cell>eating chips</cell><cell cols="2">eating doughnuts -0.103</cell></row><row><cell></cell><cell>eating ice creams</cell><cell>eating cake</cell><cell>-0.100</cell></row><row><cell>Failure</cell><cell cols="2">rock scissors paper slapping drinking drinking shots</cell><cell>+0.176 +0.158</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18 :</head><label>18</label><figDesc>Confusion Score Delta for 2D models. Lower delta means larger gain in discriminative power of these two classes. Top-5 and Lowest-2 entries are displayed.</figDesc><table><row><cell>Case</cell><cell>Action 1</cell><cell>Action 2</cell><cell>∆ ij ↓</cell></row><row><cell></cell><cell>slapping</cell><cell>headbutting</cell><cell>-0.235</cell></row><row><cell></cell><cell cols="2">eating doughnuts eating hotdog</cell><cell>-0.153</cell></row><row><cell>Success</cell><cell>eating chips</cell><cell>eating hotdog</cell><cell>-0.121</cell></row><row><cell></cell><cell>faceplanting</cell><cell>drop kicking</cell><cell>-0.120</cell></row><row><cell></cell><cell cols="3">cooking chicken cooking sausages -0.110</cell></row><row><cell>Failure</cell><cell>baking cookies yawning</cell><cell>making a cake sneezing</cell><cell>+0.119 +0.104</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19 :</head><label>19</label><figDesc>Confusion Score Delta for 3D models. Lower delta means larger gain in discriminative power of these two classes. Top-5 and Lowest-2 entries are displayed.</figDesc><table><row><cell>burger</cell><cell></cell></row><row><cell>cake</cell><cell></cell></row><row><cell>carrots</cell><cell></cell></row><row><cell>chips</cell><cell></cell></row><row><cell>doughnuts</cell><cell></cell></row><row><cell>hotdog</cell><cell></cell></row><row><cell>ice cream</cell><cell></cell></row><row><cell>spaghetti</cell><cell></cell></row><row><cell>watermelon</cell><cell></cell></row><row><cell>2D Model</cell><cell>3D Model</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For example, "beekeeping" can be transformed to "beekeep", and "keeping bee".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with cotraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A short note on the kinetics-700 human action dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal attentive alignment for large-scale video domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekwon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6321" to="6330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Domain adaptation in computer vision applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning everything about anything: Webly-supervised visual concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3270" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computer vision: a modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Prentice Hall Professional Technical Reference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinit: Learning video representations without a single labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estíbaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect concepts from webly-labeled video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1746" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Do less and achieve more: Training cnns for action recognition utilizing action images from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="334" to="345" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with spatial-temporal discriminative filter banks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Modolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised selftraining of object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WACV/MOTION</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal segment networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pa3d: Pose-action 3d machine for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recognition from web data: A progressive filtering approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5303" to="5315" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Eventnet: A large scale structured concept library for complex event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangnan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="471" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5154" to="5162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">S4l: Selfsupervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Consensus-driven propagation in massive unlabeled data for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="568" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hacs: Human action clips and segments dataset for recognition and temporal localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8668" to="8678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>2019. 9</idno>
		<ptr target="https://github.com/open-mmlab/mmaction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fine-grained video categorization with redundancy reduction attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="136" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CALD-02-107</idno>
	</analytic>
	<monogr>
		<title level="j">CMU CALD tech report CMU</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
