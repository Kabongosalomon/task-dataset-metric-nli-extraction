<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2016 REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
							<email>t.rocktaschel@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Tomáš Kočiský &amp; Phil Blunsom Google DeepMind &amp;</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2016 REASONING ABOUT ENTAILMENT WITH NEURAL ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The ability to determine the semantic relationship between two sentences is an integral part of machines that understand and reason with natural language. Recognizing textual entailment (RTE) is the task of determining whether two natural language sentences are (i) contradicting each other, (ii) not related, or whether (iii) the first sentence (called premise) entails the second sentence (called hypothesis). This task is important since many natural language processing (NLP) problems, such as information extraction, relation extraction, text summarization or machine translation, rely on it explicitly or implicitly and could benefit from more accurate RTE systems <ref type="bibr" target="#b5">(Dagan et al., 2006)</ref>.</p><p>State-of-the-art systems for RTE so far relied heavily on engineered NLP pipelines, extensive manual creation of features, as well as various external resources and specialized subcomponents such as negation detection (e.g. <ref type="bibr" target="#b14">Lai and Hockenmaier, 2014;</ref><ref type="bibr" target="#b11">Jimenez et al., 2014;</ref><ref type="bibr" target="#b27">Zhao et al., 2014;</ref><ref type="bibr" target="#b2">Beltagy et al., 2015)</ref>. Despite the success of neural networks for paraphrase detection (e.g. <ref type="bibr" target="#b19">Socher et al., 2011;</ref><ref type="bibr" target="#b10">Hu et al., 2014;</ref><ref type="bibr" target="#b25">Yin and Schütze, 2015)</ref>, end-to-end differentiable neural architectures failed to get close to acceptable performance for RTE due to the lack of large high-quality datasets. An end-to-end differentiable solution to RTE is desirable, since it avoids specific assumptions about the underlying language. In particular, there is no need for language features like part-of-speech tags or dependency parses. Furthermore, a generic sequence-to-sequence solution allows to extend the concept of capturing entailment across any sequential data, not only natural language.</p><p>Recently, <ref type="bibr" target="#b3">Bowman et al. (2015)</ref> published the Stanford Natural Language Inference (SNLI) corpus accompanied by a neural network with long short-term memory units <ref type="bibr">(LSTM, Hochreiter and Schmidhuber, 1997)</ref>, which achieves an accuracy of 77.6% for RTE on this dataset. It is the first time a generic neural model without hand-crafted features got close to the accuracy of a simple lexicalized classifier with engineered features for RTE. This can be explained by the high quality 1 arXiv:1509.06664v4 [cs.CL] 1 Mar 2016 and size of SNLI compared to the two orders of magnitude smaller and partly synthetic datasets so far used to evaluate RTE systems. Bowman et al.'s LSTM encodes the premise and hypothesis as dense fixed-length vectors whose concatenation is subsequently used in a multi-layer perceptron (MLP) for classification. In contrast, we are proposing an attentive neural network that is capable of reasoning over entailments of pairs of words and phrases by processing the hypothesis conditioned on the premise.</p><p>Our contributions are threefold: (i) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment, as opposed to mapping each sentence independently into a semantic space ( §2.2), (ii) We extend this model with a neural word-by-word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( §2.4), and (iii) We provide a detailed qualitative analysis of neural attention for RTE ( §4.1). Our benchmark LSTM achieves an accuracy of 80.9% on SNLI, outperforming a simple lexicalized classifier tailored to RTE by 2.7 percentage points. An extension with word-by-word neural attention surpasses this strong benchmark LSTM result by 2.6 percentage points, setting a new state-of-the-art accuracy of 83.5% for recognizing entailment on SNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODS</head><p>In this section we discuss LSTMs ( §2.1) and describe how they can be applied to RTE ( §2.2). We introduce an extension of an LSTM for RTE with neural attention ( §2.3) and word-by-word attention ( §2.4). Finally, we show how such attentive models can easily be used for attending both ways: over the premise conditioned on the hypothesis and over the hypothesis conditioned on the premise ( §2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTMS</head><p>Recurrent neural networks (RNNs) with long short-term memory (LSTM) units <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref> have been successfully applied to a wide range of NLP tasks, such as machine translation , constituency parsing , language modeling <ref type="bibr" target="#b26">(Zaremba et al., 2014)</ref> and recently <ref type="bibr">RTE (Bowman et al., 2015)</ref>. LSTMs encompass memory cells that can store information for a long period of time, as well as three types of gates that control the flow of information into and out of these cells: input gates (Eq. 2), forget gates (Eq. 3) and output gates (Eq. 4). Given an input vector x t at time step t, the previous output h t−1 and cell state c t−1 , an LSTM with hidden size k computes the next output h t and cell state c t as</p><formula xml:id="formula_0">H = x t h t−1 (1) i t = σ(W i H + b i ) (2) f t = σ(W f H + b f ) (3) o t = σ(W o H + b o ) (4) c t = f t c t−1 + i t tanh(W c H + b c ) (5) h t = o t tanh(c t )<label>(6)</label></formula><p>where</p><formula xml:id="formula_1">W i , W f , W o , W c ∈ R 2k×k are trained matrices and b i , b f , b o , b c ∈ R k trained</formula><p>biases that parameterize the gates and transformations of the input, σ denotes the element-wise application of the sigmoid function and the element-wise multiplication of two vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RECOGNIZING TEXTUAL ENTAILMENT WITH LSTMS</head><p>LSTMs can readily be used for RTE by independently encoding the premise and hypothesis as dense vectors and taking their concatenation as input to an MLP classifier <ref type="bibr" target="#b3">(Bowman et al., 2015)</ref>. This demonstrates that LSTMs can learn semantically rich sentence representations that are suitable for determining textual entailment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">CONDITIONAL ENCODING</head><p>In contrast to learning sentence representations, we are interested in neural models that read both sentences to determine entailment, thereby reasoning over entailments of pairs of words and phrases. <ref type="figure" target="#fig_0">Figure 1</ref> shows the high-level structure of this model. The premise (left) is read by an LSTM. A second LSTM with different parameters is reading a delimiter and the hypothesis (right), but its memory</p><formula xml:id="formula_2">x 1 c 1 h 1 x 2 c 2 h 2 x 3 c 3 h 3 x 4 c 4</formula><p>h 4</p><p>x 5 c 5</p><p>h 5 state is initialized with the last cell state of the previous LSTM (c 5 in the example), i.e. it is conditioned on the representation that the first LSTM built for the premise (A). We use word2vec vectors <ref type="bibr" target="#b16">(Mikolov et al., 2013)</ref> as word representations, which we do not optimize during training. Out-ofvocabulary words in the training set are randomly initialized by sampling values uniformly from (−0.05, 0.05) and optimized during training. 1 Out-of-vocabulary words encountered at inference time on the validation and test corpus are set to fixed random vectors. By not tuning representations of words for which we have word2vec vectors, we ensure that at inference time their representation stays close to unseen similar words for which we have word2vec embeddings. We use a linear layer to project word vectors to the dimensionality of the hidden size of the LSTM, yielding input vectors x i . Finally, for classification we use a softmax layer over the output of a non-linear projection of the last output vector (h 9 in the example) into the target space of the three classes (ENTAILMENT, NEUTRAL or CONTRADICTION), and train using the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">ATTENTION</head><p>Attentive neural networks have recently demonstrated success in a wide range of tasks ranging from handwriting synthesis <ref type="bibr" target="#b6">(Graves, 2013)</ref>, digit classification <ref type="bibr" target="#b17">(Mnih et al., 2014)</ref>, machine translation , image captioning <ref type="bibr" target="#b24">(Xu et al., 2015)</ref>, speech recognition <ref type="bibr" target="#b4">(Chorowski et al., 2015)</ref> and sentence summarization <ref type="bibr" target="#b18">(Rush et al., 2015)</ref>, to geometric reasoning <ref type="bibr" target="#b23">(Vinyals et al., 2015)</ref>. The idea is to allow the model to attend over past output vectors (see <ref type="figure" target="#fig_0">Figure 1 B</ref>), thereby mitigating the LSTM's cell state bottleneck. More precisely, an LSTM with attention for RTE does not need to capture the whole semantics of the premise in its cell state. Instead, it is sufficient to output vectors while reading the premise and accumulating a representation in the cell state that informs the second LSTM which of the output vectors of the premise it needs to attend over to determine the RTE class.</p><p>Let Y ∈ R k×L be a matrix consisting of output vectors [h 1 · · · h L ] that the first LSTM produced when reading the L words of the premise, where k is a hyperparameter denoting the size of embeddings and hidden layers. Furthermore, let e L ∈ R L be a vector of 1s and h N be the last output vector after the premise and hypothesis were processed by the two LSTMs respectively. The attention mechanism will produce a vector α of attention weights and a weighted representation r of the premise via</p><formula xml:id="formula_3">M = tanh(W y Y + W h h N ⊗ e L ) M ∈ R k×L (7) α = softmax(w T M) α ∈ R L (8) r = Yα T r ∈ R k<label>(9)</label></formula><p>where W y , W h ∈ R k×k are trained projection matrices, w ∈ R k is a trained parameter vector and w T denotes its transpose. Note that the outer product W h h N ⊗ e L is repeating the linearly transformed h N as many times as there are words in the premise (i.e. L times). Hence, the intermediate attention representation m i (ith column vector in M) of the ith word in the premise is obtained from a non-linear combination of the premise's output vector h i (ith column vector in Y) and the transformed h N . The attention weight for the ith word in the premise is the result of a weighted combination (parameterized by w) of values in m i .</p><p>The final sentence-pair representation is obtained from a non-linear combination of the attentionweighted representation r of the premise and the last output vector h N using</p><formula xml:id="formula_4">h * = tanh(W p r + W x h N ) h * ∈ R k<label>(10)</label></formula><p>where W p , W x ∈ R k×k are trained projection matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">WORD-BY-WORD ATTENTION</head><p>For determining whether one sentence entails another it can be a good strategy to check for entailment or contradiction of individual word-and phrase-pairs. To encourage such behavior we employ neural word-by-word attention similar to , Hermann et al. <ref type="formula">(2015)</ref> and <ref type="bibr" target="#b18">Rush et al. (2015)</ref>. The difference is that we do not use attention to generate words, but to obtain a sentence-pair encoding from fine-grained reasoning via soft-alignment of words and phrases in the premise and hypothesis. In our case, this amounts to attending over the first LSTM's output vectors of the premise while the second LSTM processes the hypothesis one word at a time, thus generating attention weight-vectors α t over all output vectors of the premise for every word x t with t ∈ (L + 1, N ) in the hypothesis <ref type="figure" target="#fig_0">(Figure 1 C)</ref>. This can be modeled as follows:</p><formula xml:id="formula_5">M t = tanh(W y Y + (W h h t + W r r t−1 ) ⊗ e L ) M t ∈ R k×L (11) α t = softmax(w T M t ) α t ∈ R L (12) r t = Yα T t + tanh(W t r t−1 ) r t ∈ R k<label>(13)</label></formula><p>Note that r t is dependent on the previous attention representation r t−1 to inform the model about what was attended over in the previous step (see <ref type="bibr">Eq. 11 and 13)</ref>.</p><p>As in the previous section, the final sentence-pair representation is obtained from a non-linear combination of the last attention-weighted representation of the premise (here based on the last word of the hypothesis) r N and the last output vector using</p><formula xml:id="formula_6">h * = tanh(W p r N + W x h N ) h * ∈ R k<label>(14)</label></formula><p>2.5 TWO-WAY ATTENTION Inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding <ref type="bibr">(Graves and Schmidhuber, 2005)</ref>, we introduce two-way attention for RTE. The idea is to use the same model (i.e. same structure and weights) to attend over the premise conditioned on the hypothesis, as well as to attend over the hypothesis conditioned on the premise, by simply swapping the two sequences. This produces two sentence-pair representations that we concatenate for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We conduct experiments on the Stanford Natural Language Inference corpus <ref type="bibr">(SNLI, Bowman et al., 2015)</ref>. This corpus is two orders of magnitude larger than other existing RTE corpora such as Sentences Involving Compositional Knowledge (SICK, <ref type="bibr" target="#b15">Marelli et al., 2014)</ref>. Furthermore, a large part of training examples in SICK were generated heuristically from other examples. In contrast, all sentence-pairs in SNLI stem from human annotators. The size and quality of SNLI make it a suitable resource for training neural architectures such as the ones proposed in this paper.</p><p>We use ADAM <ref type="bibr" target="#b13">(Kingma and Ba, 2015)</ref> for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999. 2 For every model we perform a small grid search as opposed to using two independent LSTMs. In addition, we compare our attentive models to two benchmark LSTMs whose hidden sizes were chosen so that they have at least as many parameters as the attentive models. Since we are not tuning word vectors for which we have word2vec embeddings, the total number of parameters |θ| W+M of our models is considerably smaller. We also compare our models against the benchmark lexicalized classifier used by Bowman et al., which constructs features from the BLEU score between the premise and hypothesis, length difference, word overlap, uni-and bigrams, part-of-speech tags, as well as cross uni-and bigrams.</p><p>Conditional Encoding We found that processing the hypothesis conditioned on the premise instead of encoding each sentence independently gives an improvement of 3.3 percentage points in accuracy over Bowman et al.'s LSTM. We argue this is due to information being able to flow from the part of the model that processes the premise to the part that processes the hypothesis. Specifically, the model does not waste capacity on encoding the hypothesis (in fact it does not need to encode the hypothesis at all), but can read the hypothesis in a more focused way by checking words and phrases for contradictions and entailments based on the semantic representation of the premise. One interpretation is that the LSTM is approximating a finite-state automaton for RTE (cf. <ref type="bibr" target="#b0">Angeli and Manning, 2014)</ref>. Another difference to Bowman et al.'s model is that we are using word2vec instead of GloVe for word representations and, more importantly, do not fine-tune these word embeddings. The drop in accuracy from train to test set is less severe for our models, which suggest that fine-tuning word embeddings could be a cause of overfitting.</p><p>Our LSTM outperforms a simple lexicalized classifier by 2.7 percentage points. To the best of our knowledge, this is the first instance of a neural end-to-end differentiable model to achieve state-ofthe-art performance on a textual entailment dataset.</p><p>Attention By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159, and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding (one for the premise and one for the hypothesis conditioned on the representation of the premise). The attention model produces output vectors summarizing contextual information of the premise that is useful to attend over later when reading the hypothesis. Therefore, when reading the premise, the model does not have to build up a semantic representation of the whole premise, but instead a representation that helps attending over the right output vectors when processing the hypothesis. In contrast, the output vectors of the premise are not used by the benchmark LSTMs. Thus, these models have to build up a representation of the whole premise and carry it over through the cell state to the part that processes the hypothesis-a bottleneck that can be overcome to some degree by using attention.</p><p>Word-by-word Attention Enabling the model to attend over output vectors of the premise for every word in the hypothesis yields another 1.2 percentage point improvement compared to attending based only on the last output vector of the premise. We argue that this is due to the model being able to check for entailment or contradiction of individual words and phrases in the hypothesis, and demonstrate this effect in the qualitative analysis below.</p><p>Two-way Attention Allowing the model to also attend over the hypothesis based on the premise does not seem to improve performance for RTE. We suspect that this is due to entailment being an asymmetric relation. Hence, using the same LSTM to encode the hypothesis (in one direction) and the premise (in the other direction) might lead to noise in the training signal. This could be addressed by training different LSTMs at the cost of doubling the number of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">QUALITATIVE ANALYSIS</head><p>It is instructive to analyze which output representations the model is attending over when deciding the class of an RTE example. Note that interpretations based on attention weights have to be taken with care since the model is not forced to solely rely on representations obtained from attention (see h N in Eq. 10 and 14). In the following we visualize and discuss the attention patterns of the presented attentive models. For each attentive model we hand-picked examples from ten randomly drawn samples of the validation set.</p><p>Attention <ref type="figure" target="#fig_1">Figure 2</ref> shows to what extent the attentive model focuses on contextual representations of the premise after both LSTMs processed the premise and hypothesis respectively. Note how the model pays attention to output vectors of words that are semantically coherent with the premise ("riding" and "rides", "animal" and "camel", 2a) or in contradiction, as caused by a single word ("blue" vs. "pink", 2b) or multiple words ("swim" and "lake" vs. "frolicking" and "grass", 2c). Interestingly, the model shows contextual understanding by not attending over "yellow", the color of the toy, but "pink", the color of the coat. However, for more involved examples with longer premises we found that attention is more uniformly distributed (2d). This suggests that conditioning attention only on the last output has limitations when multiple words need to be considered for deciding the RTE class.</p><p>Word-by-word Attention Visualizations of word-by-word attention are depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. We found that word-by-word attention can easily detect if the hypothesis is simply a reordering of words in the premise (3a). Furthermore, it is able to resolve synonyms ("airplane" and "aircraft", 3c) and capable of matching multi-word expressions to single words ("garbage can" to "trashcan", 3b). It is also noteworthy that irrelevant parts of the premise, such as words capturing little meaning or whole uninformative relative clauses, are correctly neglected for determining entailment ("which also has a rope leading out of it", 3b).</p><p>Word-by-word attention seems to also work well when words in the premise and hypothesis are connected via deeper semantics or common-sense knowledge ("snow" can be found "outside" and a "mother" is an "adult", 3e and 3g). Furthermore, the model is able to resolve one-to-many relationships ("kids" to "boy" and "girl", 3d)</p><p>Attention can fail, for example when the two sentences and their words are entirely unrelated (3f).</p><p>In such cases, the model seems to back up to attending over function words, and the sentence-pair representation is likely dominated by the last output vector (h N ) instead of the attention-weighted representation (see Eq. 14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we show how the state-of-the-art in recognizing textual entailment on a large, humancurated and annotated corpus, can be improved with general end-to-end differentiable models. Our results demonstrate that LSTM recurrent neural networks that read pairs of sequences to produce a final representation from which a simple classifier predicts entailment, outperform both a neural baseline as well as a classifier with hand-engineered features. Extending these models with attention over the premise provides further improvements to the predictive abilities of the system, resulting in a new state-of-the-art accuracy for recognizing entailment on the Stanford Natural Language Inference corpus.</p><p>The models presented here are general sequence models, requiring no appeal to Natural Languagespecific processing beyond tokenization, and are therefore a suitable target for transfer learning through pre-training the recurrent systems on other corpora, and conversely, applying the models trained on this corpus to other entailment tasks. Future work will focus on such transfer learning tasks, as well as scaling the methods presented here to larger units of text (e.g. paragraphs and entire documents) using hierarchical attention mechanisms. Additionally, it would be worthwhile exploring how other, more structured forms of attention (e.g. <ref type="bibr" target="#b20">Sukhbaatar et al., 2015)</ref>, or other forms of differentiable memory (e.g. <ref type="bibr" target="#b8">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b12">Joulin and Mikolov, 2015)</ref> could help improve performance on RTE over the neural models presented in this paper. Furthermore, we aim to investigate the application of these generic models to non-natural language sequential entailment problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Recognizing textual entailment using (A) conditional encoding via two LSTMs, one over the premise and one over the hypothesis conditioned on the representation of the premise (c 5 ), (B) attention only based on the last output vector (h 9 ) or (C) word-by-word attention based on all output vectors of the hypothesis (h 7 , h 8 and h 9 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Attention visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Word-by-word attention visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the SNLI corpus.</figDesc><table><row><cell>Model</cell><cell>k</cell><cell cols="2">|θ| W+M |θ| M</cell><cell cols="3">Train Dev Test</cell></row><row><cell cols="2">Lexicalized classifier (Bowman et al., 2015) -</cell><cell>-</cell><cell>-</cell><cell>99.7</cell><cell>-</cell><cell>78.2</cell></row><row><cell>LSTM (Bowman et al., 2015)</cell><cell cols="4">100 ≈ 10M 221k 84.4</cell><cell>-</cell><cell>77.6</cell></row><row><cell>Conditional encoding, shared</cell><cell cols="2">100 3.8M</cell><cell cols="2">111k 83.7</cell><cell cols="2">81.9 80.9</cell></row><row><cell>Conditional encoding, shared</cell><cell cols="2">159 3.9M</cell><cell cols="2">252k 84.4</cell><cell cols="2">83.0 81.4</cell></row><row><cell>Conditional encoding</cell><cell cols="2">116 3.9M</cell><cell cols="2">252k 83.5</cell><cell cols="2">82.1 80.9</cell></row><row><cell>Attention</cell><cell cols="2">100 3.9M</cell><cell cols="2">242k 85.4</cell><cell cols="2">83.2 82.3</cell></row><row><cell>Attention, two-way</cell><cell cols="2">100 3.9M</cell><cell cols="2">242k 86.5</cell><cell cols="2">83.0 82.4</cell></row><row><cell>Word-by-word attention</cell><cell cols="2">100 3.9M</cell><cell cols="2">252k 85.3</cell><cell cols="2">83.7 83.5</cell></row><row><cell>Word-by-word attention, two-way</cell><cell cols="2">100 3.9M</cell><cell cols="2">252k 86.6</cell><cell cols="2">83.6 83.2</cell></row><row><cell cols="7">over combinations of the initial learning rate [1E-4, 3E-4, 1E-3], dropout 3 [0.0, 0.1, 0.2] and 2 -</cell></row><row><cell cols="7">regularization strength [0.0, 1E-4, 3E-4, 1E-3]. Subsequently, we take the best configuration based</cell></row><row><cell cols="6">on performance on the validation set, and evaluate only that configuration on the test set.</cell></row><row><cell>4 RESULTS AND DISCUSSION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results on the SNLI corpus are summarized in Table 1. The total number of model parameters, including tunable word representations, is denoted by |θ| W+M (without word representations |θ| M ). To ensure a comparable number of parameters to Bowman et al.'s model that encodes the premise and hypothesis independently using one LSTM, we also run experiments for conditional encoding where the parameters of both LSTMs are shared ("Conditional encoding, shared" with k = 100)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We found 12.1k words in SNLI for which we could not obtain word2vec embeddings, resulting in 3.65M tunable parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Standard configuration recommended by Kingma and Ba.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As in<ref type="bibr" target="#b26">Zaremba et al. (2014)</ref>, we apply dropout only on the inputs and outputs of the network.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Nando de Freitas, Samuel Bowman, Jonathan Berant, Danqi Chen, Christopher Manning, and the anonymous ICLR reviewers for their helpful comments on drafts of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Naturalli: Natural logic inference for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Representing meaning with a combination of logical form and vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.06816</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3944</biblScope>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alex Graves and Jürgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
	</analytic>
	<monogr>
		<title level="m">Neural turing machines</title>
		<editor>Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">732</biblScope>
		</imprint>
	</monogr>
	<note>Av Juan Dios Bátiz, and Av Mendizábal</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01007</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Illinois-lh: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc Vv</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Tian Tian Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
