<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Whan</forename><surname>Yoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Seo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyun</forename><surname>Moon</surname></persName>
						</author>
						<title level="a" type="main">TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Handling previously unseen tasks after given only a few training examples continues to be a tough challenge in machine learning. We propose TapNets, neural networks augmented with taskadaptive projection for improved few-shot learning. Here, employing a meta-learning strategy with episode-based training, a network and a set of per-class reference vectors are learned across widely varying tasks. At the same time, for every episode, features in the embedding space are linearly projected into a new space as a form of quick task-specific conditioning. The training loss is obtained based on a distance metric between the query and the reference vectors in the projection space. Excellent generalization results in this way. When tested on the Omniglot, miniImageNet and tieredImageNet datasets, we obtain state of the art classification accuracies under various few-shot scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot learning promises to allow machines to carry out tasks that are previously unencountered, using only a small number of relevant examples. As such, few-shot learning finds wide applications, where labeled data are scarce or expensive, which is far more often the case than not. Unfortunately, despite immense interest and active research in recent years, few-shot learning remains an elusive challenge to machine learning community. For example, while deep networks now routinely offer near-perfect classification scores on standard image test datasets given ample training, reported results on few-shot learning still fall well below the levels that would be considered reliable in crucial real world settings.</p><p>One popular way of developing few-shot learning strategies is to take a meta-learning perspective coupled with episodic training <ref type="bibr" target="#b21">(Vinyals et al., 2016;</ref><ref type="bibr" target="#b14">Ravi &amp; Larochelle, 2017;</ref><ref type="bibr" target="#b0">Chen et al., 2019)</ref>. Meta-learning seems to convey somewhat different meanings to different people, but none would disagree that it is about learning a general strategy to learn new tasks <ref type="bibr" target="#b20">(Vanschoren, 2018)</ref>. Episodic training refers to a training method in which widely varying tasks (or episodes) are presented to the learning model one by one, with each episode containing only a few labeled examples. The repetitive exposure to previously unseen tasks, each time with low samples, during this initial learning or meta-training stage seems to provide a viable option for preparing the learner for quick adaptation to new data <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref>.</p><p>Among the well-known approaches in this direction are the metric-based learners like Matching Networks <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> and Prototypical Networks <ref type="bibr" target="#b19">(Snell et al., 2017)</ref>. These methods all incorporate non-parametric, distancebased learning, where embedding space is trained to minimize a relevant distance metric across episodes before stabilizing to perform actual few-shot classification. Matching Networks train separate networks to process labeled samples and query samples, and utilize each labeled sample in the embedding space as reference points in classifying the query samples. Prototypical Networks employ only one embedding network with its per-class centroids used as classification references in the embedding space. Based on learning only a single feed-forward feature extractor, Prototypical Networks offer a surprisingly good ability to generalize to new tasks, as an inductive bias seems to settle in somehow via episodic training.</p><p>We are also interested in distance-based learning with no fine-tuning of parameters beyond the episodic meta-training stage. Relative to prior work, the unique characteristic of our method is in explicit task-dependent conditioning via linear projection of embedded features. Once the neural network outputs are projected into a new space, classification is done there based on distances from per-class reference vectors. Both the neural network and the reference vectors are learned across the sequence of episodes reflecting widely varying tasks, while the projected classification space is constructed anew specific to each episode. The projection to an alternative classification space is done via linear nulling of errors between the embedded features and the per-class references. Unlike in <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> and <ref type="bibr" target="#b19">(Snell et al., 2017)</ref>, class-representing vectors in our scheme are not the outputs of an embedding function. Rather, the references in our method are a simple set of stand-alone vectors not directly coupled to input images, although they are updated for each episode based on distances from the embedded query images projected in the classification space.</p><p>The combination of across-task learning of the network and per-class reference vectors with a quick task-adaptive conditioning of classification space allows excellent generalization. Extensive testing on the Omniglot, miniImageNet and tieredImageNet datasets show that the proposed network augmented with task-adaptive projection (TapNet) yields state of the art few-shot classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task-Adaptive Projection Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Description</head><p>TapNet consists of three key elements: an embedding network f θ , a set of per-class reference vectors Φ, and the taskdependent adaptive projection or mapping M of embedded features to a new classification space. Φ = [φ 1 ; · · · ; φ Nc ] is a matrix whose k th row is the per-class reference (row) vector φ k . M denotes projection or mapping, but sometimes would mean the projection space itself. See <ref type="figure" target="#fig_0">Fig. 1</ref>, where a new episode is being presented to the model during the sequential episodic training process. An episode consists of a support set of images/labels {(x 1 , y 1 ), ..., (x Nc , y Nc )} as well as a query set {(x 1 ,ŷ 1 ), ..., (x Nc ,ŷ Nc )}. For clear illustration, there is only one image/label pair for each of N c given classes, in either set here.</p><p>Given the new support set, as well as f θ and Φ learned through the last episode stage, a projection space M is first constructed such that the embedded feature vectors f θ (x k )'s and the class reference vectors φ k 's with matching labels align closely when projected into M. The details of projection space construction will be given shortly.</p><p>The network f θ and the reference set Φ are in turn updated according to softmax based on Euclidean distance d (·, ·) between the mapped query image and reference vectors:</p><formula xml:id="formula_0">softmax − d(M(f θ (x k )), M(φ k )) = exp − d (M(f θ (x k )), M(φ k )) l exp − d (M(f θ (x k )), M(φ l ))<label>(1)</label></formula><p>which is averaged over all classes k. Here, M(z) denotes projection of row vector z, and all vectors including the embedded feature vector f θ (x) of input x are assumed to be row vectors, unless specified otherwise. The updated f θ and Φ are passed to the next episode processing stage. The projection and parameter updates continue for each episode until all given episodes are exhausted.</p><p>Come the few-shot test stage, again a projection space is computed to align the embedded features of the presented shots appearing at the output of the network with the references, both of which are now fixed after having learned throughout the episodic meta-training process. The query image is finally compared with the references in the projection space for final classification.</p><p>In summary, the embedder f θ and the per-class reference vectors Φ are learned across varying tasks (episodes) while the projection space M is built specific to the given task, providing a quick task-dependent conditioning. This combination results in an excellent ability to generalize to new data, as extensive experimental results will verify shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Construction of Task-Adaptive Projection Space</head><p>Finding the mapping function or projection space M is based on removing misalignment between the taskembedded features and the references. To handle general cases with multiple example images per class, let c k be the per-class average of the embedded features for class k corresponding to the images out of the support set.</p><p>We wish to find a mapper M such that c k and the matching reference vector φ k are highly aligned in the mapped space. At the same time, it would be beneficial to make c k and the non-matching weights φ l for all l = k well-separated in the same space. It turns out that a simple linear projection that does not require any learning offers an effective solution.</p><p>The idea is to find a projection space where c k aligns with a modified vectorφ</p><formula xml:id="formula_1">k = φ k − 1 N c − 1 l =k φ l<label>(2)</label></formula><p>where the factor 1/(N c − 1) provides natural normalization reflecting the number of non-matching vectors. This is to say that given the error vector defined as</p><formula xml:id="formula_2">k =φ k φ k − c k c k ,<label>(3)</label></formula><p>M can be found such that the projected error vector k M is zero for every k, where M is now a matrix whose columns span the projection space. In other words, M is found by a linear nulling of errors k .φ k and c k are normalized to remove power imbalance between them. Formally, we express</p><formula xml:id="formula_3">M = null D [ 1 ; · · · ; Nc ] ,<label>(4)</label></formula><p>where D is the column dimension of M. A well-known solution is through the singular value decomposition (SVD)  Note that D can be set less than L − N c , indicating a possibility of significant dimension reduction. Our empirical observation suggests that a dimension reduction sometimes actually improves few-shot classification accuracies. Note that for SVD of an (n × m) matrix with n ≤ m, computational complexity is O(mn 2 ). The required SVD computational complexity for obtaining our projection is thus O(LN 2 c ), which is small compared to typical model complexity.</p><p>We also remark that because the solution to linear nulling as formulated above exists irrespective of particular labeling of φ k 's, we do not need to relabel the reference vectors in every episode; the same label sticks to each reference vector throughout the episodic training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training</head><p>As mentioned, training of the embedding network f θ and the reference vectors Φ is done via episodic training, following <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref>. The detailed steps of learning TapNets is provided in Algorithm 1.</p><p>For each training episode, N c classes are randomly chosen from the training set of a given dataset. Then, for each class, N s labeled samples are randomly chosen as the support set S k , and N q labeled samples are chosen as the query set Q k , without any overlapping samples between the two sets. With the support set S k , the average network output vector c k is obtained for each class (in line 5). Based on the per-class average network output vectors, error vectors are obtained for all classes (in line 6) without any relabeling on the reference vectors. Then the projection space M is computed as a null-space of the error signals, as explained in the prior subsection. For each query input, the Euclidean distances to the reference vectors in the projection space M are measured, and the training loss is computed using these distances. The average training loss is obtained over all N q query inputs for each of N c given classes (in line 11 to 14). The learnable parameters θ of the embedding network and the references Φ are now updated based on the average training loss (in line 16). This process gets repeated for every remaining episode with new classes of images and queries.</p><p>Following <ref type="bibr" target="#b19">(Snell et al., 2017)</ref>, we also use a larger number of classes than N c during episodic training, for improved performance. For example, 20-way classification is used during episodic learning or meta-training whereas 5-way classification is done in the final few-shot learning and testing. In this case, a question remains as to how 5 reference vectors are chosen in the few-shot stage for linear projection out of 20 references that have been trained. For this, we first obtain the average vectors c k for the 5 given classes and then select the 5 reference vectors closest to the c k 's and also relabel them accordingly before the linear projection is carried out. Notice that with the higher-way training employed, TapNets can easily handle cases where the number of classes for actual few-shot classification is not known in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relation to Metric-Based Meta-Learners</head><p>Two well-known metric-based few-shot learning algorithms are Matching Networks of <ref type="bibr" target="#b21">(Vinyals et al., 2016)</ref> and Prototypical Networks of <ref type="bibr" target="#b19">(Snell et al., 2017)</ref>. Matching Networks yield decisions based on matching the output of a network driven by a query sample to the output of another network fed by labeled samples. In Matching Networks, the similarities are measured between the query output and the labeled sample outputs on two separate embeddings. The labeled sample outputs are provided as reference points for estimating the label of the query. On the other hand, Prototypical Networks are trained to minimize the distance metric between the per-class average outputs and the query output from the single embedding network. The per-class average Algorithm 1 Episodic learning is done by N E episodes. Each episode E i consists of N (image, label) pairs. These N shots are composed of N s support images/labels and N q queries from each of N c given classes and N = N c (N s + N q ). L train is the loss for training learnable parameters. The Euclidean distance between two vectors is denoted as d(·, ·). D is the dimensionality of projection space M.</p><formula xml:id="formula_4">Input: Training set E = E 1 , ..., E N E where E i is an episode of N image/label pairs over N c classes. E (k) i = {S k , Q k } = {(x k,1 , y k,1 ), ..., (x k,Ns , y k,Ns )}, {(x k,1 ,ŷ k,1 ), ..., (x k,Nq ,ŷ k,Nq )} is a subset of E i corresponding to label k. 1: for i in {1, ..., N E } do 2: L train ← 0 3: for k in {1, ..., N c } do 4: c k ← 1 Ns Ns n=1 f θ (x k,n ) 5: k ← φ k −(1/(Nc−1)) l =k φ l φ k −(1/(Nc−1)) l =k φ l − c k c k 6: end for 7: M ← null D [ 1 ; · · · ; Nc ] 8: for k in {1, ..., N c } do 9:</formula><p>for q in 1, ..., N q do 10: Update θ, Φ minimizing L train via optimizer 14: end for outputs on the embedding space work as references and the embedding network is trained to make a given query output stay close to the correct reference while pushing it away from the incorrect reference points.</p><formula xml:id="formula_5">L train ← L train + 1 N c N q   d(M(f θ (x k,q )), M(φ k )) + log l exp(−d(M(f θ (x k,q )), M(φ l )))</formula><p>Our TapNet also learns to minimize distance between the projected query and per-class references. Unlike Matching Networks and Prototypical Networks, however, there is explicit task-dependent conditioning in TapNets in the form of projection into a new classification space. Assuming oneshot support and query samples for clear illustration, the distance functions utilized by three methods are compared as:</p><formula xml:id="formula_6">Matching Networks: d(f θ (x k ), g φ (x k )) Prototypical Networks: d(f θ (x k ), f θ (x k )) TapNets: d(M(f θ (x k )), M(φ k )))</formula><p>wherex k and x k are the one-shot query and support samples, respectively, for class k. Matching Networks use two separate embedding functions f θ and g φ , while Prototypical Networks rely on a single embedding function. TapNets employ one embedding network but there is a learnable reference vector set φ k 's as well as task-conditioning linear projection M. Also note that the class references in Matching Networks and Prototypical Networks are embedded features themselves, but those in TapNets are not; rather, the per-class references in TapNets are stand-alone vectors that are not directly coupled to the input images.</p><p>Built upon a base Prototypical Network model, TADAM of <ref type="bibr" target="#b12">(Oreshkin et al., 2018)</ref> employs learned metric-scaling and additional task-dependent conditioning in the form of element-wise scaling and shifts for the feature vectors of component convolutional layers, similar to <ref type="bibr" target="#b13">(Perez et al., 2018)</ref>. But this conditioning requires learning of extra fullyconnected networks, whereas the task-conditioned M in TapNets is computed directly from the embedded features of the new task and the up-to-date references. A metricbased learner utilizing a nonlinear distance metric was also suggested in , where the distance metric itself is learned together with the embedding network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Relation to Memory-Augmented Neural Network</head><p>While our TapNet is close in spirit to the metric-based learners in the form of Matching Networks and Prototypical Networks as described above, it also bears a surprisingly close connection to the memory-augmented neural network (MANN) of <ref type="bibr" target="#b18">(Santoro et al., 2016)</ref>. MANN utilizes an external memory module with its contents rapidly adapting to new samples while the read and write weights learned across episodes. The explicit form of the memory module is an external matrix array M e designed to store information extracted by the controller network for a given episode. The memory read output vector r is seen as a weighted linear combination of the columns of M e : r ← i w r i m i . The read weight is proportional to the cosine similarity of the column i of the memory with the controller network output or key vector k:</p><formula xml:id="formula_7">w r i ← exp K (k, m i ) j =i exp K (k, m(j)) where K (·, ·)</formula><p>is the cosine correlation of two vectors. Approximating the exponentiation by a linear function, i.e., exp(K (k, m i )) 1 + K (k, m i ), and further dropping the class-independent constant, we can write w r ∼ M T e k T where w r is a column vector of the read weights. Eventually, we can approximate the read output r as</p><formula xml:id="formula_8">r ∼ M e M T e k T .</formula><p>In arriving at the final decision, this memory read output is multiplied by the matrix W = [w 1 ; · · · ; w Nc ] whose row vectors are learnable per-class weights:</p><formula xml:id="formula_9">WM e M T e k T<label>(5)</label></formula><p>While the inference from the direct branch of the hidden state of the long short-term memory (LSTM) is also used in the MANN, we consider only the inference from the read vector r which utilizes information from the external memory. The resulting vector of (5) is the inner-product similarities between the read output r and weights W.</p><p>Going back to our TapNet, if we were to use the cosine similarity instead of Euclidean distance, the distance profile between the query and the per-class references would be ΦMM T (f θ (x)) T , which is essentially the same as (5) of MANN, with Φ playing the same role as W, given that the key vector k of MANN is the same as the embedded feature of the input image for TapNets. Very interestingly, an important implication here is that the external memory array M e of MANN can be interpreted as a kind of taskadaptive projection space where the similarities between the query key k and the weights W are measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Types of Optimizers</head><p>There are other types of approaches often referred to as the optimization-based meta-learners. They aim to optimize the embedding network quickly so that the fine-tuned network successfully adapts to the given task. The metalearner LSTM of <ref type="bibr" target="#b14">(Ravi &amp; Larochelle, 2017)</ref> is one such approach, where an LSTM <ref type="bibr" target="#b5">(Hochreiter &amp; Schmidhuber, 1997</ref>) is trained to optimize another learner, which performs actual classification. There, the parameters of the few-shot learner are first set to the memory state of the LSTM, and then quickly learned based on the memory update rule of the LSTM, effectively capturing the knowledge from small samples.</p><p>The model-agnostic meta-learner (MAML) of <ref type="bibr" target="#b2">(Finn et al., 2017</ref>) sets up the model for easy fine-tuning so that a small number of gradient-based updates allows the model to yield a good generalization to new tasks. The fine-tuning method of MAML has been incorporated into many other schemes, such as Reptile of <ref type="bibr" target="#b11">(Nichol &amp; Schulman, 2018)</ref> and Platipus of <ref type="bibr" target="#b3">(Finn et al., 2018)</ref>. Very recently, meta-learners with latent embedding optimization (LEO) of <ref type="bibr" target="#b17">(Rusu et al., 2018)</ref> have been introduced that attempt at task-dependent initialization of the model parameters with additional finetuning of the parameters in a low-dimensional latent space. Also, separate pre-training is required over the same training dataset in order for LEO to work properly. There also exist other meta-learners employing different forms of taskconditioning <ref type="bibr" target="#b10">(Munkhdalai et al., 2018)</ref>. A method dubbed the simple neural attentive meta-learner (SNAIL) combines an embedding network with temporal convolution and soft attention to draw from past experience while attempting precision access at the same time <ref type="bibr" target="#b9">(Mishra et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Omniglot <ref type="bibr" target="#b7">(Lake et al., 2015)</ref> is a set of images of 1623 handwritten characters from 50 alphabets with 20 examples for each class. We have used 28×28 downsized grayscale images and introduced class-level data augmentation by random angle rotation of images in multiples of 90 • degrees, as done in prior works <ref type="bibr" target="#b18">(Santoro et al., 2016;</ref><ref type="bibr" target="#b19">Snell et al., 2017;</ref><ref type="bibr" target="#b21">Vinyals et al., 2016)</ref>. 1200 characters are used for training and test is done with the remaining characters.</p><p>miniImageNet <ref type="bibr" target="#b21">(Vinyals et al., 2016</ref>) is a dataset suggested by Vinyals et al. for few-shot classification of colored images. It is a subset of the ILSVRC-12 ImageNet dataset <ref type="bibr" target="#b16">(Russakovsky et al., 2015)</ref> with 100 classes and 600 images per class. We have used the splits introduced by Ravi and Larochelle <ref type="bibr" target="#b14">(Ravi &amp; Larochelle, 2017</ref>   <ref type="bibr" target="#b8">(Liu et al., 2018)</ref>) 53.31 ± 0.89% 72.69 ± 0.74% Relation Nets (as evaluated in <ref type="bibr" target="#b8">(Liu et al., 2018))</ref> 54.48 ± 0.93% 71.31 ± 0.78% Transductive Propagation Nets <ref type="bibr" target="#b8">(Liu et al., 2018)</ref> 59.91 ± 0.94% 73.30 ± 0.75% <ref type="table">TapNet (Ours)</ref> 63.08 ± 0.15% 80.26 ± 0.12% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>For all our experiments here, we employ ResNet-12 <ref type="bibr" target="#b4">(He et al., 2016)</ref> as the embedding network (results with smaller networks are also available as discussed in Supplementary Material). ResNet-12 has four residual blocks, each of which contains three 3×3 convolution layers and convolutional shortcut connection. Each convolution layer is followed by a batch normalization layer and ReLU activation. A 2×2 max-pooling is applied at the end of each residual block. At the top of the stack of residual blocks, we also apply a global average-pooling to reduce feature dimensionality. We use different numbers of channels for the three datasets. For 5-way miniImageNet and 5way tieredImageNet, the number of channels starts with 64, and doubles after the max-pooling is applied. For 20way Omniglot classification, the number of channels starts with 64 and then increases for the subsequent blocks, although less channels are employed at later blocks than the miniImageNet and tieredImageNet cases.</p><p>The Adam optimizer <ref type="bibr" target="#b6">(Kingma &amp; Ba, 2014)</ref> with an opti-mized learning-rate decay is employed. For all experiments, the initial learning rate is 10 −3 . In the 20-way Omniglot experiment, the learning rate is reduced by half at every 4.0 × 10 4 episodes, but for 5-way miniImageNet and 5-way tieredImageNet classification, we cut the learning rate by a factor of 10 at every 2.0 × 10 4 and 4.0 × 10 4 episodes, respectively, for 1-shot experiments and every 4.0 × 10 4 and 3.0 × 10 4 episodes, respectively, for 5-shot experiments.</p><p>For meta-training of the network, we adopt the higher-way training of Prototypical Networks of <ref type="bibr" target="#b19">(Snell et al., 2017)</ref>. We used 60-way episodes for 20-way Omniglot classification, and 20-way episodes for 5-way miniImageNet and tieredImageNet classification for training. In the test phase, we have to choose 20 and 5 references among 60 and 20 vectors, respectively. In selecting only a subset of reference vectors for testing purposes, relabeling is done. For each average network output chosen in arbitrary order, the closest vector among the remaining ones in Φ is tagged with the matching label. The closeness measure is the Euclidean distance in our experiments. After choosing the closest reference vectors, the projection space M is obtained for few-shot classification. The experimental results of our meta-learner in Tables 1 and 2 are based on 60-way initial learning for 20-way Omniglot and 20-way initial learning for 5-way miniImageNet and 5-way tieredImageNet classification.</p><p>For 20-way Omniglot classification, we used 1.0 × 10 5 training episodes with 15 query samples per class. For both 5-way miniImageNet and 5-way tieredImageNet settings, 5.0 × 10 4 training episodes with 8 query samples per class are used. For all experiments, we pick the best model with the highest validation accuracy during meta-training. Additional parameter settings are considered in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>In Tables 1 and 2, few-shot classification accuracies on the Omniglot, miniImageNet and tieredImageNet datasets are compared 1 . The performance in the 20-way Omniglot experiment is evaluated by the average accuracy over randomly chosen 1.0 × 10 4 test episodes with 5 query images for each class. On the other hand, the performance in 5-way miniImageNet and tieredImageNet is evaluated by the average accuracy and a 95% confidence interval over randomly chosen 3.0 × 10 4 test episodes with 15 query images for each class.</p><p>For the 20-way Omniglot results in <ref type="table" target="#tab_2">Table 1</ref>, TapNet shows the best performance for both 1-shot and 5-shot cases. For the 5-way miniImageNet results, our TapNet shows the best 1-shot accuracy and a 5-shot accuracy comparable to the 1 Codes are available on https://github.com/istarjun/TapNet best, that of TADAM-TC, in the sense that the confidence intervals overlap. TADAM-α is a model with metric scaling but without task-conditioning, built upon Prototypical Networks with the same ResNet-12 base architecture we use. TapNet achieves a higher accuracy than TADAM-α. TADAM-TC requires additional fully-connected layers for task-conditioning.</p><p>In <ref type="table" target="#tab_6">Table 2</ref>, the results for tieredImageNet experiments are presented. We compared our method with prior work as evaluated in <ref type="bibr" target="#b8">(Liu et al., 2018)</ref>. Our TapNet also achieves the best performance for both 1-shot and 5-shot classification tasks with considerable margins.</p><p>We remark that although better results have been reported recently in <ref type="bibr" target="#b17">(Rusu et al., 2018)</ref>, the base feature extractor used there is Wide ResNet-28-10, which is considerably larger -more than twice as deep and also much wider -than our base model, ResNet-12. In addition, for the method of <ref type="bibr" target="#b17">(Rusu et al., 2018)</ref>, a separate round of pre-training is necessary over the same training set. At this point, we do not make direct performance comparison with <ref type="bibr" target="#b17">(Rusu et al., 2018)</ref>. Additional experimental results with varying network sizes are presented in Supplementary Material.  <ref type="figure">Figure 3</ref>. Minimum distance between the normalized references t-SNE plot. <ref type="figure" target="#fig_2">Fig. 2</ref> is a t-SNE visualization of network embedding space and projection space for TapNets trained with the Omniglot dataset. The reference vector φ k 's are marked as the alphabet images in the circles. We can observe that the extracted images and the corresponding reference vectors are not located closely in the embedding space, while they lie close in the projection space. Moreover, note that the projected images are not only closely located to the matching references, but also tend to be away from nonmatching references. This is due to the inclusion of the non-matching references in the modified reference vector φ k = φ k − (1/(N c − 1)) l =k φ l , utilized in defining the error vector to null in the linear projection process.</p><p>Learning trend of references Φ. In <ref type="figure" target="#fig_4">Figs. 3 and 4</ref>  <ref type="formula" target="#formula_0">(41000)</ref> Optimal <ref type="formula" target="#formula_0">(41000)</ref> Optimal <ref type="formula" target="#formula_0">(41000)</ref> Optimal <ref type="formula">(</ref>   <ref type="figure">Fig. 3</ref> shows a plot of the Euclidean minimum distance between the normalized φ k 's. The minimum distance escalates quickly during the first 1.0 × 10 4 episodes, and then increases slowly until the learning rate decay after 4.0 × 10 4 episodes. This implies that the references tend to grow apart, showing improving separation over time. The solid dot indicates the moment where the model yields the highest validation accuracy. We can develop further insights by exploring the trajectories of the vector tips of φ k 's. <ref type="figure" target="#fig_4">Fig. 4a</ref> represents a t-SNE visualization of the trajectories of 20 references. From the random initial points which are not well-separated, the references spread out for better separation, consistent with the observation from the minimum distance plot. In <ref type="figure" target="#fig_4">Fig. 4b</ref>, only 4 references are selected for a clearer illustration. For a given reference vector, the numbers shown by the pointing arrows represent time stamps as indicated by the number of episodes processed. As seen, there appears to be a sudden jump at some point as the vector tip grows radially, and as it reaches around the optimal point it no longer seems to move outward, tending to settle into a place.</p><p>Dimensionality of projection space. We study the effects of the dimensionality of projection space M on generalization performance. For the 5-way miniImageNet experiments, the full dimensionality D of M is (512 − N c ). This means that D is 492 for training and 507 for testing. We carry out the 5-way 5-shot miniImageNet classification test with projection using various values of D. In <ref type="figure">Fig. 5</ref>, we observe that the test accuracy improves rapidly as dimensionality reaches around 50 and then settles down with a slight peak around 200. In our experiments we typically use full dimension with an exception of 5-way, 5-shot miniImageNet classification, where D = 200 is used, as taught by <ref type="figure">Fig. 5</ref>.  <ref type="figure">Figure 5</ref>. Test accuracies for 5-way 5-shot miniImageNet classification with different projection space dimensionalities</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we proposed a few-shot learning algorithm aided by a linear transformer that performed task-specific null-space projection of the network output. The original feature space is linearly projected into another space where actual classification takes place. Both the embedding network and the per-class references are learned over the entire episodes while projection is specific to the given episode. The resulting combination shows an excellent generalization capability to new tasks. State of the art few-shot classification accuracies have been observed on standard image datasets. Relationships to other metric-based meta-learners as well as memory-augmented networks have been explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Architecture Details</head><p>To obtain the results presented in the main paper, we employed ResNet-12, a residual network with 12 convolutional layers, as the feature extractor of TapNets. ResNet-12 is composed of four residual blocks and four max-pooling layers. Each residual block is constructed with three layers of 3 × 3 convolutions, followed by a batch normalization layer and an ReLU activation function. Each residual connection consists of a 3 ×3 convolutional layer and a batch normalization layer, and links the input to the last activation function. At the top of the residual block stack, global average pooling is also applied to reduce dimension. The four residual blocks have 64, 128, 256 and 512 respective channels. As mentioned in the main paper, this ResNet-12 is the same embedding network used in the task-dependent adaptive metric (TADAM) scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ablation Study</head><p>In this section, we show the results of ablation studies involving the following issues: composition of training episodes, learning rate optimization, regularization hyperparameters and reference vector settings. We used the Adam optimizer for all experiments. Also, l2 regularization and dropout are employed. <ref type="figure" target="#fig_0">Fig. 1</ref> shows test accuracies for 5-way, 5-shot miniImageNet with different numbers of training classes and query samples per class. We used the Adam optimizer with an initial learning rate of 10 −3 and cut the learning rate by a factor of 10 every 4.0 × 10 4 episodes. l2 regularization with a weight decay rate of 5.0 × 10 −4 is used and dropout with ratio 0.2 is applied to every output of the max-pooling layers. As summarized in <ref type="table" target="#tab_2">Table 1</ref>, the best accuracy is obtained using  <ref type="table" target="#tab_6">Table 2</ref> shows the hyperparameter values used for TapNet experiments. For 20-way Omniglot and 5-way tieredImageNet experiments, we used step decays for the learning rate and dropout. In 5-way miniImageNet experiments, l2 regularization is also applied in addition to dropout and learning rate decays. The dropout ratio bracket indicates the dropout ratio applied to each of the four pooling layer outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Episode Composition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hyperparameters in Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Ablation Study for Reference Vectors</head><p>We test the 5-way, 5-shot miniImageNet classification accuracy while varying the settings on the reference vectors Φ. In the main paper, we use the modified reference vectors φ k = φ k − 1 Nc−1 l =k φ l when constructing the projection space, but use the original reference vectors φ k when classifying the query samples. Using the modified references in constructing projection space gives better separation among the classes in the projection space. By constructing projection space with the original reference vectors, Tap-Net achieves 75.53% accuracy, a degradation from 76.36%. Also, one can think of using the modified reference vectors in place of the original reference vectors during classification of the query samples. In this case, the classification accuracy also degrades to 74.61%.</p><p>For meta-training of TapNets, we adopted the higher way training. As a result, the number of prepared reference vectors is larger than the actual number of distinct classes  during evaluation. This leads to a study of the effect of particular reference vector selection. In the main paper, we selected those reference vectors closest to the class average vectors. We test two other possible reference selections here: one is simple random selection and the other is to select the farthest references to the class average. On the 5-way, 5-shot miniImageNet classification task, random selection shows a slightly degraded accuracy of 75.78% while the farthest reference selection yields a bit more drop to 75.11%. In summary, although choosing the reference vectors closest to the class averages gives the best performance, any particular selection seems to make only a small difference.</p><p>In Section 4.3 of the main paper, we studied the learning trend of the reference Φ based on minimum distance growth as well as visualization of the vector trajectories. As easily seen from the figures, the norm of each reference vector increased as training progressed. This naturally raises a question: would initializing the references with a larger norm be beneficial? The answer, however, turned out to be no; when the reference vectors were initialized to have a norm as large as the fully meta-trained reference vectors, the performance of TapNet actually dropped slightly to 75.78%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results for Varying Network Sizes</head><p>We now evaluate TapNets with varying embedding network sizes. The first option is the convolutional neural network (CNN) widely used in prior works such as Matching Networks or Prototypical Networks. It is based on four convolutional blocks, each of which consists of a 3×3 convolutional layer with 64 filters, stride 1 and padding along with a batch normalization layer, a ReLU activation and a 2×2 maxpooling. This CNN is denoted as "Conv4" in <ref type="table" target="#tab_7">Table 3</ref>. The second option is a smaller version of ResNet-12, like the one used in SNAIL. With this version of ResNet-12, each residual block is constructed with three 3 × 3 convolutional layers. Also, a 1 × 1 convolutional layer is used for residual connection for each block. This network consists of four residual blocks with 64, 96, 128 and 256 respective channels. We denote this network "ResNet-12-small" in comparison to the larger version of ResNet-12 in the main paper. The Adam optimizer is also used for optimizing for both Conv4 and ResNet-12-small.</p><p>We focus on 5-way miniImageNet classification and the measured accuracy results are presented in <ref type="table" target="#tab_7">Table 3</ref>. The few-shot learners are shown in four groups, depending on the required base network size. We notice significant performance differences in general as the model size/complexity changes. The first group uses the Conv4 network. Among the methods here, our TapNet achieves best 1-shot and 5shot accuracies. The methods in the second group, Transductive Propagation Networks (TPN) and Relation Networks, are also based on the Conv4 embedder, but require additional networks for certain purposes. TPN utilizes an additional convolutional block in the graph construction network, and Relation Networks use additional convolutional blocks in its relation module. Both these methods require significant extra learning efforts, compared to the learners relying mostly on the Conv4 base embedder. TPN achieves higher 1-shot and 5 shot accuracies than the methods in the first group. The next group of methods uses ResNet-12-small. SNAIL and adaResNet are compared with TapNet. TapNet again achieves the best 1-shot and 5-shot performance in this group. The methods in the last group utilize ResNet-12, which is the largest network among the feature extractors considered in this work. For 1-shot results, our TapNet once again provides the best accuracy. For 5-shot, TapNet's result is comparable to that of the best method, TADAM-TC, in the sense that the confidence intervals overlap. In summary, given the same base network, TapNet consistently gives either the best accuracy or one comparable to the best in 5-way miniImageNet classification among the well-known methods.</p><p>In <ref type="table">Table 4</ref>, we display additional results on 5-way tieredImageNet classification with Conv4 embedding. In the tieredImageNet experiment, we had a small modification to the embedding network in TapNet. We added a 2 × 2 average pooling layer on top of the Conv4 network. Also, for 1-shot tieredImageNet classification we found that it was beneficial to use the higher-shot training strategy; we adopted 4-shot meta-training for 1-shot classification. As a result, TapNet achieves the best accuracy for 5-shot classification, and the second best accuracy for 1-shot among the methods using the same Conv4 embedding network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>TapNet learning process of the matrix [ 1 ; · · · ; Nc ], namely, by taking D of the right singular vectors of the matrix, from index N c + 1 through N c +D. With L being the length of k , if L ≥ N c +D, then the projection space M with dimension D always exists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>t-SNE visualization of the network embedding space (left) and projection space (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>41000) (b) t-SNE visualization of four selected references</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Trajectories of Φ in episodic learning during the meta-training phase. First,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .</head><label>1</label><figDesc>Accuracies with different episode compositions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). For our experiment, we have used 84×84 downsized color images with a split of 64 training classes, 16 validation classes and 20 test classes.tieredImageNet<ref type="bibr" target="#b15">(Ren et al., 2018</ref>) is a dataset suggested by Ren et al. It is a larger subset of the ILSVRC-12 ImageNet dataset with 608 classes and 779,165 images in total. The classes in tieredImageNet are grouped into 34 categories corresponding to higher-level nodes in the ImageNet hierarchy curated by human<ref type="bibr" target="#b1">(Deng et al., 2009</ref>). These categories are split into 20 training, 6 validation and 8 test categories, and the training, validation and test sets contain 351, 97 and 160 classes, respectively. The split of tieredImageNet ensures that</figDesc><table /><note>the classes in the training set are distinct from those in the test set, possibly resulting in more realistic classification scenarios.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Few-shot classification accuracies for 20-way Omniglot and 5-way miniImageNet</figDesc><table><row><cell>20-way Omniglot</cell><cell>5-way miniImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Hyperparameter settings for meta-trainingModel N c N q learning rate lr decay step lr decay ratio l2 decay rate dropout ratio</figDesc><table><row><cell>Omniglot 1-shot case Omniglot 5-shot case</cell><cell cols="2">60 15 60 15</cell><cell>1e-3 1e-3</cell><cell>40000 40000</cell><cell>0.5 0.5</cell><cell>--</cell><cell>[0.2,0.2,0.2,0.2] [0.2,0.2,0.2,0.2]</cell></row><row><cell>miniImageNet 1-shot case miniImageNet 5-shot case</cell><cell cols="2">20 12 20 8</cell><cell>1e-3 1e-3</cell><cell>20000 40000</cell><cell>0.1 0.1</cell><cell>5e-4 5e-4</cell><cell>[0.2,0.2,0.2,0.2] [0.3,0.2,0.2,0.2]</cell></row><row><cell>tieredImageNet 1-shot case tieredImageNet 5-shot case</cell><cell>30 20</cell><cell>8 8</cell><cell>1e-3 1e-3</cell><cell>40000 30000</cell><cell>0.1 0.1</cell><cell>--</cell><cell>[0.2,0.2,0.2,0.2] [0.2,0.2,0.2,0.2]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Few-shot classification accuracies for 5-way miniImageNet ± 0.89% 72.69 ± 0.74% Relation Nets 54.48 ± 0.93% 71.31 ± 0.78% Transductive Propagation Nets 59.91 ± 0.94% 73.30 ± 0.75% TapNet (Ours, Conv4) 57.11 ± 0.12% 73.66 ± 0.09%</figDesc><table><row><cell>5-way miniImageNet</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Number of parameters required for the learners</figDesc><table><row><cell>Method</cell><cell>Feature extractor</cell><cell>Additional Conv layer</cell><cell>Additional learnable parts</cell></row><row><cell>Matching Networks Prototypical Networks MAML TapNet (Conv4)</cell><cell>112k 112k 112k 112k</cell><cell>----</cell><cell>--12k (FC layer) 46k (Parameters of Φ)</cell></row><row><cell>Relation Nets Transductive Prop. Nets</cell><cell>112k 112k</cell><cell>111k 37k (Graph Construction)</cell><cell>4k (FC layer) -</cell></row><row><cell>SNAIL (ResNet-12-small) adaResNet TapNet (ResNet-12-small)</cell><cell>2.2M 2.2M 2.2M</cell><cell>1.3M + α (Attention + TC) 0.3M -</cell><cell>--5k (Parameters of Φ)</cell></row><row><cell>TADAM-α TADAM-TC TapNet (ResNet-12)</cell><cell>9.4M 9.4M 9.4M</cell><cell>---</cell><cell>-1.2M (FC layer) 10k (Parameters of Φ)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea. Correspondence to: Sung Whan Yoon &lt;shyoon8@kaist.ac.kr&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea. Correspondence to: Sung Whan Yoon &lt;shyoon8@kaist.ac.kr&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by the ICT R&amp;D program of Institute for Information &amp; Communications Technology </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Number of Network Parameters</head><p>It would be useful to understand the required complexity levels of the learning methods compared in this work. We in particular look at the number of network parameters used in each method. We focus on the number of parameters for the convolutional layer and other important learnable parts which are directly related to the learning efforts of the network. In particular, the other learnable parts include the fully connected (FC) layers in some cases and the standalone linear weights for the class reference vectors in Tap-Nets. The Conv4 network requires 112,320 parameters in total. The residual networks rely on considerably larger numbers of learnable parameters. ResNet-12-small runs on 2.2 million parameters, while ResNet-12 requires 9.4 million parameters approximately. <ref type="table">Table 5</ref> includes the number of parameters necessary for implementing any additional convolutional layers or learnable parts for each method. In the case of SNAIL, we marked the number of additional parameters simply as α, since the numbers of parameters used for attention blocks and temporal convolution blocks there are hard to estimate due to the lack of available detail descriptions. We note that for the same number of parameters, the convolutional layer, which utilizes a sliding window to repeat many multiply/add operations, requires substantially higher computational complexity than the other types of learnable parts considered in <ref type="table">Table 5</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic modelagnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Workshop on Meta-Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metalearning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJcSzz-CZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960v2</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03548</idno>
		<title level="m">Meta-learning: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
