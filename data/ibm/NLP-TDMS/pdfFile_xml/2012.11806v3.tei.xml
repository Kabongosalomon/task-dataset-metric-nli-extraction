<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
							<email>robby.tan@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yale-NUS College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph and Temporal Convolutional Networks for 3D Multi-person Pose Estimation in Monocular Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the recent progress, 3D multi-person pose estimation from monocular videos is still challenging due to the commonly encountered problem of missing information caused by occlusion, partially out-of-frame target persons, and inaccurate person detection. To tackle this problem, we propose a novel framework integrating graph convolutional networks (GCNs) and temporal convolutional networks (TCNs) to robustly estimate camera-centric multi-person 3D poses that does not require camera parameters. In particular, we introduce a human-joint GCN, which unlike the existing GCN, is based on a directed graph that employs the 2D pose estimator's confidence scores to improve the pose estimation results. We also introduce a human-bone GCN, which models the bone connections and provides more information beyond human joints. The two GCNs work together to estimate the spatial frame-wise 3D poses, and can make use of both visible joint and bone information in the target frame to estimate the occluded or missing human-part information. To further refine the 3D pose estimation, we use our temporal convolutional networks (TCNs) to enforce the temporal and human-dynamics constraints. We use a joint-TCN to estimate person-centric 3D poses across frames, and propose a velocity-TCN to estimate the speed of 3D joints to ensure the consistency of the 3D pose estimation in consecutive frames. Finally, to estimate the 3D human poses for multiple persons, we propose a root-TCN that estimates camera-centric 3D poses without requiring camera parameters. Quantitative and qualitative evaluations demonstrate the effectiveness of the proposed method. Our code and models are available at https://github.com/3dpose/GnTCN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Significant progress has been made in 3D human pose estimation in recent years, e.g. <ref type="bibr" target="#b40">(Sun et al. 2019a;</ref><ref type="bibr" target="#b34">Pavllo et al. 2019;</ref><ref type="bibr" target="#b8">Cheng et al. 2019</ref><ref type="bibr" target="#b7">Cheng et al. , 2020</ref>. In general, existing methods can be classified as either top-down or bottom-up. Top-down approaches use human detection to obtain the bounding box of each person, and then perform pose estimation for every person. Bottom-up approaches are human-detection free and can estimate the poses of all persons simultaneously. Topdown approaches generally demonstrate more superior performance in pose estimation accuracy, and are suitable for Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  <ref type="figure">Figure 1</ref>: Incorrect 3D multi-person pose estimation caused by person-centric pose estimation or occlusions. The personcentric estimation loses the location of each person in the scene (2 nd row) and existing method suffers from missing information due to occlusion (3 rd row).</p><p>many applications that require high pose estimation precision <ref type="bibr" target="#b34">(Pavllo et al. 2019;</ref><ref type="bibr" target="#b7">Cheng et al. 2020)</ref>; while bottom-up approaches are better in efficiency <ref type="bibr" target="#b5">(Cao et al. 2017</ref><ref type="bibr" target="#b4">(Cao et al. , 2019</ref>. In this paper, we aim to further improve 3D pose estimation accuracy, and thus push forward the frontier of the top-down approaches.</p><p>Most top-down methods focus on single person and define a 3D pose in a person-centric coordinate system (e.g., pelvis-based origin), which cannot be extended to multiple persons. Since for multiple persons, all the estimated skeletons need to reside in a single common 3D space in correct locations. The major problem here is that by applying the person-centric coordinate system, we lose the location of each person in the scene, and thus we do not know where to put them, as shown in <ref type="figure">Fig. 1</ref>, second row. Another major problem of multiple persons is the missing information of the target persons, due to occlusion, partially out-of-frame, inaccurate person detection, etc. For instance, inter-person occlusion may confuse human detection <ref type="bibr" target="#b27">(Lin and Lee 2020;</ref><ref type="bibr" target="#b38">Sárándi et al. 2020)</ref>, causing erroneous pose estimation <ref type="bibr" target="#b26">(Li et al. 2019;</ref><ref type="bibr" target="#b43">Umer et al. 2020)</ref>, and thus affect the 3D pose estimation accuracy (as shown in <ref type="figure">Fig. 1, third row)</ref>. Addressing these problems is critical for multi-person 3D pose estimation from monocular videos.</p><p>In this paper, we exploit the use of the visible human joints and bone information spatially and temporally utilizing GCNs (Graph Convolutional Networks) and TCNs <ref type="bibr">(Temporal Convolutional Networks)</ref>. Unlike most existing GCNs, which are based on undirected graphs and only consider the connection of joints, we introduce a directed graph that can capture the information of both joints and bones, so that the more reliably estimated joints/bones can influence the unreliable ones caused by occlusions (instead of treating them equally as in undirected graphs). Our human-joint GCN (in short, joint-GCN) employs the 2D pose estimator's heatmap confidence scores as the weights to construct the graph's edges, allowing the high-confidence joints to correct low-confidence joints in our 3D pose estimation. While our human-bone GCN (in short, bone-GCN) makes use of the confidence scores of the part affinity field <ref type="bibr" target="#b4">(Cao et al. 2019)</ref> to provide complementary information to the joint GCN. The features produced by the joint-and bone-GCNs are concatenated and fed into our fully connected layers to estimate a person-centric 3D human pose.</p><p>Our GCNs focus on recovering the spatial information of target persons in a frame-by-frame basis. To increase the accuracy across the input video, we need to put more constraints temporally, both in terms of the smoothness of the motions and the correctness of the whole body dynamics (i.e., human dynamics). To achieve this, we first employ a joint-TCN that takes a sequence of the 3D poses produced by the GCN module as input, and estimate the person-centric 3D pose of the central frame. The joint-TCN imposes a smoothness constraint in its prediction and also imposes the constraints of human dynamics. However, the joint-TCN can estimate only person-centric 3D poses (not camera-centric). Also, the joint-TCN is not robust to occlusion. To resolve the problems, we introduce two new types of TCNs: root-TCN and velocity-TCN.</p><p>Relying on the output of the joint-TCN, our root-TCN produces the camera-centric 3D poses, where the X, Y, Z coordinates of the person center, i.e. the pelvis, are in the camera coordinate system. The root-TCN is based on the weak perspective camera model, and does not need to be trained with large variation of camera parameters, since it estimates only the relative depth, Z/f . Our velocity-TCN takes the person-centric 3D poses and the velocity from previous frames as input, and estimates the velocity at the current frame. Our velocity-TCN estimates the current pose based on the previous frames using motion cues. Hence, it is more robust to missing information, such as in the case of occlusion. The reason is because the joint-TCN focuses on the correlations between past and future frames regardless of the trajectory, while the velocity-TCN focuses on the motion prediction, and thus makes the estimation more robust.</p><p>In summary, our contributions are listed as follows.</p><p>• Novel directed graph-based joint-and bone-GCNs to estimate 3D poses that can predict human 3D poses even though the information of the target person is incomplete due to occlusion, partially out-of-frame, inaccurate human detection, etc.</p><p>• Root-TCN that can estimate the camera-centric 3D poses using the weak perspective projection without requiring camera parameters.</p><p>• Combination of velocity-and joint-TCNs that utilize velocity and human dynamics for robust 3D pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>3D human pose estimation in video Recent 3D human pose estimation methods utilize temporal information via recurrent neural network (RNN) <ref type="bibr" target="#b15">(Hossain and Little 2018;</ref><ref type="bibr" target="#b24">Lee, Lee, and Lee 2018;</ref><ref type="bibr" target="#b9">Chiu et al. 2019)</ref> or TCN <ref type="bibr" target="#b34">(Pavllo et al. 2019;</ref><ref type="bibr" target="#b8">Cheng et al. 2019;</ref><ref type="bibr" target="#b41">Sun et al. 2019b;</ref><ref type="bibr" target="#b7">Cheng et al. 2020)</ref> improve the temporal consistency and show promising results on single-person video datasets such as HumanEva-I, Human3.6M, and MPI-INF-3DHP <ref type="bibr" target="#b39">(Sigal, Balan, and Black 2010;</ref><ref type="bibr" target="#b16">Ionescu et al. 2014;</ref><ref type="bibr" target="#b29">Mehta et al. 2017a</ref>), but they still suffer from the inter-person occlusion issue when applying to multi-person videos. Although a few works take occlusion into account <ref type="bibr" target="#b13">(Ghiasi et al. 2014;</ref><ref type="bibr" target="#b6">Charles et al. 2016;</ref><ref type="bibr" target="#b2">Belagiannis and Zisserman 2017;</ref><ref type="bibr" target="#b8">Cheng et al. 2019</ref><ref type="bibr" target="#b7">Cheng et al. , 2020</ref>, in a top-down framework, it is difficult to reliably estimation 3D multi-person human poses in videos due to erroneous detection and occlusions. Moreover, none of these method estimate camera-centric 3D human poses.</p><p>Monocular 3D human pose estimation Earlier approaches that tackle camera-centric 3D human pose from monocular camera require camera parameters as input or assume fixed camera pose to project the 2D posture into camera-centric coordinate <ref type="bibr" target="#b32">(Mehta et al. 2017b</ref><ref type="bibr" target="#b30">(Mehta et al. , 2019</ref><ref type="bibr" target="#b34">Pavllo et al. 2019)</ref>. As a result, these methods are inapplicable for wild videos where camera parameters are not available. Removing the requirement of camera parameters has drawn researcher's attention recently. <ref type="bibr" target="#b33">Moon et al. (Moon, Chang, and Lee 2019)</ref> first propose to learn a correction factor for a person's root depth estimation from a single image. Several recent works <ref type="bibr" target="#b25">(Li et al. 2020;</ref><ref type="bibr" target="#b27">Lin and Lee 2020;</ref><ref type="bibr" target="#b47">Zhen et al. 2020)</ref> show improved performance compared with <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>. <ref type="bibr" target="#b25">Li et al. (Li et al. 2020</ref>) develop an integrated method for detection, person-centric pose, and depth estimation from a single image. Lin et al. <ref type="bibr" target="#b27">(Lin and Lee 2020)</ref> propose to formulate the depth regression as a bin index estimation problem. <ref type="bibr" target="#b47">Zhen et al. (Zhen et al. 2020)</ref> propose to estimate 2.5D representation of body parts first and then reconstruct 3D human pose. Unlike their approach, our method is video-based where temporal information is utilized by TCN on top of GCN output, which leads to improved 3D pose estimation.  <ref type="figure">Figure 2</ref>: The framework of our approach. The 2D poses and part affinity field for each bounding box are fed into our joint-and bone-GCNs to obtain the full 3D poses (left). After obtaining all poses in the video, they are grouped by IDs which is provided by pose tracker, and fed into the the joint-, root-and velocity-TCN to obtain the camera-centric 3D pose estimation (right).</p><p>GCN for pose estimation Graph convolutional network (GCN) has been applied to 2D or 3D human pose estimation in recent years <ref type="bibr" target="#b46">(Zhao et al. 2019;</ref><ref type="bibr" target="#b3">Cai et al. 2019;</ref><ref type="bibr" target="#b10">Ci et al. 2019;</ref><ref type="bibr" target="#b35">Qiu et al. 2020)</ref>. <ref type="bibr" target="#b46">Zhao et al. (Zhao et al. 2019</ref>) propose a graph neural network architecture to capture local and global node relationship and apply the proposed GCN for single-person 3D pose estimation from image. Ci et al <ref type="bibr" target="#b10">(Ci et al. 2019</ref>) explore different network structures by comparing fully connected network and GCN and develop a locally connected network to improve the representation capability for single-person 3D human pose estimation from image as well. <ref type="bibr" target="#b3">Cai et al. (Cai et al. 2019</ref>) construct an undirected graph to model the spatial-temporal dependencies between different joints for single-person 3D pose estimation from video data. <ref type="bibr" target="#b35">Qiu et al. (Qiu et al. 2020</ref>) develop a dynamic GCN framework for multi-person 2D pose estimation from a image. Our method is different from all these methods in terms of we propose to use directed graph to incorporate heatmap and part affinity field confidence in graph construction, which brings the benefit of overcoming the limitation of human detection on top-down pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The overview of our framework is shown as <ref type="figure">Fig. 2</ref>. Having obtained the 2D poses from the 2D pose estimator, the poses are normalized so that they are centered at the root point, which is at the hip of human body. Each pose is then fed into our joint-and bone-GCNs to obtain its 3D full pose, despite the input 2D pose might be incomplete. Finally, a 3D full pose sequence is fed into the joint-TCN, root-TCN, and velocity-TCN to obtain the camera-centric 3D human poses that have smooth motion and comply with natural human dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-GCN and Bone GCN</head><p>Existing top-down methods are erroneous when the target human bounding box is incorrect, due to missing information (occlusion, partially out-of-frame, blur, etc.). To address this common problem, we introduce joint-GCN and bone-GCN that can correct the 3D poses from inaccurate 2D pose estimator. These GCNs work on a frame-by-frame basis.</p><p>Following the structure of the human body, we assign the coordinates (x i , y i ) of the human joints from the 2D pose estimator to each vertex of our graph, and establish connections between each pair of the joints. Unlike most GCNs, which are based on an undirected graph, we propose a GCN based on a directed graph. The directed graph allows us to propagate information more from high-confident joints to low-confident ones, and thus reduces the risk of propagating erroneous information (e.g., occluded joints or missing joints) in the graph. In other words, the low-confident joints contribute less to the message propagation than the highconfident ones. Details of the directed graph are available in the supplementary material.</p><p>The joint-GCN uses the 2D joints as the vertices and the confidence scores of the 2D joints as the edge weights, while the bone-GCN uses the confidence scores of part affinity field <ref type="bibr" target="#b5">(Cao et al. 2017)</ref>) as the edge weights. The features produced by the two GCNs are concatenated together and fed to a Multi Layer Perceptron to obtain the person-centric 3D pose estimation.</p><p>In GCNs, the message is propagated according to adjacent matrix, which indicates the edge between each pair of vertices. The adjacency matrix is formed by the following rule:</p><formula xml:id="formula_0">A i,j = max(H i )e −order(i,j) (i = j) max(H i )(i = j) ,<label>(1)</label></formula><p>where H is the heatmap from the 2D pose estimator. order(i, j) stands for the number of the order of neighboring vertices, which means the number of hops required to reach vertex j from vertex i. This formation of adjacency imposes more weight for close vertices and less for distance vertices.</p><p>The forward propagation of each GCN layer can be expressed as:</p><formula xml:id="formula_1">h i = σ(F (h i−1 )W i ),<label>(2)</label></formula><p>where F is the feature transformation function, and W is the learnable parameter of layer i. To learn a network with strong generalization ability, we follow the idea of Graph SAGE <ref type="bibr" target="#b14">(Hamilton, Ying, and Leskovec 2017)</ref> to learn a generalizable aggregator, which is formulated as:</p><formula xml:id="formula_2">F (h i ) = Ah i ⊕ h i ,<label>(3)</label></formula><p>where h i is the output of layer i in the GCN and ⊕ stands for the concatenation operation. A is the normalized adjacency matrix. Since our method is based on a directed graph, which uses a non-symmetric adjacency matrix, the normalization is</p><formula xml:id="formula_3">A i,j = Ai,j Dj instead of A i,j = Ai,j √ DiDj</formula><p>in <ref type="bibr" target="#b20">(Kipf and Welling 2016)</ref>, D i and D j are the indegree of vertices i and j, respectively. This normalization ensures that the indegree of each vertex sums to 1, which prevents numerical instability. Our joint-GCN considers only human-joints and does not include the information of bones, which can be critical for the cases when the joints are missing due to occlusion or other reasons. To exploit the bone information, we created a bone-GCN. First, we construct the incidence matrix I n of shape [#bones, #joints] to represent the bone connections, where each row represents an edge and the columns represent vertices. For each bone, the parent joint is assigned with −1 and the child joint is assigned with 1. Second, the incidence matrix I n is multiplied with the joint matrix J to obtain the bone matrix B, which will be further fed into our bone-GCN.</p><p>In joint matrix J, each row stands for the 2D coordinate (x, y) of a joint. Unlike our joint-GCN, where the adjacency matrix is drawn from the joint heatmap produced by 2D pose estimator, our human-bone GCN utilizes the confidence scores from the part affinity field, following the method of <ref type="bibr" target="#b5">(Cao et al. 2017)</ref>, as the adjacency. Finally, the outputs from our human-joint GCN and human-bone GCN are concatenated together and fed into an MLP (Multi-layer Perceptron). The loss function we use is the L2 loss between GCN 3D joints output P GCN and 3D ground-truth skeleton P , which is L GCN = || P − P GCN || 2 2 . In the training stage, to obtain sufficient variation and to increase the robustness of our GCNs, we use not only the results from our 2D pose estimator, but also augmented data from our ground-truths. Each joint is assigned with a random confidence score and random noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root-TCN</head><p>In most of the videos, the projection can be modelled as weak perspective:</p><formula xml:id="formula_4">x y 1 = 1/Z f 0 c x 0 f c y 0 0 1 X Y Z ,<label>(4)</label></formula><p>where x and y are the image coordinates, X, Y and Z are the camera coordinates. f, c x , c y stands for the focal length and camera centers, respectively. Thus we have:</p><formula xml:id="formula_5">X = Z f (x − c x ) Y = Z f (y − c y ).<label>(5)</label></formula><p>By assuming (c x , c y ) as the image center, which is applicable for most cameras, the only parameters we need to estimate is depth Z and focal length f . To be more practical, we jointly estimate Z/f , instead of estimating them separately. This enables our method to be able to take wild videos that the camera parameters are unknown.</p><p>According to the weak perspective assumption, the scale of a person in a frame indicates the depth in the camera coordinates. Hence, we propose a network, root Temporal Convolutional Network (root-TCN), to estimate the Z/f from 2D pose sequences. We first normalize each 2D pose by scaling the average joint-to-pelvis distance to 1, using a scale factor s. Then we concatenate the normalized pose p, scale factor s, as well as the person's center in the frame as c, and feed a list of such concatenated features in a local temporal window into the TCN for depth estimation in the camera coordinates.</p><p>As directly learning Z/f is not easy to converge, we transform this regression problem into a classification problem. For each video, we divide the depth into N discrete ranges, set to 60 in our experiments, and our root-TCN outputs a vector with length N as {x 1 , ..., x N }, where x i indicates the probability that Z/f is within the ith discrete range. Then, we apply Soft-argmax to this vector to get the final continuous estimation of the depth as:</p><formula xml:id="formula_6">[ Z f ] t = Soft-argmax ( f R ( p t−n:t+n , c t−n:t+n , s t−n:t+n )),<label>(6)</label></formula><p>where t is the time stamp, and n is half of the temporal window's size. This improves the training stability and reduces the risk of large errors.</p><p>The loss function for the depth estimation is defined as the mean squared error between the ground truth and predictions, expressed as</p><formula xml:id="formula_7">L Root = ( Z f −Ẑ f ) 2 ,</formula><p>where Z/f is the predicted value, andẐ/f denotes the ground truth. According to Eq.(5), we can calculate the coordinates for the person's center as P t D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-TCN and Velocity-TCN</head><p>To increase the accuracy of the 3D poses across the input video, we impose temporal constraints, by employing a temporal convolutional network (TCN) <ref type="bibr" target="#b7">(Cheng et al. 2020</ref>) that takes a sequence of consecutive 3D poses as input. We call this TCN a joint-TCN, which is trained using various 3D poses and their augmentation, and hence capture human dynamics. The joint-TCN outputs the person-centric 3D pose, P D . The TCN utilizes temporal information to interpolate the poses of occluded frames with temporal information. However, when persons get close and occlude each other, there may be fewer visible joints belonging to a person and more distracting joints from other persons. To resolve the problem, in addition to the joint-TCN, we propose a velocity-based estimation network, velocity-TCN, which takes the 3D joints and their velocities as input, and predicts the velocity of all joints as:</p><formula xml:id="formula_8">V t = (v t x , v t y , v t z ) = TCN v (p t−n:t−1 , V t−n:t−1 ),<label>(7)</label></formula><p>where p stands for the 2D pose and V t denotes the velocity at time t. TCN v is the velocity-TCN. The velocity here is proportional to 1/f according to Eq. (5). We normalize the velocity both in training and testing. With estimated V t , we can obtain the coordinate P t S = P t−1 + V t , where P t S and P t−1 are estimated coordinates at time t and t − 1. The calculation of P t−1 is discussed later in Eq. <ref type="formula" target="#formula_9">(8)</ref>.</p><p>The joint-TCN predicts the joints by interpolating the past and future poses, while our velocity-TCN predicts the future poses using motion cues. Both of them are able to handle the occlusion frames, but the joint-TCN focuses on the connection between past and future frames regardless of the trajectory, while the velocity-TCN focuses on the motion prediction, which can handle a motion drift. To leverage the benefits of both, we introduce an adaptive weighted average of their estimated coordinates P t D and P t S . We utilize the 2D pose tracker <ref type="bibr" target="#b43">(Umer et al. 2020</ref>) to detect and track human poses in the video. In the tracking procedure, we regard the heatmaps with less than 0.5 confidence value as occluded joints, and the pose with less than 30% non-occluded joints as the occluded pose. By doing this, we obtain the occlusion information for both joints and frames. Note that the values here are obtained empirically through our experiments. Suppose we find an occlusion duration from T start occ to T end occ for a person, then we generate the final coordinates as:</p><formula xml:id="formula_9">P t = w t P t D + (1 − w t )P t S ,<label>(8)</label></formula><formula xml:id="formula_10">where w t v = e −min(t−T start occ ,T end occ −t)</formula><p>. For frames that are closer to occlusion duration boundaries, we trust P t D more; for those are far from occlusion boundaries, we trust P t S more. The velocity-TCN loss is the L2 loss between the predicted 3D points at t and ground-truth 3D points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>MuPoTS-3D is a 3D multi-person testing set with both indoor and outdoor scenes <ref type="bibr" target="#b31">(Mehta et al. 2018</ref>). The groundtruth 3D pose of each person in the video is obtained from multi-view markerless capture, which is suitable for evaluating 3D multi-person pose estimation performance in both person-centric and camera-centric coordinates. Unlike previous methods (Moon, Chang, and Lee 2019) using the training set (MuCo-3DHP) to train their models and then do evaluation on MuPoTS-3D, we use MuPoTS-3D for testing only without fine-tuning. 3DPW is an outdoor multi-person dataset for 3D human pose reconstruction <ref type="bibr" target="#b44">(von Marcard et al. 2018)</ref>. Following previous methods <ref type="bibr" target="#b18">(Kanazawa et al. 2019;</ref><ref type="bibr" target="#b41">Sun et al. 2019b</ref>), we use 3DPW for testing only without any fine-tuning. The ground-truth of 3DPW is SMPL 3D mesh model <ref type="bibr" target="#b28">(Loper et al. 2015)</ref>, where the definition of joints differs from the one commonly used in 3D human pose estimation (skeletonbased) like Human3.6M <ref type="bibr" target="#b42">(Tripathi et al. 2020)</ref>, so it is unfair to evaluate skeleton-based methods on it even after joint adaption or scaling. To perform a fair comparison, we select an occlusion subset from the 3DPW test set (please refer to the supplementary material for details). And the performance change of a method between the full test set and the subset indicates how well the method can handle the missing information problem caused by occlusions.   Human3.6M is a widely used dataset and benchmark for 3D human pose estimation <ref type="bibr" target="#b16">(Ionescu et al. 2014)</ref>. It contains 3.6 million single-person indoor images captured by the MoCap system, which is suitable for evaluation of singleperson pose estimation and camera-centric coordinates prediction. Following previous works <ref type="formula">(</ref> and PCK abs are used for cameracentric pose estimation evaluation. Each GCN and TCN is trained for 100 epochs with initial learning rate 1e − 3, more details are available in the supplementary material. Ablation Studies In <ref type="table" target="#tab_1">Table 1</ref>, we provide the results of an ablation study to validate the major components of the proposed framework. MuPoTS-3D is used as it has been used for 3D multi-person pose evaluation in personcentric and camera-centric coordinates <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>. AU C rel and PCK metrics are used to evaluate person-centric 3D pose estimation performance, AP root 25 and PCK abs metrics are used to evaluate camera-centric 3D pose (i.e., camera-centric coordinate) estimation following <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>.</p><p>In particular, we use the joint-TCN with time window 1 plus a root-TCN with time window 1 as a baseline for both person-centric and camera-centric coordinate estimation as shown at the 1st row in <ref type="table" target="#tab_1">Table 1</ref>. We use the baseline with ground-truth bounding box (i.e., perfect 2D tracking) as a second baseline to illustrate even with perfect detection bounding box the baseline still performs poorly because it cannot deal with occlusion and distracting joints from other persons. On the contrary, we can see significant performance (e.g., 18% ∼ 29% improvement against the baseline in  PCK abs ) improvements after adding the proposed GCN and TCN modules as shown in row 3 -5 in <ref type="table" target="#tab_1">Table 1</ref>. The benefits from the TCNs are slightly larger than those from the joint and bone GCNs as temporal information is used by the TCNs while GCNs only use frame-wise information.</p><p>Lastly, our full model shows the best performance, with 53% improvement against the baseline in terms of PCK abs .</p><p>We perform a second ablation study to break down the different pieces in our GCN and TCN modules to show the effectiveness of each individual component in <ref type="table" target="#tab_2">Table 2</ref>. We observe the undirected graph-based GCN performs the worst as shown in the 1st row. Our joint-GCN, bone-GCN, and data augmentation (applying random cropping and scaling) show improved performance in row 2 -4. On top of the full GCN module, we also show the contribution of the joint-TCN and velocity-TCN in row 5 -6 (row 6 is our full model). Similar to <ref type="table" target="#tab_1">Table 1</ref> we can see the TCN module brings more improvement compared with the GCN module as temporal information is used. Quantitative Results To compare with the state-of-theart (SOTA) methods in 3D multi-person human pose estimation, we perform evaluations on MuPoTS-3D as shown in <ref type="table" target="#tab_4">Table 3</ref>. Please note our network is trained only on Human3.6M to have a fair comparison with other methods <ref type="bibr" target="#b8">(Cheng et al. 2019</ref><ref type="bibr" target="#b7">(Cheng et al. , 2020</ref>. As the definition of keypoints in MuPoTS-3D is different from the one where our model is trained on, we use joint adaptation <ref type="bibr" target="#b42">(Tripathi et al. 2020)</ref> to transfer the definition of keypoints. Among the methods in <ref type="table" target="#tab_4">Table 3</ref>, <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019;</ref><ref type="bibr" target="#b27">Lin and Lee 2020;</ref><ref type="bibr" target="#b25">Li et al. 2020</ref>) are fine-tuned on 3D training set MuCo-3DHP.</p><p>Regarding to the performance on MuPoTS-3D, our camera-centric pose estimation accuracy beat the SOTA <ref type="bibr" target="#b25">(Li et al. 2020</ref>) by 4.3% on PCK abs . A few papers reported their results on AP root 25 , where (Moon, Chang, and Lee 2019) is 31.0, (Lin and Lee 2020) is 39.4, and our result is 45.2, where we beat the SOTA (Lin and Lee 2020) by 14.7%. We also compare with other methods on person-centric 3D pose estimation, and get improvement of 4.5% on PCK against the SOTA (Lin and Lee 2020). Please note we do not finetune on MuCo-3DHP like others <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019;</ref><ref type="bibr" target="#b27">Lin and Lee 2020;</ref><ref type="bibr" target="#b25">Li et al. 2020)</ref>, which is the training dataset for evaluation on MuPoTS-3D. Moreover, the SOTA method (Lin and Lee 2020) on person-centric metric PCK shows poor performance on PCK abs (ours vs theirs:   <ref type="bibr" target="#b25">(Li et al. 2020</ref>) on camera-centric metric PCK abs has mediocre performance on PCK (ours vs theirs: 87.5 vs 82.0, 6.7% improvement). All of these results clearly show that our method not only surpasses all existing methods, but also is the only method that is well-balanced in both person-centric and camera-centric 3D multi-person pose estimation. 3DPW dataset <ref type="bibr" target="#b44">(von Marcard et al. 2018</ref>) is a new 3D multi-person human pose dataset that contains multi-person outdoor scenes for person-centric pose estimation evaluation. Following previous works <ref type="bibr" target="#b18">(Kanazawa et al. 2019;</ref><ref type="bibr" target="#b41">Sun et al. 2019b)</ref>, 3DPW is only used for testing and the PA-MPJPE values on test set are shown in <ref type="table" target="#tab_6">Table 4</ref>. As discussed in the Datasets section, the ground-truth definitions are different between 3D pose reconstruction and estimation where the ground-truth of 3DPW is SMPL mesh model, even we follow <ref type="bibr" target="#b42">(Tripathi et al. 2020)</ref> to perform joint adaptation to transform the estimated joints but still have a disadvantage, and the PA-MPJPE values cannot objectively reflect the performance of skeleton-based pose estimation methods.</p><p>As aforementioned, we select a subset out of the original test set with the largest detection errors, and run the code of the top-performing methods in <ref type="table" target="#tab_6">Table 4</ref> on this subset for comparison. <ref type="table" target="#tab_6">Table 4</ref> shows that even with the disadvantage of different definition of joints, our method is the 3rd best on the original testing test, and becomes the 2nd best on the subset where the difference to the best one <ref type="bibr" target="#b21">(Kocabas, Athanasiou, and Black 2020)</ref> is greatly shrunk. More importantly, the δ of PA-MPJPE between the original testing set and the subset in the 4th column in <ref type="table" target="#tab_6">Table 4</ref>, our method shows the least error increase compared with all other topperforming methods. In the particular, the two best methods <ref type="bibr" target="#b22">(Kolotouros et al. 2019;</ref><ref type="bibr" target="#b21">Kocabas, Athanasiou, and Black 2020)</ref> on the original testing set show 29.7 and 30.6 mm error increase while our method shows only 21.5 mm. The performance change of PA-MPJPE between the original testing set and the subset clearly demonstrates that our method is the best in terms of solving the missing information problem which is critical for 3D multi-person pose estimation.  In order to further illustrate the effectiveness of both person-centric and camera-centric 3D pose estimation of our method, we perform evaluations on the widely used singleperson dataset, Human3.6M. To evaluate camera-centric pose estimation, we use mean root position error (MPRE), a new evaluation metric proposed by <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>. Our result is 88.1 mm, the result of (Moon, Chang, and Lee 2019) is 120 mm, the result of (Lin and Lee 2020) is 77.6 mm. Our method outperforms the result of (Moon, Chang, and Lee 2019) by a large margin: 31.9 mm error reduction and 26% improvement. Although depth estimation focused method <ref type="bibr" target="#b27">(Lin and Lee 2020)</ref> shows better camera-centric performance on this single-person dataset Human3.6M, their camera-centric result on multi-person dataset MuPoTS-3D is much worse than ours (ours vs. theirs in PCK abs : 45.7 -35.2, 29.8% improvement). Cameracentric 3D human pose estimation is for multi-person pose estimation, good performance only on single-person dataset is not enough to solve the problem.</p><p>To compare with most of the existing methods that evaluate person-centric 3D pose estimation on Human3.6M using MPJPE and PA-MPJPE, we report our results using the same metrics in <ref type="table" target="#tab_8">Table 5</ref>. As Human3.6M contains only single-person videos, we do not expect to our method bring much improvement. It is observed that our method is comparable with the SOTA methods. In addition, although our method shows improved performance over others that use kinematic constraints <ref type="bibr" target="#b45">(Wandt and Rosenhahn 2019;</ref><ref type="bibr" target="#b8">Cheng et al. 2019</ref>) because of our GCNs and TCNs, adding kinematic constraints could potentially improve our performance further <ref type="bibr" target="#b0">(Akhter and Black 2015;</ref><ref type="bibr" target="#b23">Kundu et al. 2020)</ref>. Qualitative Results As shown in <ref type="figure">Figure 3</ref>, our full model can better handle occlusions and incorrect detection compared with the baselines and the relative positions among all persons are well captured without camera parameters. More comparisons against SOTA methods and qualitative results on wild videos are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We propose a new framework to unify GCNs and TCNs for camera-centric 3D multi-person pose estimation. The proposed method successfully handles missing information due to occlusion, out-of-frame, inaccurate detections, etc., in videos and produces continuous pose sequences. Experiments on different datasets validate the effectiveness of our framework as well as our individual modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In the following, additional information regarding implementation details, visualization of the occlusion subset, additional quantitative and qualitative evaluations, pose tracking illustration, and failure cases are provided as supplementary materials to better understand our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details Graph Convolutional Layers</head><p>For the Graph Convolutional Networks (GCNs), we utilize two branches which incorporate the confidence scores from heatmap and Part Affinity Field, respectively, named the joint-GCN and bone-GCN. Different from previous methods which use an undirected graph for the feature propagation, we propose to use directed graphs in our GCNs. As shown in <ref type="figure">Fig. 4</ref>, the outward edges of the low-confident joints have the same weights as the high-confident joints in the conventional undirected graphs, even though the low-confident ones are probably wrongly estimated. In contrast, with the help of directed graphs, the outward edges of the low-confident joints have less weights, thus have less impact on the feature propagation. <ref type="figure">Figure 4</ref>: Difference between the undirected and directed graphs. The undirected graph propagates the message with same weight while the directed graph constrains the outward propagation for low-confident joints.</p><p>The detailed structure of our GCN branch is shown in <ref type="figure">Fig.  5</ref>. The network consists of two encoding fully connected (FC) layers at both sides and several graph convolutional (GC) layers in between. Inspired by Graph SAGE (Hamilton, Ying, and Leskovec 2017) to learn a better generalized feature aggregator, in each GC layer, we concatenate the features before and after the feature propagation and followed by one fully connected layer.</p><p>In our experiments, the output channels are set to 512 for all layers, and 3 GC layers are included in each GCN branch. We use the original implementation of HRNet <ref type="bibr" target="#b40">(Sun et al. 2019a</ref>) as the 2D pose estimator and extract PAF from original OpenPose <ref type="bibr" target="#b4">(Cao et al. 2019</ref>). In addition, we randomly crop the input image by [0, 0.5w] to simulate the cases, where persons are out-of-frame (w is the long side of the cropped image). The GCN is trained for 100 epochs with the Adam (Kingma and Ba 2014) optimizer. Learning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Convolutional Network</head><p>Our TCNs for estimating the person-centric pose (joint-TCN), depth (root-TCN), and velocity (velocity-TCN) all have 4 residual blocks with dilation rate of 3,9,27,81, respectively. The temporal window length is set to 243 for all our TCNs in all experiments. We only use Human3.6M dataset for training the TCNs for fair comparison with most previous methods; however, there are a few methods <ref type="bibr" target="#b1">(Arnab, Doersch, and Zisserman 2019;</ref><ref type="bibr" target="#b22">Kolotouros et al. 2019;</ref><ref type="bibr" target="#b33">Moon, Chang, and Lee 2019)</ref> using additional 3D datasets such as MPI-INF-3DHP <ref type="bibr" target="#b29">(Mehta et al. 2017a)</ref> or LSP <ref type="bibr" target="#b17">(Johnson and Everingham 2010)</ref>. We use the same TCN structure (1D convolutional Network) as shown in <ref type="figure">Fig. 6</ref> for the joint-TCN, root-TCN, and velocity-TCN. In the TCN structure, a residual block is repeatedly used and consists of two convolutional layers with skip connections. The first convolutional layer has a kernel size k and dilation rate d, the second one has a kernel size 3 and dilation rate 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-offrame</head><p>Occlusion Extreme Heavy In our experiments, the network is trained with the Adam optimizer (Kingma and Ba 2014) for 100 epochs. The learning rate is set to 1e − 3 in the beginning and scaled by 0.1 every 40 epochs. The augmentation method proposed by <ref type="bibr" target="#b7">(Cheng et al. 2020</ref>) is used to enhance the robustness to occlusion. The training takes about 40 hours in single Nvidia RTX 2080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Selected Subset</head><p>As described in the experiment section of our main paper, we select a subset of 3DPW <ref type="bibr" target="#b44">(von Marcard et al. 2018</ref>) based on the IoU between the detected bounding box and the groundtruth bounding box from all three sets (train/validation/test) in 3DPW. Note, 3DPW is used only for evaluation (not finetuning). When computing the IoU, we find that due to the occlusion or out-of-frame problems, the ground truth bounding box of a target person may be incomplete (see examples in <ref type="figure" target="#fig_4">Fig 7)</ref>. When this happens, we choose to re-project the 3D skeleton back to the 2D image plane with the camera parameters provided in the 3DPW dataset, and calculate the ground truth bounding box of the target person based on the re-projected 2D pose. <ref type="figure" target="#fig_4">Fig. 7</ref> shows examples of the selected frames with small IoU (half of the body is occluded or out-of-frame) and extreme small IoU (almost entirely outof-frame or occluded), where the IoU scores are in the range of [0, 0.6]. <ref type="figure" target="#fig_5">Fig. 8</ref> shows the histogram of IoUs between the detection and ground-truth bounding boxes of all frames in 3DPW. We choose the frames with smallest 1 st to 10000 th IoU as the subset, which corresponds to the range [0, 0.597]. As we can see that even the IoU histogram peaks around 0.7, there are still many frames where the IoU between the detection bounding box and ground-truth is equal or smaller than 0.6.</p><p>As the examples shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, the detection bounding box with IoU scores in the range of [0, 0.6] is insufficient for reliable 3D human pose estimation. Therefore, it is clear that addressing the missing information problem caused by incorrect detection, out-of-frame or occlusion is critical to reliably estimating 3D multi-person poses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>Following the literature <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019;</ref><ref type="bibr" target="#b34">Pavllo et al. 2019)</ref>, Mean Per Joint Position Error (MPJPE), Procrustes analysis MPJPE (PA-MPJPE), Percentage of Correct 3D Keypoints (PCK), and area under PCK curve from various thresholds (AU C rel ) are used to evaluate person-centric 3D human pose estimation. Average precision of 3D human root location (AP root 25 ) and PCK abs , which is PCK without root alignment to evaluate the absolute camera-centric coordinates, are used to evaluate 3D multi-person camera-centric pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Quantitative Results</head><p>The detailed results and comparison with the state-of-theart (SOTA) methods on MuPoTS-3D dataset <ref type="bibr" target="#b31">(Mehta et al. 2018</ref>) are shown in Tab. 6 7 8 9. P CK and P CK abs are used as evaluation metrics to measure the person-centric and camera-centric 3D pose estimation accuracy, which are the same as the metrics used in the quantitative results section of our main paper <ref type="table" target="#tab_4">(Table 3</ref>). One can observe that our method shows more consistent improvement compared to those of the SOTA methods in all the four evaluations.</p><p>In particular, Tab. 6 7 shows the P CK and P CK abs evaluations for matched poses and Tab. 8 9 show the evaluations for all poses, where we follow <ref type="bibr" target="#b31">(Mehta et al. 2018;</ref><ref type="bibr" target="#b33">Moon, Chang, and Lee 2019;</ref><ref type="bibr" target="#b47">Zhen et al. 2020)</ref>.</p><p>According to Tab. 6, we observe that our method performs better than the SOTA methods in 15 out of 20 sequences and in the average score. However, in sequence 10, the size of persons around the image border is abnormal due to camera distortion, which results in the wrong depth estimation of our method. As we use the Human3.6M dataset to train our network, where little distortion exists, the estimation accuracy of our method is negatively affected by this image distortion.</p><p>In the Tab. 7, we observe a constant improvement of the estimation accuracy in the most sequences (17 out of 20), except for sequence 9 compared to <ref type="bibr">(Moon, Chang, and Lee</ref> Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 - <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>  3 Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>   2 Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg <ref type="bibr" target="#b36">(Rogez, Weinzaepfel, and Schmid 2017)</ref>   3 Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref>   2019). The estimation error is caused by image distortion as well. Different from sequence 10, where the person moves towards the center of the image, resulting in the higher camera-centric estimation error for P CK abs , in sequence 9, the person is constantly standing at the distortion area yielding a higher person-centric pose estimation error for P CK of our method. For more comprehensive evaluation, we also show the results for all poses, where missing poses are counted as wrong in Tab. 8 and 9. The performance of each method drops as missing detection are counted as wrong for all joints belonging to the target person. Our method maintains high performance while other methods drop significantly due to misdetections. Noticeably, since the P CK and P CK abs do not punish the false positives, <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019)</ref> includes many false-positive bounding boxes, and thus their method shows constant performance for both matched and all poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Qualitative Results</head><p>To compare with the SOTA methods visually, we run the open-source code of the recent methods <ref type="bibr" target="#b33">(Moon, Chang, and Lee 2019;</ref><ref type="bibr" target="#b47">Zhen et al. 2020)</ref>. The 3D multi-person pose estimation results of each method on the frames where occlusion occurs are shown in <ref type="figure" target="#fig_9">Fig. 11</ref>. Since other methods only use frame-wise information, their performance suffers seriously from occlusions, while our temporal-based method Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 - <ref type="bibr" target="#b36">(Rogez, Weinzaepfel, and Schmid 2017)</ref>  2 Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg <ref type="bibr" target="#b36">(Rogez, Weinzaepfel, and Schmid 2017)</ref>   <ref type="table">Table 9</ref>: P CK on MuPoTS-3D dataset for all poses. produces more robust and accurate pose estimation results.</p><p>To illustrate that our method is able to deal with the outof-frame problem, we show three cases where the wrongly estimated poses are caused by out-of-frame and occlusion in <ref type="figure" target="#fig_6">Fig. 9</ref>. In a top-down 3D pose estimation framework, once the out-of-frame problem happens, if maximum responses in 2D heatmap are chosen to generate the 3D poses, large error will be produced; since the true joints are out of the image boundary. With our GCNs, the full-body poses are inferred, which is not constrained by the image size. The reprojected the 2D pose from the 3D pose estimation on the original image space is provided to better visualize the effectiveness of our method.</p><p>Additionally, we show the estimation results for in-thewild videos as in <ref type="figure" target="#fig_10">Fig. 12</ref>. Our method can produce reason-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Tracking illustration</head><p>To perform human pose tracking, we follow <ref type="bibr" target="#b43">(Umer et al. 2020)</ref> to evaluate the probability of assigning a joint to an existing or a newly appearing person. In particular, the pose tracking module considers both appearance similarities with accumulated templates, image patches for the target person in the previous frames, and the motion smoothness according to the predicted camera-centric coordinates. In <ref type="figure" target="#fig_8">Figure  10</ref>, we can see that without using pose tracking, the identity change problem is shown in the first column, while using pose tracking can fix this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure Cases</head><p>Typical failure cases of our method are shown in <ref type="figure" target="#fig_11">Fig. 13</ref>. As our method belongs to the top-down human pose estimation approach, severe false human detection (i.e., consistently missing or duplicated) is likely to affect the performance of our method (the upper example in <ref type="figure" target="#fig_11">Fig. 13</ref>). More-   over, due to the data scarcity of 3D ground-truth (i.e., limited pose variations), our method like many others may not generalize well on wild video. In particular, as our 3D pose estimation is trained only on Human 3.6M dataset <ref type="bibr" target="#b16">(Ionescu et al. 2014</ref>), which has limited poses variations, once unusual poses occur in the testing video (the lower example in <ref type="figure" target="#fig_11">Fig. 13</ref>), our method may not be able to generalize well. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b15">Hossain and Little 2018;</ref><ref type="bibr" target="#b34">Pavllo et al. 2019;</ref><ref type="bibr" target="#b45">Wandt and Rosenhahn 2019)</ref>, the subject 1,5,6,7,8 are used for training, and 9 and 11 for testing. Evaluation and Implementation MPJPE, PA-MPJPE, PCK, and AU C rel are used for person-centric pose estimation evaluation. AP root 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Structure of the GCN branch. FC represents the fully connected layer and GC represents the graph convolutional layer.TCN structureResidual block The structure of the TCN. k and d indicates the kernel size and dilation rate, respectively. Number in the bracket [·] represents the number of output channel.rate is set to 1e − 3 in beginning and scaled by 0.1 every 40 epochs. The training takes about 18 hours on single Nvidia RTX 2080Ti GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Sample selected frames of heavy and extreme occlusion. Yellow: Ground-truth bounding box. Red: Detection result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>SelectedFigure 8 :</head><label>8</label><figDesc>The histogram of IoU between the detection and ground truth bounding box (reprojected from 3D) on 3DPW. The occlusion subset is chosen from 1 st to 10000 th smallest IOU which is [0, 0.597].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Results of our GCN module in both the 3D human pose estimation and reprojected 2D poses back to the image space. Left two columns: the existing top-down 3D human pose estimation results; right two columns: our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Pose estimation results with and without performing pose tracking. Same color indicates the same identity. able predictions when occlusion occurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative comparison with the SOTA 3D multi-person camera-centric pose estimation methods on MuPoTS-3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>3D camera-centric multi-person pose estimation results of our method on the wild videos (PoseTrack dataset) with occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Failure cases. Upper: false human detection, miss detection is highlighted with red bounding box. Lower: unusual pose. Face regions are masked in wild video (upper example), video clip in lower example is from MPII dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on MuPoTS-3D dataset. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>AP root 25</cell><cell cols="3">AU C rel PCK PCK abs</cell></row><row><cell>Joint* GCN</cell><cell>24.1</cell><cell>27.3</cell><cell>73.1</cell><cell>25.6</cell></row><row><cell>Joint GCN</cell><cell>28.5</cell><cell>30.1</cell><cell>76.8</cell><cell>29.0</cell></row><row><cell>Joint + Bone* GCN</cell><cell>28.4</cell><cell>31.9</cell><cell>78.1</cell><cell>29.7</cell></row><row><cell>Joint + Bone GCN</cell><cell>33.4</cell><cell>37.9</cell><cell>82.6</cell><cell>34.3</cell></row><row><cell>Joint + Bone + Aug.</cell><cell>35.4</cell><cell>39.7</cell><cell>83.2</cell><cell>35.1</cell></row><row><cell>Joint TCN</cell><cell>43.1</cell><cell>45.8</cell><cell>86.2</cell><cell>42.6</cell></row><row><cell>Joint + Velocity</cell><cell>45.2</cell><cell>48.9</cell><cell>87.5</cell><cell>45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Ablation study on our proposed Joint and Bone GCNs and TCNs. * stands for the GCN structure with undi- rected graph. We keep the GCN as the best one (joint + bone + aug.) to perform an ablation study on TCN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative evaluation on multi-person 3D dataset, MuPoTS-3D. Best in bold, second best underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Quantitative evaluation using PA-MPJPE in mil-</cell></row><row><cell>limeter on original 3DPW test set and its occlusion subset.</cell></row><row><cell>* denotes extra 3D datasets were used in training. Best in</cell></row><row><cell>bold, second best underlined.</cell></row></table><note>45.7 -35.2, 29.8% improvement), and the SOTA method</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Examples of results from our whole framework compared with different baseline results. First row shows the images from two video clips; second row shows the results from the baseline described in Ablation Studies; third row shows the result of the GCN module; last row shows the results of the whole framework. Wrong estimations are labeled with red circles.</figDesc><table><row><cell></cell><cell>Frame 368</cell><cell>Frame 395</cell><cell cols="2">Frame 410</cell><cell>Frame 460</cell><cell>Frame 185</cell><cell>Frame 200</cell><cell>Frame 215</cell><cell>Frame 230</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 3: Group</cell><cell>Method</cell><cell></cell><cell cols="3">MPJPE PA-MPJPE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Hossain et al., (2018)</cell><cell>51.9</cell><cell>42.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Wandt et al., (2019)*</cell><cell>50.9</cell><cell>38.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Person-</cell><cell cols="2">Pavllo et al., (2019)</cell><cell>46.8</cell><cell>36.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>centric</cell><cell cols="2">Cheng et al., (2019)</cell><cell>42.9</cell><cell>32.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Kocabas et al., (2020)</cell><cell>65.6</cell><cell>41.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Kolotouros et al. (2019)</cell><cell>41.1</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Moon et al., (2019)</cell><cell>54.4</cell><cell>35.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Camera-</cell><cell cols="2">Zhen et al., (2020)</cell><cell>54.1</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>centric</cell><cell cols="2">Li et al., (2020)</cell><cell>48.6</cell><cell>30.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>40.9</cell><cell>30.4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation on Human3.6M for normalized and camera-centric 3D human pose estimation. * denotes ground-truth 2D labels are used. Best in bold, second best underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>59.5 45.3 51.4 46.2 53.0 27.4 23.7 26.4 39.1 23.6 (Zhen et al. 2020) 42.1 41.4 46.5 16.3 53.0 26.4 47.5 18.7 36.7 73.5 Ours 64.7 61.1 59.4 63.1 52.6 43.1 31.9 35.2 53.0 28.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>18.3 14.9 38.2 29.5 36.8 23.6 14.4 20.0 18.8 25.4 31.8 (Zhen et al. 2020) 46.0 22.7 24.3 38.9 47.5 34.2 35.0 20.0 38.7 64.8 38.7 Ours 37.6 26.7 46.3 44.5 50.2 47.9 39.4 23.5 61.0 56.1 46.3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>P CK abs on MuPoTS-3D dataset for matched poses.</figDesc><table><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>S6</cell><cell>S7</cell><cell>S8</cell><cell>S9</cell><cell>S10</cell><cell>-</cell></row><row><cell cols="11">(Rogez, Weinzaepfel, and Schmid 2017) 69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69.0 78.1</cell><cell></cell></row><row><cell cols="11">(Rogez, Weinzaepfel, and Schmid 2019) 88.0 73.3 67.9 74.6 81.8 50.1 60.6 60.8 78.2 89.5</cell><cell></cell></row><row><cell>(Dabral et al. 2018)</cell><cell cols="10">85.8 73.6 61.1 55.7 77.9 53.3 75.1 65.5 54.2 81.3</cell><cell></cell></row><row><cell>(Moon, Chang, and Lee 2019)</cell><cell cols="10">94.4 78.6 79.0 82.1 86.6 72.8 81.9 75.8 90.2 90.4</cell><cell></cell></row><row><cell>(Mehta et al. 2018)</cell><cell cols="10">81.0 64.3 64.6 63.7 73.8 30.3 65.1 60.7 64.1 83.9</cell><cell></cell></row><row><cell>(Mehta et al. 2017b)</cell><cell cols="10">88.4 70.4 68.3 73.6 82.4 46.4 66.1 83.4 75.1 82.4</cell><cell></cell></row><row><cell>(Zhen et al. 2020)</cell><cell cols="10">89.9 88.3 78.9 78.2 87.6 54.0 88.5 71.6 70.3 89.2</cell><cell></cell></row><row><cell>Ours</cell><cell cols="10">91.2 90.9 81.7 82.4 88.9 85.0 94.7 91.3 81.5 93.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>P CK on MuPoTS-3D dataset for matched poses.</figDesc><table><row><cell>Method</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>S4</cell><cell>S5</cell><cell>S6</cell><cell>S7</cell><cell>S8</cell><cell>S9</cell><cell>S10</cell><cell>-</cell></row><row><cell cols="11">(Moon, Chang, and Lee 2019) 59.5 44.7 51.4 46.0 52.2 27.4 23.7 26.4 39.1 23.6</cell><cell></cell></row><row><cell>(Zhen et al. 2020)</cell><cell cols="10">41.6 33.4 45.6 16.2 48.8 25.8 46.5 13.4 36.7 73.5</cell><cell></cell></row><row><cell>Ours</cell><cell cols="10">64.7 59.3 59.4 63.1 52.6 42.7 31.9 35.2 51.8 28.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>P CK abs on MuPoTS-3D dataset for all poses.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1 (Rogez, Weinzaepfel, and Schmid 2019) 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5 (Dabral et al. 2018) 85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8 (Moon, Chang, and Lee 2019) 94.4 77.5 79.0 81.9 85.3 72.8 81.9 75.7 90.2 90.4 (Mehta et al. 2018) 81.0 59.9 64.4 62.8 68.0 30.3 65.0 59.2 64.1 83.9 (Mehta et al. 2017b) 88.4 65.1 68.2 72.5 76.2 46.2 65.8 64.1 75.1 82.4 (Zhen et al. 2020) 88.8 71.2 77.4 77.7 80.6 49.9 86.6 51.3 70.3 89.2 Ours 91.2 88.4 81.7 82.4 88.9 84.5 94.7 91.3 78.3 93.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>50.2 51.0 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8 (Rogez, Weinzaepfel, and Schmid 2019) 69.2 73.8 66.2 56.0 74.1 82.1 78.1 72.6 73.1 61.0 70.6</figDesc><table><row><cell>(Dabral et al. 2018)</cell><cell>76.4 70.1 65.3 51.7 69.5 87.0 82.1 80.3 78.5 70.7 71.3</cell></row><row><cell>(Moon, Chang, and Lee 2019)</cell><cell>79.2 79.9 75.1 72.7 81.1 89.9 89.6 81.8 81.7 76.2 81.8</cell></row><row><cell>(Mehta et al. 2018)</cell><cell>67.2 68.3 60.6 56.5 59.9 79.4 79.6 66.1 66.3 63.5 65.0</cell></row><row><cell>(Mehta et al. 2017b)</cell><cell>74.1 72.4 64.4 58.8 73.7 80.4 84.3 67.2 74.3 67.8 70.4</cell></row><row><cell>(Zhen et al. 2020)</cell><cell>72.3 81.7 63.6 44.8 79.7 86.9 81.0 75.2 73.6 67.2 73.5</cell></row><row><cell>Ours</cell><cell>83.9 80.6 89.4 90.3 88.1 93.4 90.5 87.4 87.9 86.9 87.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research/project is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2929257</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personalizing Human Video Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8118" to="8125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion-Aware Networks for 3D Human Pose Estimation in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action-agnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1423" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3D human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12929" to="12941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing Occluded People</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Hu-man3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">bmvc</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning 3D Human Dynamics From Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to Reconstruct 3D Human Pose and Shape via Modelfitting in the Loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kinematic-Structure-Preserved Representation for Unsupervised 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11312" to="11319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HDNet: Human Depth Estimation for Multi-Person Camera-Space Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">XNect: Real-time Multi-person 3D Human Pose Estimation with a Single RGB Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.00837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DGCN: Dynamic Graph Convolutional Network for Efficient Multi-Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11924" to="11931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sárándi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07227</idno>
		<title level="m">MeTRAbs: Metric-Scale Truncation-Robust Heatmaps for Absolute 3D Human Pose Estimation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03473</idno>
		<title level="m">PoseNet3D: Unsupervised 3D Human Shape and Pose Estimation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selfsupervised Keypoint Correspondences for Multi-Person Pose Estimation and Tracking in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Umer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SMAP: Single-Shot Multi-Person Absolute 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
