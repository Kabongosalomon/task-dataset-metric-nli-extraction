<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Context Aggregation Network for Image Dehazing and Deraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shandong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
							<email>liaojing8871@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">City University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
							<email>lihengzhang1993@knights.ucf.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Hou</surname></persName>
							<email>houdd@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Cloud</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ganghua@microsoft.com</email>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Cloud</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Context Aggregation Network for Image Dehazing and Deraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image dehazing aims to recover the uncorrupted content from a hazy image. Instead of leveraging traditional lowlevel or handcrafted image priors as the restoration constraints, e.g., dark channels and increased contrast, we propose an end-to-end gated context aggregation network to directly restore the final haze-free image. In this network, we adopt the latest smoothed dilation technique to help remove the gridding artifacts caused by the widely-used dilated convolution with negligible extra parameters, and leverage a gated sub-network to fuse the features from different levels. Extensive experiments demonstrate that our method can surpass previous state-of-the-art methods by a large margin both quantitatively and qualitatively. In addition, to demonstrate the generality of the proposed method, we further apply it to the image deraining task, which also achieves the state-of-the-art performance. Code has been made available at https://github.com/cddlyf/GCANet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the existence of turbid medium (e.g., dusk, smoke, and other particles) in the atmosphere, images taken in such atmospheric phenomena are subject to visible quality degradation, such as contrast and saturation loss. Taking these degraded images as input, many vision-based systems, originally designed with the assumption of clean capture environments, may be easily troubled with drastic performance decrease. Given that, image dehazing has been extensively studied to restore the clean image from the corrupted input, to serve as the preprocessing step of the aforementioned systems.</p><p>In this literature, the hazing processing is often repre-sented with the physical corruption model:</p><formula xml:id="formula_0">I(x) = J(x)t(x) + A(1 − t(x))<label>(1)</label></formula><p>where I(x) and J(x) are the degraded hazy image and the target haze-free scene radiance respectively. A is the global atmospheric light, and t(x) is the medium transmission map, which is dependent on the unknown depth information. Most previous dehazing methods first estimate the transmission map t(x) or the atmospheric light A, then try to recover the final clean image J(x). But the first step is a very challenging problem because both the transmission map t(x) and the atmospheric light A are often unknown in the real scenarios.</p><p>To compensate for the lost information during the corruption procedure, many traditional methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> leverage some image priors and visual cues to estimate the transmission maps and atmospheric light. For example, <ref type="bibr" target="#b15">[16]</ref> maximizes the local contrast of the target image by using the prior that the contrast of degraded images is often drastically decreased. <ref type="bibr" target="#b16">[17]</ref> proposes the dark channel prior based on the assumption that image patches of outdoor haze free images often have low-intensity values. <ref type="bibr" target="#b1">[2]</ref> relies on the assumption that haze-free image colors are well approximated by a few hundred distinct colors and proposes a non-local prior-based dehazing algorithm. However, these priors do not always hold, so they may not work well in certain real cases.</p><p>With the latest advances of deep learning, many CNNbased methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref> are proposed by leveraging a large scale training datasets. Compared to traditional methods as described above, CNN-based methods attempt to directly regress the intermediate transmission map or the final clean image, and achieve superior performance and robustness. <ref type="bibr" target="#b2">[3]</ref> presents an end-to-end network to estimate the intermediate transmission map. <ref type="bibr" target="#b21">[22]</ref> reformulates the atmospheric scattering model to predict the final clean image through a light-weight CNN. <ref type="bibr" target="#b31">[32]</ref> creates three different derived input images from the original hazy image and fuses the dehazed results out of these derived inputs. <ref type="bibr" target="#b41">[42]</ref> incorporates the physical model in Equation <ref type="formula" target="#formula_0">(1)</ref> into the network design and uses two sub-networks to regress the transmission map and atmospheric light respectively.</p><p>In this paper, we propose a new end-to-end gated context aggregation network (denoted as "GCANet") for image dehazing. Since dilated convolution is widely used to aggregate context information for its effectiveness without sacrificing the spatial resolution <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9]</ref>, we also adopt it to help obtain more accurate restoration results by covering more neighbor pixels. However, the original dilated convolution will produce so-called "gridding artifacts" <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15]</ref>, because adjacent units in the output are computed from completely separate sets in the input when the dilation rate is larger than one. Recently, <ref type="bibr" target="#b36">[37]</ref> analyzes the dilation convolution in a compositional way and proposes to smooth the dilated convolution, which can greatly reduce this gridding artifacts. Hence, we also incorporate this idea in our context aggregation network. As demonstrated in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b26">27]</ref>, fusing different levels of features is often beneficial for both low-level and high-level tasks. Inspired by it, we further propose a gated sub-network to determine the importance of different levels and fuse them based on their corresponding importance weights. <ref type="bibr" target="#b31">[32]</ref> also uses a gated fusion module in their network, but they directly fuse the dehazing results of different derived input images rather than the intermediate features.</p><p>To validate the effectiveness of the proposed GCANet, we compare it with previous state-of-the-art methods on the recent dehazing benchmark dataset RESIDE <ref type="bibr" target="#b22">[23]</ref>. Experiments demonstrate that our GCANet outperforms all the previous methods both qualitatively and quantitatively by a large margin. Furthermore, we conduct comprehensive ablation studies to understand the importance of each component. To show the generality of the proposed GCANet, we have also applied it to the image deraining task, which can also obtain superior performance over previous stateof-the-art image deraining methods.</p><p>To summarize, our contributions are three-fold as below:</p><p>• We propose a new end-to-end gated context aggregation network GCANet for image dehazing, in which the smoothed dilated convolution is used to avoid the gridding artifacts and a gated subnetwork is applied to fuse the features of different levels.</p><p>• Experiments show that GCANet can obtain much better performance than all the previous state-of-the-art image dehazing methods both qualitatively and quantitatively. We also provide comprehensive ablation studies to validate the importance and necessity of each component.</p><p>• We further apply our proposed GCANet to the image deraining task, which also outperforms previous stateof-the-art image deraining methods and demonstrates its generality.</p><p>The remainder of the paper is organized as follows. We will first summarize related work in Section 2, then give our main technical details in Section 3. Finally, we will provide comprehensive experiments results and ablation studies in Section 4 and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single image dehazing is the inverse recovery procedure of the physical corruption procedure defined in Equation <ref type="bibr" target="#b0">(1)</ref>, which is a highly ill-posed problem because of the unknown transmission map and global atmospheric light. In the previous several decades, many different image dehazing methods are proposed to tackle this challenging problem, which can be roughly divided into traditional priorbased methods and modern learning-based methods. The most significant difference between these two types is that the image priors are handcrafted in the former type but are learned automatically in the latter type.</p><p>In the traditional prior-based methods, many different image statistics priors are leveraged as extra constraints to compensate for the information loss during the corruption procedure. For example, <ref type="bibr" target="#b10">[11]</ref> propose a physically grounded method by estimating the albedo of the scene. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> discover and improve the effective dark channel prior to calculate the intermediate transmission map more reliably. <ref type="bibr" target="#b33">[34]</ref> use Markov Random Field to maximize the local contrast of an image by assuming that the local contrast of a clear image is higher than that of a hazy image. Based on the observation that small image patches typically exhibit a one-dimensional distribution in the RGB color space, <ref type="bibr" target="#b11">[12]</ref> recently propose a color-line method for image dehazing and <ref type="bibr" target="#b1">[2]</ref> propose a non-local path prior to characterize the clean images. These dedicatedly handcrafted priors , however, hold for some cases, but they are not always robust to handle all the cases.</p><p>Recently, learning-based methods are proposed for image dehazing by leveraging the large-scale datasets and the powerful parallelism of GPU. In these type of methods, the image priors are automatically learned from the training dataset by the neural network and saved in the network weights. Their main differences typically lie in the learning targets and the detailed network structures. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref> propose an end-to-end CNN network and multi-scale network respectively to predict the intermediate transmission maps. However, inaccuracies in the estimation of the transmission map always lead to low-quality dehazed results. <ref type="bibr" target="#b21">[22]</ref> encode the transmission map and the atmospheric light into one variable, and then use a lightweight network to predict it. <ref type="bibr" target="#b41">[42]</ref> design two different sub-networks for the prediction of the transmission map and the atmospheric light by following the physical model defined in Equation <ref type="bibr" target="#b0">(1)</ref>. We propose an end-to-end gated context aggregation network for image dehazing but different from these methods, our proposed GCANet is designed to directly regress the residue between the hazy image and the target clean image. Moreover, our network structure definitely distinguish from the previous ones, which is quite lightweight but can achieve much better results than all the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will introduce the architecture of the proposed gated context aggregation network GCANet. As shown in <ref type="figure">Figure 1</ref>, given a hazy input image, we first encode it into feature maps by the encoder part, then enhance them by aggregating more context information and fusing the features of different levels without downsampling. Specifically, the smoothed dilated convolution and an extra gate sub-network are leveraged. The enhanced feature maps will be finally decoded back to the original image space to get the target haze residue. By adding it onto the input hazy image, we will get the final haze free image.</p><p>Smoothed Dilated Convolution Modern image classification networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref> often integrate multi-scale contextual information via successive pooling and subsampling layers that reduce resolution until a global prediction is obtained. However, for dense prediction tasks like segmentation, the contradiction is the required multi-scale contextual reasoning and the lost spatial resolution information during downsampling. To solve this problem, <ref type="bibr" target="#b40">[41]</ref> proposes a new dilated convolutional layer, which supports exponential expansion of the receptive field without loss of resolution or coverage. In the one-dimension case, given a 1-D input f , the output of the regular convolutional layer w with kernel size k is:</p><formula xml:id="formula_1">(f ⊗ w)(i) = k j=1 f [i + j]w[j]<label>(2)</label></formula><p>where one output point cover total k input points, so the receptive field is k. But for the dilated convolution, it can be viewed as "convolution with a dilated filter", which can be represented as:</p><formula xml:id="formula_2">(f ⊗ r w)(i) = k j=1 f [i + r * j]w[j]<label>(3)</label></formula><p>where r is the dilation rate, and the dilated convolution will degenerate to regular convolution when r = 1. To understand the dilated convolution in an intuitive way, we can view it as inserting r−1 zeros between two adjacent weights of w. In this way, the dilated convolution can increase the original receptive field from k to r * (k − 1) + 1 without reducing the resolution. Despite of the effectiveness of the dilated convolution, it will produce the so-called gridding artifacts, which is also noticed in previous papers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15]</ref>. To understand this issue more clearly, a very recent work <ref type="bibr" target="#b36">[37]</ref> analyzes the dilated convolution in a compositional way. The illustration of gridding artifacts is shown in <ref type="figure">Figure 2</ref>, where the case of one dilated convolutional layers with r = 2, k = 3 is analyzed. Considering the four neighbor pixels of the next layer, they and their dependent units in the previous layer are marked with four different colors respectively. We can easily find that these four neighor pixels are related to totally different sets of previous units in the previous layer. In other words, there is no dependency among the input units or the output units in the dilated convolution. This is why it will potentially cause the inconsistencies, i.e. gridding artifacts.</p><p>To alleviate it, <ref type="bibr" target="#b36">[37]</ref> proposes to add interaction among the input units before dilated convolution or output units after dilated convolution by adding an extra convolutional layer of kernel size (2r − 1). In this paper, we choose to add the dependency of input units by default. Need to note that, <ref type="bibr" target="#b36">[37]</ref> adopts a separable and shared convolution as the extra convolutional layer rather than the vanilla one. "Separable" means the separable convolution idea from <ref type="bibr" target="#b7">[8]</ref>, while "shared" means the convolution weights are shared for all the channels. In this way, this special convolutional layer has a constant parameter size (2r − 1) 2 , which is independent of the feature channel number. <ref type="figure">Figure 2</ref> is one illustration of smoothed dilated convolution.</p><p>Gated Fusion Sub-network As shown in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref>, fusing the features from different levels is often beneficial both for low-level and high-level tasks. To implement this idea, <ref type="bibr" target="#b26">[27]</ref> uses the feature pyramids to fuse high-level semantic feature maps at all scales, and <ref type="bibr" target="#b41">[42]</ref> leverages the densely connected networks. In this paper, we adopt a different way by incorporation of an extra gated fusion sub-network G. Specifically, we first extract the feature maps from different levels F l , F m , F h , and feed them into the gated fusion subnetwork. The output of the gated fusion sub-network are three different importance weights (M l , M m , M h ), which correspond to each feature level respectively. Finally, these three features maps F l , F m , F h from different levels are linearly combined with the regressed importance weights.</p><formula xml:id="formula_3">(M l , M m , M h ) = G(F l , F m , F h ) F o = M l * F l + M m * F m + M h * F h<label>(4)</label></formula><p>The combined feature map F o will be further fed into the decoder to get the target haze residue. In this paper, our gated fusion sub-network consists of only one convolutional  <ref type="figure">Figure 1</ref>. The overall network structure of the proposed GCANet, which follows a basic auto-encoder structure. It consists of three convolution blocks as the encoder part, and one deconvolution block and two convolution blocks as the decoder part. Several smoothed dilated resblocks are inserted between them to aggregate context information without gridding artifacts. To fuse the features from different levels, an extra gate fusion sub-network is leveraged. During the runtime, the GCANet will predict the residue between the target clean image and the hazy input image in an end-to-end way. <ref type="figure">Figure 2</ref>. The illustration of gridding artifacts of dilated convolution and the proposed smoothed dilated convolution in <ref type="bibr" target="#b36">[37]</ref>: the four different points in next layer i are indicated by different colors, it can be seen that they are related to completely different sets of units of previous layer, which will potential cause the gridding artifacts. By contrast, the smoothed dilated convolution, which adds the dependency among the input units with an extra separable and shared convolutional layer before the dilated convolution. layer with kernel size 3x3, whose input is the concatenation of F l , F m , F h and output channel number is 3.</p><p>Network Structure Following the similar network design principle in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>, our overall network structure are also designed as a simple auto-encoder, where seven residual blocks are inserted between the encoder and decoder to enhance its learning capacity. Specifically, three convolutional layers are first used to encode the input hazy image into the feature maps as the encoder part, where only the last convolutional layer downsamples the feature maps by 1/2 once. Symmetrically, one deconvolutional layer with stride 1/2 is used to upsample the feature map to the original resolution in the decoder part, then the following two convolutional layers convert the feature maps back to the image space to get the final target haze residue. For the intermediate residual blocks, we call them "Smoothed Dilated Resblock" , because we have replaced all the orig-inal regular convolutional layers with the aforementioned smoothed dilated convolutional layers. The dilation rates of these seven residual blocks are setted as (2, 2, 2, 4, 4, 4, 1) respectively. To obtain a good tradeoff between the performance and runtime, we set the channel number of all the intermediate convolutional layers as 64. Note that except for the last convolutional layer and every extra separable and shared convolutional layer in the smoothed dilated convolution layer, we put an instance normalization layer <ref type="bibr" target="#b34">[35]</ref> and ReLU layer after each convolutional layer. In the experiment part, we will show instance normalization is more suitable than batch normalization for the image dehazing task.</p><p>As demonstrated in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>, besides the input image, precalculating the edge of the input image and feeding them into the network as the auxiliary information is very helpful to the network learning. Hence, by default, we also adopt this simple idea and concatenate the pre-calculated edge with the input hazy image along the channel dimension as the final inputs of GCANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>In previous learning-based image dehazing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, the simple Mean Square Error loss is adopted. Following the same strategy, we also use this simple loss by default. But different from these methods, our learning target is the residue between the haze free image and the input hazy one:</p><formula xml:id="formula_4">r = J − Î r = GCAN et(I) L = r − r 2 (5)</formula><p>where r andr are the ground truth and predicted haze residue respectively. During runtime, we will addr onto the input hazy image to get the final predicted haze free image. Need to emphasize that designing better loss function is not the focus of this paper, but our proposed GCANet should be able to generalize to better designed losses. For example, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref> find the perceptual loss <ref type="bibr" target="#b19">[20]</ref> and GAN loss can improve the final dehazing results. However, even only with the above simple loss, our method can still achieve the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Implementation Details For experiments, we first validate the effectiveness of the proposed GCANet on the image dehazing task, then demonstrate its generality by further applying it to image deraining task. To train these two tasks, we all directly adopt the available benchmark datasets both for training and evaluation. For each task, we compare our method with many previous state-of-the-art methods. Without losing generality, we use almost the same training strategy for these two tasks. By default, the whole network is trained for 100 epochs with the Adam optimizer. The default initial learning rate is set to 0.01 and decayed by 0.1 for every 40 epochs. All the experiments are trained with the default batch size to 12 on 4 GPUs.</p><p>Dataset Setup For the image hazing task, we find most previous state-of-the-art methods leverage available depth datasets to synthesize their own hazy datasets based on the physical corruption model in Equation <ref type="formula" target="#formula_0">(1)</ref>, and conduct evaluation only on these specific datasets. Direct comparisons on these datasets are not fair. Recently, <ref type="bibr" target="#b22">[23]</ref> proposes a image dehazing benchmark RESIDE, which consists of large-scale training and testing hazy image pairs synthesized from depth and stereo datasets. To compare with stateof-the-art methods, they use many different evaluation metrics and conduct comprehensive comparisons among them. Although their test dataset consists of both indoor and outdoor images, they only report the quantitative results for the indoor parts. Following their strategy, we also compare our method on indoor dataset quantitatively and outdoor dataset qualitatively.</p><p>Similar to image hazing, there also exist several different large-scale synthetic datasets for image deraining. Most recently, <ref type="bibr" target="#b42">[43]</ref> has developed a new dataset containing raining density labels (e.g. light, medium and heavy) for densityaware image deraining. Although we do not need the raindensity label information in our method, we still adopt this dataset for fair comparison. In this dataset, a total of 12000 training rainy images are synthesized with different orientations and scales with Photoshop.</p><p>Quantitative and Qualitative Evaluation for image dehazing In this part, we will compare our method with previous state-of-the-art image dehazing methods both quantitatively and qualitatively. <ref type="table" target="#tab_0">Table 1</ref>, six different state-of-the-art methods are used for quantitative evaluation: DCP <ref type="bibr" target="#b16">[17]</ref>, CAP <ref type="bibr" target="#b45">[46]</ref>, GRM <ref type="bibr" target="#b3">[4]</ref>, AOD-Net <ref type="bibr" target="#b21">[22]</ref>, DehazeNet <ref type="bibr" target="#b2">[3]</ref>, and GFN <ref type="bibr" target="#b31">[32]</ref>. Among them, the first three are traditional prior-based methods and the last three are learning-based methods. For convenience, all the results except GFN shown in the Table 1 are directly cited from <ref type="bibr" target="#b22">[23]</ref>. For GFN <ref type="bibr" target="#b31">[32]</ref>, the latest state-of-the-art dehazing method, they have also reported the results on the RESIDE SOTS indoor dataset in their paper. Although various evaluation metrics are proposed in <ref type="bibr" target="#b22">[23]</ref>, we only adopt PSNR and SSIM, the most widely used metrics in previous methods. It can be seen that our proposed GCANet outperforms all previous dehazing methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>We further show the dehazing results of two indoor and three outdoor hazy images in <ref type="figure" target="#fig_1">Figure 3</ref> for qualitative comparisons. From these visual results, we can easily observe that DCP <ref type="bibr" target="#b16">[17]</ref> and CAP <ref type="bibr" target="#b45">[46]</ref> will make the brightness of the dehazed results relatively dark, which is because of their underlying prior assumptions. For AOD-Net <ref type="bibr" target="#b21">[22]</ref>, we find that it is often unable to entirely remove the haze from the input. Although GFN <ref type="bibr" target="#b31">[32]</ref> can achieve quite good dehazing results in some cases, our GCANet is the best one which can both preserve the original brightness and remove the haze as much as possible from the input.</p><p>Ablation Analysis To understand the importance of each component in our GCANet, we have conducted ablation analysis with and without each specific component. Specifically, we focus on three major components: with / without the smoothed dilation, with / without the gated fusion subnetwork, and with instance normalization / batch normalization. Correspondingly, four different network configurations are evaluated on the image dehazing task, and we incrementally add one component to each configuration at a time. As shown in <ref type="table">Table 3</ref>, the final performance keeps raising in these experiments. However, one interesting observation is that it seems the biggest gain comes from instance normalization in place of batch normalization. Therefore, we further add one experiment by using instance normalization only without smoothed dilation and gated fusion network. Unsurprisingly, it can still achieve slightly better results than the first configuration with batch normalization, but the gain is smaller than the aforementioned one. That is to say, by combing all the designed components together, larger gains can be achieved than only applying one or some of them.</p><p>To further validate the effectiveness of our smoothed dilated resblock in alleviating the gridding artifacts, we compare it with the previous widely-used exponentially dilated resblock <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>, where the dilation rates of adjacent resblocks are increased exponentially (e.g., <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32</ref>     <ref type="table">Table 3</ref>. Detailed ablation analysis for each component with different training configurations, which shows that the combination of all the designed components is the best. are also very curious about whether the proposed GCANet can be applied to the image deraining task. Specifically, we leverage the training dataset synthesized in <ref type="bibr" target="#b42">[43]</ref>, and compare our method with seven different image deraining methods: DSC <ref type="bibr" target="#b27">[28]</ref>, GMM <ref type="bibr" target="#b25">[26]</ref>, CNN <ref type="bibr" target="#b12">[13]</ref>, JORDER <ref type="bibr" target="#b39">[40]</ref>, DDN <ref type="bibr" target="#b13">[14]</ref>, JBO <ref type="bibr" target="#b44">[45]</ref> and DID-MDN <ref type="bibr" target="#b42">[43]</ref>. Note that all the results are cited from <ref type="bibr" target="#b42">[43]</ref>. Surprisingly, as shown in <ref type="table" target="#tab_1">Table 2</ref>, our GCANet even outperforms previous best method <ref type="bibr" target="#b42">[43]</ref> with more than 3 dB in PSNR. We also provide one deraining example in <ref type="figure">Figure 2</ref> for visual comparison. It can be seen that many previous methods like CNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> often tend to under-derain the image, and some unexpected patterns may appear in the deraining results of JORDER <ref type="bibr" target="#b39">[40]</ref>. To see more details, we crop and zoom-in one local patch from the sky region. It is easy to observe that the deraining result of our GCANet is much clearer than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose an end-to-end gated context aggregation network for image dehazing. To eliminate the gridding artifacts from the dilated convolution, a latest smoothed dilated technique is used. Moreover, a gated subnetwork is leveraged to fuse the features of different levels. Despite of the simplicity of the proposed method, it is better than the previous state-of-the-art image dehazing methods by a large margin. We further apply the proposed network to the image deraining task, which can also obtain and state-of-the-art performance. In the future, we will try more facy losses used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref> and consider to extend to video dehazing like <ref type="bibr" target="#b4">[5]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparisons with different dehazing methods for indoor and outdoor hazy images, and the last row is one real hazy example. It can be seen that our GCANet is the best one which can remove the underlying haze while maintaining the original brightness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 ,</head><label>5</label><figDesc>the gridding artifacts and color shift often happen near the object boundaries and texture regions when the exponentially dilated resblocks are used. By contrast, our smoothed dilated resblocks can address this problem and preserve the original color fidelity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Two dehazing examples to show the superority of smoothed dilated resblocks (right column) and regular exponentially dilated resblocks (left colum). Obviously, our smoothed dilated resblocks improve the gridding artifacts and produce much better dehazing results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>). As shown in the two representative dehazing examples in DCP<ref type="bibr" target="#b16">[17]</ref> CAP<ref type="bibr" target="#b45">[46]</ref> GRM<ref type="bibr" target="#b3">[4]</ref> AOD-Net<ref type="bibr" target="#b21">[22]</ref> DehazeNet<ref type="bibr" target="#b2">[3]</ref> GFN<ref type="bibr" target="#b31">[32]</ref> GCANet Quantitative comparisons of image dehazing on the SOTS indoor dataset from RESIDE. Obviously, Our GCANet outperforms all the previous state-of-the-art image dehazing methods by a very large margin.</figDesc><table><row><cell>PSNR</cell><cell>16.62</cell><cell>19.05</cell><cell>18.86</cell><cell>19.06</cell><cell>21.14</cell><cell>22.30</cell><cell>30.23</cell></row><row><cell>SSIM</cell><cell>0.82</cell><cell>0.84</cell><cell>0.86</cell><cell>0.85</cell><cell>0.86</cell><cell>0.88</cell><cell>0.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Generality to Image Deraining Task The task of image deraining is very similar to image dehazing, which aims to remove the rain-streak component from a corrupted image captured in the rainy environment. Though our focus is to design a good network structure for image dehazing, we DSC<ref type="bibr" target="#b27">[28]</ref> GMM<ref type="bibr" target="#b25">[26]</ref> CNN<ref type="bibr" target="#b12">[13]</ref> JORDER<ref type="bibr" target="#b39">[40]</ref> DDN<ref type="bibr" target="#b13">[14]</ref> JBO<ref type="bibr" target="#b44">[45]</ref> DID-MDN<ref type="bibr" target="#b42">[43]</ref> GCANet Quantitative comparison results (PSNR) of the image deraining task on the DID-MDN test dataset. Although our GCANet is mainly designed for image dehazing, it generalizes very well for the image deraining task.Figure 4. One visual example deraining result for the different state-of-the-art deraining methods. Obviously, previous methods like CNN<ref type="bibr" target="#b12">[13]</ref>, JORDER<ref type="bibr" target="#b39">[40]</ref> tend to under-derain the image, and our GCANet can achieve the best deraining results.PSNR   27.57 28.12 28.72 30.23 28.45    </figDesc><table><row><cell>21.44</cell><cell>22.75</cell><cell>22.07</cell><cell>24.32</cell><cell>27.33</cell><cell>23.05</cell><cell>27.95</cell><cell>31.68</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image dehazing by multi-scale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3271" to="3282" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Robust image and video dehazing with visual artifact suppression via gradient residual minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coherent online video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Computer Vision (ICCV)</title>
		<meeting>Intl. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stylebank: An explicit representation for neural image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2516" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1610" to="02357" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Decouple learning for parameterized image operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08186</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generic deep architecture for single image reflection removal and image smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3258" to="3267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Clearing the skies: A deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective use of dilated convolutions for segmenting small object instances in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1442" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards fog-free invehicle vision systems through contrast restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep exemplar-based colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aod-net: Allin-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04143</idno>
		<title level="m">Reside: A benchmark for single image dehazing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Single image dehazing via conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recurrent squeezeand-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rain streak removal using layer priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Removing rain from a single image via discriminative sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3397" to="3405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nighttime haze removal using color transfer pre-processing and dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2012 19th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="957" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00213</idno>
		<title level="m">Gated fusion network for single image dehazing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Instance normalization: the missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smoothed dilated convolutions for improved dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2486" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved single image dehazing using dark channel prior and multi-scale retinex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent System Design and Engineering Application (ISDEA), 2010 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="848" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast image dehazing using improved dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Science and Technology (ICIST), 2012 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="663" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Densely connected pyramid dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Density-aware single image deraining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07412</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-scale single image dehazing using perceptual pyramid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="902" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint bilayer optimization for single-image rain streak removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
