<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Speech Recognition via Local Prior Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
						</author>
						<title level="a" type="main">Semi-Supervised Speech Recognition via Local Prior Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For sequence transduction tasks like speech recognition, a strong structured prior model encodes rich information about the target space, implicitly ruling out invalid sequences by assigning them low probability. In this work, we propose local prior matching (LPM), a semi-supervised objective that distills knowledge from a strong prior (e.g. a language model) to provide learning signal to a discriminative model trained on unlabeled speech. We demonstrate that LPM is theoretically well-motivated, simple to implement, and superior to existing knowledge distillation techniques under comparable settings. Starting from a baseline trained on 100 hours of labeled speech, with an additional 360 hours of unlabeled data, LPM recovers 54% and 73% of the word error rate on clean and noisy test sets relative to a fully supervised model on the same data. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fully supervised learning remains the mainstream paradigm for state-of-the-art automatic speech recognition (ASR). These systems require huge annotated data sets <ref type="bibr" target="#b4">Chiu et al., 2018;</ref><ref type="bibr" target="#b15">Hannun et al., 2014;</ref><ref type="bibr" target="#b1">Amodei et al., 2016)</ref>, which are time-consuming and expensive to collect. This hinders the development of accurate ASR for low resource languages <ref type="bibr" target="#b41">(Precoda, 2013)</ref>. In fact, out of over 6,000 spoken languages, fewer than 150 are supported by commercial ASR service providers. In sharp contrast to how we teach machines to recognize speech, humans do not learn by listening to thousands of hours of speech and simultaneously reading the corresponding transcriptions. Instead, humans possess an inherent ability to learn from vast quantities of unlabeled speech <ref type="bibr" target="#b5">(Chomsky, 1986;</ref><ref type="bibr" target="#b27">Kuhl, 2004;</ref><ref type="bibr" target="#b13">Glass, 2012;</ref><ref type="bibr" target="#b11">Dupoux, 2018)</ref>. Consider conversing with someone with a strong accent. Even when the speaker pronounces several words in an unusual way, one can often correctly understand the sentence. We argue that the source of indirect supervision in processing unlabeled speech comes from prior knowledge about the world and the context of the speech.</p><p>Inspired by this, we devise a semi-supervised learning framework termed local prior matching (LPM). We apply LPM to speech recognition allowing an ASR model to learn from unlabeled speech by leveraging a strong language model. Given an unlabeled utterance, a proposal model first generates a set of hypotheses. The language model (LM) then produces a target distribution for the ASR model to match, enabling distillation of prior knowledge into the ASR model.</p><p>We evaluate LPM on the LibriSpeech corpus <ref type="bibr" target="#b36">(Panayotov et al., 2015)</ref>, using 100 hours of labeled speech to build a baseline. With the addition of 360 hours of unlabeled data, LPM recovers 54% and 73% of the word error rate (WER) on a clean and noisy test set relative to a completely supervised model on the full 460 hours. By augmenting LPM with another 500 hours, for a total of 860 hours of unlabeled speech, LPM surpasses the performance of using 460 hours of labeled data. We also conduct extensive ablation studies in order to demonstrate the significance of each proposed component. Our main contributions are as follows:</p><p>• We propose an intuitive yet theoretically wellmotivated learning objective that can leverage large quantities of unpaired speech and text.</p><p>• We achieve a state-of-the-art result in WER recovery with unlabelled data on a standard ASR benchmark.</p><p>• We show that LPM can scale to 60,000 hours of unlabelled speech and yield further gains in WER.</p><p>Compared to adversarial training <ref type="bibr" target="#b30">(Liu et al., 2019)</ref>, back-translation <ref type="bibr" target="#b17">(Hayashi et al., 2018)</ref>, and cycleconsistency , LPM achieves significantly better performance without the need to jointly train additional modules. Compared to knowledge distillation, (1) LPM distills from a prior rather than a posterior, (2) considers multiple hypotheses in a principled manner and (3) improves the proposal model which results in improved WER. We show that LPM achieves better WER compared to a strong pseudo-label baseline <ref type="bibr" target="#b22">(Kahn et al., 2019a)</ref> as well as other forms of knowledge distillation. Let x denote an utterance of speech and y a transcription. We assume speech is generated following a two-step process:</p><formula xml:id="formula_0">y ∼ p y x ∼ p x|y ,<label>(1)</label></formula><p>where the text y is first generated from the language model (LM), p y , and the speech x is then generated from a text-tospeech (TTS) model, p x|y , conditioned on y. The posterior, p y|x , is then the ASR model of interest. In a typical supervised learning setting, one has access to a labeled dataset D l , which contains paired samples {(x (i) , y (i) )} N i=1 drawn from the joint distribution, p xy (x, y). An ASR model q y|x can be trained by minimizing the marginal weighted cross entropy E px [H(p y|x , q y|x )], which can be estimated with (x,y)∈D l − log q y|x (y | x) from samples. In the semi-supervised learning setting, we have additional unpaired speech D s u and text D t u , both of which can be many times larger than the paired dataset D l . We wish to exploit this unpaired data to improve the ASR model. To that end, we propose a method to estimate p y|x for an unlabeled example x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Through the Lens of Generative Modeling</head><p>A natural way to approximate the posterior, p y|x , is to estimate a TTS model, p x|y , from paired data, an LM, p y from unpaired text and apply Bayes' theorem</p><formula xml:id="formula_1">p y|x (y | x) = p y (y) p x|y (x | y) ŷ∈Y p y (ŷ) p x|y (x |ŷ)</formula><p>.</p><p>(2)</p><p>However, for sequence transduction tasks, the cardinality of the output space Y is infinite hence marginalization is intractable. Luckily, the denominator in equation 2 can often be approximated by summing over a set of hypotheses proposed by a trained ASR model, as done in sequence discriminative training <ref type="bibr" target="#b39">(Povey, 2005;</ref><ref type="bibr">Veselỳ et al., 2013)</ref> and differentiable beam search decoding . Such an approximation is reasonable because only text sequences that are linguistically and acoustically plausible will contribute non-negligible probability in the marginalization, and there are very few of them for a given utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Local Prior Matching</head><p>Let B(x) be the set of beam search hypotheses generated by a proposal ASR model r y|x with a beam size of k. By replacing Y with B(x), our estimated posterior becomes:</p><formula xml:id="formula_2">p y|x (y | x) = p y (y) p x|y (x | y) ŷ∈B(x) p y (ŷ) p x|y (x |ŷ) 1(y ∈ B(x)),</formula><p>where 1(·) is the indicator function used to ensurep y|x is a valid distribution. <ref type="table" target="#tab_1">Table 1</ref> shows the reference text (Ref.) and the hypotheses generated by a supervised ASR model trained on 100 hours of paired data (Sup.) for two utterances. We make three key observations. (1) The hypotheses are acoustically similar to each other (e.g., "stealing" / "still ing"), indicating that the acoustic probability p x|y between the hypotheses may be very close.</p><p>(2) One can often tell which hypotheses are wrong without listening to the speech because they are semantically unreasonable and grammatically incorrect in multiple locations.</p><p>(3) The third column shows the linguistic scores log p y from an LM trained on unpaired text D t u . Within each utterance, linguistic scores align well with linguistic plausibility.</p><p>Based on the above observations, we assume that the posterior probability between hypotheses are dominated by p y , whereas p x|y can be treated as a constant. Therefore, our final posterior approximation can be written as</p><formula xml:id="formula_3">p y|x (y | x) = p y (y) ŷ∈B(x) p y (ŷ) 1(y ∈ B(x)). (3)</formula><p>The approximated posterior only requires computing language model probabilities of the beam search hypotheses. We refer to equation 3 as the local prior, since it is the prior re-normalized with intended support only in the neighborhood of the unknown target transcription. We also propose local prior matching (LPM) as a semi-supervised objective for training an ASR model q y|x with unlabeled speech x:</p><formula xml:id="formula_4">L lpm (q y|x ; x, p y , r y|x , k) = H(p y|x=x , q y|x=x ) = − y∈B(x) p y (y) ŷ∈B(x) p y (ŷ) log q y|x (y | x).</formula><p>The LPM objective minimizes the cross entropy between the local prior and the model distribution, and is minimized when q y|x =p y|x . Intuitively, LPM encourages the ASR model to assign posterior probabilities proportional to the linguistic probabilities of the proposed hypotheses, similar to how humans recognize speech when ambiguity exists (e.g., "let her" / "led her" / "letter"). For clarity, we term q y|x the online model, and let q y|x (y | x; θ q ) and r y|x (y | x; θ r ) denote the models with parameters θ q and θ r , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Choice of Proposal Model</head><p>The quality of the posterior approximationp y|x depends on the proposal model r y|x . Instead of using a fixed proposal model throughout the entire training process, we consider two strategies for updating r y|x with q y|x .</p><p>On-policy beam search The first approach always uses the online model q y|x as the proposal model. This means θ r = θ q and is effectively a form of on-policy beam search, Off-policy beam search While the on-policy method benefits from the immediate improvement of the online model, it also suffers immediately if the gradient update from a mini-batch deteriorates performance. This can result in instability during optimization. We consider a second option which does not tie θ r and θ q but instead updates θ r with θ q every T steps only when the performance of the online model q y|x is better than that of the proposal model r y|x by some metric. We refer to the second option as off-policy beam search. To avoid overfitting to the training set, we use the character error rate (CER) on the validation set as the metric for the proposal model update. We set T = 1000 for all experiments with the off-policy beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Filtering Hypotheses Using Estimated Lengths</head><p>As noted in <ref type="bibr" target="#b6">Chorowski &amp; Jaitly (2017)</ref>, sequence-tosequence ASR models sometimes predict end-of-sentence (EOS) tokens too early or generate looping n-grams, resulting in hypotheses that are significantly shorter or longer than the set of acoustically matched texts for a given utterance. Of the two failure modes, the former is more harmful when using LPM. The reason is that the LPM objective assumes all hypotheses obtained from the beam search are acoustically reasonable, and weights each of them by linguistic plausibility given by an LM. While LMs are effective in discriminating plausibility between sentences of similar length, we find empirically they tend to assign higher probabilities to shorter sentences than to longer sentences, even when the longer ones are more plausible and grammatically correct than the shorter ones. As a result, truncated hypotheses are assigned higher weights than acoustically matched but longer ones, which in turn encourages earlier prediction of EOS tokens and forms a catastrophic feedback loop particularly with the on-policy beam search.</p><p>To address this issue, we propose a simple filtering heuristic based on the text length. Before training the model, a text length L is estimated for each unlabeled speech sample x. During training, only hypotheses with length close to L are retained for the LPM objective computation. Let len(y) denote the length of y. We keep a hypothesis y only if r lb · L ≤ len(y) ≤ r ub · L , where r lb and r ub are the text length lower and upper bound ratios, respectively. Several methods can be used to estimate the text length on an unlabeled utterance, including the average speaking rate <ref type="bibr" target="#b38">(Peng et al., 2019)</ref> or a phoneme/syllable segmentation <ref type="bibr" target="#b0">(Adell &amp; Bonafonte, 2004;</ref><ref type="bibr" target="#b43">Scharenborg et al., 2010;</ref><ref type="bibr">Wang et al., 2017)</ref>. In this work, we estimate the length by using using that of the best hypothesis generated from the initial proposal model, generated with either ASR-only greedy decoding or ASR+LM beam search decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Our work builds on a large body of work in semi-supervised learning for ASR. Research in this direction can be classified based on the required modules and the objectives used to learn from unpaired data.</p><p>In <ref type="bibr" target="#b10">Drexler &amp; Glass (2018)</ref> and <ref type="bibr" target="#b24">Karita et al. (2018)</ref>, biencoder network architectures are used, which map text and speech to representations in a shared space with their corresponding encoder, and then apply a shared decoder to map from the shared space to the text space. Another line of work adds a TTS model <ref type="bibr">(Tjandra et al., 2017;</ref><ref type="bibr" target="#b3">Baskar et al., 2019)</ref> or a text-to-encoding (TTE) model <ref type="bibr" target="#b17">(Hayashi et al., 2018;</ref><ref type="bibr" target="#b18">Hori et al., 2019)</ref> in the loop of ASR training, which can be utilized for back-translation style data augmentation <ref type="bibr" target="#b44">(Sennrich et al., 2016)</ref> or cycle-consistency training <ref type="bibr">(Zhu et al., 2017)</ref>. <ref type="bibr" target="#b30">Liu et al. (2019)</ref> treats ASR as a generative model that conditions on speech instead of random noise vectors, and adopts the generative adversarial network (GAN) <ref type="bibr" target="#b14">(Goodfellow et al., 2014)</ref> framework in order to improve the fidelity of ASR-generated texts. All the aforementioned methods involve additional modules that must be jointly optimized with the ASR model and require finding a careful balance between multiple training objectives. In contrast, LPM only requires a pre-trained LM and optimizes a principled cross-entropy objective.</p><p>Knowledge distillation (KD) <ref type="bibr" target="#b8">(Cui et al., 2017;</ref><ref type="bibr" target="#b37">Parthasarathi &amp; Strom, 2019)</ref> and weak distillation (also known as selftraining or pseudo-labeling) <ref type="bibr">(Veselỳ et al., 2017;</ref><ref type="bibr" target="#b31">Manohar et al., 2018;</ref><ref type="bibr" target="#b29">Li et al., 2019;</ref><ref type="bibr" target="#b22">Kahn et al., 2019a</ref>) have also achieved great success in semi-supervised learning for ASR. In KD, a student posterior model learns from a teacher posterior model by minimizing the cross entropy between the two distributions on the unlabeled speech data. Because of this, we expect KD to yield better student models when when the teacher distribution starts out better than that of the student. On the other hand, in weak distillation the teacher distribution is replaced with its mode. Hence, weak KD is equivalent to training on the unlabeled data with a supervised maximum likelihood objective, using the labels predicted by the teacher model. In this case, the teacher model can be the same as the student model, a case commonly known as self-training or pseudo-labelling. To obtain pseudo-labels with higher quality, LMs are used for shallow fusion decoding <ref type="bibr" target="#b6">(Chorowski &amp; Jaitly, 2017)</ref>, which requires expensive hyperparameter search on a held-out set. This can be viewed as interpolating between an estimated prior and posterior to obtain a better teacher model to distill from.</p><p>We can view LPM as a type of knowledge distillation, but with three key differences. First, LPM distills directly from a prior instead of a posterior, enabling seamless integration of available context (e.g., the previous sentence or other modalities). Second, LPM considers multiple hypotheses and provides a principled way to weight them, while weak distillation typically uses only one hypothesis  or assumes a uniform distribution when multiple hy-potheses are used <ref type="bibr" target="#b22">(Kahn et al., 2019a)</ref>. Third, LPM uses an improving proposal model with a stable update strategy. This is difficult to implement with pseudo-labels generated with an LM because the hyperparameters used in decoding should be updated as the teacher changes. We demonstrate the significance of these differences in our experiments.</p><p>Aside from semi-supervised learning, this work is also related to unsupervised domain adaptation, where unlabeled speech of the target domain is provided. Unlike the proposed method, previous studies focus on learning domain invariant features <ref type="bibr">(Sun et al., 2017;</ref><ref type="bibr" target="#b32">Meng et al., 2017;</ref> or data augmentation through learned speech transformations <ref type="bibr" target="#b20">(Hsu et al., 2017;</ref>. Our work also shares a similar motivation as posterior regularization (PR) <ref type="bibr" target="#b12">(Ganchev et al., 2010)</ref>. Both methods aim to incorporates prior knowledge to improve a posterior, though PR achieves this by adding handcrafted linear constraints with a limited family of posterior distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Dataset We evaluate our approach on LibriSpeech (Panayotov et al., 2015), a crowd-sourced audio book corpus derived from the LibriVox Project. The training set contains 960 hours of speech, officially split into three sets: trainclean-100, train-clean-360, and train-other-500, where the first two sets are easier and the third set is noisier and more accented. Similarly, the development and test sets are also split according to difficulty, resulting in four partitions: {dev, test}×{clean, other}, each of which contains roughly five hours of speech. In this work we use train-clean-100 as the paired speech data, and the other two training splits as the unpaired data. We also examine how well LPM scales to a much larger amount of unlabelled data. To do this, we use the recently released Libri-Light corpus <ref type="bibr" target="#b23">(Kahn et al., 2019b)</ref> which contains roughly 60k hours of unlabelled audio from the same domain as LibriSpeech.</p><p>We train the LM on the unpaired text data provided with LibriSpeech, which includes approximately 14,500 books collected from Project Gutenberg. Some of the books in the text corpus overlap with those in the LibriSpeech training set. To avoid training the LM on the ground truth text of the unlabeled speech, we exclude the 997 overlapping books from the text data. We follow the same recipe as <ref type="bibr" target="#b22">Kahn et al. (2019a)</ref> to pre-process the remaining text.</p><p>Neural Network Architecture The proposal model r y|x and the online model q y|x are sequence-to-sequence neural networks <ref type="bibr" target="#b2">(Bahdanau et al., 2016;</ref><ref type="bibr" target="#b6">Chorowski &amp; Jaitly, 2017)</ref> with the same time-depth separable (TDS) architecture proposed in <ref type="bibr" target="#b16">Hannun et al. (2019)</ref>. The encoder is fully convolutional, composed of TDS blocks which reduce the number of parameters while keeping the receptive field large. The decoder is a single layer recurrent neural network (RNN) with gated recurrent units (GRUs), equipped with a singleheaded inner-product key-value attention <ref type="bibr">(Vaswani et al., 2017)</ref> for querying information from the encoder outputs. Unless otherwise stated, we follow the recipe of <ref type="bibr" target="#b22">Kahn et al. (2019a)</ref> which uses fewer TDS blocks in the encoder compared to <ref type="bibr" target="#b16">Hannun et al. (2019)</ref> in order to generalize better when trained on the smaller LibriSpeech train-clean-100. The output of the decoder at each step is a posterior distribution over 5,000 word pieces. The word pieces are generated with the SentencePiece toolkit <ref type="bibr" target="#b26">(Kudo &amp; Richardson, 2018)</ref> using transcripts from train-clean-100.</p><p>To enable efficient evaluation of the language model probabilities, which is required at each training step, we use the gated convolutional language model architecture (ConvLM) proposed in <ref type="bibr" target="#b9">Dauphin et al. (2017)</ref>, which achieves competitive performances compared to recurrent models while significantly reducing the latency. We use the same 5,000 word-piece vocabulary for the LM which is trained with the same model configuration and recipe as Zeghidour et al. <ref type="bibr">(2018)</ref>. The trained ConvLM achieves a token perplexity of 34.24 on the development set.</p><p>Optimization We use both paired and unpaired data to optimize q y|x . To simplify the optimization procedure, the model is provided with either a paired or an unpaired batch at each step, alternated with a fixed ratio m l : m u . When given a paired batch of n samples, {(x (i) , y (i) )} n i=1 , the model minimizes the standard crossentropy loss, 1 n i − log q y|x (y (i) | x (i) ). When provided with an unpaired batch {x (i) } n i=1 , the model minimizes a weighted LPM loss, α n · i L lpm (q y|x ; x (i) , p y , r y|x , k). The weight α and the mixing ratio m l : m u are used to balance the supervised and self-training objectives. For regularization we use 20% dropout <ref type="bibr">(Srivastava et al., 2014)</ref>, 10% label smoothing, 1% decoder input sampling, and 1% word piece sampling <ref type="bibr" target="#b25">(Kudo, 2018)</ref> following <ref type="bibr" target="#b22">Kahn et al. (2019a)</ref>. We use SGD without momentum to train the online model with an initial learning rate of 5e-2. To achieve a good CER on the development sets, the model is trained for at least 1.6M steps (paired and unpaired) with a batch size of 16 (8 GPUs × 2 per GPU). The learning rate is annealed by a factor of two every 0.64M steps. All experiments in this paper are implemented in the wav2letter++ framework <ref type="bibr" target="#b40">(Pratap et al., 2018)</ref>.</p><p>Initialization To initialize the proposal model and the online model, we consider three checkpoints from a baseline model trained on train-clean-100 for a varying number of steps using only the supervised objective. The three checkpoints, whose parameters are denoted as θ A , θ B , and θ C , are trained for about 300k / 40k / 16k steps, achieving average development set CERs of 13% / 20% / 38%, respectively.</p><p>We hypothesize that initializing the proposal model from the best checkpoint leads to a better approximationp y|x to the posterior. In contrast, <ref type="bibr" target="#b22">Kahn et al. (2019a)</ref> observe that training from scratch achieves consistently better performance than starting from a well-trained model, thus we hypothesize that initializing the online model with an earlier checkpoint may lead to better results. We initialize θ r = θ A and θ q = θ C unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>The best supervised model trained only on train-clean-100 (θ A ) achieves a (no LM) WER of 14.00%/37.02% on dev-clean/dev-other, respectively. Unless otherwise stated, we use train-clean-360 as the unpaired speech dataset, (r lb , r ub ) = (0.95, 1.05) for length filtering with reference lengths obtained from ASR-only greedy decoding. <ref type="table" target="#tab_2">Table 2</ref> shows how the WER varies with the beam size and the mixing ratio. For all mixing ratios, the model improves the most from a beam size of k = 1 to k = 2, showing the benefit of considering multiple hypotheses. The improvement is greater when a higher mixing ratio of unpaired-topaired speech is used. In addition, we note that the LM is effectively unused when k = 1 because the LM probability assigned to each hypothesis is normalized within the beam. If there is only one hypothesis, it will be assigned an approximate posterior probability of one. The amount of improvement diminishes with larger beam sizes, and the performance even starts to degrade beyond k = 4 when using a higher mixing ratio. This may result from the inclusion of worse hypotheses which have a better score under the LM. We use the best setting for the following experiments with a mixing ratio m l : m u = 1 : 4, a beam size k = 4, and an LPM weight α = 0.2. We present detailed results varying the LPM weight in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Beam Size, Mixing Ratio, and LPM Weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Proposal Model Update and Model Initialization</head><p>In addition to the two update strategies proposed in Section 2.4, termed On and Off (better), we experiment with two additional strategies. The first, Off (never), uses a fixed proposal model throughout training. The second, Off (always), updates the proposal model with the online model every T steps (i.e., set θ r ← θ q ) regardless of performance.</p><p>The full results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Four key takeaways are as follows: (1) For all combinations of (r y|x , q y|x ) initialization, off-policy (never) is the worst. This demonstrates the importance of updating the proposal model to generate better hypotheses during training. (2) Off-policy (always) consistently outperforms on-policy. We observe that training is significantly stabilized by reducing the proposal model update frequency from every step to every 1,000 steps. The effect is particularly prominent when initializing r y|x and q y|x from an earlier checkpoint (9.62% vs 20.02% on dev-clean, and 27.51% vs 45.62% on dev-other).</p><p>(3) Off-policy (better) achieves the best WER in all settings and outperforms off-policy (always) by a larger margin when initializing from an earlier checkpoint. (4) Unlike the other strategies, off-policy (better) demonstrates consistent improvement when using a less-trained initial online model. In the following experiments, we initialize models with θ r = θ A and θ q = θ C unless otherwise specified.  <ref type="table" target="#tab_9">Table 4</ref> shows the impact of length filtering with different proposal model update strategies, where both the proposal model and the online model are initialized with θ A . As discussed in Section 2.5, on-policy suffers more than off-policy without length filtering. If the proposal model is never updated, then length filtering does not affect the final WER. We hypothesize that length filtering keeps the proposal model stable during training. In addition, we also compare three reference lengths: the oracle length, the predicted length from ASR + LM decoding, and that from ASR-only decoding. We observe that the WER does not differ much when using different reference lengths for filtering. Detailed results are shown in the Supplementary Material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Length Filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Choice of Language Models</head><p>We study how the quality of the LM affects the results using the same ConvLM but trained for a varying number of steps.</p><p>We quantify LM quality with the token perplexity (PPL) on the development set. <ref type="table" target="#tab_9">Table 5</ref> shows a clear positive correlation between the LM quality and the final WER. This is expected given that the better LM results in a more accurate posterior approximation,p y|x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with Knowledge Distillation</head><p>We next study different weak KD strategies and compare them with LPM. When multiple hypotheses (k &gt; 1) are used for KD, the student model matches against a uniform target distribution as done in <ref type="bibr" target="#b22">Kahn et al. (2019a)</ref>. In addition, while KD typically considers a fixed teacher, we include another variant that adopts the same off-line teacher update strategy as LPM to disentangle the effect. Results are shown in <ref type="table" target="#tab_5">Table 6</ref> with three key takeaways. (1) Using an improving teacher leads to better performance.</p><p>(2) Distilling from a posterior that combines the ASR and LM models achieves better results; however, this hinders the use of an improving model as discussed in Section 3.</p><p>(3) Incorporating multiple hypotheses can slightly improve the performance even with a uniform target. Nonetheless, the gain is noticeably smaller than matching with our proposed local prior, which can be seen by comparing the "(ASR, Imp, 4)" rows, where the only difference is the target distribution to match. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Final Results and Comparison to Prior Work</head><p>The best performing model is trained for 3.2M steps, with a learning rate annealed by a factor of two every 1.28M steps when using 360 hours of unpaired speech, and every 0.64M steps when using 860 hours of unpaired speech. Reference lengths for filtering are obtained from ASR+LM beam search decoding. We compare LPM to fully supervised models and a number of semi-supervised ASR techniques in <ref type="table" target="#tab_9">Table 7</ref>. Among the listed studies, pseudo labeling (PL) is the most comparable alternative as we follow the same experimental setup, and, more importantly, it achieved the previous state-of-the-art results on LibriSpeech when using train-clean-100 as paired data and train-other-360 as unpaired speech. We give a more detailed table comparing to prior work in the Supplemental Material.</p><p>The upper half of <ref type="table" target="#tab_9">Table 7</ref> shows greedy decoding results without an LM. For the fully supervised model, when removing train-clean-360 the WER increases by 6.86% on test-clean and 13.36% on test-other. Using train-clean-360 speech without transcripts, LPM reduces the absolute WER by 5.64% and 12.21% on the two test sets, which recovers 82% and 91%, respectively, of the WER drop from removing the labels. Adding noisier train-other-500 to the unpaired set (total 860hr D s u ) further reduces the WER, and LPM achieves a better WER on the noisy sets (dev-other and test-other) compared to the supervised model trained on 460 hours of clean paired data. In addition, LPM outper-forms PL in all settings. This trend is consistent even when decoding with a strong ConvLM.</p><p>The last row in the upper and lower halves of <ref type="table" target="#tab_9">Table 7</ref> show the results of using LPM on the 60k hours of unlabelled speech from Libri-Light. We see that the WER improves by another 15% and 9% relative over using the 860hr dataset on the clean and other test sets respectively. When training on the 60k hours we use a batch size of 128, a beam size of k = 8and no learning rate decay. Reference lengths are from ASR-greedy decoding and we filter empty transcriptions, yielding 55.8k hours of training data. We also use a larger TDS model with the same architecture as <ref type="bibr" target="#b16">Hannun et al. (2019)</ref> (11 TDS blocks instead of 9). To disentangle the effect of the larger model from more unlabelled data with LPM, we also trained the larger model on the 860hr dataset. In this case, we did not see a gain in WER, suggesting that the improvement is due to LPM with more unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Hypothesis Quality of Unlabeled Training Speech</head><p>As discussed in Section 5.2, updating the proposal model is crucial to improve the hypotheses used during training. To quantify the improvement, <ref type="table" target="#tab_9">Table 8</ref> shows WERs on the unlabeled data of an LPM model at the beginning and at the end of training. Note that this is a proxy of quality for LPM, since multiple hypotheses are used when setting k ≥ 2. We compare this to the WER of pseudo-labels (PL) generated with an LM. Although generating hypotheses without an LM is initially worse, as training progresses, the proposal model of LPM produces better predictions on both trainclean-360 and train-other-500 than the fixed ones used in PL. Furthermore, the WER on train-other-500 is much higher for PL (21.51%) than for LPM at the end of training (13.00%), which explains why LPM achieves much better WER than PL when using the full 860hr of unpaired data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Linguistic Plausibility</head><p>We expect models trained with LPM to generate more semantically and grammatically correct text since the ASR model receives direct supervision from the LM. <ref type="table" target="#tab_1">Table 1</ref> shows the proposed hypotheses for two utterances using a supervised baseline model and a model trained with LPM. The baseline model proposes erroneous hypotheses which are easy to discard even without the audio. On the other hand, LPM generates hypotheses that are both grammatically and semantically plausible, with acceptable substitution errors in some cases (e.g., might/would).</p><p>We also notice in <ref type="table" target="#tab_1">Table 1</ref> that the LM probabilities correlate well with linguistic plausibility for texts of similar lengths. Motivated by this observation, we propose to quantify linguistic knowledge of an ASR model by measuring the LM <ref type="table" target="#tab_9">Table 7</ref>. Results of baselines and the proposed methods. Word error rate recovered (WERR) measures the improvment relative to the gap between supervised model trained on 100hr and 460hr of data, computed as WERR(x) = (WER 100hr − x)/(WER 100hr − WER 460hr ). "-" means the number is not available, or the supervised results are not provided and hence WERR cannot be computed.</p><formula xml:id="formula_5">D l Du LM dev WER (%) test WER (%) test WERR (%) clean</formula><p>other clean other BT <ref type="bibr" target="#b17">(Hayashi et al., 2018)</ref> 100hr 360hr (T) None 23.5 -23.6 -11.9 -Crit-LM <ref type="bibr" target="#b30">(Liu et al., 2019)</ref> 100hr 360hr (T) None 19.1 -19.2 ---Cycle-TTE  100hr 360hr (S) None 21.5 -21.5 -27.6 -Cycle-TTS <ref type="bibr" target="#b3">(Baskar et al., 2019)</ref> 100hr 360hr (S) None --17.9 ---Cycle-TTS <ref type="bibr" target="#b3">(Baskar et al., 2019)</ref> 100hr 360hr (S+T) None --17.5 ---PL (ASR) <ref type="bibr" target="#b22">(Kahn et al., 2019a)</ref> 100hr <ref type="formula">360hr</ref>  perplexity of the hypotheses on the development set obtained using ASR-only greedy decoding. Results are shown in <ref type="table" target="#tab_7">Table 9</ref>. The ground truth text has the lowest perplexity on both sets as expected. While all models are worse on dev-other than on dev-clean, LPM exhibits the smallest perplexity difference between the two sets, demonstrating that it successfully distills knowledge from the LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduce local prior matching, a semi-supervised learning objective for speech recognition, and demonstrate noteable reductions in WER with the addition of unpaired audio and text. We also perform an extensive empirical study to demonstrate the importance of various configurations of LPM. While LPM is motivated by how humans learn to recognize speech, the proposed method can be applied to other sequence transduction tasks including machine translation <ref type="bibr" target="#b44">(Sennrich et al., 2016)</ref> and text summarization <ref type="bibr" target="#b35">(Nallapati et al., 2016)</ref>, provided a good prior for the domain. As LPM distills knowledge from a prior, it will be most effective when p x|y is easy to model and p y is more complex and hence difficult to learn with a limited amount of data.</p><p>We consider two promising directions for future work with LPM. First, we hypothesize that LPM can further benefit by incorporating more context in the prior, including previous sentences and signal from other modalities when available. Second, endowing the model with ability to dynamically select which examples to use for semi-supervision may further improve the effectiveness of LPM.   <ref type="table" target="#tab_9">Table A</ref>.2 shows the LPM results when using different reference length estimates. As discussed in Section 5.3, the WER does not differ much when using different estimates, because we use the reference length to compute a range for filtering for each utterance.  <ref type="table" target="#tab_9">Table A</ref>.3 shows additional results of the comparison with knowledge distillation (KD) with varying initial teacher quality. In addition to the three key takeaways discussed in Section 5.5, the table here demonstrates that using an improving teacher can also reduce the sensitivity to its initial quality.  <ref type="table" target="#tab_9">Table A</ref>.4 presents the character error rates (CERs) of the supervised models and the proposed methods.  <ref type="table" target="#tab_9">Table A</ref>.5 shows the detailed results of semi-supervised ASR studies in the literature and the proposed methods. Word error rate recovered (WERR) for each baseline is computed using the supervised model WERs reported in its paper. <ref type="table" target="#tab_9">Table A</ref>.5. A more comprehensive comparison with semi-supervised ASR studies using LibriSpeech, including the performances of the baseline/topline supervised model used in each study, since they differ significantly across different papers. D l and Du denote the amount of paired and unpaired data used in each experiment, and (S)/(T)/(S+T) denote the type of the unpaired data, corresponding to speech/text/both, respectively. Experiments with the asterisk sign ( * ) contain results that are not reported in the original paper, but are obtained from the authors of the paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2002.10336v1 [cs.CL] 24 Feb 2020</figDesc><table><row><cell>2. Method</cell></row><row><cell>2.1. Preliminaries</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Beam search hypotheses from the supervised model (Sup.) trained on 100hr D l and the LPM model (LPM) trained on 100hr D l and 360hr D s u . Hypotheses are ranked by the ASR score q y|x . The reference transcription (Ref.) is also shown for each example.</figDesc><table><row><cell cols="3">Model Rank log p y</cell><cell>Beam Search Hypotheses (k = 4)</cell></row><row><cell>Ref.</cell><cell>-</cell><cell>-34.14</cell><cell>she walk ed very fast after she left the house $</cell></row><row><cell></cell><cell>1</cell><cell>-47.58</cell><cell>she looked very thought after she left the house $</cell></row><row><cell>Sup.</cell><cell>2 3</cell><cell>-38.54 -53.47</cell><cell>she looked very fat after she left the house $ she what very thought after she left the house $</cell></row><row><cell></cell><cell>4</cell><cell>-83.85</cell><cell>she w o u l t very thought after she left the house $</cell></row><row><cell></cell><cell>1</cell><cell>-32.24</cell><cell>she walk ed very fast as she left the house $</cell></row><row><cell>LPM</cell><cell>2 3</cell><cell>-34.14 -33.10</cell><cell>she walk ed very fast after she left the house $ she looked very fast as she left the house $</cell></row><row><cell></cell><cell>4</cell><cell>-36.59</cell><cell>she looked very fast after she left the house $</cell></row><row><cell>Ref.</cell><cell>-</cell><cell>-68.01</cell><cell>oh if i had imagined him still in such distress sure ly i might have done something to help him $</cell></row><row><cell></cell><cell>1</cell><cell>-110.11</cell><cell>i before i had imagined him steal ing such distress sure ly i why have done something to help you $</cell></row><row><cell>Sup.</cell><cell>2 3</cell><cell>-107.55 -107.81</cell><cell>i before i had imagined him steal ing such distress sure ly i want have done something to help you $ i before i had imagined him still ing such distress sure ly i want have done something to help you $</cell></row><row><cell></cell><cell>4</cell><cell>-107.10</cell><cell>i before i had imagined him steal ing such distress sure ly i want of done something to help you $</cell></row><row><cell></cell><cell>1</cell><cell>-72.55</cell><cell>oh if i had imagined him still in such distress sure ly i would have done something to help you $</cell></row><row><cell>LPM</cell><cell>2 3</cell><cell>-71.35 -69.29</cell><cell>oh if i had imagined him still in such distress sure ly i might have done something to help you $ oh if i had imagined him still in such distress sure ly i would have done something to help him $</cell></row><row><cell></cell><cell>4</cell><cell>-85.62</cell><cell>oh if i had imagined him still in such distress sure ly i won't have done something to help you $</cell></row><row><cell cols="4">since the model used to generate hypotheses is also the</cell></row><row><cell cols="3">model we update.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>We vary the mixing ratio m l : mu and the beam size k. An LPM weight α = 0.2 is used.</figDesc><table><row><cell>D s u</cell><cell>m l : mu</cell><cell>k = 1</cell><cell cols="3">dev-clean / dev-other WER (%) k = 2 k = 4 k = 8</cell><cell>k = 16</cell></row><row><cell></cell><cell>4 : 1</cell><cell cols="5">10.75 / 31.62 10.60 / 30.96 10.25 / 30.67 10.14 / 30.17 10.09 / 29.99</cell></row><row><cell>360hr</cell><cell>1 : 1 1 : 4</cell><cell>10.43 / 29.76 11.09 / 29.89</cell><cell>9.56 / 28.83 9.34 / 27.45</cell><cell>9.37 / 28.10 9.00 / 26.47</cell><cell>9.06 / 27.35 9.15 / 26.52</cell><cell>8.88 / 27.25 9.36 / 27.00</cell></row><row><cell></cell><cell>1 : 9</cell><cell cols="2">12.11 / 30.89 10.11 / 27.71</cell><cell>9.76 / 27.08</cell><cell cols="2">10.17 / 27.41 10.30 / 27.62</cell></row><row><cell cols="2">860hr 1 : 4</cell><cell>10.59 / 26.05</cell><cell>9.37 / 23.85</cell><cell>8.68 / 22.53</cell><cell>8.37 / 21.56</cell><cell>8.37 / 21.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>We vary the proposal update strategies and the initial model weights for both r y|x and q y|x .</figDesc><table><row><cell cols="2">Init r y|x r y|x update</cell><cell cols="3">dev-clean / dev-other WERs Init θq = θA Init θq = θB Init θq = θC</cell></row><row><cell></cell><cell>On</cell><cell>9.50 / 28.29</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>θr = θA</cell><cell>Off (never) Off (always)</cell><cell cols="3">11.19 / 31.74 11.14 / 31.69 11.24 / 31.53 9.40 / 27.79 9.27 / 27.33 9.52 / 27.34</cell></row><row><cell></cell><cell>Off (better)</cell><cell>9.20 / 27.42</cell><cell>9.14 / 26.80</cell><cell>9.00 / 26.47</cell></row><row><cell></cell><cell>On</cell><cell>N/A</cell><cell>10.17 / 28.35</cell><cell>N/A</cell></row><row><cell>θr = θB</cell><cell>Off (never) Off (always)</cell><cell cols="3">13.61 / 35.39 13.95 / 35.43 13.56 / 35.62 9.50 / 27.81 9.56 / 27.58 9.79 / 27.44</cell></row><row><cell></cell><cell>Off (better)</cell><cell>9.30 / 27.34</cell><cell>9.26 / 27.01</cell><cell>9.15 / 26.63</cell></row><row><cell></cell><cell>On</cell><cell>N/A</cell><cell>N/A</cell><cell>20.20 / 45.62</cell></row><row><cell>θr = θC</cell><cell>Off (never) Off (always)</cell><cell cols="3">20.59 / 44.09 22.95 / 46.43 23.42 / 46.89 9.52 / 27.89 9.46 / 27.35 9.62 / 27.51</cell></row><row><cell></cell><cell>Off (better)</cell><cell>9.44 / 27.34</cell><cell>9.31 / 27.26</cell><cell>9.43 / 27.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>A comparison of WER with and without length filtering. Comparing py with different token perplexity.</figDesc><table><row><cell>r y|x update</cell><cell cols="2">dev-clean / dev-other WER No filtering With filtering</cell></row><row><cell>On-Policy</cell><cell>26.65 / 59.07</cell><cell>9.50 / 28.29</cell></row><row><cell>Off-Policy (never)</cell><cell cols="2">11.18 / 31.83 11.19 / 31.74</cell></row><row><cell cols="2">Off-Policy (always) 13.99 / 35.52</cell><cell>9.40 / 27.79</cell></row><row><cell>Off-Policy (better)</cell><cell>11.42 / 31.56</cell><cell>9.20 / 27.42</cell></row><row><cell cols="3">py PPL dev-clean / dev-other WER</cell></row><row><cell>34.24</cell><cell>9.00 / 26.47</cell><cell></cell></row><row><cell>64.22</cell><cell>10.08 / 26.92</cell><cell></cell></row><row><cell>97.87</cell><cell>10.90 / 27.97</cell><cell></cell></row><row><cell>142.12</cell><cell>11.53 / 28.74</cell><cell></cell></row><row><cell>180.71</cell><cell>13.18 / 30.74</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison with various knowledge distillation configurations. Each configuration is parameterized by (ASR/ASR+LM, Fix/Imp, k), where ASR/ASR+LM indicates whether hypotheses are generated from ASR-only decoding or ASR+LM shallow fusion decoding, Fix/Imp indicates whether the teacher model is fixed or improving (using our proposed off-policy update strategy), and k indicates the number of hypotheses used for each utterance.</figDesc><table><row><cell cols="2">Method Param</cell><cell>dev-clean / dev-other WER</cell></row><row><cell></cell><cell>(ASR+LM,Fix,1)</cell><cell>9.60 / 29.00</cell></row><row><cell></cell><cell>(ASR,Fix,1)</cell><cell>12.24 / 33.25</cell></row><row><cell></cell><cell>(ASR,Fix,2)</cell><cell>11.94 / 32.19</cell></row><row><cell>KD</cell><cell>(ASR,Fix,4) (ASR,Fix,8)</cell><cell>11.60 / 32.26 11.77 / 32.07</cell></row><row><cell></cell><cell>(ASR,Imp,1)</cell><cell>11.79 / 30.09</cell></row><row><cell></cell><cell>(ASR,Imp,2)</cell><cell>11.79 / 29.89</cell></row><row><cell></cell><cell>(ASR,Imp,4)</cell><cell>12.09 / 30.21</cell></row><row><cell></cell><cell>(ASR,Imp,8)</cell><cell>12.19 / 29.88</cell></row><row><cell>LPM</cell><cell>(ASR,Imp,4)</cell><cell>9.00 / 26.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>LM token perplexity of ground truth texts and hypotheses obtained with greedy decoding.</figDesc><table><row><cell></cell><cell>D l</cell><cell>D s u</cell><cell cols="2">LM perplexity dev-clean dev-other</cell></row><row><cell>Ground Truth</cell><cell>N/A</cell><cell>N/A</cell><cell>39.94</cell><cell>43.26</cell></row><row><cell>Supervised</cell><cell cols="2">100hr N/A</cell><cell>96.13</cell><cell>313.38</cell></row><row><cell>Supervised</cell><cell cols="2">460hr N/A</cell><cell>58.76</cell><cell>164.77</cell></row><row><cell>PL (ASR)</cell><cell cols="2">100hr 360hr</cell><cell>87.36</cell><cell>273.14</cell></row><row><cell cols="3">PL (ASR+LM) 100hr 360hr</cell><cell>64.07</cell><cell>170.72</cell></row><row><cell>LPM</cell><cell cols="2">100hr 360hr</cell><cell>61.73</cell><cell>159.72</cell></row><row><cell>LPM</cell><cell cols="2">100hr 860hr</cell><cell>59.84</cell><cell>125.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Srivastava, N., Hinton, G.,Krizhevsky, A., Sutskever, I.,   and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1): 2014.Sun, S., Zhang, B., Xie, L., and Zhang, Y. An unsupervised deep domain adaptation approach for robust speech recognition. Neurocomputing, 257:79-87, 2017.Table A.1 shows the results of varying the LPM weight α, as mentioned in Section 5.1. For this set of experiments, a mixing ratio m l : m u = 1 : 4 and a beam size k = 4 is used.</figDesc><table><row><cell>Semi-Supervised Speech Recognition via Local Prior Matching -</cell></row><row><cell>Supplementary Materials</cell></row><row><cell>Tjandra, A., Sakti, S., and Nakamura, S. Listening while speaking: Speech chain by deep learning. In Proc. IEEE A. Additional Results</cell></row><row><cell>Workshop on Automfatic Speech Recognition and Under-</cell></row><row><cell>standing (ASRU), 2017.</cell></row><row><cell>Tjandra, A., Sakti, S., and Nakamura, S. End-to-end feed-</cell></row><row><cell>back loss in speech chain framework via straight-through</cell></row><row><cell>estimator. In Proc. International Conference on Acoustics,</cell></row><row><cell>Speech and Signal Processing (ICASSP), 2019.</cell></row><row><cell>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,</cell></row><row><cell>L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention</cell></row><row><cell>is all you need. In Proc. Neural Information Processing</cell></row><row><cell>Systems (NeurIPS), 2017.</cell></row><row><cell>Veselỳ, K., Ghoshal, A., Burget, L., and Povey, D. Sequence-</cell></row><row><cell>discriminative training of deep neural networks. In Inter-</cell></row><row><cell>speech, 2013.</cell></row><row><cell>Veselỳ, K., Burget, L., and Cernockỳ, J. Semi-supervised</cell></row><row><cell>DNN training with word selection for ASR. In Proc. An-</cell></row><row><cell>nual Conference of International Speech Communication</cell></row><row><cell>Association (INTERSPEECH), 2017.</cell></row><row><cell>Wang, Y.-H., Chung, C.-T., and Lee, H.-y. Gate activation</cell></row><row><cell>signal analysis for gated recurrent neural networks and</cell></row><row><cell>its correlation with phoneme boundaries. In Proc. An-</cell></row><row><cell>nual Conference of International Speech Communication</cell></row><row><cell>Association (INTERSPEECH), 2017.</cell></row><row><cell>Zeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Syn-</cell></row><row><cell>naeve, G., and Collobert, R. Fully convolutional speech</cell></row><row><cell>recognition. arXiv preprint arXiv:1812.06864, 2018.</cell></row><row><cell>Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired</cell></row><row><cell>image-to-image translation using cycle-consistent adver-</cell></row><row><cell>sarial networks. In Proc. IEEE International Conference</cell></row><row><cell>on Computer Vision (ICCV), 2017.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="2">.1. Results of varying LPM weight α.</cell></row><row><cell>α</cell><cell>dev WER (%) clean other</cell></row><row><cell cols="2">2e-2 10.86 31.59</cell></row><row><cell cols="2">5e-2 10.08 28.92</cell></row><row><cell>1e-1</cell><cell>9.24 27.62</cell></row><row><cell>2e-1</cell><cell>9.00 26.47</cell></row><row><cell>5e-1</cell><cell>9.41 26.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table A.2. Length filtering using different reference lengths.</figDesc><table><row><cell cols="2">Reference Length L dev-{clean / other}</cell></row><row><cell>Oracle</cell><cell>8.85 / 26.39</cell></row><row><cell>ASR + LM Dec</cell><cell>8.99 / 26.36</cell></row><row><cell>ASR-only Dec</cell><cell>9.00 / 26.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table A.3. Comparison with various knowledge distillation configurations. Each configuration is parameterized by (ASR/ASR+LM, Fix/Imp, k), where ASR/ASR+LM indicates whether hypotheses are generated from ASR-only decoding or ASR+LM shallow fusion decoding, Fix/Imp indicates whether the teacher model is fixed or improving (using our proposed off-policy update strategy), and k indicates the number of hypotheses used for each utterance. Init r y|x = A Init r y|x = B Init r y|x = C</figDesc><table><row><cell cols="2">Method Param</cell><cell cols="3">dev-clean / dev-other WER (%)</cell></row><row><cell></cell><cell>(ASR+LM,Fix,1)</cell><cell>9.60 / 29.00</cell><cell cols="2">10.98 / 32.09 20.43 / 44.37</cell></row><row><cell></cell><cell>(ASR,Fix,1)</cell><cell cols="3">12.24 / 33.25 15.30 / 38.25 28.92 / 53.07</cell></row><row><cell></cell><cell>(ASR,Fix,2)</cell><cell cols="3">11.94 / 32.19 14.63 / 36.85 26.81 / 49.32</cell></row><row><cell>KD</cell><cell>(ASR,Fix,4) (ASR,Fix,8)</cell><cell cols="3">11.60 / 32.26 14.79 / 36.35 26.44 / 49.23 11.77 / 32.07 14.48 / 36.50 26.81 / 49.59</cell></row><row><cell></cell><cell>(ASR,Imp,1)</cell><cell cols="3">11.79 / 30.09 14.29 / 32.10 18.81 / 35.84</cell></row><row><cell></cell><cell>(ASR,Imp,2)</cell><cell cols="3">11.79 / 29.89 13.15 / 30.97 17.94 / 34.91</cell></row><row><cell></cell><cell>(ASR,Imp,4)</cell><cell cols="3">12.09 / 30.21 13.46 / 31.22 16.64 / 34.56</cell></row><row><cell></cell><cell>(ASR,Imp,8)</cell><cell cols="3">12.19 / 29.88 13.54 / 31.44 15.51 / 33.04</cell></row><row><cell>LPM</cell><cell>(ASR,Imp,4)</cell><cell>9.00 / 26.47</cell><cell>9.15 / 26.63</cell><cell>9.43 / 27.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table A.4. Character error rate (CER) results of the supervised models and the proposed methods.</figDesc><table><row><cell></cell><cell>D l</cell><cell>D s u</cell><cell>LM</cell><cell cols="4">dev CER (%) clean other clean other test CER (%)</cell></row><row><cell>Supervised</cell><cell cols="2">100hr N/A</cell><cell>None</cell><cell>6.20</cell><cell>20.27</cell><cell>6.80</cell><cell>22.14</cell></row><row><cell>Supervised</cell><cell cols="2">460hr N/A</cell><cell>None</cell><cell>2.86</cell><cell>13.06</cell><cell>3.37</cell><cell>13.73</cell></row><row><cell>Local Prior Matching</cell><cell cols="2">100hr 360hr</cell><cell>None</cell><cell>3.79</cell><cell>14.00</cell><cell>3.87</cell><cell>14.81</cell></row><row><cell>Local Prior Matching</cell><cell cols="2">100hr 860hr</cell><cell>None</cell><cell>3.52</cell><cell>11.14</cell><cell>3.60</cell><cell>12.08</cell></row><row><cell cols="4">Local Prior Matching (large model) 100hr 60,000hr None</cell><cell>2.88</cell><cell>10.01</cell><cell>3.01</cell><cell>10.45</cell></row><row><cell>Supervised</cell><cell cols="2">100hr N/A</cell><cell>ConvLM</cell><cell>3.83</cell><cell>17.03</cell><cell>3.86</cell><cell>18.52</cell></row><row><cell>Supervised</cell><cell cols="2">460hr N/A</cell><cell>ConvLM</cell><cell>1.65</cell><cell>9.51</cell><cell>1.79</cell><cell>9.47</cell></row><row><cell>Local Prior Matching</cell><cell cols="2">100hr 360hr</cell><cell>ConvLM</cell><cell>2.65</cell><cell>11.56</cell><cell>2.81</cell><cell>11.96</cell></row><row><cell>Local Prior Matching</cell><cell cols="2">100hr 860hr</cell><cell>ConvLM</cell><cell>2.51</cell><cell>8.70</cell><cell>2.70</cell><cell>9.70</cell></row><row><cell cols="4">Local Prior Matching (large model) 100hr 60,000hr ConvLM</cell><cell>2.32</cell><cell>7.61</cell><cell>2.19</cell><cell>8.53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Massachusetts Institute of Technology 2 Facebook AI Research. Correspondence to: Wei-Ning Hsu &lt;wnhsu@mit.edu&gt;, Awni Hannun &lt;awni@fb.com&gt;. 1 Code and models are available at https://github.com/ facebookresearch/wav2letter/tree/master/ recipes/models/local_prior_match</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Jacob Kahn, Qiantong Xu, Tatiana Likhomanenko, Anuroop Sriram, Vineel Pratap, Vitaliy Liptchinsky, Ronan Collobert for their help and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b3">(Baskar et al., 2019)</ref> </div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards phone segmentation for concatenative speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth ISCA Workshop on Speech Synthesis</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence-tosequence ASR using unpaired speech and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andčernockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Knowledge of language: Its nature, origin, and use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Greenwood Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fully differentiable beam search decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge distillation across ensembles of multilingual models for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nussbaum-Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining end-to-end and adversarial training for low-resource speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Drexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Spoken Language Technology Workshop (SLT)</title>
		<meeting>IEEE Spoken Language Technology Workshop (SLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="43" to="59" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards unsupervised speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Science, Signal Processing and their Applications (ISSPA)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequenceto-sequence speech recognition with time-depth separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTER-SPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTER-SPEECH)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Back-translation-style data augmentation for end-to-end ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Spoken Language Technology Workshop (SLT)</title>
		<meeting>IEEE Spoken Language Technology Workshop (SLT)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cycle-consistency training for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extracting domain invariant features by unsupervised learning for robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding</title>
		<meeting>IEEE Workshop on Automfatic Speech Recognition and Understanding</meeting>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Self-training for end-toend speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07875</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Early language acquisition: cracking the speech code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn1533</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="831" to="843" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Acoustic modeling for Google home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caroselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Chin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised training for end-to-end models via weak distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial training of end-to-end speech recognition using a criticizing language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised training of acoustic models using latticefree MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation with domain separation networks for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mazalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Automfatic Speech Recognition and Understanding</title>
		<meeting>IEEE Workshop on Automfatic Speech Recognition and Understanding</meeting>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adversarial teacher-student learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attentive adversarial learning for domain-invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Conference on Natural Language Learning (CoNLL)</title>
		<meeting>ACL Conference on Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H K</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>International Conference on Acoustics, Speech and Signal essing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08459</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Parallel neural text-to-speech. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Discriminative training for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07625</idno>
		<title level="m">The fastest open-source speech recognition system</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-mainstream languages and speech recognition: Some challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Precoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CALICO journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="243" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Project</forename><surname>Gutenberg</surname></persName>
		</author>
		<ptr target="https://www.gutenberg.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised speech segmentation: An analysis of the hypothesized phone boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Scharenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernestus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1084" to="1095" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
