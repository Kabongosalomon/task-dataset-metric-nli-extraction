<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Keyword Spotting using Neural Architecture Search and Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Peter</surname></persName>
							<email>david.peter@student.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Processing and Speech Communication Laboratory</orgName>
								<orgName type="institution">Graz University of Technology Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Roth</surname></persName>
							<email>roth@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Processing and Speech Communication Laboratory</orgName>
								<orgName type="institution">Graz University of Technology Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
							<email>pernkopf@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Processing and Speech Communication Laboratory</orgName>
								<orgName type="institution">Graz University of Technology Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Keyword Spotting using Neural Architecture Search and Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: keyword spotting</term>
					<term>neural architecture search</term>
					<term>quantization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces neural architecture search (NAS) for the automatic discovery of end-to-end keyword spotting (KWS) models in limited resource environments. We employ a differentiable NAS approach to optimize the structure of convolutional neural networks (CNNs) operating on raw audio waveforms. After a suitable KWS model is found with NAS, we conduct quantization of weights and activations to reduce the memory footprint. We conduct extensive experiments on the Google speech commands dataset. In particular, we compare our endto-end approach to mel-frequency cepstral coefficient (MFCC) based systems. For quantization, we compare fixed bit-width quantization and trained bit-width quantization. Using NAS only, we were able to obtain a highly efficient model with an accuracy of 95.55% using 75.7k parameters and 13.6M operations. Using trained bit-width quantization, the same model achieves a test accuracy of 93.76% while using on average only 2.91 bits per activation and 2.51 bits per weight.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic speech recognition (ASR) is becoming increasingly important for user interaction with everyday consumer devices. ASR systems are typically complex and computation-intensive, i.e. running ASR in always-on mode results in a steady high energy consumption. This is especially problematic for mobile devices whose batteries are drained quickly when running ASR permanently.</p><p>A common solution is to run a low-cost keyword spotting (KWS) system that is listening permanently only for a limited set of prespecified keywords. Upon detection of such a keyword, a full ASR system is triggered which then listens for a rich set of user commands. The requirements of a KWS system are: (i) The system should be resource-efficient to mitigate the aforementioned energy problem, (ii) it should run in realtime and, (iii) it should be accurate to maintain a high userexperience.</p><p>In KWS, deep neural networks (DNNs) have become the state-of-the-art. In <ref type="bibr" target="#b0">[1]</ref> several models from the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> are evaluated on the Google speech commands dataset <ref type="bibr" target="#b5">[6]</ref>. They compare their models in terms of accuracy, memory requirements and number of operations (i.e. the number of multiplications and additions) per forward pass. To allow for easy deployment on microcontrollers they train their models using 32 bit float numbers and quantize the weights after training to 8 bit fixed-point numbers. They argue that fixed point numbers have been shown to suffice to run DNNs with minimal loss in accuracy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>There are many aspects to consider when designing resource-efficient DNNs for KWS. In this paper, we will focus specifically on the following important aspects: The DNN architecture, comparison between spectral MFCC feature and raw audio processing as well as quantization of weights and activations.</p><p>Recently, neural architecture search (NAS) became a popular technique to automate the design of DNN architectures. A NAS algorithm searches for the best performing DNN architecture within a given search space using appropriate search heuristics. Popular NAS approaches use concepts such as reinforcement learning <ref type="bibr" target="#b9">[10]</ref>, gradient based methods <ref type="bibr" target="#b10">[11]</ref> or evolutionary methods <ref type="bibr" target="#b11">[12]</ref> for exploring the search space. In the context of resource-efficient DNNs, NAS techniques have also been used to find DNN architectures that are specifically tailored to the underlying hardware <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> by, for instance, additionally minimizing memory requirements, number of operations, or latency of the resulting model. Therefore, NAS techniques are well-suited for finding DNNs that run on mobile phones or embedded devices. In the context of KWS, NAS has been used successfully by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> to find efficient and small DNN architectures.</p><p>KWS with DNNs is typically performed on hand-crafted speech features such as MFCCs that are extracted from raw audio waveforms. Extracting MFCCs involves computing the Fourier transform. However, performing the Fourier transform is computationally expensive and might exceed the capabilities of resource-constrained devices. Therefore, Ibrahim et al. <ref type="bibr" target="#b16">[17]</ref> proposed to use simpler speech features derived in the timedomain. The features, referred to as Multi-Frame Shifted Time Similarity (MFSTS), are obtained by computing constrained lag autocorrelations on overlapping speech frames to form a 2D map. A temporal convolutional neural network (TCN) <ref type="bibr" target="#b17">[18]</ref> is then used to classify keywords on MFSTSs.</p><p>However, hand-crafted features such as MFSTSs and MFCCs may not be optimal for KWS. Therefore recent works have proposed to directly feed the DNNs with raw audio waveforms. In <ref type="bibr" target="#b18">[19]</ref>, a CNN for speaker recognition is proposed that encourages to learn parametrized sinc functions as kernels in the first layer. This layer is referred to as SincConv layer. During training, a low and high cutoff frequency per kernel is determined. Therefore, a custom filter bank is derived by training the SincConv layer that is specifically tailored to the desired application. SincConvs have also been recently applied to KWS tasks <ref type="bibr" target="#b19">[20]</ref>.</p><p>Quantization-aware training uses the straight-through estimator (STE) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> to approximate the gradient of piecewise constant quantizers by the non-zero gradient of some other function. The STE has been used for example in the training of binarized neural networks (BNNs) where the resolution of both weights and activations is reduced to binary values {âˆ’1, 1}. Another method for quantizing DNNs involves a Bayesian approach to learn weight distributions over discrete weights <ref type="bibr" target="#b22">[23]</ref>. Recently, the STE has been used to also learn the weight and activation bit-widths (i.e. number of bits) during training <ref type="bibr" target="#b23">[24]</ref>. We will refer to this type of training as trained bit-width quantization.</p><p>In this paper, we propose NAS for the automatic discovery of small and resource-efficient end-to-end models for KWS. We utilize the techniques from ProxylessNAS <ref type="bibr" target="#b12">[13]</ref> to find suitable end-to-end KWS models. During NAS, we establish a tradeoff between the model accuracy and the number of operations. We compare our end-to-end KWS models obtained by NAS to MFCC based systems. Once the efficient full-precision endto-end KWS model has been found, we compare two weight and activation quantization methods. Both methods perform quantization-aware training from scratch on the full-precision end-to-end KWS model. In the first method, the weight and activation bit-widths are fixed during training. In the second method, trained bit-width quantization is performed. Our contributions are the following:</p><p>â€¢ We apply NAS to obtain efficient end-to-end KWS models operating on raw audio waveforms instead of hand crafted features. Recent works such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b14">15</ref>] rely on hand crafted features such as MFCCs when performing NAS for KWS.</p><p>â€¢ We perform a thorough comparison between KWS on raw audio waveforms and KWS on MFCCs in terms of accuracy, number of operations and number of model parameters.</p><p>â€¢ We compare two quantization methods for weight and activation quantization. In particular, we perform fixed bit-width and trained bit-width quantization on end-toend KWS models to further reduce the memory footprint.</p><p>The outline of the paper is as follows: In Section 2 we present our NAS configuration, the feature extraction using SincConvs and the weight quantization methods utilized in this paper. The experimental setup is shown in Section 3. In Section 4, we discuss the results of our experiments. Finally, Section 5 provides the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Neural architecture search</head><p>Our goal is to find well performing architectures for different computing regimes. To achieve this, we use multi-objective ProxylessNAS <ref type="bibr" target="#b12">[13]</ref> to discover DNNs optimized for accuracy and number of operations. Note that this implicitly optimizes for the model size as well.</p><p>ProxylessNAS constructs an overparameterized model with multiple parallel candidate operations per layer as the base model. The overparameterized model is trained together with a set of architecture parameters specifying probabilities over the candidate operations. Once training has finished, for each layer the most probable candidate operation is selected. <ref type="table">Table 1</ref> shows the overparameterized model used in this paper. It consists of five stages with two input stages (i) (ii), two intermediate stages (iii) (iv) and one output stage (v). Stages <ref type="table">Table 1</ref>: NAS model used for KWS. K denotes the kernel size, S the stride, C the number of channels and L the number of layers per stage. Stages (i) and (ii) and (v) are fixed. For stage (iii) and (iv), the parameters e (expansion rate), k (kernel size) and whether an identity layer is selected or not is optimized using NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head><p>Type</p><formula xml:id="formula_0">K S C L (i) SincConv 400 160 1 1 (ii) Conv 3x3 2, 2 10 1 (iii) MBC[e] / Identity [k]Ã—[k] 2, 2 20 3 (iv) MBC[e] / Identity [k]Ã—[k] 2, 2 40 3 (v) Conv 1Ã—1 1, 1 80 1 Global Avg. Pooling - - - 1 Fully connected - - - 1</formula><p>(i), (ii) and (v) are fixed whereas stages (iii) an (iv) are optimized using NAS. We use mobile inverted bottleneck convolutions (MBCs) <ref type="bibr" target="#b25">[26]</ref> as our main building blocks in stages (iii) and (iv). MBCs have two learnable parameters, the expansion rate e and the size k of the quadratic kÃ—k convolution kernel. MBCs consist of three separate convolutions, one 1Ã—1 convolution followed by a depthwise-separable 3Ã—3 convolution followed again by a 1Ã—1 convolution. The first two convolutions apply batch normalization and ReLU activation functions. The third convolution only applies batch normalization. The first and third 1Ã—1 convolution change the number of feature maps by the expansion rate factor of e and 1/e respectively. Stride (as stated in <ref type="table">Table 1</ref>) is only applied to the first convolution of each stage.</p><p>During NAS, we allow MBCs with expansion rates e âˆˆ {1, 2, 3, 4, 5, 6} and kernel sizes k âˆˆ {3, 5, 7} for selection. We also include the zero operation which effectively results in an identity layer <ref type="bibr" target="#b12">[13]</ref>. For blocks where the input feature map size is equal to the output feature map size we include skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature extraction using SincConvs</head><p>SincNet <ref type="bibr" target="#b18">[19]</ref> uses parametrized sinc functions as filters in the first layer. This layer performs a convolution of the raw audio input x with an arbitrary number of parametrized sinc functions called SincNet filters. For a single SincNet filter, the output y given the filter g[n, Î¸] parametrized by Î¸ is simply</p><formula xml:id="formula_1">y[n] = x[n] * g[n, Î¸].<label>(1)</label></formula><p>In SincNet, the choice for the filter function g is</p><formula xml:id="formula_2">g[n, f1, f2] = 2f2 sinc(2Ï€f2n) âˆ’ 2f1 sinc(2Ï€f1n),<label>(2)</label></formula><p>where the sinc function is defined as sinc(x) = sin(x)/x and Î¸ = (f1, f2). This choice of g can be seen as a bandpass filter in the frequency domain with f1 and f2 being the low and high cut-off frequencies of the bandpass filter. The magnitude of the bandpass filter in the frequency domain is therefore</p><formula xml:id="formula_3">G[f, f1, f2] = rect f 2f2 âˆ’ rect f 2f1<label>(3)</label></formula><p>where rect(f ) is the rectangular function defined as</p><formula xml:id="formula_4">rect(f ) = 1 if |f | â‰¤ 1 2 0 if |f | &gt; 1 2 .<label>(4)</label></formula><p>SincConv filters have a much smaller memory footprint than 1D-Convs where the filter kernel is fully learnable. To derive a single filter g[n, f1, f2], only two parameters, the lower cutoff frequency f1 and the upper cutoff frequency f2 are needed, whereas for convolutions with arbitrary filters, the number of parameters to store is equal to the length of the filter. During runtime however, SincConvs and 1D-Convs of similar length need the same amount of memory to store the filter kernels. SincConv filter kernels are precomputed once before runtime. However, the computational cost of computing the SincConv filter kernels can typically be neglected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Weight and activation quantization</head><p>For the architecture discovered by NAS, we compare two quantization methods for quantization of weights and activations, namely fixed bit-width quantization and trained bit-width quantization.</p><p>We perform quantization-aware training using the quantization framework brevitas <ref type="bibr" target="#b26">[27]</ref>. We quantize weights and activations of all layers, except for the input layer.</p><p>In quantization-aware training, quantized tensors are obtained from real-valued auxiliary tensors by applying a quantization function Q. During backpropagation, the gradients of the auxiliary tensors are obtained using the STE to estimate the gradient of Q. The quantized tensors are typically integer numbers encoded to k bits. A factor Î±, called the dynamic range <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, is used to map the integer numbers of the quantized tensor to real-valued numbers. The scaling factor typically increases the performance quite substantially.</p><p>For quantized weights, we select Î± to be the maximal absolute value of the auxiliary weight tensor. For quantized activations, we select Î± differently depending on the bit width. Binary (i.e. 1 bit) quantization of activations is performed using a constant scaling factor of Î± = 1. When the activation bit-width is larger or equal to 2 bits, the scaling factor Î± is declared as a trainable parameter that is optimized using a gradient based approach.</p><p>For trained bit-width quantization we optimized the following loss function</p><formula xml:id="formula_5">L = LCE + Î»w Â· Bw + Î»a Â· Ba (5)</formula><p>where LCE is the cross-entropy loss, Î»w, Î»a are hyperparameters and Bw, Ba are the average weight and activation bit-width of the model respectively. For our experiments, the hyperparameters were selected as Î»w = 0.04 and Î»a = 0.04. Note that in brevitas, trained bit-width quantization is limited to bitwidths larger or equal to 2. However, brevitas is currently under active development and future versions may allow 1 bit activations and weights for trained bit-width quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We use the first version of the Google speech commands dataset <ref type="bibr" target="#b5">[6]</ref>. It consists of 65.000 1-second long audio files sampled with 16 bit at 16 kHz sampling frequency. Every audio file contains one utterance of an English word spoken by one person. The words are grouped into 30 different classes. We follow the procedure of <ref type="bibr" target="#b30">[31]</ref> and use the following 10 classes "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop" and "Go" from the dataset. Likewise, we also include an "unknown" class which is a blend of randomly selected samples from the remaining 20 classes. Furthermore, a "silence" class is added. The "silence" class is artificially generated and consists of 1-second audio files containing a random slice of audio from a randomly selected noise sample provided by the Google Speech commands dataset. We follow the procedure of <ref type="bibr" target="#b30">[31]</ref> and perform data augmentation by applying a random time shift and adding background noise to the raw audio waveforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction</head><p>End-to-end KWS models presented in this paper do not need any hand-crafted feature extraction since they directly classify keywords from the raw audio waveforms. However, we will later compare end-to-end KWS models with models that use MFCCs as input. For models using MFCCs as input, the raw audio waveforms are first filtered with a low pass filter f low = 40Hz and a highpass filter f high = 4kHz. We then extract between 10 and 30 MFCCs per 40ms frame with a stride length of 20ms. The number of MFCCs is varied in the experiments. Before performing classification on raw audio waveforms, we select a window length of 25ms and a hop length of 10ms to split up the raw audio waveform into frames. Therefore, at a sampling frequency fs = 16kHz, the SincConv filter length is 400 and the hop length is 160. Before filtering with the Sinc-Conv, a Hamming window is applied to the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KWS from raw audio waveforms using NAS</head><p>We compare our end-to-end KWS models to models using MFCCs <ref type="bibr" target="#b14">[15]</ref>. We introduce a width multiplier m that scales the number of channels of the model by a factor of m = 1, m = 0.5 or m = 2. The width multiplier does not effect the SincConv layer at the input. For the SincConv layer, the number of filters was set to 40, 60, and 80. After a width-multiplier and the number of SincConv filters is selected, NAS is performed to select the layers in stage (iii) and (iv) of our NAS model (c.f. <ref type="table">Table 1)</ref>. A tradeoff between the model size and accuracy is established by varying the regularization parameter Î² âˆˆ {0, 1, 2, 4, 8, 16} to obtain end-to-end KWS models of different sizes (for details on Î² see <ref type="bibr" target="#b14">[15]</ref>). To assess the difference between SincConvs and 1D-Convs, we also performed NAS using models with 1D-Conv instead of SincConv. However, here we only select a widthmultiplier of m = 1. <ref type="figure" target="#fig_1">Figure 1</ref> shows the performance of SincConv and MFCC models. For better visibility, we only include models on the Pareto frontier. We also include 1D-Conv models although none of the models contributes to the Pareto frontier. The number of operations of a model corresponds to the circle area. We can observe that 1D-Conv models perform worse than SincConv models with regards to test accuracy, number of operations and number of parameters. This indicates that using parametrized sinc functions instead of fully learnable filter kernels provide a substantial benefit in the performance of end-to-end KWS models. We can also observe that SincConv models need less parameters than MFCC models to achieve the same test accuracy. However, the number of operations is slightly larger in SincConv models.</p><p>Using NAS only, we were able to obtain efficient models. We highlighted one model from <ref type="figure" target="#fig_1">Figure 1</ref>    blue arrow) that will be quantized in Section 4.2. This model achieves an accuracy of 95.55% using only 75.7k parameters and 13.6M operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weight and activation quantization</head><p>After NAS is performed and a suitable full-precision end-to-end KWS model is found, we quantize the weights and activations to further reduce the memory footprint of the model. We select the model marked with a blue arrow from <ref type="figure" target="#fig_1">Figure 1</ref>. We first perform fixed bit-width quantization for the weights and activations. The results are visualized in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>We observe that the most notable impact on performance is encountered when the activation bit-width is reduced to 2 or even 1 bit. When the activation bit-width is 4 bits, a slight performance impact is observed. However, when using 8 bit activations, the performance is similar to the full precision model irrespective of the weight bit-width. We also perform trained bit-width quantization on the same model. However, now the weight and activation bit-width are optimized together with model parameters using backpropagation. The weight and activation bit-widths per layer after training are visualized in <ref type="figure" target="#fig_3">Figure 3</ref>. For MBC blocks, we report the average per-weight and per-activation bit width over the three convolutions.</p><p>The trained bit-width model visualized in <ref type="figure" target="#fig_3">Figure 3</ref> needs on average 2.91 bits for quantized activations and 2.51 bits for quantized weights and achieves a test accuracy of 93.76%. Compared to a fixed bit-width model with 2 bit activations and 2 bit weights with a test accuracy of 90.35%, the trained bitwidth model outperforms the fixed bit-width model by 3.41% while using only slightly more bits on average. However, hardware implementation of trained bit-width quantization is more difficult.</p><p>We also observe that layers at the input and the output need more bits than intermediate layers. This result is in line with the vast literature where it is common practice to leave the input and output layers at full precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Resource-efficient DNNs are the key components in modern keyword spotting (KWS) systems. We used neural architecture search (NAS) to obtain efficient end-to-end convolutional neural networks (CNNs) for KWS. Our end-to-end KWS models utilize a SincConv at the input layer to perform classification on raw audio waveforms. To make our results comparable, we performed NAS on the Google speech commands dataset. We compared our end-to-end KWS models to mel-frequency cepstral coefficient (MFCC) based systems. We also compared two weight and activation quantization methods that help to further reduce the memory footprint. By establishing a tradeoff between the model accuracy and the model size, we show that trained bit-width quantization can be used to obtain more competitive models than simply using fixed bit-width quantization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(marked with a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Test accuracy vs number of parameters of KWS models obtained using NAS. Models on the Pareto frontier are emphasized. The number of operations corresponds to the circle area. The model marked with an arrow is quantized in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Test accuracy vs weight bit-width vs activation bitwidth of an end-to-end KWS model using SincConvs. The model was trained from scratch using quantization-aware training and fixed bit-widths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Weight and activation bit-widths of an end-to-end KWS model using SincConvs. The model was trained from scratch using quantization-aware training and trained bitwidths.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hello Edge: Keyword Spotting on Microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1711.07128</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Small-footprint Keyword Spotting using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Small-footprint Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association (ISCA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1478" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ã–</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fougner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1606" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Max-pooling Loss Training of Long Short-term Memory Networks for Smallfootprint Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchapagesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Strom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="474" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Throughput-Optimized OpenCLbased FPGA Accelerator for Large-Scale Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B K</forename><surname>Vrudhula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going Deeper with Embedded FPGA Platform for Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<idno>abs/1703.03073</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical Representations for Efficient Architecture Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MnasNet: Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resource-efficient DNNs for Keyword Spotting using Neural Architecture Search and Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural Architecture Search for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jui</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1982" to="1986" />
		</imprint>
	</monogr>
	<note>Interspeech (IS), 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Keyword Spotting using Time-Domain Features in a Temporal Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huisken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>De Gyvez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="313" to="319" />
		</imprint>
	</monogr>
	<note>in 22nd Euromicro Conference on Digital System Design (DSD</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">State-of-the-Art Speech Recognition with Sequence-to-Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speaker Recognition from Raw Waveform with SincNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>KÃ¼rzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Waschneck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7454" to="7458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Coursera</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>video lectures</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>LÃ©onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<idno>abs/1308.3432</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training Discrete-Valued Neural Networks with Sign Activations Using Weight Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>FrÃ¶ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning (ECML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixed Precision DNNs: All you need is a good parametrization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>GarcÃ­a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Autokws: Keyword spotting with differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2009.03658</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>MobileNetV2: Inverted Residuals and Linear Bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Xilinx/brevitas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pappalardo</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3333552</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3333552" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differentiable Quantization of Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cardinaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>GarcÃ­a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gural</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learned Step Size quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Honk: A PyTorch Reimplementation of Convolutional Neural Networks for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1710.06554</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
