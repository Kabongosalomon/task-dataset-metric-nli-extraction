<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Visual Transformers with Hierarchical Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept of Data Science and AI</orgName>
								<orgName type="department" key="dep2">Faculty of IT</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Visual Transformers with Hierarchical Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classification. However, the routine of the current ViT model is to maintain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the sequence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a great benefit that we can increase the model capacity by scaling dimensions of depth/width/resolution/patch size without introducing extra computational complexity due to the reduced sequence length. Moreover, we empirically find that the average pooled visual tokens contain more discriminative information than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive experiments on the image classification task. With comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Equipped with the self-attention mechanism that has strong capability of capturing long-range dependencies, Transformer <ref type="bibr" target="#b36">[37]</ref> based models have achieved significant breakthroughs in many computer vision (CV) and natural language processing (NLP) tasks, such as machine translation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>, image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref>, segmentation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b38">39]</ref> and object detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47]</ref>. However, the good performance of Transformers comes at a high computational cost. For example, a single Transformer model requires more than 10G Mult-Adds to translate a sentence of only 30 words. Such a huge computational complexity hinders the widespread adoption of Transformers, especially on resource-constrained devices, such as smart phones.</p><p>To improve the efficiency, there are emerging efforts to design efficient and scalable Transformers. On the one † Corresponding author. E-mail: bohan.zhuang@monash.edu  hand, some methods follow the idea of model compression to reduce the number of parameters and computational overhead. Typical methods include knowledge distillation <ref type="bibr" target="#b17">[18]</ref>, low-bit quantization <ref type="bibr" target="#b27">[28]</ref> and pruning <ref type="bibr" target="#b10">[11]</ref>. On the other hand, the self-attention mechanism has quadratic memory and computational complexity, which is the key efficiency bottleneck of Transformer models. The dominant solutions include kernelization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref>, low-rank decomposition <ref type="bibr" target="#b40">[41]</ref>, memory <ref type="bibr" target="#b28">[29]</ref>, sparsity <ref type="bibr" target="#b3">[4]</ref> mechanisms, etc. Despite much effort has been made, there still lacks specific efficient designs for Visual Transformers considering taking advantage of characteristics of visual patterns. In particular, ViT models maintain a full-length sequence in the forward pass across all layers. Such a design can suffer from two limitations. Firstly, different layers should have different redundancy and contribute differently to the accuracy and efficiency of the network. This statement can be supported by existing compression methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22]</ref>, where each layer has its optimal spatial resolution, width and bitwidth. As a result, the full-length sequence may contain huge redundancy. Secondly, it lacks multi-level hierarchical representations, which is well known to be essential for the success of image recognition tasks.</p><p>To solve the above limitations, we propose to gradually downsample the sequence length as the model goes deeper. Specifically, inspired by the design of VGG-style <ref type="bibr" target="#b31">[32]</ref> and ResNet-style <ref type="bibr" target="#b12">[13]</ref> networks, we partition the ViT blocks into several stages and apply the pooling operation (e.g., average/max pooling) in each stage to shrink the sequence length. Such a hierarchical design is reasonable since a recent study <ref type="bibr" target="#b5">[6]</ref> shows that a multi-head self-attention layer with a sufficient number of heads can express any convolution layers. Moreover, the sequence of visual tokens in ViT can be analogous to the flattened feature maps of CNNs along the spatial dimension, where the embedding of each token can be seen as feature channels. Hence, our design shares similarities with the spatial downsampling of feature maps in CNNs. To be emphasized, the proposed hierarchical pooling has several advantages. (1) It brings considerable computational savings and improves the scalability of current ViT models. With comparable floating-point operations (FLOPs), we can scale up our HVT by expanding the dimensions of width/depth/resolution. In addition, the reduced sequential resolution also empowers the partition of the input image into smaller patch sizes for high-resolution representations, which is needed for low-level vision and dense prediction tasks. (2) It naturally leads to the generic pyramidal hierarchy, similar to the feature pyramid network (FPN) <ref type="bibr" target="#b22">[23]</ref>, which extracts the essential multi-scale hidden representations for many image recognition tasks. In addition to hierarchical pooling, we further propose to perform predictions without the class token. Inherited from NLP, conventional ViT models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> equip with a trainable class token, which is appended to the input patch tokens, then refined by the self-attention layers, and is finally used for prediction. However, we argue that it is not necessary to rely on the extra class token for image classification. To this end, we instead directly apply average pooling over patch tokens and use the resultant vector for prediction, which achieves improved performance.</p><p>Our contributions can be summarized as follows:</p><p>• We propose a hierarchical pooling regime that gradually reduces the sequence length as the layer goes deeper, which significantly improves the scalability and the pyramidal feature hierarchy of Visual Transformers. The saved FLOPs can be utilized to improve the model capacity and hence the performance.</p><p>• Empirically, we observe that the average pooled visual tokens contain richer discriminative patterns than the class token for classification.</p><p>• Extensive experiments show that, with comparable FLOPs, our HVT outperforms the competitive baseline DeiT on image classification benchmarks, including ImageNet and CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Visual Transformers. The powerful multi-head selfattention mechanism has motivated the studies of applying Transformers on a variety of CV tasks. In general, current Visual Transformers can be mainly divided into two categories. The first category seeks to combine convolution with self-attention. For example, Carion et al. <ref type="bibr" target="#b2">[3]</ref> propose DETR for object detection, which firstly extracts visual features with CNN backbone, followed by the feature refinement with Transformer blocks. BotNet <ref type="bibr" target="#b33">[34]</ref> is a recent study that replaces the convolution layers with multiheaded self-attention layers at the last stage of ResNet.</p><p>Other works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b16">17]</ref> also present promising results with this hybrid architecture. The second category aims to design a pure attention-based architecture without convolutions. Recently, Ramachandran et al. <ref type="bibr" target="#b25">[26]</ref> propose a model which replaces all instances of spatial convolutions with a form of self-attention applied to ResNet. Hu et al. <ref type="bibr" target="#b15">[16]</ref> propose LR-Net <ref type="bibr" target="#b15">[16]</ref> that replaces convolution layers with local relation layers, which adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. Axial-DeepLab <ref type="bibr" target="#b39">[40]</ref> is also proposed to use Axial-Attention <ref type="bibr" target="#b14">[15]</ref>, a generalization form of self-attention, for Panoptic Segmentation. Dosovitskiy et al. <ref type="bibr" target="#b9">[10]</ref> first transfers Transformer to image classification. The model inherits a similar architecture from standard Transformer in NLP and achieves promising results on Ima-geNet, whereas it suffers from prohibitively expensive training complexity. To solve this, the following work DeiT <ref type="bibr" target="#b35">[36]</ref> propose a more advanced optimization strategy and a distillation token, with improved accuracy and training efficiency. Moreover, T2T-ViT <ref type="bibr" target="#b43">[44]</ref> aims to overcome the limitations of simple tokenization of input images in ViT and propose to progressively structurize the image to tokens to capture rich local structural patterns. Nevertheless, the previous literature all assumes the same architecture to the NLP task, without the adaptation to the image recognition tasks. In this paper, we propose several simple yet effective modifications to improve the scalability of current ViT models.</p><p>Efficient Transformers. Transformer-based models are resource-hungry and compute-intensive despite their stateof-the-art performance. We roughly summarize the efficient Transformers into two categories. The first category focuses on applying generic compression techniques to speed up the inference, either based on quantization <ref type="bibr" target="#b45">[46]</ref>, pruning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref>, and distillation <ref type="bibr" target="#b30">[31]</ref> or seeking to use Neural Architecture Search (NAS) <ref type="bibr" target="#b37">[38]</ref> to explore better configurations. Another category aims to solve the quadratic complexity issue of the self-attention mechanism. A representative approach <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> is to express the self-attention  To reduce the redundancy in the full-length patch sequence and construct a hierarchical representation, we propose to progressively pool visual tokens to shrink the sequence length. To this end, we partition the ViT <ref type="bibr" target="#b9">[10]</ref> blocks into several stages. At each stage, we insert a pooling layer after the first Transformer block to perform down-sampling. In addition to the pooling layer, we perform predictions using the resultant vector of average pooling the output visual tokens of the last stage instead of the class token only.</p><p>weights as a linear dot-product of kernel functions and make use of the associative property of matrix products to reduce the overall self-attention complexity from O(n 2 ) to O(n). Moreover, some works alternatively study diverse sparse patterns of self-attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, or consider the lowrank structure of the attention matrix <ref type="bibr" target="#b40">[41]</ref>, leading to linear time and memory complexity with respect to the sequence length. There are also some NLP literatures that tend to reduce the sequence length during processing. For example, Goyal et al. <ref type="bibr" target="#b11">[12]</ref> propose PoWER-BERT, which progressively eliminates word tokens during the forward pass. Funnel-Transformer <ref type="bibr" target="#b6">[7]</ref> presents a pool-query-only strategy, pooling the query vector within each self-attention layer. However, there are few literatures targeting improving the efficiency of the ViT models. To compromise FLOPs, current ViT models divide the input image into coarse patches (i.e., large patch size), hindering their generalization to dense predictions. In order to bridge this gap, we propose a general hierarchical pooling strategy that significantly reduces the computational cost while enhancing the scalability of important dimensions of the ViT architectures, i.e., depth, width, resolution and patch size. Moreover, our generic encoder also inherits the pyramidal feature hierarchy from classic CNNs, potentially benefiting many downstream recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first briefly revisit the preliminaries of Visual Transformers <ref type="bibr" target="#b9">[10]</ref> and then introduce our proposed Hierarchical Visual Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Let I ∈ R H×W ×C be an input image, where H, W and C represent the height, width, and the number of channels, respectively. To handle a 2D image, ViT first splits the image into a sequence of flattened 2D patches</p><formula xml:id="formula_0">X = [x 1 p ; x 2 p ; ...; x N p ], where x i p ∈ R P 2 C is the i-th patch of the input image and [·]</formula><p>is the concatenation operation. Here, N = HW/P 2 is the number of patches and P is the size of each patch. ViT then uses a trainable linear projection that maps each vectorized patch to a D dimension patch embedding. Similar to the class token in BERT <ref type="bibr" target="#b8">[9]</ref>, ViT appends a learnable embedding x cls ∈ R D to the sequence of patch embeddings. To retain positional information, ViT introduces an additional learnable positional embeddings E ∈ R (N +1)×D . Mathematically, the resulting representation of the input sequence can be formulated as</p><formula xml:id="formula_1">X 0 = [x cls ; x 1 p W; x 2 p W; ...; x N p W] + E,<label>(1)</label></formula><p>where W ∈ R P 2 C×D is a learnable linear projection parameter. Then, the resulting sequence of embeddings serves as the input to the Transformer encoder <ref type="bibr" target="#b36">[37]</ref>. Suppose that the encoder in a Transformer consists of L blocks. Each block contains a multi-head self-attention (MSA) layer and a position-wise multi-layer perceptron (MLP). For each layer, layer normalization (LN) <ref type="bibr" target="#b0">[1]</ref> and residual connections <ref type="bibr" target="#b12">[13]</ref> are employed, which can be formulated as follows</p><formula xml:id="formula_2">X l−1 = X l−1 + MSA(LN(X l−1 )),<label>(2)</label></formula><formula xml:id="formula_3">X l = X l−1 + MLP(LN(X l−1 )),<label>(3)</label></formula><p>where l ∈ [1, ..., L] is the index of Transformer blocks.</p><p>Here, a MLP contains two fully-connected layers with a GELU non-linearity <ref type="bibr" target="#b13">[14]</ref>. In order to perform classification, ViT applies a layer normalization layer and a fullyconnected (FC) layer to the first token of the Transformer encoder's output X 0 L . In this way, the output prediction y can be computed by</p><formula xml:id="formula_4">y = FC(LN(X 0 L )).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Visual Transformer</head><p>In this paper, we propose a Hierarchical Visual Transformer (HVT) to reduce the redundancy in the full-length patch sequence and construct a hierarchical representation. In the following, we first propose a hierarchical pooling to gradually shrink the sequence length and hence reduce the computational cost. Then, we propose to perform predictions without the class token. The overview of the proposed HVT is shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Hierarchical Pooling</head><p>We propose to apply hierarchical pooling in ViT for two reasons: (1) Recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref> on Transformers show that tokens tend to carry redundant information as it goes deeper. Therefore, it would be beneficial to reduce these redundancies through the pooling approaches. (2) The input sequence projected from image patches in ViT can be seen as flattened CNN feature maps with encoded spatial information, hence pooling from the nearby tokens can be analogous to the spatial pooling methods in CNNs.</p><p>Motivated by the hierarchical pipeline of VGG-style <ref type="bibr" target="#b32">[33]</ref> and ResNet-style <ref type="bibr" target="#b12">[13]</ref> networks, we partition the Transformer blocks into M stages and apply downsampling operation to each stage to shrink the sequence length. Let {b 1 , b 2 , . . . , b M } be the indexes of the first block in each stage. At the m-th stage, we apply a 1D max pooling operation with a kernel size of k and stride of s to the output of the Transformer block b m ∈ {b 1 , b 2 , . . . , b M } to shrink the sequence length.</p><p>Note that the positional encoding is important for a Transformer since the positional encoding is able to capture information about the relative and absolute position of the token in the sequence <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3]</ref>. In Eq. (1) of ViT, each patch is equipped with positional embedding E at the beginning. However, in our HVT, the original positional embedding E may no longer be meaningful after pooling since the sequence length is reduced after each pooling operation. In this case, positional embedding in the pooled sequence needs to be updated. Moreover, previous work <ref type="bibr" target="#b6">[7]</ref> in NLP also find it important to complement positional information after changing the sequence length. Therefore, at the m-th stage, we introduce an additional learnable positional embedding E bm to capture the positional information, which can be formulated aŝ</p><formula xml:id="formula_5">X bm = MaxPool1D(X bm ) + E bm ,<label>(5)</label></formula><p>where X bm is the output of the Transformer block b m . We then forward the resulting embeddingsX bm into the next Transformer block b m + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prediction without the Class Token</head><p>Previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36]</ref> make predictions by taking the class token as input in classification tasks as described in Eq. (4). However, such structure relies solely on the single class token with limited capacity while discarding the remaining sequence that is capable of storing more discriminative information. To this end, we propose to remove the class token in the first place and predict with the remaining output sequence on the last stage. Specifically, given the output sequence without the class token on the last stage X L , we first apply average pooling, then directly apply an FC layer on the top of the pooled embeddings and make predictions. The process can be formulated as y = FC(AvgPool(LN(X L ))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complexity Analysis</head><p>In this section, we begin with analyzing the block-wise compression ratio and then extending to the encoder-wise case. Following ViT <ref type="bibr" target="#b9">[10]</ref>, we use FLOPs to measure the computational cost of a Transformer. Let n be the number of tokens in a sequence and d is the dimension of each token. The FLOPs of a Transformer block φ BLK (n, d) can be computed by</p><formula xml:id="formula_7">φ BLK (n, d) = φ M SA (n, d) + φ M LP (n, d), = 12nd 2 + 2n 2 d,<label>(7)</label></formula><p>where φ M SA (n, d) and φ M LP (n, d) are the FLOPs of the MSA and MLP, respectively. Details about Eq. <ref type="formula" target="#formula_7">(7)</ref> can be found in the supplementary material. Without loss of generality, suppose that the sequence length n is reduced by half after performing hierarchical pooling. In this case, the block-wise compression ratio α can be computed by</p><formula xml:id="formula_8">α = φ BLK (n, d) φ BLK (n/2, d) = 24d + 4n 12d + n .<label>(8)</label></formula><p>In practice, the dimension of each self-attention head is usually constant. A model with a fixed number of selfattention heads in each layer is equal to a constant d. In that case, Eq. (8) indicates that a longer sequence will lead to a larger α, correspondingly result in more complexity reduction between blocks. Furthermore, we have lim n→∞ α = 4 with a fixed d and lim d→∞ α = 2 with a fixed n. Therefore, the block-wise compression ratio α is bounded by (2, 4), i.e., α ∈ (2, 4). DeiT-S and our HVT-S-1 correspond to the small setting in DeiT, except that our model applies a pooling operation and performing predictions without the class token. The resolution of the feature maps from ResNet50 conv1 and conv4 2 are 112×112 and 14×14, respectively. For DeiT and HVT, the feature maps are reshaped from tokens. For our model, we interpolate the pooled sequence to its initial length then reshape it to a 2D map.</p><p>Next we analyse the encoder-wise compression ratio β. Let t be the number of Transformer blocks and g be the number of stages in HVT. For ViT, we then have the encoder-wise FLOPs of</p><formula xml:id="formula_9">Φ V iT (n, d, t) = t · φ BLK (n, d).<label>(9)</label></formula><p>With t = g, our model can achieve maximum FLOPs reduction by applying pooling operation after every block. Next, suppose every time n is reduced by half, the encoderwise FLOPs for our HVT can be computed by</p><formula xml:id="formula_10">Φ HV T (n, d, t) = φ BLK (n, d) + φ BLK (n, d) α 1 + φ BLK (n, d) α 1 α 2 + ... + φ BLK (n, d) t−1 i=1 α i ,<label>(10)</label></formula><p>where α i is the FLOPs compression ratio between block i − 1 and i. With a constant α, Eq. (10) can be seen as the summation of a geometric sequence with a common ratio of α −1 . In that case, the encoder-wise compression ratio β can be formulated as</p><formula xml:id="formula_11">β = Φ V iT Φ HV T = t(1 − α −1 ).<label>(11)</label></formula><p>As α −1 ∈ (1/4, 1/2), we can get β ∈ (t/2, 3t/4), which indicates that our model can be reduced at least t/2 times of FLOPs when applying pooling operation after every block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of Hierarchical Pooling</head><p>In CNNs, feature maps are usually downsampled to smaller sizes in a hierarchical way <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13]</ref>. In this paper, we show that this principle can be applied to ViT models by comparing the visualized feature maps from ResNet conv4 2, DeiT-S <ref type="bibr" target="#b35">[36]</ref> block1 and HVT-S-1 block1 in <ref type="figure">Figure 3</ref>. From the figure, in ResNet, the initial feature maps after the first convolutional layer contain rich edge information. After feeding the features to consecutive convolutional layers and a pooling layer, the output feature maps tend to preserve more high-level discriminative information. In DeiT-S, following the ViT structure, although the image resolution for the feature maps has been reduced to 14 × 14 by the initial linear projection layer, we can still observe clear edges and patterns. Then, the features get refined in the first block to obtain sharper edge information. In contrast to DeiT-S that refines features at the same resolution level, after the first block, the proposed HVT downsamples the hidden sequence through a pooling layer and reduces the sequence length by half. We then interpolate the sequence back to 196 and reshape it to 2D feature maps. We can find that the hidden representations contain more abstract information with high discriminative power, which is similar to ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scalability of HVT</head><p>The computational complexity reduction equips HVT with strong scalability in terms of width/depth/patch size/resolution. Take DeiT-S for an example, the model consists of 12 blocks and 6 heads. Given a 224×224 image with a patch size of 16, the computational cost of DeiT-S is around 4.6G FLOPs. By applying four pooling operations, our method is able to achieve nearly 3.3× FLOPs reduction. Furthermore, to re-allocate the reduced FLOPs, we may construct wider or deeper HVT-S, with 11 heads or 48 blocks, then the overall FLOPs would be around 4.51G and 4.33G, respectively. Moreover, we may consider a longer sequence by setting a smaller patch size or using a larger resolution. For example, with a patch size of 8 and an image resolution of 192×192, the FLOPs for HVT-S is around 4.35G. Alternatively, enlarging the image resolution into 384×384 will lead to 4.48G FLOPs. In all of the above mentioned cases, the computational costs are still lower than that of DeiT-S while the model capacity is enhanced.</p><p>It is worth noting that finding a principled way to scale up HVT to obtain the optimal efficiency-vs-accuracy tradeoff remains an open question. At the current stage, we take an early exploration by evenly partitioning blocks and following model settings in DeiT <ref type="bibr" target="#b35">[36]</ref> for a fair comparison. In fact, the improved scalability of HVT makes it possible for using Neural Architecture Search (NAS) to automatically find optimal configurations, such as EfficientNet <ref type="bibr" target="#b34">[35]</ref>. We leave for more potential studies for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Compared methods. To investigate the effectiveness of HVT, we consider two architectures in DeiT <ref type="bibr" target="#b35">[36]</ref> for comparisons: HVT-Ti: HVT with the tiny setting. HVT-S: HVT with the small setting. For convenience, we use "Architecture-M " to represent the model with M pooling stages, e.g., HVT-S-1. Since DeiT <ref type="bibr" target="#b35">[36]</ref> and ViT <ref type="bibr" target="#b9">[10]</ref> use the same model architectures and DeiT achieves better performance, we only consider DeiT for comparisons.</p><p>Datasets and Evaluation metrics. We evaluate our proposed HVT on two image classification benchmark datasets: CIFAR-100 <ref type="bibr" target="#b20">[21]</ref> and ImageNet <ref type="bibr" target="#b29">[30]</ref>. We measure the performance of different methods in terms of the Top-1 and Top-5 accuracy. Following DeiT <ref type="bibr" target="#b35">[36]</ref>, we measure the computational cost by FLOPs. Moreover, we also measure the model size by the number of parameters (Params).</p><p>Implementation details. For experiments on ImageNet, we train our models for 300 epochs with a total batch size of 1024. The initial learning rate is 0.0005. We use AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with a momentum of 0.9 for optimization. We set the weight decay to 0.025. For fair comparisons, All the models are evaluated on ImageNet. Solid lines denote the Top-1 accuracy (y-axis on the right). Dash lines denote the training loss (y-axis on the left).</p><p>we keep the same data augmentation strategy as DeiT <ref type="bibr" target="#b35">[36]</ref>.</p><p>For the downsampling operation, we use max pooling by default. The kernel size k and stride s are set to 3 and 2, respectively. Besides, all learnable positional embeddings are initialized in the same way as DeiT. More detailed settings on the other hyper-parameters can be found in DeiT. For experiments on CIFAR-100, we train our models with a total batch size of 128. The initial learning rate is set to 0.000125. Other hyper-parameters are kept the same as those on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Main Results</head><p>We compare the proposed HVT with DeiT and report the results in <ref type="table" target="#tab_1">Table 1</ref>. From the results, with a hierarchical pooling, our HVT achieves nearly 2× FLOPs reduction. However, the significant FLOPs reduction also leads to performance degradation in both the tiny and small settings. Additionally, the performance drop of HVT-S-1 is smaller than that of HVT-Ti-1. For example, for HVT-S-1, it only leads to 1.80% drop in the Top-1 accuracy. In contrast, it results in 2.56% drop in the Top-1 accuracy for HVT-Ti-1. It can be attributed to that, compared with HVT-Ti-1, HVT-S-1 is more redundant with more parameters. Therefore, applying hierarchical pooling to HVT-S-1 can significantly reduce redundancy while maintaining performance.</p><p>We also compare the scaled HVT with DeiT under similar FLOPs. Specifically, we enlarge the embedding dimensions and add extra heads in HVT-Ti. From <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure">Figure 4</ref>, by re-allocating the saved FLOPs to scale up the model, HVT can converge to a better solution and yield improved performance. For example, the Top-1 accuracy on ImageNet can be improved considerably by 3.03% in the tiny setting. More empirical studies on the effect of model  <ref type="bibr" target="#b35">[36]</ref> on ImageNet. "Embedding Dim" refers to the dimension of each token in the sequence. "#Heads" and "#Blocks" are the number of self-attention heads and blocks in Transformer, respectively. "FLOPs" is measured with a 224×224 image. "Ti" and "S" are short for the tiny and small settings, respectively. "Architecture-M " denotes the model with M pooling stages. "Scale" denotes that we scale up the embedding dimension and/or the number of self-attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Embedding   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>Effect of the prediction without the class token. To investigate the effect of the prediction without the class token, we train DeiT-Ti with and without the class token and show the results in <ref type="table" target="#tab_3">Table 2</ref>. From the results, the models without the class token outperform the ones with the class token. The performance gains mainly come from the extra discriminative information stored in the entire sequence without the class token. Note that the performance improvement on CIFAR-100 is much larger than that on ImageNet. It may be attributed that CIFAR-100 is a small dataset, which lacks varieties compared with ImageNet. Therefore, the model trained on CIFAR-100 benefits more from the increase of model's discriminative power.</p><p>Effect of different pooling stages. We train HVT-S with different pooling stages M ∈ {0, 1, 2, 3, 4} and show the results in <ref type="table" target="#tab_5">Table 4</ref>. Note that HVT-S-0 is equivalent to the DeiT-S without the class token. With the increase of M , HVT-S achieves better performance with decreasing FLOPs. One possible reason is that HVT-S is very redundant on CIFAR-100. Applying hierarchical pooling to HVT-S significantly reduces redundancy in the full-length patch sequence and hence achieving better performance.</p><p>Effect of different downsampling operations. To investigate the effect of different downsampling operations, we train HVT-S-4 with three downsampling strategies: convolution, average pooling and max pooling. As <ref type="table" target="#tab_4">Table 3</ref> shows, downsampling with convolution performs the worst even it introduces additional FLOPs and parameters. Besides, average pooling performs slightly better than convolution in terms of the Top-1 accuracy. Compared with the two settings, HVT-S-4 with max pooling performs much better as it significantly surpasses average pooling by 5.05% in the Top-1 accuracy and 2.17% in the Top-5 accuracy. The result is consistent with the common sense <ref type="bibr" target="#b1">[2]</ref> that max pooling performs well in a large variety of settings. To this end, we use max pooling in all other experiments by default.</p><p>Effect of model scaling. One of the important advantages of the proposed hierarchical pooling is that we can re-allocate the saved computational cost for better model capacity by constructing a model with a wider, deeper, larger  resolution or smaller patch size configuration. Similar to the CNNs literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, we study the effect of model scaling in the following. Based on HVT-S-4, we first construct deeper models by increasing the number of blocks in Transformers. Specifically, we train 4 models with different number of blocks L ∈ {12, 16, 20, 24}. As a result, each pooling stage for different models would have 3, 4, 5, and 6 blocks, respectively. We train 4 models on CIFAR-100 and report the results in <ref type="table" target="#tab_6">Table 5</ref>. From the results, we observe no more gains by stacking more blocks in HVT.</p><p>Based on HVT-Ti-4, we then construct wider models by increasing the number of self-attention heads. To be specific, we train 4 models with different numbers of selfattention heads, i.e., 3, 6, 12, and 16, on CIFAR-100 and report the results in <ref type="table" target="#tab_7">Table 6</ref>. From the results, our models achieve better performance with the increase of width. For example, the model with 16 self-attention heads outperforms those with 3 self-attention heads by 6.79% in the Top-1 accuracy and 1.38% in the Top-5 accuracy.</p><p>Based on HVT-S-4, we further construct models with larger input image resolutions. Specifically, we train 4 models with different input image resolutions, i.e., 160, 224, 320, and 384, on CIFAR-100 and report the results in <ref type="table" target="#tab_8">Table 7</ref>. From the results, with the increase of image resolution, our models achieve better performance. For example, the model with the resolution of 384 outperforms those with the resolution of 160 by 2.47% in the Top-1 accuracy and 1.12% in the Top-5 accuracy. Nevertheless, increasing image resolutions also leads to high computational cost. To make a trade-off between computational cost and accuracy, we set the image resolution to 224 by default.</p><p>We finally train HVT-S-4 with different patch sizes P ∈ {8, 16, 32} and show the results in <ref type="table" target="#tab_9">Table 8</ref>. From the results, HVT-S-4 performs better with the decrease of patch  size. For example, when the patch size decreases from 32 to 8, our HVT-S achieves 9.14% and 4.03% gain in terms of the Top-1 and Top-5 accuracy. Intuitively, a smaller patch size leads to fine-grained image patches and helps to learn high-resolution representations, which is able to improve the classification performance. However, with a smaller patch size, the length of the patch sequence will be longer, which significantly increases the computational cost. To make a balance between the computational cost and accuracy, we set the patch size to 16 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have presented a Hierarchical Visual Transformer, termed HVT, for image classification. In particular, the proposed hierarchical pooling can significantly compress the sequential resolution to save computational cost in a simple yet effective form. More importantly, this strategy greatly improves the scalability of visual Transformers, making it possible to scale various dimensionsdepth, width, resolution and patch size. By re-allocating the saved computational cost, we can scale up these dimensions for better model capacity with comparable or fewer FLOPs. Moreover, we have empirically shown that the visual tokens are more important than the single class token for class prediction. Note that the scope of this paper only targets designing our HVT as an encoder. Future works may include extending our HVT model to decoder and to solve other mainstream CV tasks, such as object detection and semantic/instance segmentation. In addition, it would be interesting to find a principled way to scale up HVT that can achieve better accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Performance comparisons on ImageNet. With comparable GFLOPs (1.25 vs. 1.39), our proposed Scale HVT-Ti-4 surpasses DeiT-Ti by 3.03% in Top-1 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed Hierarchical Vision Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :Figure 3 :</head><label>13</label><figDesc>Linear Projection, N = 196 HVT-S-1: Block1, N = 97 Feature visualization of ResNet50 [13], DeiT-S [36] and our HVT-S-1 trained on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 Figure 4 :</head><label>44</label><figDesc>Performance comparisons of DeiT-Ti (1.25G FLOPs) and the proposed Scale HVT-Ti-4 (1.39G FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparisons between HVT and DeiT</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of the prediction without the class token. "CLS" denotes the class token.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs (G) Params (M)</cell><cell cols="4">ImageNet Top-1 Acc. (%) Top-5 Acc. (%) Top-1 Acc. (%) Top-5 Acc. (%) CIFAR-100</cell></row><row><cell>DeiT-Ti with CLS</cell><cell>1.25</cell><cell>5.54</cell><cell>72.20</cell><cell>91.10</cell><cell>64.49</cell><cell>89.27</cell></row><row><cell>DeiT-Ti without CLS</cell><cell>1.25</cell><cell>5.54</cell><cell>72.42 (+0.22)</cell><cell>91.55 (+0.45)</cell><cell>65.93 (+1.44)</cell><cell>90.33 (+1.06)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">: Performance comparisons on HVT-S-4 with three</cell></row><row><cell cols="6">downsampling operations: convolution, max pooling and</cell></row><row><cell cols="6">average pooling. We report the Top-1 and Top-5 accuracy</cell></row><row><cell cols="2">on CIFAR-100.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Model Operation FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>HVT-S</cell><cell>Conv</cell><cell>1.47</cell><cell>23.54</cell><cell>69.75</cell><cell>92.12</cell></row><row><cell>HVT-S</cell><cell>Avg</cell><cell>1.39</cell><cell>21.77</cell><cell>70.38</cell><cell>91.39</cell></row><row><cell>HVT-S</cell><cell>Max</cell><cell>1.39</cell><cell>21.77</cell><cell>75.43</cell><cell>93.56</cell></row><row><cell cols="4">scaling can be found in Section 5.2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Performance comparisons on HVT-S with different</cell></row><row><cell cols="5">pooling stages M . We report the Top-1 and Top-5 accuracy</cell></row><row><cell cols="2">on CIFAR-100.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">M FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>0</cell><cell>4.57</cell><cell>21.70</cell><cell>71.99</cell><cell>92.44</cell></row><row><cell>1</cell><cell>2.40</cell><cell>21.74</cell><cell>74.27</cell><cell>93.07</cell></row><row><cell>2</cell><cell>1.94</cell><cell>21.76</cell><cell>75.37</cell><cell>93.69</cell></row><row><cell>3</cell><cell>1.62</cell><cell>21.77</cell><cell>75.22</cell><cell>93.90</cell></row><row><cell>4</cell><cell>1.39</cell><cell>21.77</cell><cell>75.43</cell><cell>93.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparisons on HVT-S-4 with different number of Transformer blocks. We report the Top-1 and Top-5 accuracy on CIFAR-100.</figDesc><table><row><cell cols="5">#Blocks FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>12</cell><cell>1.39</cell><cell>21.77</cell><cell>75.43</cell><cell>93.56</cell></row><row><cell>16</cell><cell>1.72</cell><cell>28.87</cell><cell>75.32</cell><cell>93.30</cell></row><row><cell>20</cell><cell>2.05</cell><cell>35.97</cell><cell>75.35</cell><cell>93.35</cell></row><row><cell>24</cell><cell>2.37</cell><cell>43.07</cell><cell>75.04</cell><cell>93.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Performance comparisons on HVT-Ti-4 with dif-</cell></row><row><cell cols="5">ferent number of self-attention heads. We report the Top-1</cell></row><row><cell cols="4">and Top-5 accuracy on CIFAR-100.</cell><cell></cell></row><row><cell cols="5">#Heads FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>3</cell><cell>0.38</cell><cell>5.58</cell><cell>69.51</cell><cell>91.78</cell></row><row><cell>6</cell><cell>1.39</cell><cell>21.77</cell><cell>75.43</cell><cell>93.56</cell></row><row><cell>12</cell><cell>5.34</cell><cell>86.01</cell><cell>76.26</cell><cell>93.39</cell></row><row><cell>16</cell><cell>9.39</cell><cell>152.43</cell><cell>76.30</cell><cell>93.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance comparisons on HVT-S-4 with different image resolutions. We report the Top-1 and Top-5 accuracy on CIFAR-100. Resolution FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc.</figDesc><table><row><cell>(%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance comparisons on HVT-S-4 with different patch sizes P . We report the Top-1 and Top-5 accuracy on CIFAR-100.</figDesc><table><row><cell cols="5">P FLOPs (G) Params (M) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>8</cell><cell>6.18</cell><cell>21.99</cell><cell>77.29</cell><cell>94.22</cell></row><row><cell>16</cell><cell>1.39</cell><cell>21.77</cell><cell>75.43</cell><cell>93.56</cell></row><row><cell>32</cell><cell>0.37</cell><cell>22.55</cell><cell>68.15</cell><cell>90.19</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamás</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compressing BERT: studying the effects of weight pruning on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Andrews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RepL4NLP@ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Power-bert: Accelerating BERT inference via progressive word-vector elimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamitra</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Raje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogish</forename><surname>Chakaravarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (gelus). arXiv: Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<title level="m">Tinybert: Distilling bert for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and François Fleuret</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="68" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully quantized transformer for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Prato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Charlaix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Trevor Cohn, Yulan He, and Yang Liu</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compressive transformers for longrange sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HAT: hardware-aware transformers for efficient natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7675" to="7688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ternarybert: Distillation-aware ultra-low bit BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="509" to="521" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deformable DETR: deformable transformers for end-to-end object detection. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
