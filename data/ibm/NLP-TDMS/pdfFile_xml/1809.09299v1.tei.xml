<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triply Supervised Decoder Networks for Joint Detection and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelongli@opt.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Triply Supervised Decoder Networks for Joint Detection and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Joint object detection and semantic segmentation can be applied to many fields, such as self-driving cars and unmanned surface vessels. An initial and important progress towards this goal has been achieved by simply sharing the deep convolutional features for the two tasks. However, this simple scheme is unable to make full use of the fact that detection and segmentation are mutually beneficial. To overcome this drawback, we propose a framework called TripleNet where triple supervisions including detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder network. Classagnostic segmentation supervision provides an objectness prior knowledge for both semantic segmentation and object detection. Besides the three types of supervisions, two light-weight modules (i.e., inner-connected module and attention skip-layer fusion) are also incorporated into each layer of the decoder. In the proposed framework, detection and segmentation can sufficiently boost each other. Moreover, class-agnostic and class-aware segmentation on each decoder layer are not performed at the test stage. Therefore, no extra computational costs are introduced at the test stage. Experimental results on the VOC2007 and VOC2012 datasets demonstrate that the proposed TripleNet is able to improve both the detection and segmentation accuracies without adding extra computational costs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection and semantic segmentation are two fundamental and important tasks in the field of computer vision. In recent few years, object detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> and semantic segmentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b0">1]</ref> with deep convolutional networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref> have achieved great progress, respectively. Most state-of-the-art methods only focus on one single task, which does not join object detection and semantic segmentation together. However, joint object detection and semantic segmentation is very necessary and im- The d featur</p><p>The en featur <ref type="figure">Figure 1</ref>. Some architectures of joint detection and segmentation. (a) The last layer of the encoder is used for detection and segmentation <ref type="bibr" target="#b1">[2]</ref>. (b) The branch for detection is refined by the branch for segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47]</ref>. (c) Each layer of the decoder detects objects of different scales, and the fused layer is for segmentation <ref type="bibr" target="#b6">[7]</ref>. (d) The proposed PairNet. Each layer of the decoder is simultaneously for detection and segmentation. (e) The proposed TripleNet, which has three types of supervisions and some lightweight modules.</p><p>portant in many applications, such as self-driving cars and unmanned surface vessels. In fact, object detection and semantic segmentation are highly related. On the one hand, semantic segmentation usually used as a multi-task supervision can help improve object detection <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>. On the other hand, object detection can be used as a prior knowledge to help improve performance of semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Due to application requirements and task relevance, joint object detection and semantic segmentation has gradually attracted the attention of researchers. <ref type="figure">Fig. 1</ref> summarizes three typical methods of joint object detection and semantic segmentation. <ref type="figure">Fig. 1(a)</ref> shows the simplest and most naive way where one branch for object detection and one branch for semantic segmentation are in parallel attached to the last layers of the encoder <ref type="bibr" target="#b1">[2]</ref>. In <ref type="figure">Fig. 1(b)</ref>, the branch for object detection is further refined by the features from the branch for semantic segmentation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47]</ref>. Recently, the encoder-decoder network is further used for joint object de-tection and semantic segmentation. In <ref type="figure">Fig. 1(c)</ref>, each layer of the decoder is used for multi-scale object detection, and the concatenated feature map from different layers of the decoder is used for semantic segmentation <ref type="bibr" target="#b6">[7]</ref>. The above methods have achieved great success for detection and segmentation. However, the performance is still far from the strict demand of real applications such as self-driving cars and unmanned surface vessels. One possible reason is that the mutual benefit between the two tasks is not fully exploited.</p><p>To exploit mutual benefit for joint object detection and semantic segmentation tasks, in this paper, we propose to impose three types of supervisions (i.e., detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision) on each layer of the decoder network. Meanwhile, the light-weight modules (i.e., the inner-connected module and attention skip-layer fusion) are also incorporated. The corresponding network is called TripleNet (see <ref type="figure">Fig. 1</ref>(e)). It is noted that we also propose to only impose the detection-oriented supervision and class-aware segmentation supervision on each layer of the decoder, which is called PairNet (see <ref type="figure">Fig. 1(d)</ref>). The contributions of this paper can be summarized as follows:</p><p>(1) Two novel frameworks (i.e., PairNet and TripleNet) for joint object detection and semantic segmentation are proposed. In TripleNet, the detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are imposed on each layer of the decoder. Meanwhile, two light-weight modules (i.e., the inner-connected module and attention skip-layer fusion) is also incorporated into each layer of the decoder.</p><p>(2) A lot of synergies are gained from TripleNet. Both detection and segmentation accuracies are significantly improved. The improvement is not at expense of incurring extra computational costs because the class-agnostic segmentation and class-aware segmentation are not performed in each layer of the decoder at the test stage .</p><p>(3) Experiments on the VOC 2007 and VOC 2012 datasets are conducted to demonstrate the effectiveness of the proposed TripleNet.</p><p>The rest of this paper is organized as follows. Section 2 reviews some related works of object detection and semantic segmentation. Section 3 introduces our proposed method in detail. Experiments are shown in Section 4. Finally, it is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, a review of object detection and semantic segmentation is firstly given. After that, some related works of joint object detection and semantic segmentation are further introduced.</p><p>Object detection It aims to classify and locate objects in an image. Generally, the methods of object detection can be divided into two main classes: two-stage methods and one-stage methods. Two-stage methods firstly extract some candidate object proposals from an image and then classify these candidate proposals into the specific object categories. R-CNN <ref type="bibr" target="#b10">[11]</ref> and its variants (e.g., Fast RCNN <ref type="bibr" target="#b9">[10]</ref> and Faster RCNN <ref type="bibr" target="#b35">[36]</ref>) are the most representative frameworks among the two-stage methods. Based on R-CNN series, researchers have done many improvements <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>. To accelerate detection speed, Dai et al. <ref type="bibr" target="#b5">[6]</ref> proposed R-FCN which uses position-sensitive feature maps for proposal classification and bounding box regression. To output multi-scale feature maps with strong semantics, Lin et al. <ref type="bibr" target="#b21">[22]</ref> proposed feature pyramid network (FPN) based on skip-layer connection and top-down pathway. Recently, Cai et al. <ref type="bibr" target="#b3">[4]</ref> trained a sequence of object detectors with increasing IoU thresholds to improve detection quality.</p><p>One-stage methods directly predict object class and bounding box in a single network. YOLO <ref type="bibr" target="#b34">[35]</ref> and SSD <ref type="bibr" target="#b28">[29]</ref> are two of the earliest proposed one-stage methods. After that, many variants are proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50]</ref>. DSSD <ref type="bibr" target="#b8">[9]</ref> and RON <ref type="bibr" target="#b17">[18]</ref> use the encoder-decoder network to add context information for multi-scale object detection. To train object detector from scratch, DSOD <ref type="bibr" target="#b37">[38]</ref> uses dense layer-wise connections on SSD for deep supervision. Instead of using in-network feature maps of different resolutions for multi-scale object detection, STDN <ref type="bibr" target="#b49">[50]</ref> uses scale-transferrable module to generate different highresolution feature maps from last feature map. To solve class imbalance in the training stage, RetinaNet <ref type="bibr" target="#b25">[26]</ref> introduces focal loss to downweight the contribution of easy samples.</p><p>Semantic segmentation It aims to predict the semantic label of each pixel in an image, which has achieved significant progress based on fully convolutional networks (i.e., FCN <ref type="bibr" target="#b29">[30]</ref>). Generally, the methods of semantic segmentation can be also divided into two main classes: encoderdecoder methods and spatial pyramid methods. Encoderdecoder methods contain two subnetworks: an encoder subnetwork and a decoder subnetwork. The encoder subnetwork extracts strong semantic features and reduces spatial resolution of feature maps, which is usually based on the classical CNN models (e.g., VGG <ref type="bibr" target="#b38">[39]</ref>, ResNet <ref type="bibr" target="#b14">[15]</ref>, DenseNet <ref type="bibr" target="#b16">[17]</ref>) pre-trained on ImageNet <ref type="bibr" target="#b36">[37]</ref>. The decoder subnetwork gradually upsamples the feature maps of encoder subnetwork. DeconvNet <ref type="bibr" target="#b31">[32]</ref> and SegNet <ref type="bibr" target="#b0">[1]</ref> use max-pooling indices of the encoder subnetwork to upsample the feature maps. To extract context information, some methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b44">45]</ref> adopt skip-layer connection to combine the feature maps from the encoder and decoder subnetworks.</p><p>Spatial pyramid methods adopt the idea of spatial pyramid pooling <ref type="bibr" target="#b12">[13]</ref> to extract multi-scale information from the last output feature maps. Chen et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>   proposed to use multiple convolutional layers of different atrous rates in parallel (called ASPP) to extract multi-scale features. Instead of using convolutional layers of different atrous rates, Zhao et al. <ref type="bibr" target="#b48">[49]</ref> proposed pyramid pooling module (called PSPnet), which downsamples and upsamples the feature maps in parallel. Yang et al. <ref type="bibr" target="#b42">[43]</ref> proposed to use dense connection to cover object scale range densely. Joint object detection and semantic segmentation It aims to simultaneously detect objects and predict pixel semantic labels by a single network. Recently, researchers have done some attempts. Yao et al. <ref type="bibr" target="#b43">[44]</ref> proposed to use the graphical model to holistic scene understanding. Teichmann et al. <ref type="bibr" target="#b39">[40]</ref> proposed to join object detection and semantic segmentation by sharing the encoder subnetwork. Kokkinos <ref type="bibr" target="#b20">[21]</ref> also proposed to integrate multiple computer vision tasks together. Mao et al. <ref type="bibr" target="#b30">[31]</ref> found that joint semantic segmentation and pedestrian detection can help improve performance of pedestrian detection. The similar conclusion is also demonstrated by SDS-RCNN <ref type="bibr" target="#b1">[2]</ref>. Meanwhile, joint instance semantic segmentation and object detection is also proposed <ref type="bibr" target="#b6">[7]</ref>. Recently, Dvornik et al. <ref type="bibr" target="#b6">[7]</ref> proposed a real-time framework (called BlitzNet) for joint object detection and semantic segmentation. It is based on the encoder-decoder network, where each layer of the decoder is used to detect objects of different scales and multiscale fused layer is used for semantic segmentation.</p><p>Though joint object detection and semantic segmentation has been explored recently, we argue that there is still room for further improvement. Thus, in this paper, we give some more exploration on joint object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed methods</head><p>In recent years, the fully convolutional networks (FCN) with encoder-decoder structure have achieved great success on object detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9]</ref> and semantic segmentation <ref type="bibr" target="#b0">[1]</ref>, respectively. For example, DSSD <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> and RetinaNet <ref type="bibr" target="#b25">[26]</ref> use different layers of the decoder to detect objects of different scales, respectively. By using the encoderdecoder structure, SegNet <ref type="bibr" target="#b0">[1]</ref> and LargeKernel <ref type="bibr" target="#b32">[33]</ref> generate high-resolution logits for semantic segmentation. Based on above observations, a very natural and simple idea is that FCN with encoder-decoder is suitable for joint object detection and semantic segmentation.</p><p>In this section, we give a detailed introduction about the proposed paired supervision decoder network (i.e., PairNet) and triply supervised decoder network (i.e., TripleNet) for joint object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Paired supervision decoder network (PairNet)</head><p>Based on the encoder-decoder structure, a feature pyramid network is naturally proposed to join object detection and semantic segmentation. Namely, the supervision of object detection and semantic segmentation is added to each layer of the decoder, which is called PairNet. On the one hand, PairNet uses different layers of the decoder to detect objects of different scales. On the other hand, instead of using the last high-resolution layer for semantic segmentation which is adopted by most state-of-the-art methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>, PairNet uses each layer of the decoder to respectively parse pixel semantic labels. Though the proposed PairNet is very simple and naive, it has not been explored for joint object detection and semantic segmentation to the best of our knowledge. <ref type="figure" target="#fig_2">Fig. 2(a)</ref> gives the detailed architecture of PairNet. The input image firstly goes through a fully convolutional network with encoder-decoder structure. The encoder gradually down-samples the feature map. In this paper, the famous ResNet-50 or ResNet101 <ref type="bibr" target="#b14">[15]</ref> (i.e., res1-res4) and some new added residual blocks (i.e., res5-res7) construct the encoder. The decoder gradually maps the low-resolution feature map to the high-resolution feature map. To enhance context information, skip-layer fusion is used to fuse the feature map from the decoder and the corresponding feature map from the encoder. <ref type="figure" target="#fig_2">Fig. 2(b)</ref> gives the illustration of skip-layer fusion. The feature maps in the decoder is firstly upsampled by bilinear interpolation and then concatenated with the corresponding feature maps of the same resolution in the encoder. After that, the concatenated feature maps go through a residual unit to generate the output feature maps.</p><p>To join object detection and semantic segmentation, each layer of the decoder is further split into two different branches. The branch of object detection consists of a 3 × 3 convolutional layer and two sibling 1 × 1 convolutional layers for object classification and bounding box regression. The branch of object detection at different layers is used to detect objects of different scales. Specifically, the branch at front layer of the decoder with low resolution is used to detect large-scale objects, while the branch at latter layer with high resolution is used to detect small-scale objects.</p><p>The branch of semantic segmentation consists of a 3 × 3 convolutional layer to generate the logits. There are two different ways to compute the segmentation loss. The first one is that the segmentation logits are upsampled to the same resolution of ground-truth, and the second one is that the ground-truth is downsampled to the same resolution of the logits. We found that the first strategy have a little better performance, which is adopted in the follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triply supervised decoder network (TripleNet)</head><p>To further improve the performance of joint object detection and semantic segmentation, triply supervision decoder network (called TripleNet) is further proposed, where detection-oriented supervision, class-aware segmentation supervision, and class-agnostic segmentation supervision are added on each layer of the decoder. <ref type="figure" target="#fig_3">Fig. 3(a)</ref> gives the detailed architecture of TripleNet. Compared to PairNet, TripleNet add some new modules (i.e., multiscale fused segmentation, the inner-connected module, and class-agnostic segmentation supervision). In the following section, we introduce these modules in detailed.</p><p>Multiscale fused segmentation It has been demonstrated that multi-scale features are useful for semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b45">46]</ref>. To use multi-scale features of different layers in the decoder for better semantic segmentation, the feature maps of different layers in the decoder are upsampled to the same spatial resolution and concatenated together. After that, a 3 × 3 convolutional layer is used to generate the segmentation logits. Compared to the segmentations based on one layer of the decoder, multilayer fused features can make better use of context information. Thus, multilayer fused segmentation is used for final prediction at the test stage. Meanwhile, the semantic segmentation based on each layer of the decoder can be seen as a deep supervision for feature learning.</p><p>The inner-connected module In Section 3.1, PairNet only shares the base network for object detection and semantic segmentation, while the branches of object detection and semantic segmentation have no cross. To further help object detection, an inner-connected module is proposed to refine object detection by the logits of semantic segmentation. <ref type="figure" target="#fig_3">Fig. 3(b)</ref> shows the inner-connected module in layer i. The feature map in layer i first goes through a 3 × 3 convolutional layer to produce the segmentation logits for the branch of semantic segmentation. Meanwhile, the segmentation logit goes through two 3 × 3 convolutional layers to generate new feature map which are further concatenated with feature maps in layer i. Based on concatenated feature maps, a 3 × 3 convolutional layer is used to generate the feature map for the branch of object detection.</p><p>Class-agnostic segmentation supervision Semantic segmentation mentioned above is class-aware, which aims to simultaneously identify specific object categories and the  <ref type="table" target="#tab_1">Table 1</ref>. Ablation experiments of PairNet and TripleNet on the VOC2012-val-seg set. The backbone model is ResNet50 <ref type="bibr" target="#b14">[15]</ref>, and the input image is rescaled to the size of 300 × 300. "MFS" means multiscale fused segmentation, "IC" means inner-connected module, "CAS" means class-aware segmentation, and "ASF" means attention skip-layer fusion.</p><p>background. We argue that class-aware semantic segmentation may ignore the discrimination between objects and the background. Therefore, class-agnostic segmentation supervision module is further added to each layer of the decoder. Specifically, a 3 × 3 convolutional layer is added to generate the logits of class-agnostic semantic segmentation. To generate the ground-truth of class-agnostic semantic segmentation, the objects of different categories are set as one category, and the background is set as another category. Attention skip-layer fusion In Section 3.1, PairNet simply fuses the feature maps of the decoder and the corresponding feature maps of the encoder. Generally, the features from the layer of the encoder have relatively low-level semantic, and that from the layer of decoder have relatively high-level semantic. To enhance informative features and suppress less useful features from the encoder by the features from the decoder, Squeeze-and-Excitation (SE) <ref type="bibr" target="#b15">[16]</ref> block is used. The input of a SE block is the layer of the decoder, and the output of SE block is used to scale the layer of the encoder. After that, the layer of the decoder and the scaled layer of the encoder is concatenated for fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To demonstrate the effectiveness of proposed methods and compare with same state-of-the-art methods, some experiments on the famous VOC 2007 and VOC 2012 datasets <ref type="bibr" target="#b7">[8]</ref> are conducted in this section.</p><p>The PASCAL VOC challenge <ref type="bibr" target="#b7">[8]</ref> has been held annually since 2006, which consists of three principal challenges (i.e., image classification, object detection, and semantic segmentation). Among these annual challenges, the VOC 2007 and VOC 2012 datasets are usually used to evaluate the performance of object detection and semantic segmentation, which have 20 object categories. The VOC 2007 dataset contains 5011 trainval images and 4952 test images. The VOC 2012 dataset is split into three subsets (i.e., train, val, and test). The train set con-tains 5717 images for detection and 1464 images for semantic segmentation (called VOC12-train-seg). The val set contains 5823 images for detection and 1449 images for segmentation (called VOC12-val-seg). The test set contains 10991 images for detection and 1456 for segmentation. To enlarge the training set for semantic segmentation, the additional segmentation data provided by <ref type="bibr" target="#b11">[12]</ref> is used, which contains 10582 training images (called VOC12-trainaug-seg).</p><p>For object detection, mean average precision (i.e., mAP) is used to performance evaluation. On the PASCAL VOC datasets, mAP is calculated under the IoU threshold of 0.5. For semantic segmentation, mean intersection over union (i.e., mIoU) is used for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation experiments on the VOC 2012 dataset</head><p>In this subsection, experiments are conducted on the PASCAL VOC 2012 to validate the effectiveness of proposed method. On the PASCAL VOC 2012, the set of VOC12-trainaug-seg is used for training and the set of VOC12-val-seg is used for performance evaluation, where they have the ground truth of both object detection and semantic segmentation. The input images are rescaled to the size of 300 × 300, and the size of mini-batch is 32. The total number of iteration in the training stage is 40k, where the learning rate of first 25k iterations is 0.0001, that of following 10k iterations is 0.00001, and that of last 5k iterations is 0.000001.</p><p>The top part of <ref type="table" target="#tab_1">Table 1</ref> shows the ablation experiments of PairNet. When all different layers of the decoder are only used for multi-scale object detection (i.e., <ref type="table" target="#tab_1">Table 1</ref>(a)), mAP of object detection is 78.0%. When all different layers of the decoder are used for semantic segmentation (i.e., <ref type="table" target="#tab_1">Table  1</ref>(c)), mIoU of semantic segmentation is 72.5%. When all the different layer of the decoder is used for object detection and semantic segmentation together (i.e., <ref type="figure">Table 1(d)</ref>), mAP and mIoU of PairNet are 78.9% and 73.1%, respectively. Namely, PairNet can improve both object detection and semantic segmentation, which indicates that joint ob-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT of det</head><p>GT of seg only det (  ject detection and semantic segmentation on each layer of the decoder is useful. Meanwhile, the method using all the different layers of the decoder for segmentation (i.e., <ref type="table" target="#tab_1">Table   1</ref>(c)) has better performance than the method only using the last layer of the decoder for segmentation (i.e.,  <ref type="figure" target="#fig_4">Fig. 4(a)</ref>, the examples of detection and segmentation both improved by joint detection and segmentation are given. For example, in the first row, "only det" and "only seg" both miss three potted plant, while PairNet only misses one potted plant and TripleNet does not miss any potted plant. In <ref type="figure" target="#fig_4">Fig. 4(b)</ref>, the examples of detection result improved are shown. For example, in the first row, "only detect" can only detect one ship, PairNet can detect three ships, and TripleNet can detect four ships. In <ref type="figure" target="#fig_4">Fig. 4(c)</ref>, the examples of segmentation results improved are shown. For example, in the second row, "only seg" recognize blue bag as motorbike, but PairNet and TripleNet can recognize the blue bag as background.</p><p>Meanwhile, the proposed PairNet and TripleNet are also compared to the related BlitzNet <ref type="bibr" target="#b6">[7]</ref>. For fair comparison, BltizNet are re-implemented in the similar parameter settings as the proposed PairNet and TripleNet. PairNet which simply joins detection and segmentation in each layer of the decoder has been already comparable with BlitzNet. TripleNet outperforms BlitzNet on both object detection and semantic segmentation, which demonstrates that the proposed method can make full use of the mutual information to improve the two tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-art methods on the VOC2012 test dataset</head><p>In this section, the proposed PairNet is compared with some state-of-the-art methods on the VOC 2007 dataset. Among these methods, SSD <ref type="bibr" target="#b28">[29]</ref>, RON <ref type="bibr" target="#b17">[18]</ref>, DSSD <ref type="bibr" target="#b8">[9]</ref>, DES <ref type="bibr" target="#b46">[47]</ref>, RefineDet <ref type="bibr" target="#b47">[48]</ref>, and DFPR <ref type="bibr" target="#b18">[19]</ref> are only used for object detection, ParseNet <ref type="bibr" target="#b26">[27]</ref>, Deeplab V2 <ref type="bibr" target="#b4">[5]</ref>, DPN <ref type="bibr" target="#b27">[28]</ref>, RefineNet <ref type="bibr" target="#b24">[25]</ref>, PSPNet <ref type="bibr" target="#b48">[49]</ref>, DFPN <ref type="bibr" target="#b44">[45]</ref> are only used for semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with some state-of-the-art methods on the VOC 2007 test dataset</head><p>In this section, the proposed TripleNet and some stateof-the-art methods (i.e., SSD <ref type="bibr" target="#b28">[29]</ref>, DES <ref type="bibr" target="#b46">[47]</ref>, DSSD <ref type="bibr" target="#b8">[9]</ref>, STDN <ref type="bibr" target="#b49">[50]</ref>, BlitzNet <ref type="bibr" target="#b6">[7]</ref>, RefineDet <ref type="bibr" target="#b24">[25]</ref>), and DFPR <ref type="bibr" target="#b18">[19]</ref>   methods are only evaluated on object detection. <ref type="table" target="#tab_7">Table 4</ref> shows mAP of these methods. mAP of TripleNet is 82.7%, which is higher than that of all state-of-the-art methods.</p><formula xml:id="formula_0">- - - - - - - - - - - - - - - - - - - - RefineDet512 [48] VGG16 81.8 - - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed two fully convolutional networks (i.e., PairNet and TripleNet) for joint object detection and semantic segmentation. PairNet simultaneously predicts objects of different scales by different layers and parses pixel semantic labels by all different layers. TripleNet adds four modules (i.e, multiscale fused segmentation, inner-connected module, class-agnostic segmentation supervision, and attention skip-layer fusion) to PairNet. Experiments demonstrate that TripleNet can achieve stateof-the-art performance on both object detection and semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>attention skip-layer fusion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The proposed PairNet for joint object detection and semantic segmentation. (a) The detailed architecture of PairNet. Each layer of the decoder is simultaneously used for detection and segmentation. (b) The skip-layer fusion used in PairNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The proposed TripleNet for joint object detection and semantic segmentation. (a) The detailed architecture of TripleNet. (b) The inner-connected module. (c) attention skip-layer fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of detection or segmentation results of the methods inTable 1(i.e., "only det", "only seg", PairNet, and TripleNet).(a) demonstrates that detection and segmentation can be both improved by PairNet and TripleNet. (b) demonstrates that detection is mainly improved by PairNet or TripleNet. (c) demonstrates that segmentation is mainly improved by PairNet or TripleNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>(a))</cell><cell>only seg (Table 1(b))</cell><cell>Our PairNet (Table 1(d))</cell><cell>Our TripleNet (Table 1(g))</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 Table 2 .Table 1 .</head><label>121</label><figDesc>Comparison of BlitzNet, the proposed PairNet, and the proposed TripleNet. All the methods are re-implemented in the same parameter settings. The first two columns are ground-truth of detection and segmentation. The results of only detection inTable1(a) and only segmentation in Table 1(c) are shown in the third and forth columns. The results of Pair-Net in Table 1(d) and TripleNet in Table 1(g) are shown in fifth to eighth columns. In</figDesc><table><row><cell>(d)).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Results of object detection (mAP) and semantic segmentation (mIoU) on VOC 2012 test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>shows object detect results (mAP) and semantic segmentation results (mIoU) of these methods on th VOC2012 test set. It can been seen most state-of-the-art methods can only output detection results (i.e., SSD, RON, DSSD, DES, RefineDet, and DFPR) or segmentation result (i.e., FCN, ParseNet, DeepLab, DPN, PSPNet, and DFPN). Only BlitzNet and our proposed TripleNet can simultaneously output the results of object detection and semantic segmentation. mAP and mIoU of BlitzNet are 79.0% and 75.6%, while mAP and mIoU of TripleNet are 81.0% and 82.9%. Thus, TripleNet outperforms BlitzNet by 2.0% on object detection and 7.3% on semantic segmentation. It can be also seen that TripleNet almost achieves state-of-the-art performance on both object detection and semantic segmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>are further compared on the VOC 2007 test set. Because only the ground-truth of object detection is provided, these method backbone mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv SSD300 [29] VGG16 77.5 79.5 83.9 76.0 69.6 50.5 87.0 85.7 88.1 60.3 81.5 77.0 86.1 87.5 84.0 79.4 52.3 77.9 79.5 87.6 76.8 SSD512 [29] VGG16 79.5 84.8 85.1 81.5 73.0 57.8 87.8 88.3 87.4 63.5 85.4 73.2 86.2 86.7 83.9 82.5 55.6 81.7 79.0 86.6 80.0 DES300 [47] VGG16 79.7 83.5 86.0 78.1 74.8 53.4 87.9 87.3 88.6 64.0 83.8 77.2 85.9 88.6 87.5 80.8 57.3 80.2 80.4 88.5 79.5 DES512 [47] VGG16 81.7 87.7 86.7 85.2 76.3 60.6 88.7 89.0 88.0 67.0 86.9 78.0 87.2 87.9 87.4 84.4 59.2 86.1 79.2 88.1 80.5 DSSD321 [9] ResNet101 78.6 81.9 84.9 80.5 68.4 53.9 85.6 86.2 88.9 61.1 83.5 78.7 86.7 88.7 86.7 79.7 51.7 78.0 80.9 87.2 79.4 DSSD513 [9] ResNet101 81.5 86.6 86.2 82.6 74.9 62.5 89.0 88.7 88.8 65.2 87.0 78.7 88.2 89.0 87.5 83.7 51.1 86.3 81.6 85.7 83.7 STDN300 [50] DenseNet169 78.1 81.1 86.9 76.4 69.2 52.4 87.7 84.2 88.3 60.2 81.3 77.6 86.6 88.9 87.8 76.8 51.8 78.4 81.3 87.5 77.8 STDN513 [50] DenseNet169 80.9 86.1 89.3 79.5 74.3 61.9 88.5 88.3 89.4 67.4 86.5 79.5 86.4 89.2 88.5 79.3 53.0 77.9 81.4 86.6 85.5 BlitzNet300 [7] ResNet50 79.1 86.7 86.2 78.9 73.1 47.6 85.7 86.1 87.7 59.3 85.1 78.4 86.3 87.9 84.2 79.1 58.5 82.5 81.7 85.7 81.8 BlitzNet512 [7] ResNet50 81.5 87.0 87.6 83.5 75.7 59.1 87.6 88.0 88.8 64.1 88.4 80.9 87.5 88.5 86.9 81.5 60.6 86.5 79.3 87.5 81.7</figDesc><table><row><cell>RefineDet320 [48]</cell><cell>VGG16</cell><cell>80.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>89.3 84.9 79.9 75.6 55.4 88.2 88.6 88.6 63.3 87.9 78.8 87.3 87.7 85.5 80.5 55.4 81.1 79.6 87.8 78.5 DFPR512 [19] ResNet101 82.4 92.0 88.2 81.1 71.2 65.7 88.2 87.9 92.2 65.8 86.5 79.4 90.3 90.4 89.3 88.6 59.4 88.4 75.3 89.2 78.5 TripleNet300 ResNet50 79.3 81.4 85.0 79.5 72.1 53.7 85.3 85.9 87.8 62.5 85.1 78.7 87.8 88.6 85.7 79.5 56.8 80.7 79.2 88.7 81.4 TripleNet512 ResNet50 82.4 88.9 86.9 85.0 77.5 61.5 87.7 88.2 89.2 66.0 88.3 79.6 87.5 88.8 87.3 82.3 62.4 86.1 81.7 89.2 82.4 TripleNet512 ResNet101 82.7 88.9 87.8 83.7 79.6 62.9 87.9 88.3 88.5 67.5 89.1 81.2 88.0 89.5 87.9 83.3 58.7 85.1 83.4 88.8 84.1 Results of object detection (mAP) on the VOC 2007 test set.</figDesc><table><row><cell>DFPR300 [19]</cell><cell>ResNet101</cell><cell>77.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Illuminating pedestrians via simultaneous detection &amp; segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cascade R-CNN: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BlitzNet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep feature pyramid reconguration for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UberNet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graininess-aware deep feature learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ParseNet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DSOD: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DenseASPP for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single-shot object detection with enriched semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scaletransferrable object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
