<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
							<email>ivana.balazevic@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
							<email>carl.allen@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs are structured representations of real world facts. However, they typically contain only a small subset of all possible facts. Link prediction is a task of inferring missing facts based on existing ones. We propose TuckER, a relatively straightforward but powerful linear model based on Tucker decomposition of the binary tensor representation of knowledge graph triples. TuckER outperforms previous state-of-the-art models across standard link prediction datasets, acting as a strong baseline for more elaborate models. We show that TuckER is a fully expressive model, derive sufficient bounds on its embedding dimensionalities and demonstrate that several previously introduced linear models can be viewed as special cases of TuckER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vast amounts of information available in the world can be represented succinctly as entities and relations between them. Knowledge graphs are large, graph-structured databases which store facts in triple form (e s , r, e o ), with e s and e o representing subject and object entities and r a relation. However, far from all available information is currently stored in existing knowledge graphs and manually adding new information is costly, which creates the need for algorithms that are able to automatically infer missing facts.</p><p>Knowledge graphs can be represented as a third-order binary tensor, where each element corresponds to a triple, 1 indicating a true fact and 0 indicating the unknown (either a false or a missing fact). The task of link prediction is to predict whether two entities are related, based on known facts already present in a knowledge graph, i.e. to infer which of the 0 entries in the tensor are indeed false, and which are missing but actually true.</p><p>A large number of approaches to link prediction so far have been linear, based on various methods of factorizing the third-order binary tensor <ref type="bibr" target="#b16">(Nickel et al., 2011;</ref><ref type="bibr" target="#b28">Yang et al., 2015;</ref><ref type="bibr" target="#b25">Trouillon et al., 2016;</ref><ref type="bibr" target="#b9">Kazemi and Poole, 2018)</ref>. Recently, state-of-the-art results have been achieved using non-linear convolutional models <ref type="bibr" target="#b4">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b0">Balažević et al., 2019)</ref>. Despite achieving very good performance, the fundamental problem with deep, non-linear models is that they are nontransparent and poorly understood, as opposed to more mathematically principled and widely studied tensor decomposition models.</p><p>In this paper, we introduce TuckER (E stands for entities, R for relations), a straightforward linear model for link prediction on knowledge graphs, based on Tucker decomposition <ref type="bibr" target="#b27">(Tucker, 1966)</ref> of the binary tensor of triples, acting as a strong baseline for more elaborate models. Tucker decomposition, used widely in machine learning <ref type="bibr" target="#b18">(Schein et al., 2016;</ref><ref type="bibr" target="#b1">Ben-Younes et al., 2017;</ref><ref type="bibr" target="#b30">Yang and Hospedales, 2017)</ref>, factorizes a tensor into a core tensor multiplied by a matrix along each mode. It can be thought of as a form of higherorder SVD in the special case where matrices are orthogonal and the core tensor is "all-orthogonal" <ref type="bibr" target="#b12">(Kroonenberg and De Leeuw, 1980)</ref>. In our case, rows of the matrices contain entity and relation embeddings, while entries of the core tensor determine the level of interaction between them. Subject and object entity embedding matrices are assumed equivalent, i.e. we make no distinction between the embeddings of an entity depending on whether it appears as a subject or as an object in a particular triple. Due to the low rank of the core tensor, TuckER benefits from multi-task learning by parameter sharing across relations. A link prediction model should have enough expressive power to represent all relation types (e.g. symmetric, asymmetric, transitive). We thus show that TuckER is fully expressive, i.e. given any ground truth over the triples, there exists an assignment of values to the entity and relation embeddings that accurately separates the true triples from false ones. We also derive a dimensionality bound which guarantees full expressiveness.</p><p>Finally, we show that several previous stateof-the-art linear models, RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref>, ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> and <ref type="bibr">SimplE (Kazemi and Poole, 2018)</ref>, are special cases of TuckER.</p><p>In summary, key contributions of this paper are:</p><p>• proposing TuckER, a new linear model for link prediction on knowledge graphs, that is simple, expressive and achieves state-of-theart results across all standard datasets; • proving that TuckER is fully expressive and deriving a bound on the embedding dimensionality for full expressiveness; and • showing how TuckER subsumes several previously proposed tensor factorization approaches to link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several linear models for link prediction have previously been proposed: RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref> optimizes a scoring function containing a bilinear product between subject and object entity vectors and a full rank relation matrix. Although a very expressive and powerful model, RESCAL is prone to overfitting due to its large number of parameters, which increases quadratically in the embedding dimension with the number of relations in a knowledge graph.</p><p>DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref> is a special case of RESCAL with a diagonal matrix per relation, which reduces overfitting. However, the linear transformation performed on entity embedding vectors in DistMult is limited to a stretch.  <ref type="bibr">cock, 1927)</ref>, in which subject and object entity embeddings for the same entity are independent (note that DistMult is a special case of CP). Sim-plE's scoring function alters CP to make subject and object entity embedding vectors dependent on each other by computing the average of two terms, first of which is a bilinear product of the subject entity head embedding, relation embedding and object entity tail embedding and the second is a bilinear product of the object entity head embedding, inverse relation embedding and subject entity tail embedding.</p><p>Recently, state-of-the-art results have been achieved with non-linear models: ConvE <ref type="bibr" target="#b4">(Dettmers et al., 2018)</ref> performs a global 2D convolution operation on the subject entity and relation embedding vectors, after they are reshaped to matrices and concatenated. The obtained feature maps are flattened, transformed through a linear layer, and the inner product is taken with all object entity vectors to generate a score for each triple. Whilst results achieved by ConvE are impressive, its reshaping and concatenating of vectors as well as using 2D convolution on word embeddings is unintuitive. HypER <ref type="bibr" target="#b0">(Balažević et al., 2019)</ref> is a simplified convolutional model, that uses a hypernetwork to generate 1D convolutional filters for each relation, extracting relation-specific features from subject entity embeddings. The authors show that convolution is a way of introducing sparsity and parameter tying and that HypER can be understood in terms of tensor factorization up to a non-linearity, thus placing HypER closer to the well established family of factorization models. The drawback of HypER is that it sets most elements of the core weight tensor to 0, which amounts to hard regularization, rather than letting the model learn which parameters to use via soft regularization.</p><p>Scoring functions of all models described above and TuckER are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Let E denote the set of all entities and R the set of all relations present in a knowledge graph. A triple is represented as (e s , r, e o ), with e s , e o ∈ E denoting subject and object entities respectively and r ∈ R the relation between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Link Prediction</head><p>In link prediction, we are given a subset of all true triples and the aim is to learn a scoring function φ  <ref type="bibr" target="#b28">(Yang et al., 2015)</ref> e s , w r , e o w r ∈ R de O(n e d e + n r d e ) ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> Re( e s , w r , e o ) w r ∈ C de O(n e d e + n r d e ) ConvE <ref type="bibr" target="#b4">(Dettmers et al., 2018)</ref> f (vec(f ([e s ; w r ] * w))W)e o w r ∈ R dr O(n e d e + n r d r ) SimplE (Kazemi and Poole, 2018) 1 2 ( h es , w r , t eo + h eo , w r −1 , t es ) w r ∈ R de O(n e d e + n r d e ) HypER <ref type="bibr" target="#b0">(Balažević et al., 2019)</ref> f (vec(e s * vec −1 (w r H))W)e o w r ∈ R dr O(n e d e + n r d r ) TuckER (ours) W × 1 e s × 2 w r × 3 e o w r ∈ R dr O(n e d e + n r d r ) <ref type="table">Table 1</ref>: Scoring functions of state-of-the-art link prediction models, the dimensionality of their relation parameters, and significant terms of their space complexity. d e and d r are the dimensionalities of entity and relation embeddings, while n e and n r denote the number of entities and relations respectively. e o ∈ C de is the complex conjugate of e o , e s , w r ∈ R dw×d h denote a 2D reshaping of e s and w r respectively, h es , t es ∈ R de are the head and tail entity embedding of entity e s , and w r −1 ∈ R dr is the embedding of relation r −1 (which is the inverse of relation r). * is the convolution operator, · denotes the dot product and × n denotes the tensor product along the n-th mode, f is a non-linear function, and W ∈ R de×de×dr is the core tensor of a Tucker decomposition.</p><p>that assigns a score s = φ(e s , r, e o ) ∈ R which indicates whether a triple is true, with the ultimate goal of being able to correctly score all missing triples. The scoring function is either a specific form of tensor factorization in the case of linear models or a more complex (deep) neural network architecture for non-linear models. Typically, a positive score for a particular triple indicates a true fact predicted by the model, while a negative score indicates a false one. With most recent models, a non-linearity such as the logistic sigmoid function is typically applied to the score to give a corresponding probability prediction p = σ(s) ∈ [0, 1] as to whether a certain fact is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tucker Decomposition</head><p>Tucker decomposition, named after Ledyard R. Tucker <ref type="bibr" target="#b26">(Tucker, 1964)</ref>, decomposes a tensor into a set of matrices and a smaller core tensor. In a three-mode case, given the original tensor X ∈ R I×J×K , Tucker decomposition outputs a tensor Z ∈ R P ×Q×R and three matrices A ∈ R I×P ,</p><formula xml:id="formula_0">B ∈ R J×Q , C ∈ R K×R : X ≈ Z × 1 A × 2 B × 3 C,<label>(1)</label></formula><p>with × n indicating the tensor product along the nth mode. Factor matrices A, B and C, when orthogonal, can be thought of as the principal components in each mode. Elements of the core tensor Z show the level of interaction between the different components. Typically, P , Q, R are smaller than I, J, K respectively, so Z can be thought of as a compressed version of X . Tucker decomposition is not unique, i.e. we can transform Z without affecting the fit if we apply the inverse transformation to A, B and C (Kolda and Bader, 2009).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tucker Decomposition for Link Prediction</head><p>We propose a model that uses Tucker decomposition for link prediction on the binary tensor representation of a knowledge graph, with entity embedding matrix E that is equivalent for subject and object entities, i.e. E = A = C ∈ R ne×de and relation embedding matrix R = B ∈ R nr×dr , where n e and n r represent the number of entities and relations and d e and d r the dimensionality of entity and relation embedding vectors. We define the scoring function for TuckER as:</p><formula xml:id="formula_1">φ(e s , r, e o ) = W × 1 e s × 2 w r × 3 e o ,<label>(2)</label></formula><p>where e s , e o ∈ R de are the rows of E representing the subject and object entity embedding vectors, w r ∈ R dr the rows of R representing the relation embedding vector and W ∈ R de×dr×de is the core tensor. We apply logistic sigmoid to each score φ(e s , r, e o ) to obtain the predicted probability p of a triple being true. Visualization of the TuckER architecture can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. As proven in Section 5.1, TuckER is fully expressive. Further, its number of parameters increases linearly with respect to entity and relation embedding dimensionality d e and d r , as the number of entities and relations increases, since the number of parameters of W depends only on the entity and relation embedding dimensionality and not on the number of entities or relations. By having the core tensor W, unlike simpler models such as DistMult, Com-plEx and SimplE, TuckER does not encode all the learned knowledge into the embeddings; some is stored in the core tensor and shared between all entities and relations through multi-task learning.</p><p>Rather than learning distinct relation-specific matrices, the core tensor of TuckER can be viewed as containing a shared pool of "prototype" relation matrices, which are linearly combined according to the parameters in each relation embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>Since the logistic sigmoid is applied to the scoring function to approximate the true binary tensor, the implicit underlying tensor is comprised of −∞ and ∞. Given this prevents an explicit analytical factorization, we use numerical methods to train TuckER. We use the standard data augmentation technique, first used by <ref type="bibr" target="#b4">Dettmers et al. (2018)</ref> and formally described by <ref type="bibr" target="#b13">Lacroix et al. (2018)</ref> </p><formula xml:id="formula_2">L = − 1 ne ne i=1 (y (i) log(p (i) ) + (1 − y (i) )log(1 − p (i) )),</formula><p>(3) where p ∈ R ne is the vector of predicted probabilities and y ∈ R ne is the binary label vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Full Expressiveness and Embedding Dimensionality</head><p>A tensor factorization model is fully expressive if for any ground truth over all entities and relations, there exist entity and relation embeddings that accurately separate true triples from the false. As shown in <ref type="bibr" target="#b24">(Trouillon et al., 2017)</ref>, ComplEx is fully expressive with the embedding dimensionality bound d e = d r = n e · n r . Similarly to Com-plEx, <ref type="bibr" target="#b9">Kazemi and Poole (2018)</ref> show that SimplE is fully expressive with entity and relation embeddings of size d e = d r = min(n e · n r , γ + 1), where γ represents the number of true facts. They further prove other models are not fully expressive: DistMult, because it cannot model asymmetric relations; and transitive models such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> and its variants FTransE <ref type="bibr" target="#b6">(Feng et al., 2016)</ref> and <ref type="bibr">STransE (Nguyen et al., 2016)</ref>, because of certain contradictions that they impose between different relation types. By Theorem 1, we establish the bound on entity and relation embedding dimensionality (i.e. decomposition rank) that guarantees full expressiveness of TuckER.</p><p>Theorem 1. Given any ground truth over a set of entities E and relations R, there exists a TuckER model with entity embeddings of dimensionality d e = n e and relation embeddings of dimensionality d r = n r , where n e = |E| is the number of entities and n r = |R| the number of relations, that accurately represents that ground truth.</p><p>Proof. Let e s and e o be the n e -dimensional onehot binary vector representations of subject and object entities e s and e o respectively and w r the n r -dimensional one-hot binary vector representation of relation r. For each subject entity e (i) s , relation r (j) and object entity e (k) o , we let the i-th, j-th and k-th element respectively of the corresponding vectors e s , w r and e o be 1 and all other elements 0. Further, we set the ijk element of the tensor W ∈ R ne×nr×ne to 1 if the fact (e s , r, e o ) holds and -1 otherwise. Thus the product of the entity embeddings and the relation embedding with the core tensor, after applying the logistic sigmoid, accurately represents the original tensor.</p><p>The purpose of Theorem 1 is to prove that TuckER is capable of potentially capturing all information (and noise) in the data. In practice however, we expect the embedding dimensionalities needed for full reconstruction of the underlying binary tensor to be much smaller than the bound stated above, since the assignment of values to the tensor is not random but follows a certain structure, otherwise nothing unknown could be predicted. Even more so, low decomposition rank is actually a desired property of any bilin- ear link prediction model, forcing it to learn that structure and generalize to new data, rather than simply memorizing the input. In general, we expect TuckER to perform better than ComplEx and SimplE with embeddings of lower dimensionality due to parameter sharing in the core tensor (shown empirically in Section 6.4), which could be of importance for efficiency in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relation to Previous Linear Models</head><p>Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL (Nickel et al., 2011) Following the notation introduced in Section 3.2, the RESCAL scoring function (see <ref type="table">Table 1</ref>) has the form:</p><formula xml:id="formula_3">X ≈ Z × 1 A × 3 C.</formula><p>(4) This corresponds to Equation 1 with I = K = n e , P = R = d e , Q = J = n r and B = I J the J × J identity matrix. This is also known as Tucker2 decomposition (Kolda and Bader, 2009). As is the case with TuckER, the entity embedding matrix of RESCAL is shared between subject and object entities, i.e. E = A = C ∈ R ne×de and the relation matrices W r ∈ R de×de are the d e × d e slices of the core tensor Z. As mentioned in Section 2, the drawback of RESCAL compared to TuckER is that its number of parameters grows quadratically in the entity embedding dimension d e as the number of relations increases. DistMult <ref type="bibr" target="#b28">(Yang et al., 2015)</ref> The scoring function of DistMult (see <ref type="table">Table 1</ref>) can be viewed as equivalent to that of TuckER (see Equation 1) with a core tensor Z ∈ R P ×Q×R , P = Q = R = d e , which is superdiagonal with 1s on the superdiagonal, i.e. all elements z pqr with p = q = r are 1 and all the other elements are 0 (as shown in <ref type="figure">Figure 2a</ref>). Rows of E = A = C ∈ R ne×de contain subject and object entity embedding vectors e s , e o ∈ R de and rows of R = B ∈ R nr×de contain relation embedding vectors w r ∈ R de . It is interesting to note that the TuckER interpretation of the DistMult scoring function, given that matrices A and C are identical, can alternatively be interpreted as a special case of CP decomposition <ref type="bibr" target="#b7">(Hitchcock, 1927)</ref>, since Tucker decomposition with a superdiagonal core tensor is equivalent to CP decomposition. Due to enforced symmetry in subject and object entity mode, DistMult cannot learn to represent asymmetric relations.</p><p>ComplEx <ref type="bibr" target="#b25">(Trouillon et al., 2016)</ref> Bilinear models represent subject and object entity embeddings as vectors e s , e o ∈ R de , relation as a matrix W r ∈ R de×de and the scoring function as a bilinear product φ(e s , r, e o ) = e s W r e o . It is trivial to show that both RESCAL and DistMult belong to the family of bilinear models. As explained by Kazemi and Poole (2018), ComplEx can be considered a bilinear model with the real and imaginary part of an embedding for each entity concatenated in a single vector, [Re(e s ); Im(e s )] ∈ R 2de for subject, [Re(e o ); Im(e o )] ∈ R 2de for object, and a relation matrix W r ∈ R 2de×2de , constrained so that its leading diagonal contains duplicated elements of Re(w r ), its d e -diagonal elements of Im(w r ) and its -d e -diagonal elements of -Im(w r ), with all other elements set to 0, where d e and -d e represent offsets from the leading diagonal.</p><p>Similarly to DistMult, we can regard the scoring function of ComplEx (see <ref type="table">Table 1</ref>) as equivalent to the scoring function of TuckER (see Equation 1), with core tensor Z ∈ R P ×Q×R , P = Q = R = 2d e , where 3d e elements on different tensor diagonals are set to 1, d e elements on one tensor diagonal are set to -1 and all other elements are set to 0 (see <ref type="figure">Figure 2b</ref>). This shows that the scoring function of ComplEx, which computes a bilinear product with complex entity and relation embeddings and disregards the imaginary part of the obtained result, is equivalent to a hard regularization of the core tensor of TuckER in the real domain.</p><p>SimplE <ref type="bibr" target="#b9">(Kazemi and Poole, 2018)</ref> The authors show that SimplE belongs to the family of bilinear models by concatenating embeddings for head and tail entities for both subject and object into vectors [h es ; t es ] ∈ R 2de and [h eo ; t eo ] ∈ R 2de and constraining the relation matrix W r ∈ R 2de×2de so that it contains the relation embedding vector 1 2 w r on its d e -diagonal and the inverse relation embedding vector 1 2 w r −1 on its -d e -diagonal and 0s elsewhere. The SimplE scoring function (see <ref type="table">Table 1</ref>) is therefore equivalent to that of TuckER (see Equation 1), with core tensor Z ∈ R P ×Q×R , P = Q = R = 2d e , where 2d e elements on two tensor diagonals are set to 1 2 and all other elements are set to 0 (see <ref type="figure">Figure 2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Representing Asymmetric Relations</head><p>Each relation in a knowledge graph can be characterized by a certain set of properties, such as symmetry, reflexivity, transitivity. So far, there have been two possible ways in which linear link prediction models introduce asymmetry into factorization of the binary tensor of triples:</p><p>• distinct (although possibly related) embeddings for subject and object entities and a diagonal matrix (or equivalently a vector) for each relation, as is the case with models such as ComplEx and SimplE; or</p><p>• equivalent subject and object entity embeddings and each relation represented by a full rank matrix, which is the case with RESCAL.</p><p>The latter approach appears more intuitive, since asymmetry is a property of the relation, rather than the entities. However, the drawback of the latter approach is quadratic growth of parameter number with the number of relations, which often leads to overfitting, especially for relations with a small number of training triples. TuckER overcomes this by representing relations as vectors w r , which makes the parameter number grow linearly with the number of relations, while still keeping the desirable property of allowing relations to be asymmetric by having an asymmetric relation-agnostic core tensor W, rather than encoding the relation-specific information in the entity embeddings. Multiplying W ∈ R de×dr×de with w r ∈ R dr along the second mode, we obtain a full rank relation-specific matrix W r ∈ R de×de , which can perform all possible linear transformations on the entity embeddings, i.e. rotation, reflection or stretch, and is thus also capable of modeling asymmetry. Regardless of what kind of transformation is needed for modeling a particular relation, TuckER can learn it from the data.</p><p>To demonstrate this, we show sample heatmaps of learned relation matrices W r for a WordNet symmetric relation "derivationally related form" and an asymmetric relation "hypernym" in <ref type="figure" target="#fig_1">Figure 3</ref>, where one can see that TuckER learns to model the symmetric relation with the relation matrix that is approximately symmetric about the main diagonal, whereas the matrix belonging to the asymmetric relation exhibits no obvious structure. 6 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We evaluate TuckER using four standard link prediction datasets (see <ref type="table" target="#tab_5">Table 2</ref>):</p><p>FB15k <ref type="bibr" target="#b2">(Bordes et al., 2013</ref>) is a subset of Freebase, a large database of real world facts. FB15k-237 <ref type="bibr" target="#b23">(Toutanova et al., 2015)</ref> was created from FB15k by removing the inverse of many relations that are present in the training set from validation and test sets, making it more difficult for simple models to do well. WN18 <ref type="figure" target="#fig_0">(Bordes et al., 2013)</ref> is a subset of Word-Net, a hierarchical database containing lexical relations between words. WN18RR <ref type="figure" target="#fig_0">(Dettmers et al., 2018)</ref> is a subset of WN18, created by removing the inverse relations from validation and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implementation and Experiments</head><p>We implement TuckER in PyTorch <ref type="bibr" target="#b17">(Paszke et al., 2017)</ref> and make our code available on GitHub. <ref type="bibr">1</ref> We choose all hyper-parameters by random search based on validation set performance. For FB15k and FB15k-237, we set entity and relation embedding dimensionality to d e = d r = 200. For WN18 and WN18RR, which both contain a significantly smaller number of relations relative to the number of entities as well as a small number of relations compared to FB15k and FB15k-237, we set d e = 200 and d r = 30. We use batch normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy, 2015)</ref> and dropout <ref type="bibr" target="#b21">(Srivastava et al., 2014)</ref> to speed up training. We find that lower dropout values (0.1, 0.2) are required for datasets with a higher number of training triples per relation and thus less risk of overfitting (WN18 and WN18RR), whereas higher dropout values (0.3, 0.4, 0.5) are required for FB15k and FB15k-237. We choose the learning rate from {0.01, 0.005, 0.003, 0.001, 0.0005} and learning rate decay from {1, 0.995, 0.99}. We find the following combinations of learning rate and learning rate decay to give the best results: (0.003, 0.99) for FB15k, (0.0005, 1.0) for FB15k-237, (0.005, 0.995) for WN18 and (0.01, 1.0) for WN18RR (see <ref type="table" target="#tab_9">Table 5</ref> in the Appendix A for a complete list of hyper-parameter values on each dataset). We train the model using Adam <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref> with the batch size 128.</p><p>At evaluation time, for each test triple we generate n e candidate triples by combining the test entity-relation pair with all possible entities E, ranking the scores obtained. We use the filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, i.e. all known true triples are removed from the candidate set except for the current test triple. We use evaluation metrics standard across the link prediction literature: mean reciprocal rank (MRR) and hits@k, k ∈ {1, 3, 10}. Mean reciprocal rank is the average of the inverse of the mean rank assigned to the true triple over all candidate triples. Hits@k measures the percentage of times a true triple is ranked within the top k candidate triples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Link Prediction Results</head><p>Link prediction results on all datasets are shown in <ref type="table" target="#tab_7">Tables 3 and 4</ref>. Overall, TuckER outperforms previous state-of-the-art models on all metrics across all datasets (apart from hits@10 on WN18 where a non-linear model, R-GCN, does better). Results achieved by TuckER are not only better than those of other linear models, such as DistMult, ComplEx and SimplE, but also better than the results of many more complex deep neural network and reinforcement learning architectures, e.g. R-GCN, MINERVA, ConvE and HypER, demonstrating the expressive power of linear models and supporting our claim that simple linear models should serve as a baseline before moving onto more elaborate models. Even with fewer parameters than ComplEx and SimplE at d e = 200 and d r = 30 on WN18RR (∼9.4 vs ∼16.4 million), TuckER consistently obtains better results than any of those models. We believe this is because TuckER exploits knowledge sharing between relations through the core tensor, i.e. multi-task learning. This is supported by the fact that the margin by which TuckER outperforms other linear models is notably increased on datasets with a large number of relations. For example, improvement on FB15k is +14% over ComplEx and +8% over SimplE on the toughest hits@1 metric. To our knowledge, ComplEx-N3 <ref type="bibr" target="#b13">(Lacroix et al., 2018)</ref> is the only other linear link prediction model that benefits from multitask learning. There, rank regularization of the embedding matrices is used to encourage a lowrank factorization, thus forcing parameter sharing between relations. We do not include their published results in <ref type="table" target="#tab_7">Tables 3 and 4</ref>, since they use the highly non-standard d e = d r = 2000 and thus a far larger parameter number (18x more parameters than TuckER on WN18RR; 5.5x on FB15k-237), making their results incomparable to those typically reported, including our own. However, running their model with equivalent parameter number to TuckER shows comparable performance, supporting our belief that the two models both attain the benefits of multi-task learning, although by different means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Influence of Parameter Sharing</head><p>The ability of knowledge sharing through the core tensor suggests that TuckER should need a lower number of parameters for obtaining good results than ComplEx or SimplE. To test this, we re-implement ComplEx and SimplE with reciprocal relations, 1-N scoring, batch normalization and dropout for fair comparison, perform random search to choose best hyper-parameters   (see <ref type="table" target="#tab_10">Table 6</ref> in the Appendix A for exact hyperparameter values used) and train all three models on FB15k-237 with embedding sizes d e = d r ∈ {20, 50, 100, 200}. <ref type="figure" target="#fig_3">Figure 4</ref> shows the obtained MRR on the test set for each model. It is important to note that at embedding dimensionalities 20, 50 and 100, TuckER has fewer parameters than Com-plEx and SimplE (e.g. ComplEx and SimplE have ∼3 million and TuckER has ∼2.5 million parameters for embedding dimensionality 100).  We can see that the difference between the MRRs of ComplEx, SimplE and TuckER is approximately constant for embedding sizes 100 and 200. However, for lower embedding sizes, the dif-ference between MRRs increases by 0.7% for embedding size 50 and by 4.2% for embedding size 20 for ComplEx and by 3% for embedding size 50 and by 9.9% for embedding size 20 for Sim-plE. At embedding size 20 (∼300k parameters), the performance of TuckER is almost as good as the performance of ComplEx and SimplE at embedding size 200 (∼6 million parameters), which supports our initial assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we introduce TuckER, a relatively straightforward linear model for link prediction on knowledge graphs, based on the Tucker decomposition of a binary tensor of known facts. TuckER achieves state-of-the-art results on standard link prediction datasets, in part due to its ability to perform multi-task learning across relations. Whilst being fully expressive, TuckER's number of parameters grows linearly with respect to the number of entities or relations in the knowledge graph. We further show that previous linear state-of-the-art models, RESCAL, DistMult, ComplEx and Sim-plE, can be interpreted as special cases of our model. Future work might include exploring how to incorporate background knowledge on individual relation properties into the existing model. <ref type="table" target="#tab_9">Table 5</ref> shows best performing hyper-parameter values for TuckER across all datasets, where lr denotes learning rate, dr decay rate, ls label smoothing, and d#k, k ∈ {1, 2, 3} dropout values applied on the subject entity embedding, relation matrix and subject entity embedding after it has been transformed by the relation matrix respectively.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the TuckER architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Learned relation matrices for a symmetric (derivationally related form) and an asymmetric (hypernym) WN18RR relation. W derivationally related form is approximately symmetric about the leading diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>MRR for ComplEx, SimplE and TuckER for different embeddings sizes on FB15k-237.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, of adding reciprocal relations for every triple in the dataset, i.e. we add (e o , r −1 , e s ) for every (e s , r, e o ). Following the training procedure introduced by<ref type="bibr" target="#b4">Dettmers et al. (2018)</ref> to speed up training, we use 1-N scoring, i.e. we simultaneously score entity-relation pairs (e s , r) and (e o , r −1 ) with all entities e o ∈ E and e s ∈ E respectively, in contrast to 1-1 scoring, where individual triples (e s , r, e o ) and (e o , r −1 , e s ) are trained one at a time. The model is trained to minimize the Bernoulli negative log-likelihood loss function. A component of the loss for one entity-relation pair with all others entities is defined as:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Constraints imposed on the values of core tensor Z ∈ R de×de×de for DistMult and Z ∈ R 2de×2de×2de for ComplEx and SimplE. Elements that are set to 0 are represented in white.</figDesc><table><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1 1</cell><cell>1 1</cell><cell>1 1 1 1 1 1 1 −1 −1 1 1 −1 1 −1 1 −1 1 −1</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2</cell><cell>1 2 2 1</cell><cell>2 1</cell><cell>2 1</cell><cell>2 1</cell><cell>2 1</cell><cell>1 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">(a) DistMult</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) ComplEx</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) SimplE</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Link prediction results on WN18RR and FB15k-237. The RotatE<ref type="bibr" target="#b22">(Sun et al., 2019)</ref> results are reported without their self-adversarial negative sampling (see Appendix H in the original paper) for fair comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>WN18</cell><cell></cell><cell></cell><cell></cell><cell>FB15k</cell><cell></cell></row><row><cell></cell><cell cols="5">Linear MRR Hits@10 Hits@3 Hits@1</cell><cell cols="4">MRR Hits@10 Hits@3 Hits@1</cell></row><row><cell>TransE (Bordes et al., 2013)</cell><cell>no</cell><cell>−</cell><cell>.892</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>.471</cell><cell>−</cell><cell>−</cell></row><row><cell>DistMult (Yang et al., 2015)</cell><cell>yes</cell><cell>.822</cell><cell>.936</cell><cell>.914</cell><cell>.728</cell><cell>.654</cell><cell>.824</cell><cell>.733</cell><cell>.546</cell></row><row><cell>ComplEx (Trouillon et al., 2016)</cell><cell>yes</cell><cell>.941</cell><cell>.947</cell><cell>.936</cell><cell>.936</cell><cell>.692</cell><cell>.840</cell><cell>.759</cell><cell>.599</cell></row><row><cell>ANALOGY (Liu et al., 2017)</cell><cell>yes</cell><cell>.942</cell><cell>.947</cell><cell>.944</cell><cell>.939</cell><cell>.725</cell><cell>.854</cell><cell>.785</cell><cell>.646</cell></row><row><cell>Neural LP (Yang et al., 2017)</cell><cell>no</cell><cell>.940</cell><cell>.945</cell><cell>−</cell><cell>−</cell><cell>.760</cell><cell>.837</cell><cell>−</cell><cell>−</cell></row><row><cell>R-GCN (Schlichtkrull et al., 2018)</cell><cell>no</cell><cell>.819</cell><cell>.964</cell><cell>.929</cell><cell>.697</cell><cell>.696</cell><cell>.842</cell><cell>.760</cell><cell>.601</cell></row><row><cell>TorusE (Ebisu and Ichise, 2018)</cell><cell>no</cell><cell>.947</cell><cell>.954</cell><cell>.950</cell><cell>.943</cell><cell>.733</cell><cell>.832</cell><cell>.771</cell><cell>.674</cell></row><row><cell>ConvE (Dettmers et al., 2018)</cell><cell>no</cell><cell>.943</cell><cell>.956</cell><cell>.946</cell><cell>.935</cell><cell>.657</cell><cell>.831</cell><cell>.723</cell><cell>.558</cell></row><row><cell>HypER (Balažević et al., 2019)</cell><cell>no</cell><cell>.951</cell><cell>958</cell><cell>.955</cell><cell>.947</cell><cell>.790</cell><cell>.885</cell><cell>.829</cell><cell>.734</cell></row><row><cell>SimplE (Kazemi and Poole, 2018)</cell><cell>yes</cell><cell>.942</cell><cell>.947</cell><cell>.944</cell><cell>.939</cell><cell>.727</cell><cell>.838</cell><cell>.773</cell><cell>.660</cell></row><row><cell>TuckER (ours)</cell><cell>yes</cell><cell>.953</cell><cell>.958</cell><cell>.955</cell><cell>.949</cell><cell>.795</cell><cell>.892</cell><cell>.833</cell><cell>.741</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Link prediction results on WN18 and FB15k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Best performing hyper-parameter values for TuckER across all datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>shows best performing hyper-parameter values for ComplEx and SimplE on FB15k-237, used to produce the result in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Best performing hyper-parameter values for ComplEx and SimplE on FB15k-237.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ibalazevic/TuckER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Ivana Balažević and Carl Allen were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hypernetwork Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MUTAN: Multimodal Tucker Fusion for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multirelational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Go for a Walk and Arrive at the Answer: Reasoning over Paths in Knowledge Bases Using Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TorusE: Knowledge Graph Embedding on a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Flexible Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Expression of a Tensor or a Polyadic as a Sum of Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sim-plE Embedding for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
		<title level="m">Tensor Decompositions and Applications. SIAM review</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="455" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Principal Component Analysis of Three-Mode Data by Means of Alternating Least Squares Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pieter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">De</forename><surname>Kroonenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="97" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Canonical Tensor Decomposition for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analogical Inference for Multi-relational Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">STransE: a Novel Embedding Model of Entities and Relationships in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge Graph Completion via Complex Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Christopher R Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4735" to="4772" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Extension of Factor Analysis to Three-Dimensional Matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to Mathematical Psychology</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="page">110119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some Mathematical Notes on Three-Mode Factor Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Multi-task Representation Learning: A Tensor Factorisation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
