<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Training for Improved Out-of-Distribution Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Winkens</surname></persName>
							<email>jimwinkens@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><forename type="middle">Guha</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stanforth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Macwilliams</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kohl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylan</forename><surname>Cemgil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M Ali</forename><surname>Eslami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
							<email>olafr@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Health</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Contrastive Training for Improved Out-of-Distribution Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reliable detection of out-of-distribution (OOD) inputs is increasingly understood to be a precondition for deployment of machine learning systems. This paper proposes and investigates the use of contrastive training to boost OOD detection performance. Unlike leading methods for OOD detection, our approach does not require access to examples labeled explicitly as OOD, which can be difficult to collect in practice. We show in extensive experiments that contrastive training significantly helps OOD detection performance on a number of common benchmarks. By introducing and employing the Confusion Log Probability (CLP) score, which quantifies the difficulty of the OOD detection task by capturing the similarity of inlier and outlier datasets, we show that our method especially improves performance in the 'near OOD' classes -a particularly challenging setting for previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The classes are sorted by increasing similarity to the inlier classes as given by the class-wise confusion log probability (CLP), see Eq. 5. Contrastive training improves OOD detection results across the board, particularly in the near OOD regime, where the outlier and inlier classes are highly similar. CIFAR-10 contains classes similar to leopard (e.g. dog, cat) but none similar to oak tree or orange.</p><p>A well-trained deep neural network f that obtains high accuracy on its test set can still make arbitrarily bad predictions when exposed to inputs drawn from an unfamiliar distribution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. This poses a significant obstacle for real world deployment, where it is typically either prohibitively expensive or outright impossible to ensure that the network is only ever exposed to data from the training distribution.</p><p>In safety-critical applications, e.g. in medical diagnosis, it would be preferable to detect inputs unfamiliar to the trained network for separate processing (for instance by a human expert), than to make potentially inaccurate predictions using the machine learning system. This problem is known as out-of-distribution (OOD) detection, open set recognition, or anomaly detection by different research communities.</p><p>Out-of-distribution detection can be performed by approximating a probability density p(x) of training inputs x, and detecting test-time OOD inputs using a threshold γ: if p(x) &lt; γ then x is considered OOD. The surprising finding of <ref type="figure">Figure 2</ref>: We show s(z) = log p(z) of a network trained to distinguish between two classes, where z is its penultimate activation. Left: Supervised training of z alone may discard input dimensions that are unnecessary for classification but necessary for OOD detection. Right: Contrastive training makes z also be sensitive to the x 2 dimension. Nalisnick et al. <ref type="bibr" target="#b24">[25]</ref> was that even powerful neural generative models trained to estimate p(x) (e.g. on CIFAR-10 images) can perform poorly at OOD detection, often assigning higher probabilities to out-of-distribution test examples (Street View House Numbers) than to in-distribution test examples.</p><p>Modern OOD detection techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> instead assign a scalar score s(z) (e.g. via an approximated probability density) to activations z in an intermediate feature space of a discriminatively trained classifier f , and use that to detect OOD inputs. The success of these approaches highly depends on the quality of the intermediate feature space defined by f . If the feature space is not sufficiently rich, the network may be blind to properties of the image that turn out to be necessary for detection of OOD inputs. Consider, for instance, the case of visual inputs. Variation in captured images is either due to semantic differences of the objects (e.g. pose, shape, texture), or due to differences in the imaging process (e.g. lighting, camera position). Depending on the application, an unfamiliar variation of either type could lead to an input being deemed out-of-distribution. We therefore desire intermediate feature spaces defined by f that capture as many semantic properties as possible, whilst also remaining sensitive to properties of the imaging process.</p><p>Supervised learning produces semantic representations, but only to the extent that those representations discriminate between classes labeled in the dataset. The network f is not incentivized to learn features (semantic or otherwise) beyond the bare minimum necessary to classify. That is why current state-of-the-art approaches to OOD detection enrich the intermediate feature space beyond what would ordinarily be learned via only supervised learning on the inlier dataset, for instance by exploiting examples from an outlier dataset (Outlier Exposure, OE, <ref type="bibr" target="#b11">[12]</ref>), or with self-supervised losses (Rotation Prediction, RP, <ref type="bibr" target="#b12">[13]</ref>). To do this, however, OE requires access to examples labeled explicitly as OOD (which can be difficult to collect in practice), and RP relies on the assumption that an unrelated auxiliary task will produce beneficial representations, which may not always hold.</p><p>The key idea of this paper is to encourage f to learn as many high-level, task-agnostic, semantic features as possible from the in-distribution dataset, so as to enable it to detect any kind of out-ofdistribution input at test time. We note recently introduced contrastive training techniques such as SimCLR <ref type="bibr" target="#b2">[3]</ref> as performant and well-motivated approaches to this end. Using a set of class-preserving transformations, SimCLR introduces a loss that pulls transformed versions of the same image closer to each other, whilst pushing all other images away. This incentivizes the model to learn features that discriminate between all dataset images, even if they belong to the same class. When combined with supervised training, f learns features that are both rich and semantically discriminative. <ref type="figure">Figure 2</ref> demonstrates this idea on a toy example, where we aim to classify points in a 2-dimensional input space (x 1 , x 2 ). The two classes can be distinguished by the first dimension x 1 alone. With only supervised training, f has no incentive to be sensitive to the x 2 dimension, making OOD detection using z impossible. Contrastive training, however, shapes z to remain sensitive to both dimensions and makes OOD detection possible. We show in extensive experiments that this approach scales to high-dimensional problems and consistently improves performance.</p><p>An additional difficulty of OOD detection lies in the evaluation of methods. Quantitative evaluation requires specification of an 'outlier' dataset at test time, which is itself a subjective design choice, as the notion of 'out-of-distribution' is task dependent. We therefore distinguish between near OOD regimes where inlier and outlier distributions are meaningfully similar, and far OOD regimes where the two are unrelated. Near OOD is encountered more often in practice, e.g. a system that detects medical pathologies will often encounter patients with atypical combinations of pathologies (near OOD) and will have to be reliable nonetheless. A completely broken sensor (far OOD) is less prevalent by comparison. We therefore advocate for quantification of the 'similarity' of inlier and outlier distributions used in evaluations, and propose a metric for this which we use in our experiments. To summarize, the key contributions of the paper are as follows:</p><p>• We propose a new approach for OOD detection that incorporates contrastive training. Our approach avoids explicit inlier and outlier density modelling in the input space, can readily be incorporated into existing training setups and is simple to adapt to different datasets. • We show that the approach consistently improves OOD detection across a wide spectrum of benchmarks, outperforming competitive methods such as Outlier Exposure (OE, <ref type="bibr" target="#b11">[12]</ref>). Unlike OE, our method does not require access to data from the outlier distribution during training or tuning. • We introduce 'Confusion Log Probability' (CLP) as a metric to evaluate OOD detection methods, which measures the similarity of inlier / outlier dataset pairs. Using this metric, we show that the proposed method improves OOD detection in both near and far OOD settings, but especially in near OOD settings. See <ref type="figure" target="#fig_1">Figure 1</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representations from classification networks. The vast majority of OOD detection methods use scores derived from models trained only with multi-class supervision. Hendrycks &amp; Gimpel <ref type="bibr" target="#b10">[11]</ref> propose using the maximum softmax probability (MSP) to score OOD samples, which was further improved in ODIN <ref type="bibr" target="#b18">[19]</ref> by using temperature scaling and input pre-processing. Lee et al. <ref type="bibr" target="#b17">[18]</ref> utilize standard Gaussian density estimates of the network's class-conditional intermediate activations.</p><p>Sastry &amp; Oore <ref type="bibr" target="#b30">[31]</ref> showed improvements on far OOD detection by using Gram matrices from multiple feature maps.</p><p>Alternative training strategies. Beyond proposing better scoring functions, another area of research is adapting the training strategy to improve the quality of scores . MSP scoring was improved by using strategies like confidence loss <ref type="bibr" target="#b16">[17]</ref>, auxiliary objectives <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>, margin loss <ref type="bibr" target="#b33">[34]</ref> and outlier exposure <ref type="bibr" target="#b11">[12]</ref>. A multi-head network architecture was used by Shalev et al. <ref type="bibr" target="#b32">[33]</ref> to improve the intermediate feature map for OOD detection. Similarly, an approach for novelty detection based on metric learning was presented by Masana et al. <ref type="bibr" target="#b21">[22]</ref>. Most of the above approaches make the assumption of access to OOD samples during the training process to enhance performance.</p><p>Bayesian approaches. Under the Bayesian paradigm, Blundell et al. <ref type="bibr" target="#b1">[2]</ref>, Malinin &amp; Gales <ref type="bibr" target="#b20">[21]</ref>, Chen et al. <ref type="bibr" target="#b3">[4]</ref> showed that model uncertainty estimates can be produced by learning distributions over network weights and Gal &amp; Ghahramani <ref type="bibr" target="#b6">[7]</ref> proposed Monte-Carlo dropout sampling for it.</p><p>Generative and hybrid models. An intuitive strategy for detecting OOD samples is to train a generative model from which one can compute the likelihood as an OOD score. An ensemble approach was adapted by Choi et al. <ref type="bibr" target="#b4">[5]</ref> and likelihood ratios were estimated as an OOD scoring metric in <ref type="bibr" target="#b29">[30]</ref>. While generative models are a promising avenue for OOD detection, applying them directly to the image space has not achieved state of the art results, even when combined with a classification network <ref type="bibr" target="#b25">[26]</ref>. Concurrent to our work, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> show in a surprising empirical finding that if a residual flow network is attached to the penultimate layer of a classification network, and the networks are trained together, the p(z) learned by the flow network is able to detect near OOD samples much better than baselines in the open set recognition field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>As shown in <ref type="figure">Figure 2</ref>, training using only supervised classification losses may not produce the required features for identifying OOD samples. Contrastive training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> provides a remedy to this problem, by learning a representation capable of distinguishing between all individual training samples, while incorporating existing prior knowledge about identity-preserving transformations.</p><p>For image classification, camera parameter and illumination are obvious variations. Both can be approximated by translating, scaling and rotating the image, as well as applying brightness and contrast transformations. Intuitively, the contrastive loss moves augmented versions of the same image closer together in the feature space whilst pushing all other image pairs apart <ref type="bibr" target="#b2">[3]</ref>.  Architecture. Our proposed architecture <ref type="figure" target="#fig_3">(Figure 3)</ref> for contrastive training is based on SimCLR <ref type="bibr" target="#b2">[3]</ref>, chosen for its simplicity. It is composed of an encoder network f θ , followed by two projection heads g φ and h ν . g φ maps to the class predictions and h ν maps to the low dimensional embedding over which we define the contrastive loss. The desired feature space is learned on in-distribution training samples only. For a batch of images</p><formula xml:id="formula_0">= " &gt; A A A B / X i c b V D L S g M x F M 3 U V x 1 f 4 2 P n J l g E V 2 V G B H U h F t 2 4 r N A X d M a S S d M 2 N M k M S U a o Q / F X B H G h i F s / w b 0 b 8 W / M t F 1 o 6 4 H A 4 Z x 7 u S c n j B l V 2 n W / r d z c / M L i U n 7 Z X l l d W 9 9 w N r d q K k o k J l U c s U g 2 Q q Q I o 4 J U N d W M N G J J E A 8 Z q Y f 9 y 8 y v 3 x K p a C Q q e h C T g K O u o B 2 K k T Z S y 9 m p 3 H j Q V 5 R D n y P d w 4 i l l W H L K b h F d w Q 4 S 7 w J K Z x / 2 G f x 4 5 d d b j m f f j v C C S d C Y 4 a U a n p u r I M U S U 0 x I 0 P b T x S J E e 6 j L m k a K h A n K k h H 6 Y d w 3 y h t 2 I m k e U L D k f p 7 I 0 V c q Q E P z W Q W U U 1 7 m f i f 1 0 x 0 5 y R I q Y g T T Q Q e H + o k D O o I Z l X A N p U E a z Y w B G F J T V a I e 0 g i r E 1 h t i n B m / 7 y L K k d F r 2 j 4 u m 1 W y h d g D H y Y B f s g Q P g g W N Q A l e g D K o A g z v w A J 7 B i 3 V v P V m v 1 t t 4 N G d N d r b B H 1 j v P x c F m A Y = &lt; / l a t e x i t &gt; T 2 ⇠ T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z D w d / t / Y q F 1 E z L H R 3 G y / e V l D L b c = " &gt; A A A B / X i c b V D L S g M x F M 3 4 r O N r f O z c B I v g q s w U Q V 2 I R T c u K / Q F 7 V g y a a Y N T T J D k h H q U P w V Q V w o 4 t Z P c O 9 G / B s z b R f a e i B w O O d e 7 s k J Y k a V d t 1 v a 2 5 + Y X F p O b d i r 6 6 t b 2 w 6 W 9 s 1 F S U S k y q O W C Q b A V K E U U G q m m p G G r E k i A e M 1 I P + Z e b X b 4 l U N B I V P Y i J z 1 F X 0 J B i p I 3 U d n Y r N 0 X Y U p T D F k e 6 h x F L K 8 O 2 k 3 c L 7 g h w l n g T k j / / s M / i x y + 7 3 H Y + W 5 0 I J 5 w I j R l S q u m 5 s f Z T J D X F j A z t V q J I j H A f d U n T U I E 4 U X 4 6 S j + E B 0 b p w D C S 5 g k N R + r v j R R x p Q Y 8 M J N Z R D X t Z e J / X j P R 4 Y m f U h E n m g g 8 P h Q m D O o I Z l X A D p U E a z Y w B G F J T V a I e 0 g i r E 1 h t i n B m / 7 y L K k V C 9 5 R 4 f T a z Z c u w B g 5 s A f 2 w S H w w D E o g S t Q B l W A w R 1 4 A M / g x b q 3 n q x X 6 2 0 8 O m d N d n b A H 1 j v P x i a m A c = &lt; / l a t e x i t &gt; f ✓ &lt; l a</formula><formula xml:id="formula_1">{x i } i=1...N , we define x 0 i = T 0 (x i ) and x 1 i = T 1 (x i ),</formula><p>where T 0 and T 1 denote two explicit transformations (such as crop-resize or color distortions) selected randomly from a set of transformations T . The representations of the transformed images are given as z 0</p><formula xml:id="formula_2">i = f θ (x 0 i ) and z 1 i = f θ (x 1 i )</formula><p>where f θ is an encoder, a deep network with parameters θ. The representation is learned by solving a relational classification task using cosine similarity in some embedding space. The cosine similarity between any two vectors u, w is given by sim(u, w) = u w/( u w ). A 2-layer MLP h ν maps a sample from the representation space to a lower-dimensional embedding space </p><formula xml:id="formula_3">{ẑ i }, i.e.ẑ 0 i = h ν (z 0 i ), andẑ 1 i = h ν (z 1 i ).</formula><p>where τ is a positive temperature parameter. The projection head g φ consists of a linear transformation mapping the representation space to k logits for the k in-distribution classes. The logits are trained with a standard softmax cross-entropy loss L class .</p><p>Training is performed in two stages. The first stage consists of using solely the contrastive loss L con for a large number of epochs to help learn a good representation. In the second stage, we optimize the combined loss L con + λL class for a smaller number of epochs, where λ is the supervised loss weight.</p><p>Density estimation. We detect OOD samples using a method analogous to Lee et al. <ref type="bibr" target="#b17">[18]</ref>, by fitting Gaussian distributions to the activations on the training data, which we shape in two significant ways. First, the contrastive loss encourages the network to encode all features capable of distinguishing between samples rather than only those necessary to discriminate between classes. Second, to simplify the distribution of the activation that is fitted, label smoothing is added to the cross-entropy loss L class , so as to prevent the network from spreading out the activations in an attempt to drive the logits of the correct class to infinity. This encourages tight clustering of the activations of each class, as demonstrated by Müller et al. <ref type="bibr" target="#b23">[24]</ref>.</p><p>To take advantage of this last property, our density estimation is performed class-wise, over the activations z at the penultimate layer. For each class c, we estimate an n-dimensional multivariate Gaussian N (µ c , Σ c ), with n the dimension of z. For the OOD score s(x), the highest density is taken over all the class-conditional Gaussian components. A high score s(x) indicates that the representation of a test sample in the embedding space lies close to the typical set for one of the classes. Conversely, a low score signifies that the test sample has a representation that is far from all training set examples and is therefore likely to represent an OOD example.</p><formula xml:id="formula_5">s(x) = max c −(f θ (x) − µ c ) T Σ −1 c (f θ (x) − µ c ) − log ((2π) n det Σ c ) ,<label>(2)</label></formula><p>where µ c and Σ c are obtained using the standard estimators. Unlike Lee et al. <ref type="bibr" target="#b17">[18]</ref>, we do not perform ensembling over detectors based on different layers, since determining the optimal linear combination would require access to labelled OOD data. We also do not perform input preprocessing <ref type="bibr" target="#b18">[19]</ref>. Instead, we rely on having a richer representation over which to define our distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Confusion Log Probability (CLP) as a Measure of Dataset Distance</head><p>Real world test samples may vary strongly, from falling exactly within the training distribution to falling far out of the training distribution. As a consequence, a robust model should exhibit strong OOD detection performance across the entirety of the spectrum. Current benchmarks however report only a single performance statistic for each in-distribution and OOD dataset pair, such as the area under the receiver operating characteristic curve. This type of evaluation metric can therefore not resolve the performance in detecting near versus far OOD.</p><p>Different proposals toward discerning the difficulty of OOD tasks have been made in the literature. The openness score <ref type="bibr" target="#b31">[32]</ref>, used in the Open Set Recognition community, gives a difficulty measure based on the number of classes in the training set compared to the number of classes in the test set. This measure, however, ignores the visual similarity that can exist between classes, and therefore the relative difficulty of detecting different unknown classes. Another example is the usage of the maximum mean discrepancy with an L 2 distance kernel on the image space <ref type="bibr" target="#b18">[19]</ref>. However, applying the L 2 distance directly in the image space can only identify nearly identical images. Therefore this metric cannot completely assess the spectrum of OOD cases which we seek.</p><p>We propose the confusion log probability (CLP) as a measure that is indicative of the difficulty of an OOD detection task. CLP is based on the probability with which a classifier confuses outliers with inliers, that has access to outlier samples during training. Given two labelled datasets D in and D out , with corresponding sets of classes C in and C out , we train an ensemble of N e classifiers {p j } Ne j=1 on the joint dataset D = D in ∪ D out using the extended label set C = C in ∪ C out . Once the ensemble is trained, we compute an estimate for the confusion matrix between classes on held-out test data. The expected probability of a test sample x to be predicted as class k is given by:</p><formula xml:id="formula_6">c k (x) = 1 N e Ne j=1p j (ŷ = k|x).<label>(3)</label></formula><p>Therefore the confusion of a set of test OOD samples D test with the inlier classes C in , i.e. the confusion log probability (CLP) of D test , becomes:</p><formula xml:id="formula_7">CLP Cin (D test ) = log 1 |D test | x∈Dtest k∈Cin c k (x) .<label>(4)</label></formula><p>A low CLP indicates that test samples are far OOD and a high CLP indicates that they are near OOD. Note that summing over the probability for all inlier classes C in effectively evaluates the binary classification problem of inlier versus outlier classes on outlier samples. As such CLP is asymmetric.</p><p>We can compute a class-wise CLP with D test being the test samples of this specific class, or a dataset CLP by using all samples of the test dataset as D test . The class-wise CLP is used in <ref type="figure" target="#fig_1">Fig. 1</ref>, where we show how OOD detection performance varies as a function of the distance between the CIFAR-100 classes and the inlier CIFAR-10 dataset. By using classifiers to estimate class confusions (5 ResNet-34 models in our work), we ground the measure in a notion of visual similarity rather than semantic or image space similarity. The choice of an ensemble of independently trained classifiers is motivated by their well-calibrated predictions over single classifiers <ref type="bibr" target="#b15">[16]</ref>. Further implementation details of the ensemble training setup to compute CLP and further qualitative analysis using dendrograms can be found in Appendix D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Near to far OOD spectrum benchmark. We study out-of-distribution detection on the following in-distribution dataset (D in ) and out-of-distribution dataset (D out ) pairs which we believe represent the most challenging pairs of common OOD detection benchmarks <ref type="bibr" target="#b10">[11]</ref>: We use the CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b14">[15]</ref> datasets, as well as the Street View House Numbers (SVHN) dataset <ref type="bibr" target="#b26">[27]</ref>. Note that the CIFAR-10 and CIFAR-100 classes are mutually exclusive. The distance of the dataset pairs is given by the min-max bounds of the class-wise CLP.</p><p>Evaluation metrics. As is common in the OOD detection literature, we use the area under the receiver operating characteristic curve (AUROC), with in-distribution and out-of-distribution being the labels, as the metric for OOD detection performance. Note that this is a threshold-independent and calibration-free evaluation metric. Results with additional metrics can be found in Appendix E. In addition to AUROC, we also quantify the performance on a single OOD sample using the OOD rank. For estimating this, we compare the score of the OOD sample s(x) to the scores of the inlier test dataset. The OOD rank is given by the percentage of inlier test examples that have a lower s(x) than the OOD sample. The higher the rank, the more the sample is deemed to be OOD.</p><p>Setup. We adopt a wide ResNet-50 <ref type="bibr" target="#b7">[8]</ref> as the encoder f θ , whose last layer has a fixed-dimensional output (6144-D). This output vector z is the representation on which we compute the OOD score s(x) for a given test sample x. z is followed by the supervised head g φ with label smoothing, and the contrastive head h ν with a 128-D 2-layer MLP projection with batch normalization and ReLU.</p><p>For fair comparison, we use the same architecture, the same transformation function T and scoring function for the baselines without contrastive training reported in <ref type="table">Table 6</ref>. Further training setup details are provided in Appendix B.</p><p>Results. <ref type="table" target="#tab_0">Table 1</ref> shows our main results. We observe that our proposed method improves OOD detection across the spectrum of our proposed benchmark. Specifically, on the near OOD dataset pair with CIFAR-100 as D in and CIFAR-10 as D out , we obtain an AUROC of 78.3 which is, to the best of our knowledge, a new state-of-the-art result, outperforming <ref type="bibr" target="#b17">[18]</ref>. With CIFAR-10 as D in and CIFAR-100 as D out , containing both test samples that are near as well as far OOD, we obtain an AUROC of 92.9, which is more than two points better than the next best method not using labeled out-of-distribution data during training. <ref type="figure" target="#fig_5">Figure 4a</ref> shows contrastive training helps to differentiate OOD classes that are highly similar to inlier classes, where the baseline results is worse than random performance. In the far OOD setting <ref type="figure" target="#fig_5">(Figure 4b)</ref>, contrastive training is also a significant component for further separation between z of dissimilar images and inlier dataset images. The performance gap is the largest for the high CLP regime (+18 AUROC points for −5 ≤ CLP &lt; 0), and remains for the whole spectrum <ref type="figure" target="#fig_5">(Figure 4c</ref>). On the far OOD dataset pair with CIFAR-10 as D in and SVHN as D out , we obtain an AUROC of 99.5 which is on par with the current state-of-the-art <ref type="bibr" target="#b30">[31]</ref>. Results on additional dataset pairs are reported in Appendix E. When considering the average performance on all three dataset pairs as representative of the entire near to far OOD spectrum, our method obtains an AUROC of 90.2 outperforming the previous state-of-the-art method <ref type="bibr" target="#b11">[12]</ref>. It is worth pointing out that, unlike others, our approach does not assume access to additional data or labels from an outlier distribution during training or tuning.</p><p>Impact of activation space shaping. We perform an ablation study to investigate the impact of label smoothing and contrastive training, with results reported in <ref type="table">Table 6</ref>. We report results at the task level, but also investigate the variation on a per-sample basis, by looking at the OOD rank variation between several randomly initialized runs. This allows us to quantify whether the OOD samples considered in distribution always remain the same or whether the errors vary.</p><p>We observe that while using contrastive training or label smoothing alone leads to improvements over the supervised baseline, significantly better results are obtained when the two of them are combined together to shape the activation space. We hypothesize that scoring with standard Gaussian density estimation benefits significantly from tighter class clusters obtained via label smoothing, and it is required for the combination of contrastive training and the scoring function as seen in Eq. 2 to work effectively. Furthermore, our feature shaping strategies allow for a significant reduction in variation of the OOD rank of samples between runs. We hypothesize that for the supervised baseline, only features required for class discrimination will be created while the presence of task-agnostic features useful for near OOD detection will only be present by chance due to randomness in training.</p><p>More details on additional ablation tests and discussions are available in Appendix F, where we study the impact of parameters like relative weighting between supervised and contrastive loss, temperature and model capacity on OOD detection performance.</p><p>Failure mode analysis. <ref type="figure" target="#fig_6">Figure 5</ref> shows failure cases for the baseline and our model. We show samples from D out = CIFAR-100 most likely to be incorrectly predicted as in-distribution for networks trained on D in = CIFAR-10. The percentile rank of the OOD score w.r.t. the scores of the inlier dataset indicates the success of the OOD detection (higher is better). For most of the baseline's worst mistakes, our approach succeeds in identifying that sample as an outlier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>In this paper, we proposed a simple contrastive training-based approach to OOD detection that outperforms recent methods across a variety of settings. Furthermore we introduced a new metric to quantify the difficulty of a specific OOD task, the confusion log probability (CLP), that captures the similarity of inlier and outlier datasets. We showed with extensive experiments that representations obtained through contrastive training improve OOD detection performance beyond what is possible with purely supervised training. The representations are shaped by joint training, in which the contrastive loss pushes the representations apart, even within each class, while the supervised loss acts to cluster the representations by class. Unlike previous competitive methods our approach does not require access to data from the outlier distribution during training or tuning. Moreover, in contrast to many other representation learning approaches, the underlying SimCLR <ref type="bibr" target="#b2">[3]</ref> has a well-motivated training objective, and can scale to large images and datasets.</p><p>Of course this additional training objective can not guarantee that all necessary features for near OOD classes are found. See <ref type="figure" target="#fig_6">Figure 5</ref> for a failure mode analysis for the setting D in = CIFAR-10 and D out = CIFAR-100. Here the model saw 'automobiles' and 'trucks' during training, but it was not able to distinguish the new classes 'pickup truck' and 'bus' from the existing ones. These class-pairs are however also the most challenging for fully supervised training as CLP shows.</p><p>This work only examines improvements that arise from training a richer representation z. With regards to scoring of z, we employed standard Gaussian density modeling like prior methods <ref type="bibr" target="#b13">[14]</ref>. In concurrent work, Zhang et al. <ref type="bibr" target="#b35">[36]</ref> show that scoring with a deep flow-based network can also significantly improve performance. We expect that these improvements in density estimation are complementary to our proposed contrastive training.</p><p>We use a larger neural network than is typical in the literature. This capacity is needed to encode the additional features that SimCLR training creates. We have shown that the Mahalanobis approach <ref type="bibr" target="#b13">[14]</ref> does not benefit from additional model capacity. For other baselines we can only speculate that the authors tried to add more capacity, and published the network that yielded best performance.</p><p>Another advantage of our setup is the ability to extract useful information from completely unlabelled images that can come from arbitrary distributions. Other approaches that learn a density directly from the training set (e.g. Zhang et al. <ref type="bibr" target="#b35">[36]</ref>) rely on the fact that all training examples come only from the 'in-distribution' set. Especially in real-world applications, for instance medical imaging, a large set of unlabelled images is easily available from routine imaging. Removing outlier images from this set would be an expensive manual labelling task.</p><p>All in all, this work demonstrates that the challenges in OOD detection (especially for the near OOD setting) are closely related to those in unsupervised representation learning. We believe that viewing the OOD detection problem from this perspective opens up new avenues for progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Deep neural networks have demonstrated superhuman performance across a wide range of applications, with the promise of significant positive impact on the world. Despite this, our ability to safely deploy models in real world settings is still limited. One such obstacle is the inability of existing models to accurately withhold prediction for an input that is meaningfully different from typical examples encountered during training. In contrast to human experts, deep neural network based systems tend to struggle to account for the uncertainties for taking an appropriate decision, e.g. refer a difficult case for a second opinion. Moreover, models can fail in unexpected ways, making errors difficult to identify in practice. In domains with the greatest potential for societal impact, such as medical imaging and self-driving cars, appropriate recognition of OOD inputs is essential to avoid catastrophic errors that may cause harm.</p><p>This work proposes a new scalable approach to OOD detection based on recent advancements in representation learning, and takes a step towards the ultimate goal of enabling safe real-world deployment of machine learning models in safety-critical domains. Additionally, this work also defines and recognizes the importance of evaluation of OOD detection performance in a spectrum of regimes, in particular near and far OOD settings. The experimental results have been reported on standard benchmark datasets for considerations of reproducible research, but both the OOD detection method and the evaluation approach are addressing potential issues encountered in safety critical domains. For example, in medical imaging, pathologies, poor-quality images and other artefacts that have not previously been encountered in model training are both common and important. Progress in methods for near OOD detection is therefore particularly relevant to addressing the often subtle but significant challenges of operating safely in a real world environment.</p><p>A Detailed Performance Across CLP Spectrum  In <ref type="figure" target="#fig_8">Fig. 6</ref>, we show the class-wise OOD detection performance for the full CLP spectrum with and without contrastive training with D in being CIFAR-10 and D out being the combination of CIFAR-100, SVHN, Places365 and Gaussian noise test samples. Details about the training process for CLP computation are given in Sec. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We use a Resnet-50 with a 3× width multiplier for all experiments. We first pretrain the model with a batch size of 2048 for 1000 epochs using only the contrastive loss and then finetune with a joint supervised and contrastive loss for 100 epochs in case of CIFAR-10 and 200 epochs for CIFAR-100. We use a supervised loss multiplier of λ = 100 during the finetuning stage. The models are trained using the LARS <ref type="bibr" target="#b34">[35]</ref> optimizer with momentum 0.9 and weight decay 1 × 10 −6 . Furthermore, we use an initial learning rate of 1.0, with a linear warmup for the first 30 epochs followed by a cosine decay schedule without restarts following <ref type="bibr" target="#b19">[20]</ref> for both stages. For label smoothing, we use α = 0.01 for CIFAR-10 as D in and α = 0.1 for CIFAR-100 as D in .</p><p>The data augmentation operation T as described in Section 3 follows <ref type="bibr" target="#b2">[3]</ref>, which is a sequence of random cropping followed by a random left-right flip and random color distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CLP Training Details</head><p>For computing CLP scores, we trained an ensemble of five ResNet-34 model instances. In order to accurately estimate class confusions, we would ideally like to train classifiers on a dataset that is representative of all possible images. As an approximation, we independently train each of the model instances on the union of five datasets: CIFAR-10, SVHN, CIFAR-100, Places365 and independent Gaussian noise. The entire collection has 486 classes. We used a training batch size of 1024, and ensured that examples from each of the 486 classes are uniformly sampled in a batch. We used SGD with momentum of 0.9 for training the models. The models were trained for 500 epochs with a cosine decay learning rate schedule initialized at 0.2. A weight decay parameter of 10 −6 was used for regularization. Each of the model instances only differ in the random initializations of the weights. Once all the model instances are trained, we use them to compute CLP between any given dataset pair. As a specific example, let us consider the case where the inlier dataset D in is CIFAR-10 and the outlier dataset D out is Places365, with 10 and 365 classes respectively. To calculate the CLP score of Places365 (with respect to CIFAR-10), we compute, for each of the 5 model instances, the softmax output for all the test examples in Places365. The outputs of the model ensemble instances are averaged to have a 1 × 486 vector output, where 486 is the total number of classes (in the union of all datasets). We compute the total probability of the 10 outputs corresponding to CIFAR-10 classes as D in and report the log of this probability as CLP as an estimate of confusing a Places365 example with a CIFAR-10 example. The confusion matrix of the our model is shown in <ref type="figure" target="#fig_9">Fig. 7</ref>, by combining all the classes of a given dataset. The rows indicate the true labels and columns indicate the predictions. We observe that CIFAR-10 and CIFAR-100 confuses more among themselves compared to datasets: Places365, SVHN and Gaussian noise, which are far OOD to them. Also note that these datasets have almost 100% classification accuracy. Thus the relative ordering of these far OOD datasets from in-distribution CIFAR-10/100 is only as good as the state-of-the art in calibrated uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Analysis of CLP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Class Similarities</head><p>To qualitatively ascertain our CLP estimates, we use the model jointly trained on CIFAR-10, CIFAR-100, SVHN, Places365 and Gaussian noise resulting in a 486-way classifier. We compute a confusion probability for each pair of classes i, j as</p><formula xml:id="formula_8">u i→j = 1 |D test,j | x∈Dtest,j c i (x) ,<label>(5)</label></formula><p>using the expected probability c i (x) of a test sample to be predicted as class i (Eq. (3) in the main paper), and the test set for class j, D test,j . We translate the probabilities into symmetric distances using d(i, j) = − log 1 2 (u i→j + u j→i ) .</p><p>We then use these pairwise distances to perform hierarchical agglomerative clustering (with 'average' linkage). <ref type="figure" target="#fig_10">Figure 8</ref> shows the dendrogram. We observe that the relationships captured by this distance measure are a good representation of the visual similarity of the classes: all the SVHN classes (shown in blue) are well separated from the CIFAR-10 and CIFAR-100 classes indicating that it is a far OOD dataset. CIFAR-10 (shown in red) and CIFAR-100 classes (shown in green) on the other hand are frequently confused, indicating that the two datasets are closer to one another than to SVHN. The Places365 dataset builds a separate cluster (shown in purple). Surprisingly the CIFAR-100 classses 'orange' and 'apple' are quite dissimilar to the remaining CIFAR-100 and CIFAR-10 classes and get clustered closer to the house numbers.</p><p>The individual classes within the data sets that get clustered together match very well our impression of "visual similarity". We take this as another confirmation that our proposed CLP metric can be used to quantify the difficulty of OOD tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results</head><p>As an extension to <ref type="table" target="#tab_0">Table 1</ref>, we report additional metrics for models trained with our methods. We chose to report the whole</p><p>• Area Under Receiver Operating Characteristic (AUROC) corresponds to the area under the Receiver Operating Characteristic curve, which plots the True Positive Rate (TPR) as a function of the False Positive Rate (FPR). This metric has the advantage of not requiring to choose the choice of a threshold and evaluating the OOD detector in all range of operations. It can be interpreted as the probability that, when picking an out of distribution sample and an in-distribution sample, the in-distribution sample gets considered as more in-distribution that the OOD sample. • Area Under Precision Recall (AUPR) is a closely related metric. It corresponds to the area under the Precision-Recall Curve, which plots Precision against Recall. • False Positive Rate at 95% True Positive Rate (FPR@95%TPR) is the probability that an OOD example is correctly identified when the true positive rate (TPR) is 95%. TPR is computed as T P R = T P/(T P + F N ), where TP and FN denote true positive and false negative respectively. • Detection Accuracy (DtAcc) is a measurement of the maximum classification accuracy that we can achieve between in-distribution and out-of-distribution examples, by choosing the optimal threshold.</p><p>We also report results on additional out of distribution datasets.</p><p>• Gaussian noise is a synthetic dataset composed of 32 × 32 pixel random images where the intensity of each pixel is sampled from a gaussian distribution with mean 0.5 and scale 0.25. • Places365 [37] is a dataset containing scene photographs. We rescale them from their original size down to 32 × 32 to match the size of our in-distribution images. <ref type="table">Table 3</ref>: Additional out-of-distribution detection results for our proposed method. All results are an average over five independent runs. We report results both for our method and the baseline that does not include label smoothing or contrastive training. Baseline / Ours D in D out CLP range FPR@95%TPR ↓ AUROC ↑ AUPR ↑ DtAcc ↑ CIFAR-10 <ref type="table">Table 4</ref>: Ablation study of weight between supervised loss and contrastive loss λ Din = CIFAR-100 CIFAR-100 CIFAR-100 CIFAR-10 CIFAR-10 CIFAR-10 Dout = CIFAR-10 SVHN Places365 CIFAR-100 SVHN Places365</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>classes (points) w.r.t. a fixed inlier dataset (CIFAR-10) With Contrastive training Without Contrastive training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Each point represents performance at detecting one of the classes in CIFAR-100 as outliers with respect to a network trained on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 i 1 i</head><label>11</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " O B l t K z e 6 L K B m 8 S S 8 5 0 s B m R Q e X D Q = " &gt; A A A B 6 n i c b V D L S g N B E O y N r 5 j 4 i H r 0 M h g F T 2 F X B P U W 9 O I x o p s E k i X M T m a T M b O z y 8 x s M C z 5 B C 8 e F P H q N / g D / o E 3 P 0 T P T h 4 H T S x o K K q 6 6 e 7 y Y 8 6 U t u 1 P K 7 O w u L S 8 k l 3 N 5 d f W N z Y L W 9 t V F S W S U J d E P J J 1 H y v K m a C u Z p r T e i w p D n 1 O a 3 7 v Y u T X + l Q q F o k b P Y i p F + K O Y A E j W B v p + q 5 1 2 y o U 7 Z I 9 B p o n z p Q U y / t f b + / 9 / H e l V f h o t i O S h F R o w r F S D c e O t Z d i q R n h d J h r J o r G m P R w h z Y M F T i k y k v H p w 7 R g V H a K I i k K a H R W P 0 9 k e J Q q U H o m 8 4 Q 6 6 6 a 9 U b i f 1 4 j 0 c G p l z I R J 5 o K M l k U J B z p C I 3 + R m 0 m K d F 8 Y A g m k p l b E e l i i Y k 2 6 e R M C M 7 s y / O k e l R y j k t n V y a N c 5 g g C 7 u w B 4 f g w A m U 4 R I q 4 A K B D t z D I z x Z 3 H q w n q 2 X S W v G m s 7 s w B 9 Y r z 8 n x J I q &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z o J q e / h b o b H e M F 8 y a X u y 2 f a T E b 8 = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g Q h V b g T I b E L 2 l h G 8 J J A c o a 9 z V 6 y Z G / v 2 N 0 T w 5 H f Y G O h B F v / i 6 2 d 6 I 9 x 8 1 F o 4 o O B x 3 s z z M z z Y 8 6 U t u 1 P K 7 O 2 v r G 5 l d 3 O 7 e z u 7 R / k D 4 8 a K k o k o S 6 J e C R b P l a U M 0 F d z T S n r V h S H P q c N v 3 h 1 d R v 3 l O p W C R u 9 S i m X o j 7 g g W M Y G 0 k 9 6 H L 7 p x u v m i X 7 R n Q K n E W p F g r l L 6 / K u + T e j f / 0 e l F J A m p 0 I R j p d q O H W s v x V I z w u k 4 1 0 k U j T E Z 4 j 5 t G y p w S J W X z o 4 d o 1 O j 9 F A Q S V N C o 5 n 6 e y L F o V K j 0 D e d I d Y D t e x N x f + 8 d q K D q p c y E S e a C j J f F C Q c 6 Q h N P 0 c 9 J i n R f G Q I J p K Z W x E Z Y I m J N v n k T A j O 8 s u r p H F W d s 7 L F z c m j U u Y I w s n U I A S O F C B G l x D H V w g w O A R n u H F E t a T N b F e 5 6 0 Z a z F z D H 9 g v f 0 A t 4 6 S X w = = &lt; / l a t e x i t &gt; x 0 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b q 6 8 T a A A + + P 1 m 3 G i Q x U + y X f / T w A = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g Q h V b g T I b E L 2 l h G 8 J J A c o a 9 z V 6 y Z G / v 2 N 0 T w 5 H f Y G O h B F v / i 6 2 d 6 I 9 x 8 1 F o 4 o O B x 3 s z z M z z Y 8 6 U t u 1 P K 7 O 2 v r G 5 l d 3 O 7 e z u 7 R / k D 4 8 a K k o k o S 6 J e C R b P l a U M 0 F d z T S n r V h S H P q c N v 3 h 1 d R v 3 l O p W C R u 9 S i m X o j 7 g g W M Y G 0 k 9 6 H L 7 u x u v m i X 7 R n Q K n E W p F g r l L 6 / K u + T e j f / 0 e l F J A m p 0 I R j p d q O H W s v x V I z w u k 4 1 0 k U j T E Z 4 j 5 t G y p w S J W X z o 4 d o 1 O j 9 F A Q S V N C o 5 n 6 e y L F o V K j 0 D e d I d Y D t e x N x f + 8 d q K D q p c y E S e a C j J f F C Q c 6 Q h N P 0 c 9 J i n R f G Q I J p K Z W x E Z Y I m J N v n k T A j O 8 s u r p H F W d s 7 L F z c m j U u Y I w s n U I A S O F C B G l x D H V w g w O A R n u H F E t a T N b F e 5 6 0 Z a z F z D H 9 g v f 0 A t g q S X g = = &lt; / l a t e x i t &gt; z 0 i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F S v U V Q y Q h 8 O N m m P d Q x D o g S n 0 i M g = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g Q h V b g T I b E L 2 l h G 8 J J A c o a 9 z V 6 y Z G / v 2 N 0 T 4 p H f Y G O h B F v / i 6 2 d 6 I 9 x 8 1 F o 4 o O B x 3 s z z M z z Y 8 6 U t u 1 P K 7 O 2 v r G 5 l d 3 O 7 e z u 7 R / k D 4 8 a K k o k o S 6 J e C R b P l a U M 0 F d z T S n r V h S H P q c N v 3 h 1 d R v 3 l O p W C R u 9 S i m X o j 7 g g W M Y G 0 k 9 6 H L 7 u x u v m i X 7 R n Q K n E W p F g r l L 6 / K u + T e j f / 0 e l F J A m p 0 I R j p d q O H W s v x V I z w u k 4 1 0 k U j T E Z 4 j 5 t G y p w S J W X z o 4 d o 1 O j 9 F A Q S V N C o 5 n 6 e y L F o V K j 0 D e d I d Y D t e x N x f + 8 d q K D q p c y E S e a C j J f F C Q c 6 Q h N P 0 c 9 J i n R f G Q I J p K Z W x E Z Y I m J N v n k T A j O 8 s u r p H F W d s 7 L F z c m j U u Y I w s n U I A S O F C B G l x D H V w g w O A R n u H F E t a T N b F e 5 6 0 Z a z F z D H 9 g v f 0 A u R q S Y A = = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z D F Y H U 7 8 h I v o i Q S C f 6 B g I O / X P p I = " &gt; A A A B 7 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g Q h V b g T I b E L 2 l h G 8 J J A c o a 9 z V 6 y Z G / v 2 N 0 T 4 p H f Y G O h B F v / i 6 2 d 6 I 9 x 8 1 F o 4 o O B x 3 s z z M z z Y 8 6 U t u 1 P K 7 O 2 v r G 5 l d 3 O 7 e z u 7 R / k D 4 8 a K k o k o S 6 J e C R b P l a U M 0 F d z T S n r V h S H P q c N v 3 h 1 d R v 3 l O p W C R u 9 S i m X o j 7 g g W M Y G 0 k 9 6 H L 7 p x u v m i X 7 R n Q K n E W p F g r l L 6 / K u + T e j f / 0 e l F J A m p 0 I R j p d q O H W s v x V I z w u k 4 1 0 k U j T E Z 4 j 5 t G y p w S J W X z o 4 d o 1 O j 9 F A Q S V N C o 5 n 6 e y L F o V K j 0 D e d I d Y D t e x N x f + 8 d q K D q p c y E S e a C j J f F C Q c 6 Q h N P 0 c 9 J i n R f G Q I J p K Z W x E Z Y I m J N v n k T A j O 8 s u r p H F W d s 7 L F z c m j U u Y I w s n U I A S O F C B G l x D H V w g w O A R n u H F E t a T N b F e 5 6 0 Z a z F z D H 9 g v f 0 A u p 6 S Y Q = = &lt; / l a t e x i t &gt; z j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O Z t y G X J g 5 2 U g V N Q i 9 3 Z w F W q J m R o = " &gt; A A A B 6 n i c b V D L S g N B E O y N r 5 j 4 i H r 0 M h g F T 2 F X B P U W 9 O I x o p s E k i X M T m a T M b O z y 8 x s I C 7 5 B C 8 e F P H q N / g D / o E 3 P 0 T P T h 4 H T S x o K K q 6 6 e 7 y Y 8 6 U t u 1 P K 7 O w u L S 8 k l 3 N 5 d f W N z Y L W 9 t V F S W S U J d E P J J 1 H y v K m a C u Z p r T e i w p D n 1 O a 3 7 v Y u T X + l Q q F o k b P Y i p F + K O Y A E j W B v p + q 5 1 2 y o U 7 Z I 9 B p o n z p Q U y / t f b + / 9 / H e l V f h o t i O S h F R o w r F S D c e O t Z d i q R n h d J h r J o r G m P R w h z Y M F T i k y k v H p w 7 R g V H a K I i k K a H R W P 0 9 k e J Q q U H o m 8 4 Q 6 6 6 a 9 U b i f 1 4 j 0 c G p l z I R J 5 o K M l k U J B z p C I 3 + R m 0 m K d F 8 Y A g m k p l b E e l i i Y k 2 6 e R M C M 7 s y / O k e l R y j k t n V y a N c 5 g g C 7 u w B 4 f g w A m U 4 R I q 4 A K B D t z D I z x Z 3 H q w n q 2 X S W v G m s 7 s w B 9 Y r z 8 q 0 J I s &lt; / l a t e x i t &gt; x i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G m I U e q 4 N F l U x a r + J 3 0 x y B T A A j w w = " &gt; A A A B 6 n i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M K u C G o X Y m O Z o H l A s o T Z y W w y Z H Z m m Z k V w 5 J P s L F Q x N b W v / A L 7 G z 8 F i e P Q h M P X D i c c y / 3 3 h P E n G n j u l 9 O Z m l 5 Z X U t u 5 7 b 2 N z a 3 s n v 7 t W 1 T B S h N S K 5 V M 0 A a 8 q Z o D X D D K f N W F E c B Z w 2 g s H V 2 G / c U a W Z F L d m G F M / w j 3 B Q k a w s d L N f Y d 1 8 g W 3 6 E 6 A F o k 3 I 4 X S Q f W b v Z c / K p 3 8 Z 7 s r S R J R Y Q j H W r c 8 N z Z + i p V h h N N R r p 1 o G m M y w D 3 a s l T g i G o / n Z w 6 Q s d W 6 a J Q K l v C o I n 6 e y L F k d b D K L C d E T Z 9 P e + N x f + 8 V m L C C z 9 l I k 4 M F W S 6 K E w 4 M h K N / 0 Z d p i g x f G g J J o r Z W x H p Y 4 W J s e n k b A j e / M u L p H 5 a 9 M 6 K l 1 W b R h m m y M I h H M E J e H A O J b i G C t S A Q A 8 e 4 A m e H e 4 8 O i / O 6 7 Q 1 4 8 x m 9 u E P n L c f Q x i R g Q = = &lt; / l a t e x i t &gt; x j &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O B l t K z e 6 L K B m 8 S S 8 5 0 s B m R Q e X D Q = " &gt; A A A B 6 n i c b V D L S g N B E O y N r 5 j 4 i H r 0 M h g F T 2 F X B P U W 9 O I x o p s E k i X M T m a T M b O z y 8 x s M C z 5 B C 8 e F P H q N / g D / o E 3 P 0 T P T h 4 H T S x o K K q 6 6 e 7 y Y 8 6 U t u 1 P K 7 O w u L S 8 k l 3 N 5 d f W N z Y L W 9 t V F S W S U J d E P J J 1 H y v K m a C u Z p r T e i w p D n 1 O a 3 7 v Y u T X + l Q q F o k b P Y i p F + K O Y A E j W B v p + q 5 1 2 y o U 7 Z I 9 B p o n z p Q U y / t f b + / 9 / H e l V f h o t i O S h F R o w r F S D c e O t Z d i q R n h d J h r J o r G m P R w h z Y M F T i k y k v H p w 7 R g V H a K I i k K a H R W P 0 9 k e J Q q U H o m 8 4 Q 6 6 6 a 9 U b i f 1 4 j 0 c G p l z I R J 5 o K M l k U J B z p C I 3 + R m 0 m K d F 8 Y A g m k p l b E e l i i Y k 2 6 e R M C M 7 s y / O k e l R y j k t n V y a N c 5 g g C 7 u w B 4 f g w A m U 4 R I q 4 A K B D t z D I z x Z 3 H q w n q 2 X S W v G m s 7 s w B 9 Y r z 8 n x J I q &lt; / l a t e x i t &gt; 8j 6 = i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 N h 5 X T r i e g 8 O M r 8 4 0 h a r O r X N 0 y 4 = " &gt; A A A B + X i c b V D L S g M x F M 3 U V 6 2 v U Z e K B I v g q s y I o O 6 K b l y 2 Y B / Q D i W T 3 m l j M 5 k x y R T K 0 K V / 4 c a F I m 7 d 9 D v c + Q 3 + h O l j o a 0 H A o d z 7 u W e H D / m T G n H + b I y S 8 s r q 2 v Z 9 d z G 5 t b 2 j r 2 7 V 1 V R I i l U a M Q j W f e J A s 4 E V D T T H O q x B B L 6 H G p + 7 2 b s 1 / o g F Y v E n R 7 E 4 I W k I 1 j A K N F G a t l 2 M 4 g k 4 R z f 4 6 a A B 8 x a d t 4 p O B P g R e L O S L 5 4 O C p / P x 6 N S i 3 7 s 9 m O a B K C 0 J Q T p R q u E 2 s v J V I z y m G Y a y Y K Y k J 7 p A M N Q w U J Q X n p J P k Q n x i l j U 0 E 8 4 T G E / X 3 R k p C p Q a h b y Z D o r t q 3 h u L / 3 m N R A e X X s p E n G g Q d H o o S D j W E R 7 X g N t M A t V 8 Y A i h k p m s m H a J J F S b s n K m B H f + y 4 u k e l Z w z w t X Z d P G N Z o i i w 7 Q M T p F L r p A R X S L S q i C K O q j J / S C X q 3 U e r b e r P f p a M a a 7 e y j P 7 A + f g C N m J a t &lt; / l a t e x i t &gt; T 0 ⇠ T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l p I b j k x Y N D 5 B b 5 M r R 7 t q r + 7 Y I W w = " &gt; A A A B / X i c b V D L S g M x F M 3 U V x 1 f 4 2 P n J l g E V 2 V G B H U h F t 2 4 r N A X d M a S S d M 2 N M k M S U a o Q / F X B H G h i F s / w b 0 b 8 W / M t F 1 o 6 4 H A 4 Z x 7 u S c n j B l V 2 n W / r d z c / M L i U n 7 Z X l l d W 9 9 w N r d q K k o k J l U c s U g 2 Q q Q I o 4 J U N d W M N G J J E A 8 Z q Y f 9 y 8 y v 3 x K p a C Q q e h C T g K O u o B 2 K k T Z S y 9 m p 3 L j Q V 5 R D n y P d w 4 i l l W H L K b h F d w Q 4 S 7 w J K Z x / 2 G f x 4 5 d d b j m f f j v C C S d C Y 4 a U a n p u r I M U S U 0 x I 0 P b T x S J E e 6 j L m k a K h A n K k h H 6 Y d w 3 y h t 2 I m k e U L D k f p 7 I 0 V c q Q E P z W Q W U U 1 7 m f i f 1 0 x 0 5 y R I q Y g T T Q Q e H + o k D O o I Z l X A N p U E a z Y w B G F J T V a I e 0 g i r E 1 h t i n B m / 7 y L K k d F r 2 j 4 u m 1 W y h d g D H y Y B f s g Q P g g W N Q A l e g D K o A g z v w A J 7 B i 3 V v P V m v 1 t t 4 N G d N d r b B H 1 j v P x V w m A U = &lt; / l a t e x i t &gt; T 1 ⇠ T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 4 9 u O H F y K O P 5 8 B m z g o V 7 t C / s m L s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " j M C f p C 7 a N N R u s g u J n 1 f k T J j R V z A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s B / Q h r L Z b t q l m 0 3 c n Q g l 9 E 9 4 8 a C I V / + O N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q A S c J 9 y M 6 V C I U j K K V O m G / h y O O t F + u u F V 3 D r J K v J x U I E e j X / 7 q D W K W R l w h k 9 S Y r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 3 i k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z J 4 n A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 1 E J R u C t / z y K m l d V L 1 a 9 f q + V q n f 5 H E U 4 Q R O 4 R w 8 u I Q 6 3 E E D m s B A w j O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 8 h a 5 A N &lt; / l a t e x i t &gt; f ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j M C f p C 7 a N N R u s g u J n 1 f k T J j R V z A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s B / Q h r L Z b t q l m 0 3 c n Q g l 9 E 9 4 8 a C I V / + O N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q A S c J 9 y M 6 V C I U j K K V O m G / h y O O t F + u u F V 3 D r J K v J x U I E e j X / 7 q D W K W R l w h k 9 S Y r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 3 i k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z J 4 n A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 1 E J R u C t / z y K m l d V L 1 a 9 f q + V q n f 5 H E U 4 Q R O 4 R w 8 u I Q 6 3 E E D m s B A w j O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 8 h a 5 A N &lt; / l a t e x i t &gt; f ✓ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j M C f p C 7 a N N R u s g u J n 1 f k T J j R V z A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o N 6 K X j x W s B / Q h r L Z b t q l m 0 3 c n Q g l 9 E 9 4 8 a C I V / + O N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K F Q d f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o Z e J U M 9 5 k s Y x 1 J 6 C G S 6 F 4 E w V K 3 k k 0 p 1 E g e T s Y 3 8 7 8 9 h P X R s T q A S c J 9 y M 6 V C I U j K K V O m G / h y O O t F + u u F V 3 D r J K v J x U I E e j X / 7 q D W K W R l w h k 9 S Y r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 3 i k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z J 4 n A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 1 E J R u C t / z y K m l d V L 1 a 9 f q + V q n f 5 H E U 4 Q R O 4 R w 8 u I Q 6 3 E E D m s B A w j O 8 w p v z 6 L w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w 8 h a 5 A N &lt; / l a t e x i t &gt; h ⌫ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 o Z S l 0 T f R e P z j E W C g C 7 g H s 6 o c 5 Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Q g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 b u a 3 n 1 B p n s h H M 0 k x i O l Q 8 o g z a q z k j / o 9 m f W r N b f u z k F W i V e Q G h R o 9 q t f v U H C s h i l Y Y J q 3 f X c 1 A Q 5 V Y Y z g d N K L 9 O Y U j a m Q + x a K m m M O s j n x 0 7 J m V U G J E q U L W n I X P 0 9 k d N Y 6 0 k c 2 s 6 Y m p F e 9 m b i f 1 43 M 9 F 1 k H O Z Z g Y l W y y K M k F M Q m a f k w F X y I y Y W E K Z 4 v Z W w k Z U U W Z s P h U b g r f 8 8 i p p X d S 9 y / r N w 2 W t c V v E U Y Y T O I V z 8 O A K G n A P T f C B A Y d n e I U 3 R z o v z r v z s W g t O c X M M f y B 8 / k D 3 P 2 O v g = = &lt; / l a t e x i t &gt;h ⌫ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 o Z S l 0 T f R e P z j E W C g C 7 g H s 6 o c 5 Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Q g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 b u a 3 n 1 B p n s h H M 0 k x i O l Q 8 o g z a q z k j / o 9 m f W r N b f u z k F W i V e Q G h R o 9 q t f v U H C s h i l Y Y J q 3 f X c 1 A Q 5 V Y Y z g d N K L 9 O Y U j a m Q + x a K m m M O s j n x 0 7 J m V U G J E q U L W n I X P 0 9 k d N Y 6 0 k c 2 s 6 Y m p F e 9 m b i f 1 4 3 M 9 F 1 k H O Z Z g Y l W y y K M k F M Q m a f k w F X y I y Y W E K Z 4 v Z W w k Z U U W Z s P h U b g r f 8 8 i p p X d S 9 y / r N w 2 W t c V v E U Y Y T O I V z 8 O A K G n A P T f C B A Y d n e I U 3 R z o v z r v z s W g t O c X M M f y B 8 / k D 3 P 2 O v g = = &lt; / l a t e x i t &gt; h ⌫ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 o Z S l 0 T f R e P z j E W C g C 7 g H s 6 o c 5 Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Q g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 b u a 3 n 1 B p n s h H M 0 k x i O l Q 8 o g z a q z k j / o 9 m f W r N b f u z k F W i V e Q G h R o 9 q t f v U H C s h i l Y Y J q 3 f X c 1 A Q 5 V Y Y z g d N K L 9 O Y U j a m Q + x a K m m M O s j n x 0 7 J m V U G J E q U L W n I X P 0 9 k d N Y 6 0 k c 2 s 6 Y m p F e 9 m b i f 1 4 3 M 9 F 1 k H O Z Z g Y l W y y K M k F M Q m a f k w F X y I y Y W E K Z 4 v Z W w k Z U U W Z s P h U b g r f 8 8 i p p X d S 9 y / r N w 2 W t c V v E U Y Y T O I V z 8 O A K G n A P T f C B A Y d n e I U 3 R z o v z r v z s W g t O c X M M f y B 8 / k D 3 P 2 O v g = = &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c T u c S w W A i Q K R B d r w h h e 1 f 1 U / i u E = " &gt; A A A B 7 X i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I v g q e x K Q b 0 V v X i s Y D + g X U o 2 z b a x 2 W R J s k J Z + h + 8 e F D E q / / H m / / G t N 2 D t j 4 Y e L w 3 w 8 y 8 M B H c W M / 7 R o W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U M i r V l D W p E k p 3 Q m K Y 4 J I 1 L b e C d R L N S B w K 1 g 7 H t z O / / c S 0 4 U o + 2 E n C g p g M J Y 8 4 J d Z J r W G / l 4 x 4 v 1 z x q t 4 c e J X 4 O a l A j k a / / N U b K J r G T F o q i D F d 3 0 t s k B F t O R V s W u q l h i W E j s m Q d R 2 V J G Y m y O b X T v G Z U w Y 4 U t q V t H i u / p 7 I S G z M J A 5 d Z 0 z s y C x 7 M / E / r 5 v a 6 C r I u E x S y y R d L I p S g a 3 C s 9 f x g G t G r Z g 4 Q q j m 7 l Z M R 0 Q T a l 1 A J R e C v / z y K m l d V P 1 a 9 f q + V q n f 5 H E U 4 Q R O 4 R x 8 u I Q 6 3 E E D m k D h E Z 7 h F d 6 Q Q i / o H X 0 s W g s o n z m G P 0 C f P 5 D k j y U = &lt; / l a t e x i t &gt; Schematic description of the multitask approach. x i , x j : training images. T : image transformation (cropping, brightness, etc.). f θ : encoder network. z: image represented in latent space. g φ : projection to k classes. h ν : projection to lower-dimensional embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Objective. In this embedding space, the loss function aims to maximize the cosine similarity of sample pairs originating from the same image, i.e. sim(ẑ 0 i ,ẑ 1 i ) → 1, whilst minimizing all other pairs originating from two different images, i.e. sim(ẑ a i ,ẑ b j ) → 0, with j ∈ {1, . . . , N } \ i and a, b ∈ {0, 1}. The contrastive loss for sample i is then defined as:L con,i =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Experimental results with contrastive training (CT) under the D in = CIFAR-10, D out = CIFAR-100 setting, showing the histograms of OOD scores s(x) with respect to those of a fixed inlier dataset for (a) a near OOD class and (b) a far OOD class. (c): Out-of-distribution detection performance across the sample-wise CLP spectrum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Failure mode analysis. D in = CIFAR-10; D out = CIFAR-100; GT: CIFAR-100 ground truth label. BL: baseline, CT: contrastive training. The numbers show the percentile rank of the OOD score. In brackets the class of the most activated inlier Gaussian. Green: percentile rank &gt; 50%, indicating relative success of a method at detecting that input as an outlier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>classes (points) w.r.t. a fixed inlier dataset (CIFAR-10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Each point represents performance at detecting one of the classes in CIFAR-100, SVHN, Places365 and Gaussian noise as outliers with respect to a network trained on CIFAR-10. Colored and gray markers correspond to performance from our best model with contrastive training, and our best model without contrastive training respectively. The classes are sorted by increasing similarity to the inlier classes as given by the class-wise confusion log probability (CLP). Contrastive training improves OOD detection results across the board, particularly in the near OOD regime, where the outlier and inlier classes are highly similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrix of the classifier ensemble using the combination of the five reported datasets. Rows and columns represent the true labels and the predictions respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative Analysis of CLP. Three-part dendrogram plot of the classes of CIFAR-10 (red), CIFAR-100 (green), SVHN (blue), Places365 (purple) and Gaussian noise (black) based on the expected confusion matrix combining the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Out-of-distribution detection performance (AUROC). CIFAR-100 D in = CIFAR-10 D in = CIFAR-10 D out = CIFAR-10 D out = CIFAR-100 D out = SVHN CLP= [-4.5 to -2.6] CLP=[-7.4 to -0.8] CLP=[-12.1 to -7.6]</figDesc><table><row><cell></cell><cell>Near OOD</cell><cell cols="2">Near &amp; Far OOD Far OOD</cell><cell></cell></row><row><cell cols="2">Method D in = Softmax probs. [11] 77.1</cell><cell>86.4</cell><cell>89.9</cell><cell>Average 84.5</cell></row><row><cell>ODIN [19]*</cell><cell>77.2</cell><cell>85.8</cell><cell>96.7</cell><cell>86.6</cell></row><row><cell>Mahalanobis [18]*</cell><cell>77.5</cell><cell>88.2</cell><cell>99.1</cell><cell>88.3</cell></row><row><cell>Residual flows [38]*</cell><cell>77.1</cell><cell>89.4</cell><cell>99.1</cell><cell>88.5</cell></row><row><cell cols="2">Outlier exposure [12]  † 75.7  †</cell><cell>93.3  †</cell><cell>98.4  †</cell><cell>89.1  †</cell></row><row><cell>Rotation pred. [13]  ‡</cell><cell>-</cell><cell>90.9</cell><cell>98.9</cell><cell>-</cell></row><row><cell>Gram matrix [31]</cell><cell>67.9</cell><cell>79.0</cell><cell>99.5</cell><cell>82.1</cell></row><row><cell>Ours</cell><cell>78.3</cell><cell>92.9</cell><cell>99.5</cell><cell>90.2</cell></row></table><note>* Uses data explicitly labeled as out-of-distribution for tuning† Uses data explicitly labeled as out-of-distribution for training‡ Uses additional data for pretraining</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of objective function. The baseline model only performs supervised training and we evaluate the impact of incorporating label smoothing (LS) and contrastive training (CT). We report AUROC, as well as the standard deviation of the OOD rank between 5 runs.</figDesc><table><row><cell cols="2">Training</cell><cell></cell><cell>AUROC</cell><cell></cell><cell></cell><cell>OOD rank</cell></row><row><cell cols="2">strategy</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Average standard deviation</cell></row><row><cell>LS</cell><cell>CT</cell><cell cols="2">Din = CIFAR-100 Dout = CIFAR-10 CIFAR-100 CIFAR-10</cell><cell>CIFAR-10 SVHN</cell><cell>CIFAR-100 CIFAR-10</cell><cell>CIFAR-10 CIFAR-100</cell><cell>CIFAR-10 SVHN</cell></row><row><cell>x</cell><cell>x</cell><cell>63.9 ± 0.3</cell><cell cols="2">81.1 ± 0.2 96.7 ± 0.2</cell><cell>23.3</cell><cell>20.7</cell><cell>6.7</cell></row><row><cell></cell><cell>x</cell><cell>74.1 ± 0.4</cell><cell cols="2">90.8 ± 0.1 99.2 ± 0.1</cell><cell>20.3</cell><cell>12.8</cell><cell>2.0</cell></row><row><cell>x</cell><cell></cell><cell>72.1 ± 0.4</cell><cell cols="2">90.9 ± 0.2 98.8 ± 0.2</cell><cell>23.7</cell><cell>12.5</cell><cell>2.4</cell></row><row><cell></cell><cell></cell><cell>78.3 ± 0.2</cell><cell cols="2">92.9 ± 0.2 99.5 ± 0.1</cell><cell>19.3</cell><cell>10.6</cell><cell>1.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ranging from 1 (the default ResNet-50) to 4. We observe that decreasing the ResNet-50 width to 1 leads to a significant drop on both near and far OOD dataset pairs. On the other hand, increasing the width to 4 does not result in improvement on OOD detection performance. We hypothesize that using contrastive training mandates the need for a higher capacity model in order to capture a richer representation with general task-agnostic features, and thus to achieve optimal performance compared to a supervised only baseline. Effect of temperature parameter of the contrastive loss. Finally, we run experiments to understand the importance of the temperature parameter τ used in the contrastive loss. We consider τ values ∈ [0.01, 0.1, 0.5, 1, 2] for our experiments. Unlike <ref type="bibr" target="#b2">[3]</ref> which obtains optimal performance with a τ of 0.1, we find that higher temperature leads to improved performance for the OOD detection task across the board. The optimal performance is obtained with a τ of 1. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A variational dirichlet framework for out-ofdistribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07308</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Waic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning confidence for out-of-distribution detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04865</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A. Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15637" to="15648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Why is the mahalanobis distance effective for anomaly detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kamoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00402</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training confidence-calibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-ofdistribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metric learning for novelty and anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised learning for generalizable out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hybrid models with deep and invertible features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clune</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14680" to="14691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting out-of-distribution examples with gram matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Rezende Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection using multiple semantic label representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7375" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Out-of-distribution detection using an ensemble of self supervised leave-out classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="550" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hybrid models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12506</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep residual flow for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zisselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05419</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">We investigate the sensitivity of our models to the supervised loss multiplier parameter λ in our objective function defined in Section 3 which controls the strength of the supervised loss during the finetuning stage. We consider λ ∈ [0, 1, 10, 100, 1000] for our experiments. We observe that increasing the loss multiplier leads to particularly significant improvement on the near OOD dataset pair with D in as CIFAR-100 and D out as CIFAR-10 but gains are marginal beyond λ = 100</title>
	</analytic>
	<monogr>
		<title level="m">F Additional Ablations Sensitivity to supervised loss multiplier</title>
		<imprint/>
	</monogr>
	<note>We conjecture that the large loss ratios are necessary due to the difference in scale of the supervised and contrastive loss</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Our reference model is a wide ResNet-50 with a multiplier of 3. We investigate the impact of model capacity by training models for other width multipliers</title>
		<imprint/>
	</monogr>
	<note>Impact of increasing model capacity</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
