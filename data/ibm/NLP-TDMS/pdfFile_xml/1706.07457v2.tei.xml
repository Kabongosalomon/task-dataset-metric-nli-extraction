<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial-Aware Regressions for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial-Aware Regressions for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking, which aims to continuously estimate the positions and scales of a pre-specified target, has been a hot topic for the last decades. It is widely used in numerous vision tasks, such as video surveillance, augmented reality and so on. Current algorithms have achieved very impressive results, however, many problems remain to be solved.</p><p>With the emergence of large-scale datasets, deep neural networks have shown their great capacity in object classification, image identification, to name a few. It has been verified in many prior papers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref> that trackers based on convolutional neural networks (CNNs) can significantly improve the tracking performance. Usually, these methods pretrain their networks on a large scale dataset, and finetune the networks with the ground-truth data in the first frame of a sequence. In addition to the CNN-based trackers, methods based on the kernelized correlation filter (KCF) are also very popular in recent years for the efficiency and capacity to utilize large numbers of negative samples. As is de- scribed in <ref type="bibr" target="#b16">[17]</ref>, the KCF method is essentially the kernelized ridge regression (KRR) with cyclically shifted samples. Methods based on the KCF usually take a region of interest as the input, which makes it very difficult to exploit the structural information of the target. In addition, the cyclically constructed samples also introduce the unwanted boundary effects. Compared to the KCF method, the dominating reason why the conventional KRR is not widely applied is that it has to compute a kernel matrix for large numbers of samples, which results in heavy computational load.</p><p>Both the CNN-and KRR-based trackers have limitations and they have complementarities. The CNN-based trackers usually contain a large number of parameters which are difficult to be finetuned in the tracking problem. As a result, the trained filter kernels in the convolution layer are usually highly correlated and tend to overfit the training data. On the contrary, the KRR-based trackers have limited model parameters (equal to the number of training samples), and cannot learn discriminative enough models when training samples are correlated. In addition, the existing KRR-based methods assume that each part of the target is equally important, and ignore the relationship among different parts.</p><p>In this paper, we exploit both the KRR and CNN models, and learn the complementary spatial-aware regressions for visual tracking (LSART). First, we propose a kernelized ridge regression with cross-patch similarities. We assign each similarity score a weight, and simultaneously learn this weight and ridge regression model parameters. We show that the proposed ridge regression model can be reformulated as a neural network, which is more efficiently optimized than the original form. Second, we introduce the spatially regularized kernels into the fully convolutional neural network. By imposing spatial constraints on the filter kernels, we enforce each output channel of the convolution layer to have a response for a specific localized region. We exploit the distance transform pooling layer to determine the effectiveness of the outputs from the convolution layer, and develop a two-stage training strategy to update the CNN model effectively. Finally, the heat maps obtained by the KRR and CNN models are combined to generate a final heat map for target location. Experiments on the popular datasets demonstrate that our tracker performs significantly better than other state-of-the-art methods (see <ref type="figure" target="#fig_0">Figure 1</ref> for visualized tracking results).</p><p>The main contributions of this paper can be summarized as follows:</p><p>1. We develop a spatial-aware KRR model by introducing a cross-patch similarity kernel. This model can model both regression coefficients and patch reliability, which enables our model to be robust to the unreliable patches. The regression coefficient and similarity weight vectors are simultaneously optimized via an end-to-end neural network, which is new in visual tracking and facilitates seamlessly integrating our model with deep feature extraction networks.</p><p>2. We propose the spatially regularized filter kernels for CNN, which enforces each filter kernel to focus on a localized region. We also design a two-stream training network to effectively learn network parameters, which avoids overfitting and considers rotation information.</p><p>3. We propose the complementary KRR and CNN models based on their inherent limitations. The spatial-aware KRR model focuses on the holistic object and the spatialaware CNN model focuses on small and localized regions, thereby complementing each other for better performance. 4. Our method achieves very promising tracking performance, especially on the recent VOT-2017 public dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Algorithms of visual tracking mainly focus on designing robust appearance models, which are roughly categorized as generative and discriminative models. With the progress of deep neural networks and correlation filters, discriminative appearance models are preferred in the recent works.</p><p>Trackers based on correlation filter have attracted more and more attention for the advantages in efficiency and robustness. In essential, correlation filter can be viewed as a kernelized ridge regression (KRR) model that can be speeded up in the frequency domain. Bolme et al. <ref type="bibr" target="#b2">[3]</ref> exploit the correlation filter with minimum output sum of squared error for visual tracking. As fast Fourier transform is used, the tracker achieves very fast tracking performance. In <ref type="bibr" target="#b15">[16]</ref>, Henriques et al. first incorporate kernel functions into the correlation filter, which is named as CSK. The CSK tracker can also be solved via fast Fourier transform, thus is efficient. Based on <ref type="bibr" target="#b15">[16]</ref>, the tracking method <ref type="bibr" target="#b16">[17]</ref> further improves the CSK tracker by using the histogram of gradient features. Ma et al. <ref type="bibr" target="#b22">[23]</ref> exploit complementary nature of features extracted from three layers of CNN, and use a coarse-to-fine scheme for target searching. Based on <ref type="bibr" target="#b22">[23]</ref>, an online adaptive Hedge method <ref type="bibr" target="#b25">[26]</ref> is designed, which takes both the short-term and long-term regrets into consideration. In this method, they use the CF-based tracker defined on a single CNN layer as an expert and learn the adaptive weights for different experts. Danelljan et al. propose several CF-based trackers with good performance. The SRDCF method <ref type="bibr" target="#b7">[8]</ref> tries to suppress the boundary effects of the correlation filter by multiplying the filter coefficients with spatial regularization weights produced by a Gaussian distribution. This tracker achieves very good performance even with hand-crafted features. Based on <ref type="bibr" target="#b7">[8]</ref>, they propose an adaptive decontamination method <ref type="bibr" target="#b8">[9]</ref> for the correlation filter, which adaptively learns the reliability of each training sample and eliminates the influence of contaminated ones. Furthermore, the learning process for correlation filter is conducted in the continuous spatial domain of various feature maps <ref type="bibr" target="#b9">[10]</ref>, which incorporates the sub-pixel information. These methods usually exploit the linear or Gaussian kernel to depict the similarity between the target region and a given candidate region. They inevitably ignore the intrinsic structure information within the target region, which makes the trackers be less effective in dealing with occlusion and deformation challenges. In this work, we introduce a novel kernel to model the cross-patch similarities, develop the corresponding KRR optimization model, and provide a network structure to solve it efficiently.</p><p>Compared with trackers based on correlation filters, CNN-based trackers also achieve good performance in recent years. In <ref type="bibr" target="#b20">[21]</ref>, a shallow network with two convolution layers is proposed, which learns the feature representation and classifier simultaneously. In <ref type="bibr" target="#b30">[31]</ref>, Wang et al. transfer the model pre-trained on image classification dataset to visual tracking and exploit the fully convolutional neural network for target location. Wang et al. <ref type="bibr" target="#b31">[32]</ref> propose a sequentially training fashion for neural network to avoid the overfitting problem. Tao et al. <ref type="bibr" target="#b28">[29]</ref> off-line train a Siamese deep neural network on large amounts of extra video se-quences, and directly apply this model to conduct the optimal match during the tracking process. In <ref type="bibr" target="#b24">[25]</ref>, a multidomain CNN-based tracker is proposed, in which the shared layers are used to obtain generic target representation and the domain specific layers are adopted for classification. These CNN-based trackers usually exploit the global information regarding the target object and therefore ignore its spatial layouts. To address this issue, we propose a convolution layer with spatially regularized filter kernels to focus on local regions of the target. Besides, we effectively combine the proposed KRR and CNN models to develop a robust tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatial-Aware KRR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">KRR with Cross-Patch Similarity (KRRCPS)</head><formula xml:id="formula_0">Given N training samples {(x i , y i )} N i=1 in frame t,</formula><p>the conventional ridge regression can be formulated as</p><formula xml:id="formula_1">w t = arg min wt i (y i − w t x i ) 2 + λ w t 2 ,<label>(1)</label></formula><p>where x i ∈ R d×1 denotes the feature vector for sample i, and y i is its sample label. In Eq. 1, w t ∈ R d×1 denotes the linear weight vector, and λ is a regularization constant. According to the representer theorem <ref type="bibr" target="#b27">[28]</ref>, the model parameter w t can be determined as the weighted sum of the training samples (i.e., w t = N i=1 α i t x i ). Thus, the optimization problem (1) can be rewritten as</p><formula xml:id="formula_2">α * t = arg min αt N i=1 (y i − N j=1 α t j k ij ) 2 + λ N i,j=1 α t i α t j k i,j ,<label>(2)</label></formula><p>where α t i is the weight for sample i, α t = [α t 1 , ..., α t N ] ∈ R N ×1 , and k i,j is the kernel value computed between features x i and x j . The existing kernel definitions (e.g., linear kernel and Gaussian kernel) do not fully consider the spatial layouts of the target, which limits the tracking performance.</p><p>In this work, we introduce a kernel function which considers the similarity of all pairs of patches between two samples. Especially, we divide each sample i into M patches, and obtain features for each patch m as x m i , then the kernel value between sample i and j is computed as</p><formula xml:id="formula_3">k ij = M m,n=1 β t m,n x m i x n j ,<label>(3)</label></formula><p>where β t m,n denotes the weight for the similarity score between m-and n-th patches. This kernel function has at least two advantages: (a) for each similarity score, the kernel function assigns a learnable weight to make the model adaptively focus on the similarity scores of reliable regions;  <ref type="bibr">9 ,9</ref> , , , ,</p><formula xml:id="formula_4">B C 1 t v 3 t v 4 t v 5 t v 6 t v 7 t v 8 t v 9 t v 1~9 t v 1,1 1, 1,9 , , , , n       b b b 9 ,<label>1 9 ,</label></formula><formula xml:id="formula_5">n       b b b 1,1 , 9,9 , , , , m n b b b 9 f 1 f 1~9 t v 2 t v Figure 2.</formula><p>The reformulated network structure for our KRRCPS model. In Module B, we show two examples on how to crop the feature maps for f1 and f9. By doing this, we make the responses generated by different fragments correspond to the same input ROI region. Best viewed in high resolutions with zoom in.</p><p>(b) more similarity pairs between patches are considered, thereby enhancing the discriminant ability of the model. By substituting Eq.3 into Eq.2 and introducing a regularization term, we obtain the optimization problem <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_6">α * t , β * t = arg min αt,βt J (α t , β t ) ,<label>(4)</label></formula><p>where we define J (α, β) as</p><formula xml:id="formula_7">J (α, β) = N i=1 (y i − N j=1 α t j M m,n=1 β t m,n x m j x n i ) 2 +λ 1 N i,j=1 α t i k i,j α t j + λ 2 M m,n=1 β t m,n 2 = y − M m,n=1 f m β t m,n f n α t 2 2 + λ 1 α t Kα t + λ 2 β t 2 2 , (5) where β t = β t 1,1 , β t 1,2 , ..., β t M,M is the weight vector for all cross patches, f m = [x m 1 , ..., x m N ]</formula><p>stands for the concatenated feature matrix for the m-th patch of N samples, and K denotes the kernel matrix whose (i, j)-th element is</p><formula xml:id="formula_8">k ij = M m,n=1 β t m,n x m i x n j .</formula><p>A conventional solver for the optimization problem (4) is the alternating iteration algorithm that optimizes α t and β t iteratively. If β t is fixed, the analytical solution for α t can be obtained as α t = (K + λ 1 I) −1 y (I denotes an identity matrix), whose computation complexity is O(N 3 ). If α t is fixed, β t can be updated via the gradient descent algorithm, and it is easy to know that the computation complexity for this process is O(dN 2 ). Thus, optimizing α t and β t via the alternating iteration algorithm is very time consuming for the online update process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Structure for KRRCPS</head><p>In this work, we attempt to learn α t and β t by reformulating the proposed ridge regression into a neural network. This reformulation not only provides an efficient solver but also enables the algorithm to be seamlessly integrated with state-of-the-art deep feature extraction networks.</p><p>In Eq. 5, the response term r = M m,n=1 f m β t m,n f n α t can be sequentially calculated by the following three steps:</p><formula xml:id="formula_9">A : v t n = fnαt B : bm,n = f m v t n C : r = M m,n=1 β t m,n bm,n .<label>(6)</label></formula><p>Thus, we develop an equivalent neural network of our regression model (shown in <ref type="figure">Figure 2</ref>), which takes the extracted feature map as the input and outputs a heat map for target localization. This network consists of three modules, each of which precisely corresponds to one of the three operations in Eq. 6.</p><p>Module A: Given the target location, we first crop a rectangle region around the target object, and obtain the feature map X t with size H × W × C, thus the target size projected on the feature map is h × w. Based on the projected target size, we densely crop samples and reshape each sample to a d (= h × w × C) dimensional vector. This results in a feature matrix D t ∈ R d×N , based on which the output of the weighted sum layer is obtained as</p><formula xml:id="formula_10">z = Dtαt,<label>(7)</label></formula><p>where z ∈ R d×1 denotes the response for the current layer and α t is the weight vector to be learned. We reshape the vector z to a response map with size h × w × C, and then</p><formula xml:id="formula_11">divide it into M = √ M × √ M sub-responses with size (h/ √ M ) × (w/ √ M ) × C. We use v t 1 .</formula><p>..v t M to denote these sub-responses in <ref type="figure">Figure 2</ref>. Module B: This module corresponds to the second equation of Eq. 6, which is equivalent to a convolution layer. Noticing that the feature map corresponding to f m , m ∈ {1, ..., M } is a sub-region of the input feature map X t and is different when m varies, we first obtain the feature map corresponding to f m by cropping X t (see Module B in <ref type="figure">Figure 2</ref> for example) and feed it into a convolution layer which takes v t 1 ...v t M as an ensemble of filter kernels. As we have M patches in total, the convolution layer has M outputs, each of which has the size (</p><formula xml:id="formula_12">H − h + 1) × (W − w + 1) × M .</formula><p>Module C: We concatenate the M outputs of Module B through the Concat layer, and input the concatenated feature maps into a convolution layer, whose kernel size is 1 × 1 × M 2 . This module corresponds to operation C in Eq. 6, and the filter kernel corresponds to β t .</p><p>In this work, we use the backpropagation algorithm to solve this network. The computational complexity for both forward and backward propagations is O(dN ), which is much more efficient than the original solver. Note that our network is defined based on Eq. 5 for model learning. At the detection stage in frame t, we just need to replace D t , α t and β t in the network withD t ,α t andβ t which are iteratively updated using Eq. 8 (η is the update rate).</p><formula xml:id="formula_13">D t = (1 − η)D t−1 + ηD t−1 α t = (1 − η)α t−1 + ηα t−1 β t = (1 − η)β t + ηβ t−1 .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Spatial-Aware CNN</head><p>As is described in previous papers (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>), the spatial information plays an important role in a visual tracking system. However, the spatial layouts of the target object are usually ignored in the current CNN-based tracking algorithms. What is more, the existing algorithms do not perform well when the target object suffers from a severe inplane rotation. In this work, we propose a convolution layer with spatially regularized filter kernels, through which each convolution kernel only focuses on a specific region of the target. In addition, as the training samples in visual tracking are very limited, we develop a two-stream training strategy to avoid overfitting and consider rotation information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN with Spatially Regularized Kernels (CNNSRK)</head><p>The proposed CNN framework with spatially regularized kernels is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref> (a), which consists of two convolution layers interleaved with an ReLU layer and several distance transform pooling layers. Given the input feature map, we first reshape it to 46 × 46 × C, where C is the channel number. Based on the input feature map, the first convolution layer (conv1) has a kernel size of 5 × 5, and outputs a feature map with size 46 × 46 × C 1 . The second convolution layer (conv2) has a kernel size of 3 × 3 (we divide the input feature into C 1 groups), and outputs a 46 × 46 × C 1 response map, each channel of which is a heat map for the target localization. After that, we divide the response map into several groups (C 1 /4 groups in our implementation), and sum the responses in each group through the channel dimension, which results in a 46×46×1 output for each group. Finally, the obtained response maps are fed into distance transform pooling layers, and the outputs are effectively combined to produce a final heat map for target localization. The detailed structure is shown in <ref type="figure" target="#fig_3">Figure 3</ref>   the training process. By doing this, the learned convolution layers are forced to focus on different parts of the input feature map. But the discriminant ability of their model is weak as it merely takes parts of the input feature map for consideration in the training process. In this work, instead of introducing constraints on the input feature map, we enforce constraints on the filter kernels in the convolution layer. Compared with <ref type="bibr" target="#b29">[30]</ref>, our method considers the spatial information, and fixes the filter mask during the tracking process. Let F c ∈ R K h ×Kw×Kc denotes the filter kernel weights associated with the c-th channel of the output feature map O c . We introduce the spatial regularization weights W c into the convolution layer, which has the same size as F c . By considering the spatial regularization weights, the output feature map O c can be calculated as,</p><formula xml:id="formula_14">O c = (F c W c ) * X c + b,<label>(9)</label></formula><p>where * denotes the convolution operation, stands for the Hadamard product, X c represents the input feature map and b is the bias term. For constructing W c , we first generate a binary mask M c of size K h × K w through Bernoulli distribution B(0.3) and then construct W c based on this mask,</p><formula xml:id="formula_15">W c (p, q, r) = M c (p, q),<label>(10)</label></formula><p>where p, q and r denote the indexes of a 3-dimensional matrix. Clearly, only parts of the spatial regions in W c have non-zero values, which forces the filter kernels to focus on different regions.</p><p>Distance Transform Pooling Layer: The distance transform has been used in previous studies for object detection ( <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref>). Girshick et al. <ref type="bibr" target="#b12">[13]</ref> point out that distance transform is indeed a generalization of the max pooling layer and can be expressed in a similar formula as max pooling.</p><p>Given a function y = f (x) defined on a regular grid G, its distance transform can be calculated as,</p><formula xml:id="formula_16">D f (s) = max t∈G (f (t) − d(s − t)).<label>(11)</label></formula><p>Here</p><formula xml:id="formula_17">d(s − t) is a convex quadratic function with d(s − t) = (s − t) 2 + θ(s − t)</formula><p>, where and θ are learnable parameters. The distance transform pooling layer can be used to estimate the reliability of the input feature map. Generally speaking, the larger the learned value is, the more reliable the input feature map is. When = θ = 0, this layer outputs a response with constant values, which means that the input feature map does not influence the tracking result. In this work, the distance transform pooling layer is implemented in the Caffe framework according to <ref type="bibr" target="#b13">[14]</ref> and the pooling region is bounded for efficient computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Two-Stream Training Strategy</head><p>Based on our observation that the tracking performance is inevitably influenced when severe in-plane-rotation 1 occurs, we develop a two-stream network to learn the weights of convolution and distance transform pooling layers. The proposed two-stream network is presented in <ref type="figure" target="#fig_3">Figure 3 (b)</ref>, in which the weights of the convolution layers are shared.</p><p>The upper branch of the network exploits the same feature map as the original network <ref type="figure" target="#fig_3">(Figure 3 (a)</ref>), and the lower branch uses the input feature map corresponding to the rotated target object. The previous two layers are the same as the original network, resulting in two 46 × 46 × C 1 response maps in both upper and lower branches. Then, we conduct the max-out pooling operation on these two response maps to produce a 46 × 46 × C 1 response. We compute the loss for each channel of the response maps and propagate the loss backward to learn the filters in convolution layers. After that, we fix the convolution layers and  learn the model parameters of the distance transform pooling layers. Some visual results are illustrated in <ref type="figure" target="#fig_5">Figure 4</ref>, from which we can see that the proposed two-stream training strategy facilitates dealing with severe in-plane rotation.</p><p>In addition, the reason why we do not directly perform model learning on the original network (presented in <ref type="figure" target="#fig_3">Figure 3 (a)</ref>) is to avoid the overfitting problem. For example, the original network has a total of 5 × 5 × C × C 1 + 3 × 3 × C 1 + C 1 parameters. If C = 512 and C 1 = 100, there will be a total of 1281000 parameters, which are very difficult to be learned with limited training data in visual tracking. The proposed two-stream strategy essentially decomposes the network into several sub-parts and trains each part separately, thereby facilitating avoiding overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Tracking with Spatial-Aware KRR and CNN</head><p>In this section, we describe how to exploit both spatialaware KRR and CNN models for robust visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Target Location Estimation</head><p>We conduct visual tracking by combining the responses of our KRR and CNN models. The former one captures the holistic information of the target, while the latter one focuses more on the localized region. In frame t, we crop a search region centered by the estimated object location of the last frame and then obtain a feature map X t for this region. Furthermore, the final heat map of our tracker can be computed as,</p><formula xml:id="formula_18">f (X t ) = f KRR (X t ) + γf CN N (X t ),<label>(12)</label></formula><p>where γ is a trade-off parameter. f KRR (X t ) and f CN N (X t ) denote the heat maps produced by KRR and CNN models, respectively. Finally, we find the position with highest heat map score in f (X t ) and determine it as the optimal location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scale Estimation</head><p>It is not enough to only provide the location for a target object, which may experience drastic scale variation. In this work, we also estimate the scale variation of the target after location estimation. We use S to denote the candidate scale size and H × W to denote the input feature map size. For each s l ∈ − S−1 2 , ..., S−1 2 , we crop or pad the input feature map to size a s l H × a s l W and reshape it to H × W (we use T (X t , s l ) to denote the transformed feature map for scale l). These transformed maps are then fed into a fully connected layer to output the scale scores. Finally, we choose the scale related to the largest score as the optimal state. After scale estimation, we further exploit the bounding box regression method <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> to refine the tracking result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Model Update</head><p>After obtaining the optimal location and scale, we crop the corresponding search region and extract its feature map X t . Then, we generate an ideal heat map of Gaussian distribution <ref type="bibr" target="#b16">[17]</ref> and exploit the L2 loss function for finetuning both KRR and CNN models to fit the ideal heat map. In this work, the stochastic gradient descent (SGD) method is adopted for finetuning both networks. For KRR, after model parameters α t and β t are updated, we further updateD t ,α t andβ t based on Eq. 8 . For CNN, our two-stream network (Section 4.2) is adopted for updating model parameters.</p><p>In addition, if scale variation is detected, we update the scale estimation network with the loss function L S = 1 2 y s l − f S (T (X t , s l )) 2 2 (f S denotes the score obtained by the network, y s l = exp(− 1 2σ 2 s 2 l ) is a Gaussian function).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section, we first introduce the experimental setups, and then report the experimental results of different trackers on the OTB-2015 and VOT-2017 public datasets. In addition, we verify the effectiveness of different components of the proposed method. Our source codes can be downloaded at https://github.com/ cswaynecool/LSART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>The proposed tracker is implemented with MAT-LAB2014a on an Intel 4.0 GHz CPU with 32G memory, and runs at around 1fps during online tracking. We use the Caffe toolbox <ref type="bibr" target="#b17">[18]</ref> to implement the networks, whose forward and backward operations are conducted on a Nvidia Titan X GPU. We divide the target into 9 (3 × 3) patches in the kernelized ridge regression model. The trade-off parameters λ 1 , λ 2 are set as 0.001 and 0.001 respectively. The learning rate η is set as 0.2 in the first ten frames, and changed to a smaller learning rate (e.g. 0.001) during the tracking process. All the networks are trained with the SGD method, and the learning rates for α t and β t in the KRR network are set as 8e-9 and 1.6, while the learning rate for each layer in the CNN network is fixed as 8e-7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">OTB-2015 Dataset</head><p>The OTB-2015 dataset <ref type="bibr" target="#b33">[34]</ref> is one of the most commonly used benchmarks in evaluating different trackers. This dataset includes 100 challenging image sequences with 11 different attributes, such as illumination variation, background clutter, scale variation, fast motion, in-plane rotation and so on. In this dataset, we exploit the the output of the Conv4-3 layer of the VGG-16 net as the basic feature for our tracker, and compare our LSART method with 12 state-of-the-art trackers including DSST <ref type="bibr" target="#b5">[6]</ref>, ECO <ref type="bibr" target="#b4">[5]</ref>, CCOT <ref type="bibr" target="#b9">[10]</ref>, SRDCF <ref type="bibr" target="#b7">[8]</ref>, KCF <ref type="bibr" target="#b16">[17]</ref>, DeepSRDCF <ref type="bibr" target="#b6">[7]</ref>, CF2 <ref type="bibr" target="#b22">[23]</ref>, LCT <ref type="bibr" target="#b23">[24]</ref>, SRDCFdecon <ref type="bibr" target="#b8">[9]</ref>, HDT <ref type="bibr" target="#b25">[26]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref> and MEEM <ref type="bibr" target="#b34">[35]</ref>. We exploit the one-pass evaluation (OPE) for all the trackers and report both the precision and success plots for comparison. The precision plots aim to measure the percentage of frames in which the distance between the tracked result and the ground-truth is under a threshold, while the success plots aim to measure the successfully tracked frames with various thresholds. Following <ref type="bibr" target="#b32">[33]</ref>, in the precision plots, we use the distance precision rate at threshold 20 for ranking, while in the success plots, we use the area under curve (AUC) for ranking. <ref type="figure">Figure 5</ref> illustrates both precision and success plots over all 100 videos in this dataset. We can see that our LSART method achieves the best performance with a precision score 92.3% and the second best result with an AUC score of 67.2%. Overall, in OTB-2015, our tracker has comparable performance compared to the existing best tracker ECO.</p><p>Besides, we evaluate different trackers using 8 attributes and report their precision plots in <ref type="figure" target="#fig_7">Figure 6</ref>. The results show that our tracker achieves very promising performance in handling most of the challenges, especially for deformation and in-plane rotation. First, the proposed LSART method improves the second best one by 4.3% in the attribute of deformation. This improvement is mainly because that our tracker determines the reliability of the target object adaptively and is insusceptible to the unreliable regions. In addition, for in-plane rotation, our method improves the ECO method by 1.8%. The underlying reason is that our tracker exploits the two-stream network to learn the rotation information of the target object effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">VOT-2017 Public Dataset</head><p>For more thorough evaluations, we test our LSART tracker on the VOT-2017 public dataset <ref type="bibr" target="#b19">[20]</ref> in comparison with 11 state-of-the-art methods, including ECO <ref type="bibr" target="#b4">[5]</ref>, CCOT <ref type="bibr" target="#b9">[10]</ref>, CFCF <ref type="bibr" target="#b14">[15]</ref>, MCPF <ref type="bibr" target="#b35">[36]</ref>, CRT <ref type="bibr" target="#b3">[4]</ref>, ECOhc <ref type="bibr" target="#b4">[5]</ref>, MEEM <ref type="bibr" target="#b34">[35]</ref>, FSTC <ref type="bibr" target="#b30">[31]</ref>, Staple <ref type="bibr" target="#b1">[2]</ref>, KCF <ref type="bibr" target="#b16">[17]</ref>   <ref type="figure">Figure 5</ref>. Precision and success plots on the OTB-2015 dataset in terms of OPE rule. In the legend, we show the distance precision rates at threshold 20 and area under curve (AUC) scores, based on which different trackers are ranked.  <ref type="bibr" target="#b7">[8]</ref>. Since many top-ranked trackers (e.g. CFCF, ECO and CCOT) exploit a combination of CNN and handcrafted features, we extend our feature set with HOG and Color Naming like CCOT in this dataset. The VOT-2017 public dataset is one of the most recent datasets for evaluating online model-free single-object trackers, and includes 60 public image sequences with different challenging factors. Following the evaluation protocol of VOT-2017 <ref type="bibr" target="#b19">[20]</ref>, we adopt the expected average overlap (EAO), accuracy and robustness raw values (A, R) and no-reset-based average overlap (AO) to compare different trackers. The detailed comparisons are reported in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, we can conclude that the proposed LSART method achieves the top-ranked performance in terms of EAO, R and AO criteria while maintaining a competitive accuracy. Especially, our LSART tracker has the best performance among the compared trackers in terms of the EAO measure, which is the most important metric on the VOT dataset. Compared with the second best tracker (CFCF), the proposed method achieves a relative performance gain of 12.94%. In addition, our tracker achieves a substantial improvement over the popular ECO method, with a relative gain of 15.36% in EAO. Note that our tracker has an EAO of 0.275 without the hand-crafted features, which still out-   performs CCOT and is competitive among the compared trackers. The OPE rule is also adopted to evaluate different trackers and the AO values are reported to demonstrate their performance. From the last column in <ref type="table" target="#tab_1">Table 1</ref>, we can see that our method achieves comparable performance compared to the MCPF tracker and improves the ECO method by a relative gain of 8.71%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Ablation Studies</head><p>In this paper, we propose two complementary spatial aware regressions for visual tracking, which are respectively the kernelized ridge regression model with crosspatch similarity and convolution neural network with the spatially regularized kernels. Here, we conduct ablation analysis to evaluate each component of our tracker. With different experimental settings, we obtain the following 4 variants of our tracker, which are respectively named as "Baseline", "Baseline+CPS", "Baseline+SRK" and "Base-line+CPS+SRK" (LSART). We use the shorthand "Baseline" to denote the method that directly combines the conventional KRR and CNN models, and adopt the abbreviations "CPS", "SRK" to denote the cross-patch similarity kernel and spatial regularized filter kernel. Using OTB-2015, the results of different variants are presented in <ref type="figure" target="#fig_9">Figure 7</ref>.</p><p>First, the direct combination of conventional KRR and CNN models (i.e., "Baseline") cannot achieve satisfac-tory performance (0.841 in precision score and 0.606 in AUC score). Second, the effectiveness of the CPS module can be verified comparing"Baseline+CPS" with "Baseline", which contributes to the relative performance gains of 5.35% and 6.93% in precision and success plots. The effectiveness of the SRK module can be validated by comparing "Baseline+SRK" with "Baseline". Finally, we can see that our LSART method ("Baseline+CPS+SRK") improves the original "Baseline" method by relative gains of 9.75% in precision plots and 10.89% in success plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper proposes a robust online tracker by exploiting both spatial-aware KRR and spatial-aware CNN. First, we propose a novel KRR model with cross-patch similarity (CPS). This model considers the interior structure of the target, and can adaptively determine the importance of the similarity score between two patches. We show that the KR-RCPS model can be reformulated as a neural network, and thus can be more efficiently solved. In addition, we propose a complementary CNN model which focuses more on the localized region via a spatially regularized filter kernel. Distance transform pooling layers are further exploited to determine the reliability of the convolutional layers. Finally, the above-mentioned two models are effectively combined to generate a final heat map for target location. Experimental results on two recent benchmarks demonstrate that the proposed LSART method achieves very promising tracking performance, especially on the VOT-2017 public dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example tracking results of different methods on the OTB dataset. Our tracker (LSART) has comparable results compared with the state-of-the-art algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a). Convolution Layer with Spatially Regularized Filter Kernels: The target object may experience deformations and occlusions during tracking, which makes some parts of the target object more important than others. To address this issue, Wang et al.<ref type="bibr" target="#b31">[32]</ref> modifies the implementation of the Dropout layer and keep the dropped activations fixed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Network structures for our CNNSRK model. (a) The testing network for CNNSRK. We use a convolutional neural network to estimate the target position and exploit the distance transform pooling layer to determine the effectiveness of each response map. (b) The training network for CNNSRK. We utilize the two-stage training strategy to update/train the convolution and distance transform pooling layers separately. A two-stream network is used to learn the rotation information of the target. Best viewed in color with zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Tracking results with and without two-stream training process are illustrated in (a) and (b), respectively. We can see that the proposed two-stream training strategy makes the tracker perform better when severe in-plane rotation occurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Performance evaluation on different attributes of OTB-2015 in terms of the OPE criterion. The reported attributes include deformation, background clutter, scale variation, fast motion, in-plane rotation, illumination variation, out-of-plane rotation, low resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Performance evaluation for each component of the proposed tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance evaluation for 12 state-of-the-art algorithms on the VOT-2017 public dataset. The best three results are marked in red, blue and green bold fonts respectively.</figDesc><table><row><cell>Tracker</cell><cell>EAO</cell><cell>A</cell><cell>R</cell><cell>AO</cell></row><row><cell cols="5">LSART 0.323 0.493 0.218 0.437</cell></row><row><cell>CFCF</cell><cell cols="4">0.286 0.509 0.281 0.380</cell></row><row><cell>ECO</cell><cell cols="4">0.280 0.483 0.276 0.402</cell></row><row><cell>CCOT</cell><cell cols="4">0.267 0.494 0.318 0.390</cell></row><row><cell>MCPF</cell><cell cols="4">0.248 0.510 0.427 0.443</cell></row><row><cell>CRT</cell><cell cols="4">0.244 0.463 0.337 0.370</cell></row><row><cell cols="5">ECOhc 0.238 0.494 0.435 0.335</cell></row><row><cell cols="5">MEEM 0.192 0.463 0.534 0.328</cell></row><row><cell>FSTC</cell><cell cols="4">0.188 0.480 0.534 0.334</cell></row><row><cell>Staple</cell><cell cols="4">0.169 0.530 0.688 0.335</cell></row><row><cell>KCF</cell><cell cols="4">0.135 0.447 0.773 0.267</cell></row><row><cell cols="5">SRDCF 0.119 0.490 0.974 0.246</cell></row><row><cell>SRDCF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Rotation angle is larger than 90 degrees.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This paper is partially supported by the Natural Science Foundation of China #61725202, #61502070, #61472060, NSF CAREER (No. 1149783), gifts from Adobe, Toyota, Panasonic, Samsung, NEC, Verisk and Nvidia. Chong Sun is also supported by the China Scholarship Council (CSC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragmentsbased tracking using the integral histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04215</idno>
		<title level="m">Convolutional regression for visual tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive decontamination of the training set: A unified formulation for discriminative visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1319" to="1327" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Good features to correlate for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gundogdu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alatan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sowp: Spatially ordered and weighted patch descriptor for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust online visual tracking with a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep visual tracking: Review and experimental comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="323" to="338" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hedged deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable part models with cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generalized representer theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stct: Sequentially training convolutional networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meem: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-task correlation particle filter for robust object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
