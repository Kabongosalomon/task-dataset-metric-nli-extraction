<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
							<email>keyulu@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Mit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
							<email>weihuahu@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.</p><p>Here, we present a theoretical framework for analyzing the representational power of GNNs. We formally characterize how expressive different GNN variants are in learning to represent and distinguish between different graph structures. Our framework is inspired by the close connection between GNNs and the Weisfeiler-Lehman (WL) graph isomorphism test <ref type="bibr" target="#b36">(Weisfeiler &amp; Lehman, 1968</ref>), a powerful test known to distinguish a broad class of graphs <ref type="bibr" target="#b2">(Babai &amp; Kucera, 1979)</ref>. Similar to GNNs, the WL test iteratively updates a given node's feature vector by aggregating feature vectors of its network neighbors. What makes the WL test so powerful is its injective aggregation update that maps different node neighborhoods to different feature vectors. Our key insight is that a GNN can have as large discriminative power as the WL test if the GNN's aggregation scheme is highly expressive and can model injective functions.</p><p>To mathematically formalize the above insight, our framework first represents the set of feature vectors of a given node's neighbors as a multiset, i.e., a set with possibly repeating elements. Then, the neighbor aggregation in GNNs can be thought of as an aggregation function over the multiset. Hence, to have strong representational power, a GNN must be able to aggregate different multisets into different representations. We rigorously study several variants of multiset functions and theoretically characterize their discriminative power, i.e., how well different aggregation functions can distinguish different multisets. The more discriminative the multiset function is, the more powerful the representational power of the underlying GNN.</p><p>Our main results are summarized as follows:</p><p>1) We show that GNNs are at most as powerful as the WL test in distinguishing graph structures.</p><p>2) We establish conditions on the neighbor aggregation and graph readout functions under which the resulting GNN is as powerful as the WL test.</p><p>3) We identify graph structures that cannot be distinguished by popular GNN variants, such as GCN (Kipf &amp; Welling, 2017)  and GraphSAGE (Hamilton et al., 2017a), and we precisely characterize the kinds of graph structures such GNN-based models can capture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>We develop a simple neural architecture, Graph Isomorphism Network (GIN), and show that its discriminative/representational power is equal to the power of the WL test.</p><p>We validate our theory via experiments on graph classification datasets, where the expressive power of GNNs is crucial to capture graph structures. In particular, we compare the performance of GNNs with various aggregation functions. Our results confirm that the most powerful GNN by our theory, i.e., Graph Isomorphism Network (GIN), also empirically has high representational power as it almost perfectly fits the training data, whereas the less powerful GNN variants often severely underfit the training data. In addition, the representationally more powerful GNNs outperform the others by test set accuracy and achieve state-of-the-art performance on many graph classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning with graph structured data, such as molecules, social, biological, and financial networks, requires effective representation of their graph structure <ref type="bibr" target="#b14">(Hamilton et al., 2017b)</ref>. Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs <ref type="bibr" target="#b23">(Li et al., 2016;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b34">Velickovic et al., 2018;</ref><ref type="bibr" target="#b37">Xu et al., 2018)</ref>. GNNs broadly follow a recursive neighborhood aggregation (or message passing) scheme, where each node aggregates feature vectors of its neighbors to compute its new feature vector <ref type="bibr" target="#b37">(Xu et al., 2018;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017)</ref>. After k iterations of aggregation, a node is represented by its transformed feature vector, which captures the structural information within the node's k-hop neighborhood. The representation of an entire graph can then be obtained through pooling <ref type="bibr" target="#b39">(Ying et al., 2018)</ref>, for example, by summing the representation vectors of all nodes in the graph.</p><p>Many GNN variants with different neighborhood aggregation and graph-level pooling schemes have been proposed <ref type="bibr" target="#b31">(Scarselli et al., 2009b;</ref><ref type="bibr" target="#b3">Battaglia et al., 2016;</ref><ref type="bibr" target="#b6">Defferrard et al., 2016;</ref><ref type="bibr" target="#b8">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b13">Hamilton et al., 2017a;</ref><ref type="bibr" target="#b19">Kearnes et al., 2016;</ref><ref type="bibr" target="#b21">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b23">Li et al., 2016;</ref><ref type="bibr" target="#b34">Velickovic et al., 2018;</ref><ref type="bibr" target="#b28">Santoro et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2018;</ref><ref type="bibr" target="#b29">Santoro et al., 2018;</ref><ref type="bibr" target="#b35">Verma &amp; Zhang, 2018;</ref><ref type="bibr" target="#b39">Ying et al., 2018;</ref>. Empirically, these GNNs have achieved state-of-the-art performance in many tasks such as node classification, link prediction, and graph classification. However, the design of new GNNs is mostly based on empirical intuition, heuristics, and experimental trial-anderror. There is little theoretical understanding of the properties and limitations of GNNs, and formal analysis of GNNs' representational capacity is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We begin by summarizing some of the most common GNN models and, along the way, introduce our notation. Let G = (V, E) denote a graph with node feature vectors X v for v ∈ V . There are two tasks of interest: (1) Node classification, where each node v ∈ V has an associated label y v and the goal is to learn a representation vector h v of v such that v's label can be predicted as y v = f (h v ); (2) Graph classification, where, given a set of graphs {G 1 , ..., G N } ⊆ G and their labels {y 1 , ..., y N } ⊆ Y, we aim to learn a representation vector h G that helps predict the label of an entire graph, y G = g(h G ).</p><p>Graph Neural Networks. GNNs use the graph structure and node features X v to learn a representation vector of a node, h v , or the entire graph, h G . Modern GNNs follow a neighborhood aggregation strategy, where we iteratively update the representation of a node by aggregating representations of its neighbors. After k iterations of aggregation, a node's representation captures the structural information within its k-hop network neighborhood. Formally, the k-th layer of a GNN is</p><formula xml:id="formula_0">a (k) v = AGGREGATE (k) h (k−1) u : u ∈ N (v) , h (k) v = COMBINE (k) h (k−1) v , a (k) v ,<label>(2.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>where h <ref type="bibr">(k)</ref> v is the feature vector of node v at the k-th iteration/layer. We initialize h (0) v = X v , and N (v) is a set of nodes adjacent to v. The choice of AGGREGATE (k) (·) and COMBINE (k) (·) in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph</head><p>Rooted subtree 2 WL test iterations Multiset GNN aggregation ….</p><p>Captures structures <ref type="figure">Figure 1</ref>: An overview of our theoretical framework. </p><formula xml:id="formula_1">a (k) v = MAX ReLU W · h (k−1) u , ∀u ∈ N (v) , (2.2)</formula><p>where W is a learnable matrix, and MAX represents an element-wise max-pooling. The COMBINE step could be a concatenation followed by a linear mapping W · h</p><formula xml:id="formula_2">(k−1) v , a (k) v</formula><p>as in GraphSAGE. In Graph Convolutional Networks (GCN) <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref>, the element-wise mean pooling is used instead, and the AGGREGATE and COMBINE steps are integrated as follows:</p><formula xml:id="formula_3">h (k) v = ReLU W · MEAN h (k−1) u , ∀u ∈ N (v) ∪ {v} . (2.3)</formula><p>Many other GNNs can be represented similarly to Eq. 2.1 <ref type="bibr" target="#b37">(Xu et al., 2018;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017)</ref>.</p><p>For node classification, the node representation h</p><formula xml:id="formula_4">(K) v</formula><p>of the final iteration is used for prediction. For graph classification, the READOUT function aggregates node features from the final iteration to obtain the entire graph's representation h G :</p><formula xml:id="formula_5">h G = READOUT h (K) v v ∈ G .</formula><p>(2.4)</p><p>READOUT can be a simple permutation invariant function such as summation or a more sophisticated graph-level pooling function <ref type="bibr" target="#b39">(Ying et al., 2018;</ref>.</p><p>Weisfeiler-Lehman test. The graph isomorphism problem asks whether two graphs are topologically identical. This is a challenging problem: no polynomial-time algorithm is known for it yet <ref type="bibr" target="#b10">(Garey, 1979;</ref><ref type="bibr" target="#b11">Garey &amp; Johnson, 2002;</ref><ref type="bibr" target="#b1">Babai, 2016)</ref>. Apart from some corner cases <ref type="bibr" target="#b4">(Cai et al., 1992)</ref>, the Weisfeiler-Lehman (WL) test of graph isomorphism <ref type="bibr" target="#b36">(Weisfeiler &amp; Lehman, 1968)</ref> is an effective and computationally efficient test that distinguishes a broad class of graphs <ref type="bibr" target="#b2">(Babai &amp; Kucera, 1979)</ref>. Its 1-dimensional form, "naïve vertex refinement", is analogous to neighbor aggregation in GNNs. The WL test iteratively (1) aggregates the labels of nodes and their neighborhoods, and (2) hashes the aggregated labels into unique new labels. The algorithm decides that two graphs are non-isomorphic if at some iteration the labels of the nodes between the two graphs differ.</p><p>Based on the WL test, <ref type="bibr" target="#b32">Shervashidze et al. (2011)</ref> proposed the WL subtree kernel that measures the similarity between graphs. The kernel uses the counts of node labels at different iterations of the WL test as the feature vector of a graph. Intuitively, a node's label at the k-th iteration of WL test represents a subtree structure of height k rooted at the node <ref type="figure">(Figure 1)</ref>. Thus, the graph features considered by the WL subtree kernel are essentially counts of different rooted subtrees in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORETICAL FRAMEWORK: OVERVIEW</head><p>We start with an overview of our framework for analyzing the expressive power of GNNs. <ref type="figure">Figure 1</ref> illustrates our idea. A GNN recursively updates each node's feature vector to capture the network structure and features of other nodes around it, i.e., its rooted subtree structures <ref type="figure">(Figure 1</ref>). Throughout the paper, we assume node input features are from a countable universe. For finite graphs, node feature vectors at deeper layers of any fixed model are also from a countable universe. For notational simplicity, we can assign each feature vector a unique label in {a, b, c . . .}. Then, feature vectors of a set of neighboring nodes form a multiset <ref type="figure">(Figure 1)</ref>: the same element can appear multiple times since different nodes can have identical feature vectors.</p><p>Definition 1 (Multiset). A multiset is a generalized concept of a set that allows multiple instances for its elements. More formally, a multiset is a 2-tuple X = (S, m) where S is the underlying set of X that is formed from its distinct elements, and m : S → N ≥1 gives the multiplicity of the elements.</p><p>To study the representational power of a GNN, we analyze when a GNN maps two nodes to the same location in the embedding space. Intuitively, a maximally powerful GNN maps two nodes to the same location only if they have identical subtree structures with identical features on the corresponding nodes. Since subtree structures are defined recursively via node neighborhoods ( <ref type="figure">Figure 1</ref>), we can reduce our analysis to the question whether a GNN maps two neighborhoods (i.e., two multisets) to the same embedding or representation. A maximally powerful GNN would never map two different neighborhoods, i.e., multisets of feature vectors, to the same representation. This means its aggregation scheme must be injective. Thus, we abstract a GNN's aggregation scheme as a class of functions over multisets that their neural networks can represent, and analyze whether they are able to represent injective multiset functions.</p><p>Next, we use this reasoning to develop a maximally powerful GNN. In Section 5, we study popular GNN variants and see that their aggregation schemes are inherently not injective and thus less powerful, but that they can capture other interesting properties of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BUILDING POWERFUL GRAPH NEURAL NETWORKS</head><p>First, we characterize the maximum representational capacity of a general class of GNN-based models. Ideally, a maximally powerful GNN could distinguish different graph structures by mapping them to different representations in the embedding space. This ability to map any two different graphs to different embeddings, however, implies solving the challenging graph isomorphism problem. That is, we want isomorphic graphs to be mapped to the same representation and non-isomorphic ones to different representations. In our analysis, we characterize the representational capacity of GNNs via a slightly weaker criterion: a powerful heuristic called Weisfeiler-Lehman (WL) graph isomorphism test, that is known to work well in general, with a few exceptions, e.g., regular graphs <ref type="bibr" target="#b4">(Cai et al., 1992;</ref><ref type="bibr" target="#b7">Douglas, 2011;</ref><ref type="bibr" target="#b9">Evdokimov &amp; Ponomarenko, 1999)</ref>. Lemma 2. Let G 1 and G 2 be any two non-isomorphic graphs. If a graph neural network A : G → R d maps G 1 and G 2 to different embeddings, the Weisfeiler-Lehman graph isomorphism test also decides G 1 and G 2 are not isomorphic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proofs of all</head><formula xml:id="formula_6">h (k) v = φ h (k−1) v , f h (k−1) u : u ∈ N (v) ,</formula><p>where the functions f , which operates on multisets, and φ are injective.</p><p>b) A's graph-level readout, which operates on the multiset of node features h</p><formula xml:id="formula_7">(k) v , is injective.</formula><p>We prove Theorem 3 in the appendix. For countable sets, injectiveness well characterizes whether a function preserves the distinctness of inputs. Uncountable sets, where node features are continuous, need some further considerations. In addition, it would be interesting to characterize how close together the learned features lie in a function's image. We leave these questions for future work, and focus on the case where input node features are from a countable set (that can be a subset of an uncountable set such as R n ). Lemma 4. Assume the input feature space X is countable. Let g (k) be the function parameterized by a GNN's k-th layer for k = 1, ..., L, where g (1) is defined on multisets X ⊂ X of bounded size. The range of g (k) , i.e., the space of node hidden features h (k) v , is also countable for all k = 1, ..., L.</p><p>Here, it is also worth discussing an important benefit of GNNs beyond distinguishing different graphs, that is, capturing similarity of graph structures. Note that node feature vectors in the WL test are essentially one-hot encodings and thus cannot capture the similarity between subtrees. In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures. Capturing structural similarity of the node labels is shown to be helpful for generalization particularly when the co-occurrence of subtrees is sparse across different graphs or there are noisy edges and node features <ref type="bibr" target="#b38">(Yanardag &amp; Vishwanathan, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH ISOMORPHISM NETWORK (GIN)</head><p>Having developed conditions for a maximally powerful GNN, we next develop a simple architecture, Graph Isomorphism Network (GIN), that provably satisfies the conditions in Theorem 3. This model generalizes the WL test and hence achieves maximum discriminative power among GNNs.</p><p>To model injective multiset functions for the neighbor aggregation, we develop a theory of "deep multisets", i.e., parameterizing universal multiset functions with neural networks. Our next lemma states that sum aggregators can represent injective, in fact, universal functions over multisets.</p><formula xml:id="formula_8">Lemma 5. Assume X is countable. There exists a function f : X → R n so that h(X) = x∈X f (x)</formula><p>is unique for each multiset X ⊂ X of bounded size. Moreover, any multiset function g can be decomposed as g (X) = φ x∈X f (x) for some function φ. We prove Lemma 5 in the appendix. The proof extends the setting in <ref type="bibr" target="#b40">(Zaheer et al., 2017)</ref> from sets to multisets. An important distinction between deep multisets and sets is that certain popular injective set functions, such as the mean aggregator, are not injective multiset functions. With the mechanism for modeling universal multiset functions in Lemma 5 as a building block, we can conceive aggregation schemes that can represent universal functions over a node and the multiset of its neighbors, and thus will satisfy the injectiveness condition (a) in Theorem 3. Our next corollary provides a simple and concrete formulation among many such aggregation schemes. Corollary 6. Assume X is countable. There exists a function f : X → R n so that for infinitely many choices of , including all irrational numbers, h(c, X) = (1 + ) · f (c) + x∈X f (x) is unique for each pair (c, X), where c ∈ X and X ⊂ X is a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as g (c, X) = ϕ (1 + ) · f (c) + x∈X f (x) for some function ϕ.</p><p>We can use multi-layer perceptrons (MLPs) to model and learn f and ϕ in Corollary 6, thanks to the universal approximation theorem <ref type="bibr" target="#b16">(Hornik et al., 1989;</ref><ref type="bibr" target="#b15">Hornik, 1991)</ref>. In practice, we model f (k+1) • ϕ (k) with one MLP, because MLPs can represent the composition of functions. In the first iteration, we do not need MLPs before summation if input features are one-hot encodings as their summation alone is injective. We can make a learnable parameter or a fixed scalar. Then, GIN updates node representations as</p><formula xml:id="formula_9">h (k) v = MLP (k) 1 + (k) · h (k−1) v + u∈N (v) h (k−1) u .</formula><p>( <ref type="formula">4.1)</ref> Generally, there may exist many other powerful GNNs. GIN is one such example among many maximally powerful GNNs, while being simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GRAPH-LEVEL READOUT OF GIN</head><p>Node embeddings learned by GIN can be directly used for tasks like node classification and link prediction. For graph classification tasks we propose the following "readout" function that, given embeddings of individual nodes, produces the embedding of the entire graph.</p><p>An important aspect of the graph-level readout is that node representations, corresponding to subtree structures, get more refined and global as the number of iterations increases. A sufficient number of iterations is key to achieving good discriminative power. Yet, features from earlier iterations may sometimes generalize better. To consider all structural information, we use information from all depths/iterations of the model. We achieve this by an architecture similar to Jumping Knowledge sum -multiset &gt; mean -distribution max -set &gt; Input <ref type="figure">Figure 2</ref>: Ranking by expressive power for sum, mean and max aggregators over a multiset.</p><p>Left panel shows the input multiset, i.e., the network neighborhood to be aggregated. The next three panels illustrate the aspects of the multiset a given aggregator is able to capture: sum captures the full multiset, mean captures the proportion/distribution of elements of a given type, and the max aggregator ignores multiplicities (reduces the multiset to a simple set).</p><p>vs. Between the two graphs, nodes v and v get the same embedding even though their corresponding graph structures differ. <ref type="figure">Figure 2</ref> gives reasoning about how different aggregators "compress" different multisets and thus fail to distinguish them.</p><p>Networks <ref type="bibr" target="#b37">(Xu et al., 2018)</ref>, where we replace Eq. 2.4 with graph representations concatenated across all iterations/layers of GIN: </p><formula xml:id="formula_10">h G = CONCAT READOUT h (k) v |v ∈ G k = 0, 1, . . . ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LESS POWERFUL BUT STILL INTERESTING GNNS</head><p>Next, we study GNNs that do not satisfy the conditions in Theorem 3, including GCN <ref type="bibr" target="#b21">(Kipf &amp; Welling, 2017)</ref> and <ref type="bibr">GraphSAGE (Hamilton et al., 2017a)</ref>. We conduct ablation studies on two aspects of the aggregator in Eq. 4.1: (1) 1-layer perceptrons instead of MLPs and (2) mean or max-pooling instead of the sum. We will see that these GNN variants get confused by surprisingly simple graphs and are less powerful than the WL test. Nonetheless, models with mean aggregators like GCN perform well for node classification tasks. To better understand this, we precisely characterize what different GNN variants can and cannot capture about a graph and discuss the implications for learning with graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">1-LAYER PERCEPTRONS ARE NOT SUFFICIENT</head><p>The function f in Lemma 5 helps map distinct multisets to unique embeddings. It can be parameterized by an MLP by the universal approximation theorem <ref type="bibr" target="#b15">(Hornik, 1991</ref>  <ref type="bibr" target="#b25">(Nelder &amp; Wedderburn, 1972)</ref>. Therefore, we are interested in understanding whether 1-layer perceptrons are enough for graph learning. Lemma 7 suggests that there are indeed network neighborhoods (multisets) that models with 1-layer perceptrons can never distinguish. Lemma 7. There exist finite multisets X 1 = X 2 so that for any linear mapping W ,</p><formula xml:id="formula_11">x∈X1 ReLU (W x) = x∈X2 ReLU (W x) .</formula><p>The main idea of the proof for Lemma 7 is that 1-layer perceptrons can behave much like linear mappings, so the GNN layers degenerate into simply summing over neighborhood features. Our proof builds on the fact that the bias term is lacking in the linear mapping. With the bias term and sufficiently large output dimensionality, 1-layer perceptrons might be able to distinguish different multisets. Nonetheless, unlike models using MLPs, the 1-layer perceptron (even with the bias term) is not a universal approximator of multiset functions. Consequently, even if GNNs with 1-layer perceptrons can embed different graphs to different locations to some degree, such embeddings may not adequately capture structural similarity, and can be difficult for simple classifiers, e.g., linear classifiers, to fit. In Section 7, we will empirically see that GNNs with 1-layer perceptrons, when applied to graph classification, sometimes severely underfit training data and often perform worse than GNNs with MLPs in terms of test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">STRUCTURES THAT CONFUSE MEAN AND MAX-POOLING</head><p>What happens if we replace the sum in h (X) = x∈X f (x) with mean or max-pooling as in GCN and GraphSAGE? Mean and max-pooling aggregators are still well-defined multiset functions because they are permutation invariant. But, they are not injective. <ref type="figure">Figure 2</ref> ranks the three aggregators by their representational power, and <ref type="figure">Figure 3</ref> illustrates pairs of structures that the mean and max-pooling aggregators fail to distinguish. Here, node colors denote different node features, and we assume the GNNs aggregate neighbors first before combining them with the central node labeled as v and v .</p><p>In <ref type="figure">Figure 3a</ref>, every node has the same feature a and f (a) is the same across all nodes (for any function f ). When performing neighborhood aggregation, the mean or maximum over f (a) remains f (a) and, by induction, we always obtain the same node representation everywhere. Thus, in this case mean and max-pooling aggregators fail to capture any structural information. In contrast, the sum aggregator distinguishes the structures because 2 · f (a) and 3 · f (a) give different values. The same argument can be applied to any unlabeled graph. If node degrees instead of a constant value is used as node input features, in principle, mean can recover sum, but max-pooling cannot. <ref type="figure">Fig. 3a</ref> suggests that mean and max have trouble distinguishing graphs with nodes that have repeating features. Let h color (r for red, g for green) denote node features transformed by f . <ref type="figure">Fig. 3b</ref> shows that maximum over the neighborhood of the blue nodes v and v yields max (h g , h r ) and max (h g , h r , h r ), which collapse to the same representation (even though the corresponding graph structures are different). Thus, max-pooling fails to distinguish them. In contrast, the sum aggregator still works because 1 2 (h g + h r ) and 1 3 (h g + h r + h r ) are in general not equivalent. Similarly, in <ref type="figure">Fig. 3c</ref>, both mean and max fail as 1 2 (h g + h r ) = 1 4 (h g + h g + h r + h r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MEAN LEARNS DISTRIBUTIONS</head><p>To characterize the class of multisets that the mean aggregator can distinguish, consider the example X 1 = (S, m) and X 2 = (S, k · m), where X 1 and X 2 have the same set of distinct elements, but X 2 contains k copies of each element of X 1 . Any mean aggregator maps X 1 and X 2 to the same embedding, because it simply takes averages over individual element features. Thus, the mean captures the distribution (proportions) of elements in a multiset, but not the exact multiset. Corollary 8. Assume X is countable. There exists a function f : X → R n so that for h(X) =</p><p>1 |X| x∈X f (x), h(X 1 ) = h(X 2 ) if and only if multisets X 1 and X 2 have the same distribution. That is, assuming |X 2 | ≥ |X 1 |, we have X 1 = (S, m) and X 2 = (S, k · m) for some k ∈ N ≥1 .</p><p>The mean aggregator may perform well if, for the task, the statistical and distributional information in the graph is more important than the exact structure. Moreover, when node features are diverse and rarely repeat, the mean aggregator is as powerful as the sum aggregator. This may explain why, despite the limitations identified in Section 5.2, GNNs with mean aggregators are effective for node classification tasks, such as classifying article subjects and community detection, where node features are rich and the distribution of the neighborhood features provides a strong signal for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MAX-POOLING LEARNS SETS WITH DISTINCT ELEMENTS</head><p>The examples in <ref type="figure">Figure 3</ref> illustrate that max-pooling considers multiple nodes with the same feature as only one node (i.e., treats a multiset as a set). Max-pooling captures neither the exact structure nor the distribution. However, it may be suitable for tasks where it is important to identify representative elements or the "skeleton", rather than to distinguish the exact structure or distribution. <ref type="bibr" target="#b27">Qi et al. (2017)</ref> empirically show that the max-pooling aggregator learns to identify the skeleton of a 3D point cloud and that it is robust to noise and outliers. For completeness, the next corollary shows that the max-pooling aggregator captures the underlying set of a multiset.</p><p>Corollary 9. Assume X is countable. Then there exists a function f : X → R ∞ so that for h(X) = max x∈X f (x), h(X 1 ) = h(X 2 ) if and only if X 1 and X 2 have the same underlying set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">REMARKS ON OTHER AGGREGATORS</head><p>There are other non-standard neighbor aggregation schemes that we do not cover, e.g., weighted average via attention <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref> and LSTM pooling <ref type="bibr" target="#b13">(Hamilton et al., 2017a;</ref><ref type="bibr" target="#b24">Murphy et al., 2018)</ref>. We emphasize that our theoretical framework is general enough to characterize the representaional power of any aggregation-based GNNs. In the future, it would be interesting to apply our framework to analyze and understand other aggregation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">OTHER RELATED WORK</head><p>Despite the empirical success of GNNs, there has been relatively little work that mathematically studies their properties. An exception to this is the work of <ref type="bibr" target="#b30">Scarselli et al. (2009a)</ref> who shows that the perhaps earliest GNN model <ref type="bibr" target="#b31">(Scarselli et al., 2009b)</ref> can approximate measurable functions in probability. <ref type="bibr" target="#b22">Lei et al. (2017)</ref> show that their proposed architecture lies in the RKHS of graph kernels, but do not study explicitly which graphs it can distinguish. Each of these works focuses on a specific architecture and do not easily generalize to multple architectures. In contrast, our results above provide a general framework for analyzing and characterizing the expressive power of a broad class of GNNs. Recently, many GNN-based architectures have been proposed, including sum aggregation and MLP encoding <ref type="bibr" target="#b3">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b31">Scarselli et al., 2009b;</ref><ref type="bibr" target="#b8">Duvenaud et al., 2015)</ref>, and most without theoretical derivation. In contrast to many prior GNN architectures, our Graph Isomorphism Network (GIN) is theoretically motivated, simple yet powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We evaluate and compare the training and test performance of GIN and less powerful GNN variants. 1 Training set performance allows us to compare different GNN models based on their representational power and test set performance quantifies generalization ability.</p><p>Datasets. We use 9 graph classification benchmarks: 4 bioinformatics datasets (MUTAG, PTC, NCI1, PROTEINS) and 5 social network datasets (COLLAB, IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI5K) <ref type="bibr" target="#b38">(Yanardag &amp; Vishwanathan, 2015)</ref>. Importantly, our goal here is not to allow the models to rely on the input node features but mainly learn from the network structure. Thus, in the bioinformatic graphs, the nodes have categorical input features but in the social networks, they have no features. For social networks we create node features as follows: for the REDDIT datasets, we set all node feature vectors to be the same (thus, features here are uninformative); for the other social graphs, we use one-hot encodings of node degrees. Dataset statistics are summarized in <ref type="table" target="#tab_5">Table 1</ref>, and more details of the data can be found in Appendix I.</p><p>Models and configurations. We evaluate GINs (Eqs. 4.1 and 4.2) and the less powerful GNN variants. Under the GIN framework, we consider two variants: (1) a GIN that learns in Eq. 4.1 by gradient descent, which we call GIN-, and (2) a simpler (slightly less powerful) 2 GIN, where in Eq. 4.1 is fixed to 0, which we call GIN-0. As we will see, GIN-0 shows strong empirical performance: not only does GIN-0 fit training data equally well as GIN-, it also demonstrates good generalization, slightly but consistently outperforming GIN-in terms of test accuracy. For the less powerful GNN variants, we consider architectures that replace the sum in the GIN-0 aggregation with mean or max-pooling 3 , or replace MLPs with 1-layer perceptrons, i.e., a linear mapping followed by ReLU. In <ref type="figure" target="#fig_1">Figure 4</ref> and <ref type="table" target="#tab_5">Table 1</ref>, a model is named by the aggregator/perceptron it uses. Here mean-1-layer and max-1-layer correspond to GCN and GraphSAGE, respectively, up to minor architecture modifications. We apply the same graph-level readout (READOUT in Eq. 4.2) for GINs and all the GNN variants, specifically, sum readout on bioinformatics datasets and mean readout on social datasets due to better test performance.</p><p>Following <ref type="bibr" target="#b38">(Yanardag &amp; Vishwanathan, 2015;</ref><ref type="bibr" target="#b26">Niepert et al., 2016)</ref>, we perform 10-fold crossvalidation with LIB-SVM <ref type="bibr" target="#b5">(Chang &amp; Lin, 2011)</ref>. We report the average and standard deviation of validation accuracies across the 10 folds within the cross-validation. For all configurations, 5 GNN layers (including the input layer) are applied, and all MLPs have 2 layers. Batch normalization <ref type="bibr" target="#b17">(Ioffe &amp; Szegedy, 2015)</ref> is applied on every hidden layer. We use the Adam optimizer <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref> with initial learning rate 0.01 and decay the learning rate by 0.5 every 50 epochs. The hyper-parameters we tune for each dataset are: (1) the number of hidden units ∈ {16, 32} for bioinformatics graphs and 64 for social graphs; (2) the batch size ∈ {32, 128}; (3) the dropout ratio ∈ {0, 0.5} after the dense layer <ref type="bibr" target="#b33">(Srivastava et al., 2014)</ref>; (4) the number of epochs, i.e., a single epoch with the best cross-validation accuracy averaged over the 10 folds was selected. Note that due to the small dataset sizes, an alternative setting, where hyper-parameter selection is done using a validation set, is extremely unstable, e.g., for MUTAG, the validation set only contains 18 data points. We also report the training accuracy of different GNNs, where all the hyper-parameters were fixed across the datasets: 5 GNN layers (including the input layer), hidden units of size 64, minibatch of size 128, and 0.5 dropout ratio. For comparison, the training accuracy of the WL subtree kernel is reported, where we set the number of iterations to 4, which is comparable to the 5 GNN layers.</p><p>Baselines. We compare the GNNs above with a number of state-of-the-art baselines for graph classification: (1) the WL subtree kernel <ref type="bibr" target="#b32">(Shervashidze et al., 2011)</ref>, where C-SVM <ref type="bibr" target="#b5">(Chang &amp; Lin, 2011)</ref> was used as a classifier; the hyper-parameters we tune are C of the SVM and the number of WL iterations ∈ {1, 2, . . . , 6};</p><p>(2) state-of-the-art deep learning architectures, i.e., Diffusionconvolutional neural networks (DCNN) <ref type="bibr" target="#b0">(Atwood &amp; Towsley, 2016)</ref>, PATCHY-SAN <ref type="bibr" target="#b26">(Niepert et al., 2016)</ref> and Deep Graph CNN (DGCNN) ; (3) Anonymous Walk Embeddings (AWL) <ref type="bibr" target="#b18">(Ivanov &amp; Burnaev, 2018)</ref>. For the deep learning methods and AWL, we report the accuracies reported in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">RESULTS</head><p>Training set performance. We validate our theoretical analysis of the representational power of GNNs by comparing their training accuracies. Models with higher representational power should have higher training set accuracy. <ref type="figure" target="#fig_1">Figure 4</ref> shows training curves of GINs and less powerful GNN variants with the same hyper-parameter settings. First, both the theoretically most powerful GNN, i.e. GIN-and GIN-0, are able to almost perfectly fit all the training sets. In our experiments, explicit learning of in GIN-yields no gain in fitting training data compared to fixing to 0 as in GIN-0. In comparison, the GNN variants using mean/max pooling or 1-layer perceptrons severely underfit on many datasets. In particular, the training accuracy patterns align with our ranking by the models' <ref type="table" target="#tab_5">Datasets  IMDB-B  IMDB-M  RDT-B  RDT-M5K COLLAB  MUTAG  PROTEINS  PTC  NCI1  Datasets  # graphs  1000  1500  2000  5000  5000  188  1113  344  4110  # classes  2  3  2  5  3  2  2  2  2  Avg #</ref>   representational power: GNN variants with MLPs tend to have higher training accuracies than those with 1-layer perceptrons, and GNNs with sum aggregators tend to fit the training sets better than those with mean and max-pooling aggregators.</p><p>On our datasets, training accuracies of the GNNs never exceed those of the WL subtree kernel. This is expected because GNNs generally have lower discriminative power than the WL test. For example, on IMDBBINARY, none of the models can perfectly fit the training set, and the GNNs achieve at most the same training accuracy as the WL kernel. This pattern aligns with our result that the WL test provides an upper bound for the representational capacity of the aggregation-based GNNs. However, the WL kernel is not able to learn how to combine node features, which might be quite informative for a given prediction task as we will see next.</p><p>Test set performance. Next, we compare test accuracies. Although our theoretical results do not directly speak about the generalization ability of GNNs, it is reasonable to expect that GNNs with strong expressive power can accurately capture graph structures of interest and thus generalize well. <ref type="table" target="#tab_5">Table 1</ref> compares test accuracies of GINs (Sum-MLP), other GNN variants, as well as the state-of-the-art baselines.</p><p>First, GINs, especially GIN-0, outperform (or achieve comparable performance as) the less powerful GNN variants on all the 9 datasets, achieving state-of-the-art performance. GINs shine on the social network datasets, which contain a relatively large number of training graphs. For the Reddit datasets, all nodes share the same scalar as node feature. Here, GINs and sum-aggregation GNNs accurately capture the graph structure and significantly outperform other models. Mean-aggregation GNNs, however, fail to capture any structures of the unlabeled graphs (as predicted in Section 5.2) and do not perform better than random guessing. Even if node degrees are provided as input features, mean-based GNNs perform much worse than sum-based GNNs (the accuracy of the GNN with mean-MLP aggregation is 71.2±4.6% on REDDIT-BINARY and 41.3±2.1% on REDDIT-MULTI5K).</p><p>Comparing GINs (GIN-0 and GIN-), we observe that GIN-0 slightly but consistently outperforms GIN-. Since both models fit training data equally well, the better generalization of GIN-0 may be explained by its simplicity compared to GIN-.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we developed theoretical foundations for reasoning about the expressive power of GNNs, and proved tight bounds on the representational capacity of popular GNN variants. We also designed a provably maximally powerful GNN under the neighborhood aggregation framework. An interesting direction for future work is to go beyond neighborhood aggregation (or message passing) in order to pursue possibly even more powerful architectures for learning with graphs. To complete the picture, it would also be interesting to understand and improve the generalization properties of GNNs as well as better understand their optimization landscape.</p><formula xml:id="formula_12">(0) v = h (0) v for all v ∈ G 1 , G 2 .</formula><p>So ϕ could be the identity function for k = 0. Suppose this holds for iteration k − 1, we show that it also holds for k. Substituting h</p><formula xml:id="formula_13">(k−1) v with ϕ l (k−1) v gives us h (k) v = φ ϕ l (k−1) v , f ϕ l (k−1) u : u ∈ N (v) .</formula><p>Since the composition of injective functions is injective, there exists some injective function ψ so that</p><formula xml:id="formula_14">h (k) v = ψ l (k−1) v , l (k−1) u : u ∈ N (v)</formula><p>Then we have</p><formula xml:id="formula_15">h (k) v = ψ • g −1 g l (k−1) v , l (k−1) u : u ∈ N (v) = ψ • g −1 l (k) v ϕ = ψ • g −1 is</formula><p>injective because the composition of injective functions is injective. Hence for any iteration k, there always exists an injective function ϕ such that h</p><formula xml:id="formula_16">(k) v = ϕ l (k) v</formula><p>. At the K-th iteration, the WL test decides that G 1 and G 2 are non-isomorphic, that is the multisets l (K) v are different for G 1 and G 2 . The graph neural network A's node embeddings h</p><formula xml:id="formula_17">(K) v = ϕ l (K) v</formula><p>must also be different for G 1 and G 2 because of the injectivity of ϕ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF FOR LEMMA 4</head><p>Proof. Before proving our lemma, we first show a well-known result that we will later reduce our problem to: N k is countable for every k ∈ N, i.e. finite Cartesian product of countable sets is countable. We observe that it suffices to show N × N is countable, because the proof then follows clearly from induction. To show N × N is countable, we construct a bijection φ from N × N to N as φ (m, n) = 2 m−1 · (2n − 1)</p><p>Now we go back to proving our lemma. If we can show that the range of any function g defined on multisets of bounded size from a countable set is also countable, then the lemma holds for any g (k) by induction. Thus, our goal is to show that the range of such g is countable. First, it is clear that the mapping from g(X) to X is injective because g is a well-defined function. It follows that it suffices to show the set of all multisets X ⊂ X is countable.</p><p>Since the union of two countable sets is countable, the following set X is also countable.</p><formula xml:id="formula_18">X = X ∪ {e}</formula><p>where e is a dummy element that is not in X . It follows from the result we showed above, i.e., N k is countable for every k ∈ N, that X k is countable for every k ∈ N. It remains to show there exists an injective mapping from the set of multisets in X to X k for some k ∈ N.</p><p>We construct an injective mapping h from the set of multisets X ⊂ X to X k for some k ∈ N as follows. Because X is countable, there exists a mapping Z : X → N from x ∈ X to natural numbers. We can sort the elements x ∈ X by z(x) as x 1 , x 2 , ..., x n , where n = |X|. Because the multisets X are of bounded size, there exists k ∈ N so that |X| &lt; k for all X. We can then define h as h (X) = (x 1 , x 2 , ..., x n , e, e, e...) ,</p><p>where the k − n coordinates are filled with the dummy element e. It is clear that h is injective because for any multisets X and Y of bounded size, h(X) = h(Y ) only if X is equivalent to Y . Hence it follows that the range of g is countable as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOF FOR LEMMA 5</head><p>Proof. We first prove that there exists a mapping f so that x∈X f (x) is unique for each multiset X of bounded size. Because X is countable, there exists a mapping Z : X → N from x ∈ X to natural numbers. Because the cardinality of multisets X is bounded, there exists a number N ∈ N so that |X| &lt; N for all X. Then an example of such f is f (x) = N −Z(x) . This f can be viewed as a more compressed form of an one-hot vector or N -digit presentation. Thus, h(X) = x∈X f (x) is an injective function of multisets. φ x∈X f (x) is permutation invariant so it is a well-defined multiset function. For any multiset function g, we can construct such φ by letting φ x∈X f (x) = g(X). Note that such φ is well-defined because h(X) = x∈X f (x) is injective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E PROOF OF COROLLARY 6</head><p>Proof. Following the proof of Lemma 5, we consider f (x) = N −Z(x) , where N and Z : X → N are the same as defined in Appendix D. Let h(c, X) ≡ (1 + ) · f (c) + x∈X f (x). Our goal is show that for any (c , X ) = (c, X) with c, c ∈ X and X, X ⊂ X , h(c, X) = h(c , X ) holds, if is an irrational number. We prove by contradiction. For any (c, X), suppose there exists (c , X ) such that (c , X ) = (c, X) but h(c, X) = h(c , X ) holds. Let us consider the following two cases: (1) c = c but X = X, and <ref type="formula" target="#formula_0">(2)</ref>  For any function g over the pairs (c, X), we can construct such ϕ for the desired decomposition by letting ϕ (1 + ) · f (c) + x∈X f (x) = g(c, X). Note that such ϕ is well-defined because h(c, X) = (1 + ) · f (c) + x∈X f (x) is injective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F PROOF FOR LEMMA 7</head><p>Proof. Let us consider the example X 1 = {1, 1, 1, 1, 1} and X 2 = {2, 3}, i.e. two different multisets of positive numbers that sum up to the same value. We will be using the homogeneity of ReLU.</p><p>Let W be an arbitrary linear transform that maps x ∈ X 1 , X 2 into R n . It is clear that, at the same coordinates, W x are either positive or negative for all x because all x in X 1 and X 2 are positive. It follows that ReLU(W x) are either positive or 0 at the same coordinate for all </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Mean and Max both failFigure 3: Examples of graph structures that mean and max aggregators fail to distinguish.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Training set performance of GINs, less powerful GNN variants, and the WL subtree kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>c = c. For the first case, h(c, X) = h(c, X ) implies x∈X f (x) = x∈X f (x). It follows from Lemma 5 that the equality will not hold, because withf (x) = N −Z(x) , X = X implies x∈X f (x) = x∈X f (x). Thus,we reach a contradiction. For the second case, we can similarly rewrite h(c, X) = h(c , X ) as · (f (c) − f (c )) = f (c ) + x∈X f (x) − f (c) + x∈X f (x) . (E.1) Because is an irrational number and f (c) − f (c ) is a non-zero rational number, L.H.S. of Eq. E.1 is irrational. On the other hand, R.H.S. of Eq. E.1, the sum of a finite number of rational numbers, is rational. Hence the equality in Eq. E.1 cannot hold, and we have reached a contradiction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Middle panel: rooted subtree structures (at the blue node) that the WL test uses to distinguish different graphs. Right panel: if a GNN's aggregation function captures the full multiset of node neighbors, the GNN can capture the rooted subtrees in a recursive manner and be as powerful as the WL test.</figDesc><table><row><cell>GNNs is crucial. A number of architectures for AGGREGATE have been proposed. In the pooling</cell></row><row><cell>variant of GraphSAGE (Hamilton et al., 2017a), AGGREGATE has been formulated as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, if GIN replaces READOUT in Eq. 4.2 with summing all node features from the same iterations (we do not need an extra MLP before summation for the same reason as in Eq. 4.1), it provably generalizes the WL test and the WL subtree kernel.</figDesc><table><row><cell>K .</cell><cell>(4.2)</cell></row><row><cell>By Theorem 3 and Corollary 6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Test set classification accuracies (%). The best-performing GNNs are highlighted with boldface. On datasets where GINs' accuracy is not strictly the highest among GNN variants, we see that GINs are still comparable to the best GNN because a paired t-test at significance level 10% does not distinguish GINs from the best; thus, GINs are also highlighted with boldface. If a baseline performs significantly better than all GNNs, we highlight it with boldface and asterisk.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>x in X 1 , X 2 . For the coordinates where ReLU(W x) are 0, we have x∈X1 ReLU (W x) = x∈X2 ReLU (W x). For the coordinates where W x are positive, linearity still holds. It follows from linearity that</figDesc><table /><note>x∈X ReLU (W x) = ReLU Wx∈X x where X could be X 1 or X 2 . Because x∈X1 x = x∈X2 x, we have the following as desired.x∈X1 ReLU (W x) =x∈X2 ReLU (W x)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/weihua916/powerful-gnns. 2 There exist certain (somewhat contrived) graphs that GIN-can distinguish but GIN-0 cannot. 3 For REDDIT-BINARY, REDDIT-MULTI5K, and COLLAB, we did not run experiments for max-pooling due to GPU memory constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">. This apparently holds for k = 0 because the initial node features are the same</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was supported by NSF CAREER award 1553284, a DARPA D3M award and DARPA DSO's Lagrange program under grant FA86501827838. This research was also supported in part by NSF, ARO MURI, Boeing, Huawei, Stanford Data Science Initiative, and Chan Zuckerberg Biohub. Weihua Hu was supported by Funai Overseas Scholarship. We thank Prof. Ken-ichi Kawarabayashi and Prof. Masashi Sugiyama for supporting this research with computing resources and providing great advice. We thank Tomohiro Sonobe and Kento Nozawa for managing servers. We thank Rex Ying and William Hamilton for helpful feedback. We thank Simon S. Du, Yasuo Tabei, Chengtao Li, and Jingling Li for helpful discussions and positive comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOF FOR LEMMA 2</head><p>Proof. Suppose after k iterations, a graph neural network A has A(G 1 ) = A(G 2 ) but the WL test cannot decide G 1 and G 2 are non-isomorphic. It follows that from iteration 0 to k in the WL test, G 1 and G 2 always have the same collection of node labels. In particular, because G 1 and G 2 have the same WL node labels for iteration i and i + 1 for any i = 0, ..., k − 1, G 1 and G 2 have the same collection, i.e. multiset, of WL node labels l (i) v as well as the same collection of node neighborhoods l</p><p>. Otherwise, the WL test would have obtained different collections of node labels at iteration i + 1 for G 1 and G 2 as different multisets get unique new labels.</p><p>The WL test always relabels different multisets of neighboring nodes into different new labels. We show that on the same graph G = G 1 or G 2 , if WL node labels l</p><p>u for any iteration i. This apparently holds for i = 0 because WL and GNN starts with the same node features. Suppose this holds for iteration j, if for any u, v, l</p><p>then it must be the case that</p><p>By our assumption on iteration j, we must have</p><p>In the aggregation process of the GNN, the same AGGREGATE and COMBINE are applied. The same input, i.e. neighborhood features, generates the same output. Thus, h</p><p>u for any iteration i. This creates a valid mapping φ such that h</p><p>It follows from G 1 and G 2 have the same multiset of WL neighborhood labels that G 1 and G 2 also have the same collection of GNN neighborhood features</p><p>are the same. In particular, we have the same collection of GNN node features</p><p>for G 1 and G 2 . Because the graph level readout function is permutation invariant with respect to the collection of node features, A(G 1 ) = A(G 2 ). Hence we have reached a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF FOR THEOREM 3</head><p>Proof. Let A be a graph neural network where the condition holds. Let G 1 , G 2 be any graphs which the WL test decides as non-isomorphic at iteration K. Because the graph-level readout function is injective, i.e., it maps distinct multiset of node features into unique embeddings, it sufficies to show that A's neighborhood aggregation process, with sufficient iterations, embeds G 1 and G 2 into different multisets of node features. Let us assume A updates node representations as</p><p>with injective funtions f and φ. The WL test applies a predetermined injective hash function g to update the WL node labels l</p><p>We will show, by induction, that for any iteration k, there always exists an injective function ϕ such that h</p><p>Proof. Suppose multisets X 1 and X 2 have the same distribution, without loss of generality, let us assume X 1 = (S, m) and X 2 = (S, k · m) for some k ∈ N ≥1 , i.e. X 1 and X 2 have the same underlying set and the multiplicity of each element in X 2 is k times of that in X 1 . Then we have</p><p>Now we show that there exists a function f so that 1 |X| x∈X f (x) is unique for distributionally equivalent X. Because X is countable, there exists a mapping Z : X → N from x ∈ X to natural numbers. Because the cardinality of multisets X is bounded, there exists a number N ∈ N so that |X| &lt; N for all X. Then an example of such f is f (x) = N −2Z(x) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H PROOF FOR COROLLARY 9</head><p>Proof. Suppose multisets X 1 and X 2 have the same underlying set S, then we have</p><p>Now we show that there exists a mapping f so that max x∈X f (x) is unique for Xs with the same underlying set. Because X is countable, there exists a mapping Z : X → N from x ∈ X to natural numbers. Then an example of such f :</p><p>. Such an f essentially maps a multiset to its one-hot embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I DETAILS OF DATASETS</head><p>We give detailed descriptions of datasets used in our experiments. Further details can be found in <ref type="bibr" target="#b38">(Yanardag &amp; Vishwanathan, 2015)</ref>.</p><p>Social networks datasets. IMDB-BINARY and IMDB-MULTI are movie collaboration datasets. Each graph corresponds to an ego-network for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn betwen two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre graph it is derived from. REDDIT-BINARY and REDDIT-MULTI5K are balanced datasets where each graph corresponds to an online discussion thread and nodes correspond to users. An edge was drawn between two nodes if at least one of them responded to another's comment. The task is to classify each graph to a community or a subreddit it belongs to. COLLAB is a scientific collaboration dataset, derived from 3 public collaboration datasets, namely, High Energy Physics, Condensed Matter Physics and Astro Physics. Each graph corresponds to an ego-network of different researchers from each field. The task is to classify each graph to a field the corresponding researcher belongs to.</p><p>Bioinformatics datasets. MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels. PROTEINS is a dataset where nodes are secondary structure elements (SSEs) and there is an edge between two nodes if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet or turn. PTC is a dataset of 344 chemical compounds that reports the carcinogenicity for male and female rats and it has 19 discrete labels. NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines, having 37 discrete labels.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph isomorphism in quasipolynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Canonical labelling of graphs in linear average time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludik</forename><surname>Kucera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1979" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
	<note>20th Annual Symposium on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Yi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on intelligent systems and technology (TIST)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The weisfeiler-lehman method and graph isomorphism testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Isomorphism of coloured graphs with slowly increasing multiplicity of jordan blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergei</forename><surname>Evdokimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Ponomarenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="333" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A guide to the theory of np-completeness. Computers and intractability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David S Johnson</forename><surname>Garey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and intractability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>wh freeman</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01900</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W M</forename><surname>Wedderburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series A, General</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="370" to="384" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4477" to="4486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Computational capabilities of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08090</idno>
		<title level="m">Graph capsule convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
