<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bay Area Environmental Research Institute</orgName>
								<address>
									<settlement>Moffett Field</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dibiano</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Louisiana State University</orgName>
								<address>
									<settlement>Baton Rouge</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Nemani</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NASA Ames Research Center</orgName>
								<address>
									<settlement>Moffett Field</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1080/2150704X.2019.1693071</idno>
					<note>ARTICLE HISTORY Compiled November 19, 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels.</p><p>In a preliminary version of this work, we introduced two new high resolution satellite imagery datasets (SAT-4 and SAT-6) and proposed DeepSat framework for classification based on "handcrafted" features and a deep belief network (DBN). The present paper is an extended version, we present an end-to-end framework leveraging an improved architecture that augments a convolutional neural network (CNN) with handcrafted features (instead of using DBN-based architecture) for classification.</p><p>Our framework, having access to fused spatial information obtained from handcrafted features as well as CNN feature maps, have achieved accuracies of 99.90% and 99.84% respectively, on SAT-4 and SAT-6, surpassing all the other state-ofthe-art results. A statistical analysis based on Distribution Separability Criterion substantiates the robustness of our approach in learning better representations for satellite imagery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last few years, advances in supervised Deep Learning enabled by Convolutional Neural Networks (CNN) <ref type="bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref> have given rise to powerful techniques for solving a variety of problems in computer vision and image classification <ref type="bibr" target="#b13">(Krizhevsky, Sutskever, and Hinton 2012)</ref>.</p><p>A related and equally hard problem is Satellite image scene classification that is crucial for understanding and delineating land cover. It involves terabytes of data and significant variations due to conditions in data acquisition, pre-processing, and filtering. The problem of detecting various land cover classes in general is a difficult problem considering the significantly higher intra-class variability in land cover types such as trees, grasslands, barren lands, water bodies, etc. as compared to that of CONTACT: Qun Liu. Email: qliu14@lsu.edu. This is an Accepted Manuscript of an article published by Taylor &amp; Francis Group in Remote Sensing Letters, available online: http://www.tandfonline.com/10. <ref type="bibr">1080/2150704X.2019.1693071.</ref> roads. Due to the high variability inherent in the satellite imagery data, even deep neural networks-based supervised classification methods have traditionally struggled to produce human-like performance in this area. However, recently, there has been a lot of research in this area especially in the deep learning community, with several works attempting to retrofit deep learning techniques to classification of high resolution satellite imagery <ref type="bibr" target="#b0">Basu et al. 2015a;</ref><ref type="bibr" target="#b23">Zhong et al. 2017;</ref><ref type="bibr" target="#b14">Liu and Huang 2018;</ref><ref type="bibr" target="#b19">Simo-Serra et al. 2015;</ref><ref type="bibr" target="#b1">Basu et al. 2015b</ref>). <ref type="bibr" target="#b23">Zhong et. al. (Zhong et al. 2017)</ref> proposed an agile architecture based on CNNs to learn robust intra-class diversity and the spatial information, achieving state-of-theart performance. <ref type="bibr" target="#b14">Liu and Huang (Liu and Huang 2018)</ref> proposed a framework based on triplet networks to achieve high accuracy in classifying high resolution satellite imagery. <ref type="bibr" target="#b9">Gong et. al. (Gong et al. 2018</ref>) regularized a deep structural metric learning (DSML) algorithm with a prior distribution over the parameters that tends to reduce the correlation among them. Using this technique, their framework  obtained state-of-the-art results in classification of high-resolution satellite imagery.</p><p>In a preliminary version of this work <ref type="bibr" target="#b0">(Basu et al. 2015a)</ref>, we introduced two new high resolution satellite imagery datasets called SAT-4 and SAT-6 and proposed a classification framework that extracts "handcrafted" features from an input image, normalizes them, and feeds the normalized feature vectors to a deep belief network (DBN) for classification. SAT-4 and SAT-6 cover a total area of ∼800 square kilometers at 1 m resolution and can be used to further the research and investigate the use of various learning models for high resolution satellite image classification. Both SAT-4 and SAT-6 were sampled from a much larger dataset, National Agriculture Imagery Program (NAIP) dataset, which covers the whole of continental United States and can be used to create labeled landcover maps, which can then be used for various applications, such as, measuring ground carbon content or estimating total area of rooftops for solar power generation. Among the publicly available benchmark datasets for high resolution satellite imagery classification in the remote sensing community (WWW1 n.d.), only SAT-4 and SAT-6 provide enough labeled image patches (500,000 and 405,000 respectively) to evaluate a new architecture or approach without running into overtraining issues.</p><p>The present paper is an extended version of <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>). The contributions of this paper are: (1) we present an end-to-end framework based on an improved architecture that enhances a modern CNN with handcrafted features (as opposed to the DBN-based architecture of <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>)) for high resolution satellite imagery classification. We experimentally show that our framework surpasses all existing stateof-the-art algorithms for high-resolution satellite imagery classification on both SAT-4 and SAT-6 datasets, including the original DeepSAT <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>), MLP (Zscore) , SatCNN (both Z -score and linear) , TradCNN (Z -score) , triplet networks (Liu and Huang 2018), D-DSML-Caffenet , and contrastive loss <ref type="bibr" target="#b19">(Simo-Serra et al. 2015)</ref>. It has been shown theoretically in <ref type="bibr" target="#b3">(Basu et al. 2018</ref><ref type="bibr" target="#b2">(Basu et al. , 2016</ref> CNNs, by themselves, are not able to learn representations of Haralick features from data. By augmenting CNNs with the handcrafted features, we are enhancing the discriminative power of CNNs for satellite imagery. (2) We present a statistical analysis based on Distribution Separability Criterion that substantiates the robustness of our approach in learning better representations for satellite imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In <ref type="bibr" target="#b17">(Paisitkriangkrai et al. 2015)</ref>, the authors combine the output of a CNN externally with handcrafted features, using logistic regression to create probability maps. In contrast, we augment a CNN itself with handcrafted features with a hidden layer fusing handcrafted features with CNN bottleneck representations. In <ref type="bibr" target="#b8">(Egede, Valstar, and Martinez 2017)</ref>, the authors provide a framework that fuses deep features obtained from a CNN with handcrafted statistical features for automatically estimating pain. <ref type="bibr" target="#b23">Zhong et. al. (Zhong et al. 2017)</ref> proposed an agile architecture based on CNNs to learn expressive representations that capture the large variance between the classes, achieving state-of-the-art performance. Compared to their approach, in this paper, we augment our framework with lower dimensional statistical features (that we call handcrafted features) to enable learning discriminative representations of the texture of the image. Instead of using CNNs, the authors in <ref type="bibr" target="#b24">(Zhu et al. 2017)</ref> proposed the FSSTM (Fully Sparse Semantic Topic Model) approach for high resolution imagery classification. In <ref type="bibr" target="#b25">(Zhu et al. 2018)</ref>, the authors used pretrained CaffeNet for extracting deep features to combine with semantic topics for classification. In <ref type="bibr" target="#b6">(Chaib et al. 2017)</ref>, the authors investigated feature fusion among deep features extracted from a pretrained deep model (VGG-Net) and proposed a fusion method that outperformed the stateof-the-art approaches. In this paper, we provide an end-to-end framework leveraging a CNN architecture augmented with handcrafted features rather than relying on deep feature extraction.</p><p>In <ref type="bibr" target="#b7">(Cheng et al. 2018)</ref>, the authors proposed a technique based on metric learning that minimizes the intra-class diversity and maximizes the inter-class similarity. In contrast, we rely on Haralick features to induce high distribution separability.</p><p>The authors in <ref type="bibr" target="#b14">(Liu and Huang 2018)</ref> proposed an approach based on triplet networks using a loss function that minimizes the intra-class distances and maximizes the inter-class ones. In contrast, we enhance a CNN-based framework with statistical features that discriminatively capture image texture characteristics providing improved distribution separability.</p><p>In ) the authors proposed a regularization term that increases the variation among network parameters for learning more expressive representations.</p><p>High resolution satellite imagery datasets (Van Etten, Lindenbaum, and Bacastow 2018) have been proposed as benchmarks for training and evaluating remote sensing imagery segmentation algorithms. However, for understanding satellite imagery, framing the problem of feature detection as a classification problem is important because of the higher scalability of the classification datasets that can be generated as opposed to per-pixel segmentation masks that are expensive to label. Classification techniques also form the basis for characterizing land cover. Hence, we limit the scope of this paper to classification of high resolution satellite imagery rather than exploring per-pixel segmentation techniques and datasets.</p><p>In <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>), we presented a classification framework that feeds handcrafted features extracted from an image to a DBN for classifying high resolution satellite imagery. The framework in <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>) classifies satellite imagery without considering the spatial features or correlation information from the image. In this paper, we present an improved architecture that enhances a modern CNN with handcrafted features for classification of high resolution satellite imagery. The framework presented in this paper fuses handcrafted features extracted from an image with spatial (deep) features acquired from the bottleneck layer of a CNN to obtain improved classification accuracy on the SAT-4 and SAT-6 datasets compared to (Basu </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architectural Overview</head><p>We propose an end-to-end framework that augments a modern CNN architecture with handcrafted features (texture features) to improve distribution separability for classification of satellite imagery. While the DBN-based architecture in <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>) used higher-order texture features that are important for discriminative representations for various landcover classes, it did not capture spatial contextual information. We extend <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>) by providing a new architecture that uses a CNN as a baseline model for extracting spatial contextual information and then augmenting it with the representations extracted from handcrafted feature spaces to enhance the discriminative power.</p><p>The complete architecture is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of two convolutional layers with 32 and 64 feature maps with a kernel of 3×3 for both, each accompanied with a Rectified Linear Unit (ReLU) layer. A max-pooling layer follows that with a kernel of 2×2. A Dropout layer is added after the max pooling layer with dropout rate of 0.25. This is followed by a feature fusion layer where the handcrafted features are concatenated with the CNN bottleneck representations. Then the fused features are input into a fully connected dense layer containing 32 neurons to which batch normalization is added. Following this is a ReLU layer, after which is a fully connected dense layer with 128 neurons. After this layer comes a ReLU layer, succeeding which is a dropout layer with rate 0.2. The final layer is a Softmax layer based on crossentropy loss function. The Adadelta optimizer <ref type="bibr" target="#b22">(Zeiler 2012)</ref> have been adopted in the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>The feature extraction phase computes 150 features from the input imagery. The key features that we use for classification are mean, standard deviation, variance, 2nd moment, direct cosine transforms, correlation, co-variance, autocorrelation, energy, entropy, homogeneity, contrast, maximum probability and sum of variance of the hue, saturation, intensity, and near infrared (NIR) channels as well as those of the color co-occurrence matrices. These features were shown to be useful descriptors for classi-fication of satellite imagery in previous research <ref type="bibr" target="#b10">(Haralick, Shanmugam, and Dinstein 1973)</ref>. Since two of the classes in SAT-4 and SAT-6 are trees and grasslands, we incorporate features that are useful determinants for segregation of vegetated areas from non-vegetated ones. The red band already provides a useful feature for discrimination of vegetated and non-vegetated areas based on chlorophyll reflectance. However, we also use derived features (vegetation indices derived from spectral band combinations) that are more representative of vegetation greenness -this includes the Enhanced Vegetation Index (EVI) <ref type="bibr" target="#b11">(Huete et al. 2002)</ref>, Normalized Difference Vegetation Index (NDVI) <ref type="bibr" target="#b18">(Rouse et al. 1974)</ref> and Atmospherically Resistant Vegetation Index (ARVI) <ref type="bibr" target="#b12">(Kaufman and Tanre 1992)</ref>.</p><p>The performance of our learner depends to a large extent on the selected features. Some features contribute more than others towards optimal classification. The 150 features extracted are narrowed down to 22 using a feature-ranking algorithm based on Distribution Separability Criterion <ref type="bibr" target="#b4">(Boureau, Ponce, and Lecun 2010)</ref>. Details of the feature ranking method along with the ranking for all the 22 features used in our framework are provided in Section 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Statistical Perspective based on Distribution Separability Criterion</head><p>Improving classification accuracy can be viewed as maximizing the separability between the class-conditional distributions. We can view the problem of maximizing distribution separability (Boureau, Ponce, and Lecun 2010) as maximizing the distance between distribution means and minimizing their standard deviations. <ref type="figure" target="#fig_1">Figure  2</ref> shows the histograms that represent the class-conditional distributions of the NIR channel and a sample feature extracted in our framework. As illustrated in <ref type="table">Table 2</ref>, the features extracted in our framework have a higher distance between means and a lower standard deviation as compared to the original image distributions, thereby ensuring better class separability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Feature Ranking</head><p>Following the analysis proposed in Section 3.2 above, we can derive a metric for the Distribution Separability Criterion as follows: D s = δmean δσ where δ mean indicates the mean of distance between means and δ σ indicates the mean of standard deviations  of the class conditional distributions. Maximizing D s over the feature space, a feature ranking can be obtained. <ref type="table">Table 1</ref> shows the ranking of the various features used in our framework along with the values of the corresponding distance between means δ mean , standard deviation δ σ , and Distribution Separability Criterion D s . A threshold of D s = 0.3 was used to narrow down the 22 features in <ref type="table">Table 1</ref> from among 150 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>All of our experiments were conducted on an Exxact workstation with one Intel Core i7-5930K CPU with 12 cores, four NVIDIA GeForce GTX TITAN X GPUs, and a 64 GB memory. The NVIDIA deep learning library of CuDNN of CUDA was used for acceleration and our model was developed in Keras with Tensorflow as backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Analysis</head><p>We evaluated our architecture on the SAT-4 and SAT-6 datasets <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>).</p><p>As stated above, among the publicly available benchmark datasets for high resolution satellite imagery in the remote sensing community (WWW1 n.d.), only SAT-4 and SAT-6 provide enough labeled image patches (500,000 and 405,000 respectively) to evaluate a new architecture or approach without running into overtraining issues.  ( <ref type="bibr" target="#b0">Basu et al. 2015a</ref>) while the test set has 100,000 samples with the image size and channels remaining the same. The SAT-6 training set has 324,000 training samples of 28 × 28 images each with 4 channels <ref type="bibr" target="#b0">(Basu et al. 2015a</ref>) while the test set has 81,000 samples with the image size and channels remaining the same.</p><p>To qualitatively understand the impact of augmentation with handcrafted features, in <ref type="figure" target="#fig_2">Figure 3</ref>, we visualize the learned representations and the decision boundaries for the SAT-4 dataset using t-Distributed Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b16">(Maaten and Hinton 2008)</ref>, that embeds representations in high dimensions into two dimensional space preserving the distances based on local structure. To this end, t-SNE first generates a probability distribution over point pairs in high dimensional space using a Gaussian distribution, ensuring that similar pairs have higher probability. It then generates the low dimensional mappings having the similar probability distributions wherein similarity between points is estimated using the student t-distribution. The bottom row in <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the map responses learned from the first fully connected dense layer, those learned from the second fully connected dense layer, and the decision boundaries, respectively, for a CNN augmented with handcrafted features while the top row shows the same for the same CNN without the handcrafted features (and without the feature fusion layer). It can be seen from <ref type="figure" target="#fig_2">Figure 3</ref> that fusing handcrafted features helped improve discriminative feature learning (see <ref type="figure" target="#fig_2">Figure 3</ref>(B), bottom row, where the others class is already more compactly clustered than in the top) providing robust separation of the decision boundaries (see <ref type="figure" target="#fig_2">Figure 3</ref>(C) where the bottom row shows clearer separation of the classes than the top where the classes trees, grassland, and others are not robustly separable and the intra-class distances are more). This is corroborated by the higher distances between means and the lower standard deviations for the handcrafted features as shown in <ref type="table">Table 2</ref>.</p><p>We next study the impact of the two fully connected layers after the feature fusion layer as well as that of the dropout layers on classification (testing) accuracy in  <ref type="figure" target="#fig_3">Figure 4(a)</ref> shows that removing the second dense layer (with 128 neurons) reduces the network performance with respect to accuracy of classifica-  tion. <ref type="figure" target="#fig_3">Figure 4(b)</ref> shows that a dropout layer before the feature fusion layer with rate 0.25 and one before the final layer with rate 0.2 provides the best performance in terms of classification accuracy (testing) (shown by pink line in <ref type="figure" target="#fig_3">Figure 4(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>In this section, we compare the results obtained by using our approach with those obtained using state-of-the-art methods on the SAT-4 and SAT-6 datasets. The comparison is shown in <ref type="table" target="#tab_2">Table 3</ref>. The classification accuracy obtained using our approach are 99.90% on SAT-4 and 99.84% on SAT-6. It can be seen from <ref type="table" target="#tab_2">Table 3</ref> that our framework surpasses all the existing approaches in terms of accuracy of classification <ref type="bibr" target="#b0">(Basu et al. 2015a;</ref><ref type="bibr" target="#b19">Simo-Serra et al. 2015;</ref><ref type="bibr" target="#b23">Zhong et al. 2017;</ref><ref type="bibr" target="#b15">Ma et al. 2016;</ref><ref type="bibr" target="#b9">Gong et al. 2018;</ref><ref type="bibr" target="#b14">Liu and Huang 2018)</ref>; in particular, it surpasses the next best one (Liu and Huang 2018) that uses triplet networks by 0.14% on SAT-4 and 0.13% on SAT-6. We statistically evaluate the significance of the improvement provided by our framework over <ref type="bibr" target="#b14">(Liu and Huang 2018)</ref> using the McNemar's test (since the test datasets for our framework and for <ref type="bibr" target="#b14">(Liu and Huang 2018)</ref> were same for both . For the SAT-4 dataset, using McNemar's test, we obtain the value of the test statistic χ 2 = 138.01 with degree of freedom 1 and a two-tailed p-value less than 2.2 × 10 −16 indicating that the improvement in the accuracy of classification induced by our framework is statistically significant. For the SAT-6 dataset, using McNemar's test, we obtain the value of the test statistic χ 2 = 103.01 with degree of freedom 1 and a two-tailed p-value less than 2.2 × 10 −16 indicating that the improvement in the accuracy of classification induced by our framework is statistically significant. Our approach achieves better performance than that achieved by complex triplet networks <ref type="bibr" target="#b14">(Liu and Huang 2018)</ref> by augmenting a smaller CNN, comprising only of two convolutional layers together with two fully connected layers apart from ReLU, Max-pooling, Dropout, and Softmax, with handcrafted features. The advantages of our framework are simplicity and fast training (with average training time being around 1200 seconds for both datasets as opposed to ∼2400 seconds for ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Architecture of the DeepSat V2 classification framework. et al. 2015a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Distributions of the raw NIR values for traditional deep learning algorithms and a sample handcrafted DeepSat feature (Autocorrelation of Hue Color co-occurrence matrix Boyda et al. (2017)) for various classes in SAT-4 imagery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of learned representations and decision boundaries for SAT-4 dataset. Top row, regular CNN model which has no handcrafted features fused. Bottom, proposed framework which has handcrafted features fused. (a) Feature maps learned from the first dense layer. (b) Feature maps learned from the second dense layer. (c) Decision Boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Both Figures 4(a) and 4(b) show how classification accuracy (testing) changes with the number of epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Impact on the classification performance of our framework on the datasets. (a) Using different number of fully connected layers with values. (b) Using different number of dropout layers with values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of classification accuracy (%) of various methods on SAT-4 and SAT-6 datasets.</figDesc><table><row><cell>Methods</cell><cell>SAT-4 Accuracy (%)</cell><cell>SAT-6 Accuracy (%)</cell></row><row><cell>DBN (Basu et al. 2015a)</cell><cell>81.78</cell><cell>76.47</cell></row><row><cell>SDAE (Basu et al. 2015a)</cell><cell>79.98</cell><cell>78.43</cell></row><row><cell>CNN (Basu et al. 2015a)</cell><cell>86.83</cell><cell>79.10</cell></row><row><cell>DeepSat (Basu et al. 2015a)</cell><cell>97.95</cell><cell>93.92</cell></row><row><cell>Contrastive loss (Simo-Serra et al. 2015)</cell><cell>98.74</cell><cell>98.55</cell></row><row><cell>MLP (Z -score) (Zhong et al. 2017)</cell><cell>94.76</cell><cell>97.46</cell></row><row><cell>DCNN (Ma et al. 2016)</cell><cell>98.41</cell><cell>96.04</cell></row><row><cell>TradCNN (Z -score) (Zhong et al. 2017)</cell><cell>98.43</cell><cell>98.34</cell></row><row><cell>D-DSML-CaffeNet (Gong et al. 2018)</cell><cell>99.51</cell><cell>99.42</cell></row><row><cell>SatCNN (linear) (Zhong et al. 2017)</cell><cell>99.55</cell><cell>99.58</cell></row><row><cell>SatCNN (Z -score) (Zhong et al. 2017)</cell><cell>99.69</cell><cell>99.61</cell></row><row><cell>Triplet networks (Liu and Huang 2018)</cell><cell>99.76</cell><cell>99.71</cell></row><row><cell>DeepSat V2 (The proposed method)</cell><cell>99.90</cell><cell>99.84</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an end-to-end framework based on an improved architecture that augments a CNN architecture with handcrafted features, for high resolution satellite imagery classification. We showed that augmenting a CNN with handcrafted features enhances its discriminative power for satellite imagery even compared to larger unaugmented CNN architectures ) (see <ref type="table">Table 3</ref>). Our framework outperforms all the existing approaches <ref type="bibr" target="#b0">(Basu et al. 2015a;</ref><ref type="bibr" target="#b19">Simo-Serra et al. 2015;</ref><ref type="bibr" target="#b23">Zhong et al. 2017;</ref><ref type="bibr" target="#b15">Ma et al. 2016;</ref><ref type="bibr" target="#b9">Gong et al. 2018;</ref><ref type="bibr" target="#b14">Liu and Huang 2018)</ref> in terms of classification accuracy for the SAT-4 and SAT-6 datasets. A statistical analysis based on Distribution Separability Criterion substantiates the robustness of our approach in learning better representations for satellite imagery.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepSat: a learning framework for satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><forename type="middle">R</forename><surname>Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems<address><addrLine>Bellevue, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-11-03" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Semiautomated Probabilistic Framework for Tree-Cover Delineation From 1-m NAIP Imagery Using a High-Performance Computing Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Milesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michaelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5690" to="5708" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theoretical analysis of Deep Neural Networks for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreekant</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gayaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Joint Conference on Neural Networks</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-24" />
			<biblScope unit="page" from="992" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for texture classification -A theoretical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Karki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dibiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreekant</forename><surname>Nemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gayaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of Feature Pooling in Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>-Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Machine Learning</title>
		<meeting><address><addrLine>Haifa, Isreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deploying a quantum annealing processor to detect tree cover in aerial imagery of California</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Boyda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangram</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supratik</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramakrishna R Nemani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">172505</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep feature fusion for VHR remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Souleyman</forename><surname>Chaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4775" to="4784" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When deep learning meets metric learning: remote sensing image scene classification via learning discriminative CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2811" to="2821" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing deep learned and hand-crafted features of appearance, shape, and dynamics for automatic pain estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Egede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diversity-Promoting Deep Structural Metric Learning for Remote Sensing Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="390" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Textural Features for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Its&amp;apos;hak</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on SMC-</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the radiometric and biophysical performance of the MODIS vegetation indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Didan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="195" to="213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Atmospherically resistant vegetation index (ARVI) for EOS-MODIS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>Geoscience and Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scene classification via triplet networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="237" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Satellite imagery classification based on deep convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangzeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Acad. Sci., Eng. Technol., Int. J. Comput., Elect., Autom., Control Inf. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1155" to="1159" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranam</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van-Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monitoring vegetation systems in the Great Plains with ERTS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Schell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Deering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NASA Goddard Space Flight Center</title>
		<imprint>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">SpaceNet: A Remote Sensing Dataset and Challenge Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bacastow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01232</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">List of datasets for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">N</forename><surname>Www1</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Aerial_images" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SatCNN: satellite image dataset classification using agile convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzan</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene classification based on the fully sparse semantic topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5525" to="5538" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive Deep Sparse Semantic Modeling Framework for High Spatial Resolution Image Scene Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6180" to="6195" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
