<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 4 DI ENS †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 4 DI ENS †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 4 DI ENS †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 4 DI ENS †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nprc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria 4 DI ENS †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Establishing visual correspondences under large intraclass variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details. To tackle these challenges, we represent images by "hyperpixels" that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Establishing visual correspondences under large intraclass variations, i.e., matching scenes depicting different instances of the same object categories, remains a challenging problem in computer vision. It requires analyzing scenes at different levels, from features linked to semantics and context to local image patterns, while being invariant to irrelevant instance-specific details. Recent methods have addressed this problem using deep convolutional features. Many of them <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">42]</ref> formulate this task as local region matching and learn to assign a local region in an image to a correct match in another image. Others <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45]</ref> cast it as image alignment and learn to regress the parameters of global geometric transformation, e.g., using an affine or thin plate spline model <ref type="bibr" target="#b7">[8]</ref>. These methods, however, mainly perform the prediction based on the output of the last convolutional layer, and fail to fully exploit the different levels of semantic features available to resolve the severe * The Neural Processing Research Center, Seoul, Korea † Département d'informatique de l'ENS, ENS, CNRS, PSL University, Paris, France  ambiguities in matching linked with intra-class variations.</p><p>We propose a novel dense matching method, dubbed hyperpixel flow <ref type="figure" target="#fig_1">(Figure 1)</ref>. Inspired by the hypercolumns <ref type="bibr" target="#b17">[18]</ref> used in object segmentation and detection, we represent images by "hyperpixels" that leverage different levels of features among early to late layers of a convolutional neural network and disambiguate parts of images in multiple visual aspects. The corresponding feature layers for hyperpixels are selected by a simple yet effective search process which requires only a small validation set of supervised image pairs. We show that the resultant hyperpixels provide both fine-grained and context-aware features suited for semantic correspondence and that only a few layers are sufficient and even better for the purpose, thus making hyperpixels an effective representation for light-weight computation. To obtain a geometrically consistent flow of hyperpixels, we present a real-time dense matching algorithm, regularized Hough matching (RHM), building on a recent region matching method using geometric voting <ref type="bibr" target="#b3">[4]</ref>. Furthermore, we also introduce a new large-scale dataset, SPair-71k, with more accurate and richer annotations, which facilitates indepth analysis for semantic correspondence.</p><p>Our paper makes four main contributions:</p><p>• We propose hyperpixels for establishing reliable dense correspondences between two images, which provide multi-layer features robust to local ambiguities.</p><p>• We present an efficient matching algorithm, regularized Hough matching (RHM), that achieves a speed of more than 50 fps on a GPU for 300 × 200 image pairs.</p><p>• We introduce a new dataset, SPair-71k, which contains a significantly larger number of image pairs with richer annotations than existing ones.</p><p>• The proposed method, hyperpixel flow, sets a new state of the art on standard benchmarks as well as SPair-71k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Local region matching. Early methods commonly tackle semantic correspondence by matching two sets of local regions based on handcrafted features. Liu et al. <ref type="bibr" target="#b31">[32]</ref> and Kim et al. <ref type="bibr" target="#b21">[22]</ref> use dense SIFT descriptors to establish a flow of local regions across similar but different scenes by leveraging a hierarchical optimization technique in a coarse-to-fine manner. Bristow et al. <ref type="bibr" target="#b0">[1]</ref> use LDA-whitened SIFT descriptors, making correspondence more robust to background clutter. Cho et al. <ref type="bibr" target="#b3">[4]</ref> introduce an effective voting-based algorithm based on region proposals and HOG features <ref type="bibr" target="#b5">[6]</ref> for semantic matching and object discovery.</p><p>Ham et al. <ref type="bibr" target="#b13">[14]</ref> further extend the work with a local-offset matching algorithm, and introduce a benchmark dataset with keypoint-level annotations. Taniai et al. <ref type="bibr" target="#b46">[46]</ref> tackle semantic correspondence jointly with cosegmentation, introducing a benchmark dataset annotated with dense flows and segmentation masks. All these hand-crafted representation fails to capture high-level semantics enough to discriminate complex patterns with large intra-class deformations. In this context, CNN features have emerged as good alternatives for semantic matching. Long et al. <ref type="bibr" target="#b33">[34]</ref> show that convolutional features from a CNN pretrained on classification are transferable to correspondence problems. Choy et al. <ref type="bibr" target="#b4">[5]</ref> attempt to learn a similarity metric based on a CNN using a contrastive loss with hard negative mining. Han et al. <ref type="bibr" target="#b15">[16]</ref> propose to learn a CNN end-to-end with geometric matching, which uses region proposals as matching primitives. Kim et al. <ref type="bibr" target="#b23">[24]</ref> introduce a CNN-based selfsimilarity feature for semantic correspondence, and also use it to estimate dense affine-transformation fields by an iterative discrete-continuous optimization <ref type="bibr" target="#b24">[25]</ref>. Novotny et al. <ref type="bibr" target="#b36">[37]</ref> train a geometry-aware feature in an unsupervised regime and use it for part matching and discovery by measuring confidence scores. Rocco et al. <ref type="bibr" target="#b43">[43]</ref> propose a neigh-bourhood consensus network that computes robust matching similarity using 4D convolution filters.</p><p>Global image alignment. Some methods have cast semantic correspondence as global alignment. Rocco et al. <ref type="bibr" target="#b40">[41]</ref> propose a CNN architecture which takes a correlation tensor and directly predicts global transformation parameters for geometric matching. Seo et al. <ref type="bibr" target="#b45">[45]</ref> improve it using offsetaware correlation kernels with attention. Rocco et al. <ref type="bibr" target="#b42">[42]</ref> develop a weakly-supervised learning framework using differentiable soft-inlier count loss function. Jeon et al. <ref type="bibr" target="#b19">[20]</ref> propose a pyramidal affine transformation regression network to compute the correspondence hierarchically from high-level semantics to pixel-level points. Kim et al. <ref type="bibr" target="#b22">[23]</ref> introduce a recurrent alignment network that performs iterative local transformations with a global constraint.</p><p>Multi-layer neural features. Hariharan et al. <ref type="bibr" target="#b17">[18]</ref> have shown that hypercolumns that combine features from multiple layers of CNN, improve object detection, segmentation, and part labeling. Following this work, several methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> have used multi-layer neural features with additional modules on object detection task. Fathy et al. <ref type="bibr" target="#b9">[10]</ref> propose coarse-to-fine stereo matching method that uses multi-layer features in sequence. In semantic correspondence, multi-layer neural features have rarely been explored despite its relevance. Novotny et al. <ref type="bibr" target="#b38">[39]</ref> use residual hypercolumn features to learn a set of diverse filters for object parts. Ufer and Ommer <ref type="bibr" target="#b47">[47]</ref> employ pyramids of pre-trained CNN features to localize salient feature points guided by object proposals, and match them across images using sparse graph matching. In these methods, multi-layer features are mainly used to localize salient parts and the feature layers are manually selected following previous methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. Unlike these approaches and the hypercolumn <ref type="bibr" target="#b17">[18]</ref>, we use a multi-layer neural feature as a pixel representation for dense matching and optimize feature layers via layer search for the purpose. We show that specific combinations of layers significantly affect matching performance and using only a small number of layers can achieve a remarkable performance.</p><p>Neural architecture search (NAS). The layer search for hyperpixels can be viewed as an instance of NAS <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54]</ref>. Unlike a general search space of network configurations in NAS, however, the search space in our work is limited to combinations of feature layers for visual correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hyperpixel Flow</head><p>Our method presented below, dubbed hyperpixel flow, can be divided into three steps: (1) hyperpixel construction, (2) regularized Hough matching, and (3) flow formation. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the overall architecture of our model aligned with the three steps. Each input image is fed into a convolutional neural network to create a set of hyperpixels. The hyperpixels are then used as primitives for the regularized Hough matching algorithm to build a tensor of matching confidences for all candidate correspondences. The confidence tensor is transformed into a hyperpixel flow in a post-processing step assigning a match to each hyperpixel. Three steps are detailed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hyperpixel construction</head><p>Given an image, a convolutional neural network produces a sequence of L feature maps (f 0 , f 1 , ..., f L−1 ) as intermediate outputs. We represent the original image by a hyperimage by pooling a small subset of K feature maps, optimized for semantic correspondence, and concatenating them along channels with upsampling:</p><formula xml:id="formula_0">F = f l0 , ζ(f l1 ), ζ(f l2 ), ..., ζ(f l K−1 ) ,<label>(1)</label></formula><p>where ζ denotes a function that upsamples the input feature map to the size of f l0 , the base map. We can associate with each spatial position p of the hyperimage the corresponding image coordinates, a hyperpixel feature, and its multi-scale receptive fields. Let us denote by x p the image coordinate of position p, and by f p the corresponding hyperfeature, i.e., f p = F(x p ). The hyperpixel at position p on the hyperimage is then defined as</p><formula xml:id="formula_1">h p = (x p , f p ).<label>(2)</label></formula><p>As will be seen in the next subsection, the hyperpixels are used as primitives for the subsequent matching process.</p><p>To select the optimal set of feature maps for hyperpixels, we perform a search over all convolutional layers of a given CNN so that a subsequent matching algorithm achieves the best validation performance. In our case, we use regularized Hough matching (Sec. 3.2) for the matching algorithm and the probability of correct keypoints (PCK) (Sec. 5.2) for the performance metric. For the search algorithm, we use a variant of beam search <ref type="bibr" target="#b35">[36]</ref>, which is a breadth-first search algorithm with a limited memory. Basically, at each iteration, it evaluates the effect of each candidate layer by adding it to current combinations of layers in the memory  and then replaces them with a fixed number of top performing combinations. The search process is repeated until the number of selected layers reaches the maximum number of layers allowed. Finally, we use the best combination found along the search. The detailed procedure is summarized in Algorithm 1, where we restrict base layer candidates, L base only to layers with a sufficient spatial resolution.</p><formula xml:id="formula_2">(L * , v * ) ← M .findBest(); 21 if v * &gt; v sel then 22 (L sel , v sel ) ← (L * , v * );</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regularized Hough matching</head><p>In order to establish visual correspondences, we adapt the probabilistic Hough matching (PHM), algorithm of Cho et al. <ref type="bibr" target="#b3">[4]</ref>, to hyperpixels. The key idea of PHM is to re-weight appearance similarity by Hough space voting to enforce geometric consistency. In our context, let D = (H, H ) be two sets of hyperpixels, and m = (h, h ) be a hyperpixel match where h and h are respectively elements of H and H . Given a Hough space X of possible offsets (image transformation) between the two hyperpixels, the confidence for match m, p(m|D), is computed as</p><formula xml:id="formula_3">p(m|D) ∝ p(m a ) x∈X p(m g |x) m∈H×H p(m a )p(m g |x),<label>(3)</label></formula><p>where p(m a ) represents the confidence for appearance matching and p(m g |x) is the confidence for geometric matching with an offset x, measuring how close the offset induced by m is to x. By sharing the Hough space X for all matches, PHM efficiently computes the match confidence with good empirical performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this work we compute appearance matching confidence using hyperpixel features:</p><formula xml:id="formula_4">p(m a ) = ReLU f · f f f d ,<label>(4)</label></formula><p>where the ReLU function clamps negative values to zero and the exponent d is used to emphasize the difference between the hyperpixel features. When combined with Hough voting, this similarity function with d ≥ 2 improves matching performance by suppressing noisy activations. We set d = 3 in our experiments. To compute p(m g |x), we construct a two-dimensional offset space, quantize it into a grid of bins, and use a set of center points of the bins for X . For Hough voting, each match m is assigned to the corresponding offset bin to increment the score of the bin by the appearance similarity score, p(m a ). Despite their (serial) complexity of O(|H| × |H |), the operations are mutually independent, and can thus easily be parallelized on a GPU.</p><p>Previous versions of PHM all use multi-scale region proposals <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">48]</ref> as matching primitives described with HOG <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> or a single feature map from a CNN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. While using irregular and multi-scale region proposals focuses attention on object-like regions, it requires creating a three-dimensional offset space for translation and scale changes with higher memory and computation. In contrast, the use of hyperpixels reduces the Hough space down to two dimensions and makes the voting procedure faster and simpler since all hyperpixels are homogeneous on a predefined regular grid. In addition, unlike region proposals, hyperpixels provide (quasi-)dense image features and their multilayer features improve performance in practice. In our GPU implementation, our algorithm, regularized Hough matching (RHM), runs 100 to 500 times faster than PHM (2∼20 msecs vs. 1∼2 secs), enabling real-time matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Flow formation and keypoint transfer</head><p>The raw output of RHM is a tensor of confidences for all candidate matches. It can easily be transformed into a hyperpixel flow in a post-processing step of assigning a match to each hyperpixel, e.g., by nearest-neighbor assignment. Since the base map of the hyperimage is selected among early layers, the flow is dense enough for many applications.</p><p>Transferring keypoints from an image to the corresponding points in another image is commonly used for evaluating semantic correspondences. We use a simple method for keypoint transfer using hyperpixel flow; given a keypoint x p in a source image, its neighbor hyperpixels N (x p ) are collected whose base map receptive fields cover the keypoint, and the displacement vectors from the centers of the base map receptive fields to the keypoint, denoted by {d(x q )} xq∈N (xp) , are computed. Given the hyperpixel flow T of N (x p ) predicted by our method, we apply the average of the displacements {T (x q ) + d(x q )} xq∈N (xp) to localize a corresponding keypoint in the target image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SPair-71k dataset</head><p>With growing interest in semantic correspondence, several annotated benchmarks are now available. Some popular ones are summarized in <ref type="table" target="#tab_1">Table 1</ref>. Due to the high expense of ground-truth annotations for semantic correspondence, early benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> only support indirect evaluation using a surrogate evaluation metric rather than direct matching accuracy. For example, the Caltech-101 dataset in <ref type="bibr" target="#b21">[22]</ref> provides binary mask annotations of objects of interest for 1,515 pairs of images and the accuracy of mask transfer is evaluated as a rough approximation to that of matching. Recently, Ham et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and Taniai et al. <ref type="bibr" target="#b46">[46]</ref> have introduced datasets with ground-truth correspondences. Since then, PF-WILLOW <ref type="bibr" target="#b13">[14]</ref> and PF-PASCAL <ref type="bibr" target="#b14">[15]</ref> have been used for evaluation in many papers. They contain 900 and 1,300 image pairs, respectively, with keypoint annotations for semantic parts.</p><p>All previous datasets, however, have several drawbacks: First, the amount of data is not sufficient to train and test a large model. Second, image pairs do not display much variability in viewpoint, scale, occlusion, and truncation. Third, the annotations are often limited to either keypoints or object segmentation masks, which hinders in-depth analysis. Fourth, the datasets have no clear splits for training, validation, and testing. Due to this, recent evaluations in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref> have been done with different dataset splits of PF-PASCAL. Furthermore, the splits are disjoint in terms of image pairs, but not images: some images are shared between training and testing data.  To resolve these issues, we introduce a new dataset, SPair-71k, consisting of total 70,958 pairs of images from PASCAL 3D+ <ref type="bibr" target="#b50">[50]</ref> and PASCAL VOC 2012 [9] * . The dataset is significantly larger with rich annotations and clearly organized for learning. In particular, several types of useful annotations are available: keypoints of semantic parts, object segmentation masks, bounding boxes, viewpoint, scale, truncation, and occlusion differences for image pairs, etc. <ref type="figure" target="#fig_5">Figure 3</ref> presents the statistics of SPair-71k in pie chart forms and shows a sample image pair with its annotations. For details on our dataset, we refer the readers to the website: http://cvlab.postech.ac.kr/ research/SPair-71k/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>In this section we compare the proposed method with recent state-of-the-art methods and discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>We use two CNNs as main backbone networks for hyperpixel features, ResNet-50 and ResNet-101 <ref type="bibr" target="#b18">[19]</ref> pre-trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>. All convolutional layers of the networks are used as candidate feature layers for hyperpixels. We extract the features at the end of each layer before a ReLU activation. The optimal set of hyperpixel layers, (l 0 , ..., l K−1 ), is determined by Algorithm 1 run with a validation split of a target dataset. For this beam search, we set the beam size 4 and the maximum number of layers allowed 8. For the exponent value for hyperpixel similarity, we fix d = 3 based on search using PF-PASCAL validation split. * We do not include 'dining table' and 'sofa' classes because they appear as background in most images and their semantic keypoints are too ambiguous to localize.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation metric</head><p>For evaluation on PF-WILLOW, PF-PASCAL, and SPair-71k, we use a common evaluation metric of percentage of correct keypoints (PCK), which counts the average number of correctly predicted keypoints given a tolerance threshold. Given predicted keypoint k pr and groundtruth keypoint k gt , the prediction is considered correct if Euclidean distance between them is smaller than a given threshold. The correctness c of each keypoint can be expressed as   <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42]</ref> are borrowed from <ref type="bibr" target="#b22">[23]</ref>. where w τ and h τ are the width and height of either an entire image or object bounding box, τ ∈ {img, bbox}, and α τ is a tolerance factor (in most cases, α = 0.1). Note that PCK with α bbox is a more stringent metric than one with α img . The final PCK of a benchmark is evaluated by averaging PCKs of all input image pairs. Following recent papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref>, we evaluate PF-WILLOW with α bbox and PF-PASCAL with α img using the same dataset split as in <ref type="bibr" target="#b43">[43]</ref>. For SPair-71k, we use α bbox , which is more stringent.</p><formula xml:id="formula_5">c = 1 if d(k pr , k gt ) ≤ α τ · max (w τ , h τ ) 0 otherwise,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and analysis</head><p>Hyperpixel layers. For PF-PASCAL, the hyperpixel layer results are <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref> with ResNet-50 and Interestingly, in both cases, adding the second layer significantly boosts the performance of PCK, and only a few more layers are sufficient to achieve a comparable performance with the best one. After reaching an optimized set of layers, adding more damages the performance. This result demonstrates the effectiveness of hyperpixels compared to conventional hypercolumn features. The result also implies that features resolving local-ambiguity lie in between particular layers, e.g., between layer 20 and 30 in our case.</p><p>Benchmark comparisons. <ref type="table" target="#tab_4">Table 2</ref> summarizes comparison to recent methods on three standard benchmarks: PF-PASCAL, PF-WILLOW, and Caltech-101. In this experiment, the hyperpixels tuned using the validation split of PF-PASCAL are evaluated on the test split of PF-PASCAL, and futher evaluated on PF-WILLOW and Caltech-101 for checking transferability as done in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>The results clearly show that the proposed method sets new state-of-the-art results on all the three benchmarks, proving the effectiveness of our approach. Note that all recent neural methods for semantic correspondence rely on ImageNetpretrained features, and thus their performance depends on the backbone networks (indicated by subscripts). As expected, our method using the stronger backbone of ResNet-101 improves the performance compared to using ResNet-50. Furthermore, using the backbone of FCN <ref type="bibr" target="#b29">[30]</ref> pretrained with PASCAL VOC 2012 <ref type="bibr" target="#b8">[9]</ref>, that is a superset of our target dataset <ref type="bibr" target="#b13">[14]</ref>, significantly boosts performance. This shows that our method is flexible in using backbone networks and can further improve by adopting a better one.  <ref type="table">Table 3</ref>: Per-class PCK (α bbox = 0.1) results on SPair-71k dataset. For transferred model, the original models trained on PASCAL-VOC <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">45]</ref> and PF-PASCAL <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref>, which are provided by the authors, are used for evaluation. Note that, for SPair-71k trained models, the transferred models are further finetuned on SPair-71k dataset by ourselves with our best efforts. Numbers in bold indicate the best performance and underlined ones are the second and third best.</p><formula xml:id="formula_6">Approach Model PCK Time (ms)</formula><p>Image alignment CNNGeo res101 <ref type="bibr" target="#b40">[41]</ref> 69.5 40 WeakAlign res101 <ref type="bibr" target="#b42">[42]</ref> 74.8 41 A2Net res101 <ref type="bibr" target="#b45">[45]</ref> 70.8 53 RTNs res101 <ref type="bibr" target="#b22">[23]</ref> 75.9 376</p><p>Local region matching SCNet vgg16 <ref type="bibr" target="#b15">[16]</ref> 72.2 &gt; 1000 PF HOG <ref type="bibr" target="#b13">[14]</ref> 62.5 &gt; 1000 NC-Net res101 <ref type="bibr" target="#b43">[43]</ref> 78.9 261 HPF res101 w/ all layers 74.  <ref type="table">Table 4</ref>: Inference time comparison on PF-PASCAL benchmark. Hyperpixel layers of HPF res50 * are <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref>.</p><p>Degree of supervision. Different methods in our comparison require different degrees of supervision in training as indicated in the second column of <ref type="table" target="#tab_4">Table 2</ref>. The only supervised part of our method is layer selection using a validation set, which can be very small as revealed by small-set experiments, and does not require additional learning: Instead of using all the 308 pairs of the original validation split of PF-PASCAL, the layer search algorithm is performed on k random pairs per class, for a total of 20k validation pairs. The average performances over 10 trials are shown along with their standard deviations in the set of rows with k = 1, 2, 3 at the bottom of <ref type="table" target="#tab_4">Table 2</ref>. Using as little as one sample per class (20 image pairs total) as supervisory signal gives results comparable as using all 308 pairs, outperforming the previous state of the art. Given the cost of data collection and the total amount of user-provided information in weakly-supervised methods, we thus believe that our algorithm with small k values (e.g., k = 1) is more cost effective and practical. Effect of layer search. To check the effect of layer search, we take random combinations of 8 layers (the same number chosen by our layer search) as a baseline. The average results over 10 trials are shown with their standard deviations in the last row of <ref type="table" target="#tab_4">Table 2</ref>. Their much worse performance shows that our layer search is crucial. 84.5 73.9 <ref type="table">Table 5</ref>: Ablation studies on RHM with ResNet-101.</p><p>Comparison to proposal flow approach <ref type="bibr" target="#b13">[14]</ref>. The core differences between hyperpixel flow and proposal flow <ref type="bibr" target="#b13">[14]</ref> are the changes in (1) matching primitives, from perproposal geometric descriptor to hyperpixels, in order to handle problems of local-ambiguity and (2) matching algorithms, from PHM to RHM, in order to leverage hyperpixel geometry for efficiency. In <ref type="table" target="#tab_4">Table 2</ref>, significant performance improvements on three different benchmarks demonstrate that our features encoding high-level semantics while being agnostic to instance-specific details are crucial to establish robust correspondences. In addition, as shown in <ref type="table">Table 4</ref>, the proposed voting method, RHM, with hyperpixels shows an impressive improvement in speed compared to <ref type="bibr" target="#b13">[14]</ref>.</p><p>Inference time comparison. With RHM, predicting dense correspondences for a single pair of images turns out to be much faster compared to other recent models. <ref type="table">Table 4</ref> demonstrates the comparison of per-pair inference time on PF-PASCAL. While having more than 5% improvements over current state-of-the-art approach <ref type="bibr" target="#b43">[43]</ref>, the proposed model runs 4 to 13 times faster. With a slight trade-off on performance, hyperpixels with fewer layers and larger receptive field sizes enables real-time matching.</p><p>Ablation studies on matching. To analyze the effects of RHM and its exponent factor d in similarity p(m a ), we experiment with replacing RHM with naïve nearest neighbor matching (NN) and also varying exponent d of similarity. As shown in <ref type="table">Table 5</ref>, the significant PCK gap between NN and RHM demonstrates the effectiveness of geometry matching. The performance improvement with d ≥ 2 shows its effect of suppressing noisy votes in RHM.   <ref type="table">Table 6</ref>: PCK analysis on SPair-71k. Difficulty levels of view points and scales are labeled easy, medium, and hard, while those of truncation and occlusion are indicated by none, source, target, and both.</p><p>Model analyses on SPair-71k benchmark. We evaluate several recent methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b45">45]</ref> on our new benchmark dataset. In this experiment, our method tuned using the validation split of SPair-71k is evaluated on the test split of SPair-71k. For each method in comparison, we run two versions of each model: a trained model provided by the authors and the other further finetuned by ourselves on SPair-71k training set. The results are shown in <ref type="table">Table 3</ref>. We fail to successfully train the method of <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref> on SPair-71k so that their performances drop when trained. We guess that their original learning objectives for weakly-supervised learning is fragile in presence of large view-point differences as in SPair-71k. We leave this issue for further investigation and will update the results at our benchmark page.</p><p>SPair-71k has several annotation types such as viewpoint, scale, truncation and occlusion differences. In-depth analyses of each model using these annotations are summarized in <ref type="table">Table 6</ref>. All models perform better with pairs of small differences, and view-point and scale differences significantly affect the performances. Yet, our method shows more robust results in terms of those variations compared to the others. <ref type="figure">Figure 5</ref> shows some examples where our method finds reliable correspondences even under a large view-point and scale difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a fast yet effective semantic matching method, hyperpixel flow, which leverages an optimized set of convolutional layer features pre-trained on a classification task. The impressive performance of the proposed method, which is only tuned with a small vadidation split without any end-to-end training, indicates that using relevant levels of multiple neural features is crucial in semantic correspondence. We believe further research in this direction is needed together with feature learning. To this end, we have also introduced a large-scale dataset, SPair-71k, with richer annotations for in-depth analyses, which is intended to resolve drawbacks of existing semantic correspondence datasets and to serve for supervised end-to-end learning of semantic correspondence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Features</head><label></label><figDesc>of all intermediate conv layers at the position Multi-scale receptive fields Hyperpixel Feature layer selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Hyperpixel flow. Top: The hyperpixel is a multilayer pixel representation created with selected levels of features optimized for semantic correspondence. It provides multi-scale features, resolving local ambiguities. Bottom: The proposed method, hyperpixel flow, establishes dense correspondences in real time using hyperpixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture of the proposed method. Hyperpixel flow consists of three main steps: hyperpixel construction, regularized Hough matching, and flow formation. For details, see text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 3 forall l ∈ L base do 4 v 5 M 6 end 7 M 8 ( 9 for k ← 1 to Kmax − 1 do 10 M 14 v 19 M</head><label>13456789101419</label><figDesc>Beam search for hyperpixel layers. Input: Lcand = {0, ..., L − 1}: all candidate layers Lbase: candidate layers for the base (⊂ Lcand) Nbeam: the beam size Kmax: the maximum number of layers allowed Output: Lsel: the set of selected layers 1 function SearchLayers // initialize memory buffers 2 M.init(); M .init(); // base layer search ← evaluateLayerSet({l}); .insert(({l}, v)); ← M.findBestN(N beam ); L sel , v sel ) ← M .findBest(); // layer search iterations ← evaluateLayerSet(L ∪ {l}); 15 M.insert(L ∪ {l}, v )); ← M.findBestN(N beam ); 20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>SPair-71k data statistics and an example pair with its annotations. Best viewed in electronic form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Hyperpixel layer search with ResNet-101 backbone on PF-PASCAL and SPair-71k datasets. Hyperpixel layers are in the order of selection during beam search. Dashed lines indicate PCKs when all layers of a CNN are used for hyperpixels. Best viewed in electronic form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 2 ,</head><label>2</label><figDesc><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28)</ref> with ResNet-101. For SPair-71k, the results are (0,<ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref> with ResNet-50 and (0, 8, 20, 21, 26, 28, 29, 30) with ResNet-101. In order to analyze the effect of each intermediate feature (f l0 , ..., f l K−1 ) on hyperpixel, we have measured PCK of our model on both PF-PASCAL and SPair-71k in the order of the layer selection during beam search as shown in Figure 4. The dashed lines represent PCKs using all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )Figure 5 :</head><label>a5</label><figDesc>Source image (b) Target image (c) HPF (ours) (d) CNNGeo<ref type="bibr" target="#b40">[41]</ref> (e) A2Net<ref type="bibr" target="#b45">[45]</ref> (f) WeakAlign<ref type="bibr" target="#b42">[42]</ref> (g) NC-Net<ref type="bibr" target="#b43">[43]</ref> Qualitative results on SPair-71k. The source images are transformed to target images using correspondences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Public benchmark datasets for semantic correspondence. The datasets are listed in chronological order. Research papers using the datasets for evaluation are listed in the last column. See text for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>±0.89 83.9 ±1.14 92.2 ±0.99 44.5 ±0.90 72.5 ±1.22 84.8 ±0.93 ±1.33 84.5 ±0.77 92.9 ±0.41 44.7 ±0.92 73.1 ±1.05 85.4 ±0.84 ±1.16 84.5 ±0.27 92.7 ±0.35 45.1 ±0.55 73.4 ±0.52 85.4 ±0.48 ±11.11 74.7 ±6.46 87.3 ±3.13 32.8 ±8.12 62.4 ±6.67 78.2 ±4.20</figDesc><table><row><cell>Methods</cell><cell>Supervision</cell><cell cols="3">PF-PASCAL (PCK@α img ) 0.05 0.1 0.15</cell><cell cols="3">PF-WILLOW (PCK@α bbox ) 0.05 0.1 0.15</cell><cell cols="2">Caltech-101 LT-ACC IoU</cell></row><row><cell>Identity mapping PF HOG [14]</cell><cell>-</cell><cell>12.7 31.4</cell><cell>37.0 62.5</cell><cell>60.8 79.5</cell><cell>12.2 28.4</cell><cell>27.0 56.8</cell><cell>41.7 68.2</cell><cell>0.77 0.78</cell><cell>0.44 0.50</cell></row><row><cell>CNNGeo res101 [41]</cell><cell>synthetic warp</cell><cell>41.0</cell><cell>69.5</cell><cell>80.4</cell><cell>36.9</cell><cell>69.2</cell><cell>77.8</cell><cell>0.79</cell><cell>0.56</cell></row><row><cell>A2Net res101 [45]</cell><cell>(self-supervised)</cell><cell>42.8</cell><cell>70.8</cell><cell>83.3</cell><cell>36.3</cell><cell>68.8</cell><cell>84.4</cell><cell>0.80</cell><cell>0.57</cell></row><row><cell>DCTM CAT-FCSS [24]</cell><cell></cell><cell>34.2</cell><cell>69.6</cell><cell>80.2</cell><cell>38.1</cell><cell>61.0</cell><cell>72.1</cell><cell>0.83</cell><cell>0.52</cell></row><row><cell>Weakalign res101 [42]</cell><cell>image labels</cell><cell>49.0</cell><cell>74.8</cell><cell>84.0</cell><cell>37.0</cell><cell>70.2</cell><cell>79.9</cell><cell>0.85</cell><cell>0.63</cell></row><row><cell>NC-Net res101 [43]</cell><cell>(weakly-supervised)</cell><cell>54.3</cell><cell>78.9</cell><cell>86.0</cell><cell>33.8</cell><cell>67.0</cell><cell>83.7</cell><cell>0.85</cell><cell>0.60</cell></row><row><cell>RTNs res101 [23]</cell><cell></cell><cell>55.2</cell><cell>75.9</cell><cell>85.2</cell><cell>41.3</cell><cell>71.9</cell><cell>86.2</cell><cell>-</cell><cell>-</cell></row><row><cell>UCN GoogLeNet [5]</cell><cell></cell><cell>29.9</cell><cell>55.6</cell><cell>74.0</cell><cell>24.1</cell><cell>54.0</cell><cell>66.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SCNet vgg16 [16]</cell><cell>keypoints</cell><cell>36.2</cell><cell>72.2</cell><cell>82.0</cell><cell>38.6</cell><cell>70.4</cell><cell>85.3</cell><cell>0.79</cell><cell>0.51</cell></row><row><cell>NN-Cyc res101 [28]</cell><cell></cell><cell>55.1</cell><cell>85.7</cell><cell>94.7</cell><cell>40.5</cell><cell>72.5</cell><cell>86.9</cell><cell>0.86</cell><cell>0.62</cell></row><row><cell>HPF res50 (ours) HPF res101 (ours) HPF res101-FCN (ours)</cell><cell>keypoints (validation only)</cell><cell>60.5 60.1 63.5</cell><cell>83.4 84.8 88.3</cell><cell>92.1 92.7 95.4</cell><cell>46.5 45.9 48.6</cell><cell>72.4 74.4 76.3</cell><cell>84.7 85.6 88.2</cell><cell>0.88 0.87 0.87</cell><cell>0.64 0.63 0.63</cell></row><row><cell>HPF res101 (k=1)</cell><cell>keypoints</cell><cell cols="7">59.4 0.87</cell><cell>0.63</cell></row><row><cell>HPF res101 (k=2)</cell><cell>(validation only,</cell><cell cols="7">58.3 0.87</cell><cell>0.63</cell></row><row><cell>HPF res101 (k=3)</cell><cell>small set)</cell><cell cols="7">59.4 0.87</cell><cell>0.63</cell></row><row><cell>HPF res101 (random)</cell><cell>-</cell><cell cols="7">44.5 0.85</cell><cell>0.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on standard benchmarks of semantic correspondences. Subscripts of the method names indicate backbone networks used. The second column denotes supervisory information used for training or tuning. Numbers in bold indicate the best performance and underlined ones are the second and third best. Results of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>WeakAlignres101 [42] 23.4 17.0 41.6 14.6 37.6 28.1 26.6 32.6 12.6 27.9 23.0 13.6 18.5 42.0 16.4 37.9 30.8 26.5 35.6 13.3 29.6 24.3 16.0 WeakAlignres101 [42] 22.2 17.6 41.9 15.1 38.1 27.4 27.2 31.8 12.8 26.8 22.6 14.2</figDesc><table><row><cell></cell><cell>Methods</cell><cell>aero bike bird boat bottle bus</cell><cell>car</cell><cell cols="5">cat chair cow dog horse moto person plant sheep train</cell><cell>tv</cell><cell>all</cell></row><row><cell></cell><cell>CNNGeores101 [41]</cell><cell cols="3">21.3 15.1 34.6 12.8 31.2 26.3 24.0 30.6 11.6 24.3 20.4 12.2</cell><cell>19.7</cell><cell>15.6</cell><cell>14.3</cell><cell>9.6</cell><cell>28.5 28.8 18.1</cell></row><row><cell>Transferred</cell><cell>A2Netres101 [45]</cell><cell cols="3">20.8 17.1 37.4 13.9 33.6 29.4 26.5 34.9 12.0 26.5 22.5 13.3</cell><cell>21.3</cell><cell>20.0</cell><cell>16.9</cell><cell>11.5</cell><cell>28.9 31.6 20.1</cell></row><row><cell>models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>21.3</cell><cell>22.2</cell><cell>17.9</cell><cell>10.9</cell><cell>31.5 34.8 21.1</cell></row><row><cell></cell><cell>NC-Netres101 [43]</cell><cell cols="3">24.0 16.0 45.0 13.7 35.7 25.9 19.0 50.4 14.3 32.6 27.4 19.2</cell><cell>21.7</cell><cell>20.3</cell><cell>20.4</cell><cell>13.6</cell><cell>33.6 40.4 26.4</cell></row><row><cell>SPair-71k trained models</cell><cell>CNNGeores101 [41] A2Netres101 [45] NC-Netres101 [43]</cell><cell cols="4">23.4 16.7 40.2 14.3 36.4 27.7 26.0 32.7 12.7 27.4 22.8 13.7 22.6 21.6 20.9 20.0 17.9 12.2 32.1 11.7 29.0 19.9 16.1 39.2 9.9 23.9 18.8 15.7 17.4</cell><cell>21.0 22.8 22.2 15.9</cell><cell>17.5 20.5 17.9 14.8</cell><cell>10.2 13.5 10.4 9.6</cell><cell>30.8 34.1 20.6 31.4 36.5 22.3 32.2 35.1 20.9 24.2 31.1 20.1</cell></row><row><cell cols="2">HPFres50 (ours)</cell><cell cols="3">25.3 18.5 47.6 14.6 37.0 22.9 18.3 51.1 16.7 31.5 30.8 19.1</cell><cell>23.7</cell><cell>23.8</cell><cell>23.5</cell><cell>14.4</cell><cell>30.8 37.2 27.2</cell></row><row><cell cols="2">HPFres101 (ours)</cell><cell cols="3">25.2 18.9 52.1 15.7 38.0 22.8 19.1 52.9 17.9 33.0 32.8 20.6</cell><cell>24.4</cell><cell>27.9</cell><cell>21.1</cell><cell>15.9</cell><cell>31.5 35.6 28.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>18.6 12.8 31.7 23.8 14.2 29.1 22.9 23.4 21.0 29.0 21.1 21.8 19.6 26.4 18.7 10.6 23.7 15.5 17.9 15.3 22.9 16.1 16.4 14.4 20.6 A2Net res101 [45] 30.9 13.3 7.4 26.1 21.1 12.4 25.0 17.4 20.5 17.6 24.6 18.6 17.2 16.4 22.3 WeakAlign res101 [42] 29.3 11.9 7.0 25.1 19.1 11.0 24.0 15.8 18.4 15.6 23.3 16.1 16.4 15.7 20.9 NC-Net res101 [43] 26.1 13.5 10.1 24.7 17.5 9.9 22.2 17.1 17.5 16.8 22.0 16.3 16.3 15.2 20.1 HPF res50 (ours) 35.0 18.9 13.6 32.0 25.1 15.4 29.7 24.5 23.5 22.9 29.6 22.9 22.1 21.3 27.2 HPF res101 (ours) 35.6 20.3 15.5 33.0 26.1 15.8 31.0 24.6 24.0 23.7 30.8 23.5 22.8 21.8 28.2</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="9">View-point easy medi hard easy medi hard none src Scale Truncation tgt</cell><cell cols="4">Occlusion both none src tgt</cell><cell>both</cell><cell>All</cell></row><row><cell cols="2">Identity mapping</cell><cell>7.3</cell><cell>3.7</cell><cell>2.6</cell><cell>7.0</cell><cell>4.3</cell><cell>3.3</cell><cell>6.5</cell><cell>4.8</cell><cell>3.5</cell><cell>5.0</cell><cell>6.1</cell><cell>4.0</cell><cell>5.1</cell><cell>4.6</cell><cell>5.6</cell></row><row><cell></cell><cell>CNNGeo res101 [41]</cell><cell cols="2">25.2 10.7</cell><cell>5.9</cell><cell cols="2">22.3 16.1</cell><cell>8.5</cell><cell cols="9">21.1 12.7 15.6 13.9 20.0 14.9 14.3 12.4 18.1</cell></row><row><cell>Transferred</cell><cell>A2Net res101 [45]</cell><cell cols="2">27.5 12.4</cell><cell>6.9</cell><cell cols="12">24.1 18.5 10.3 22.9 15.2 17.6 15.7 22.3 16.5 15.2 14.5 20.1</cell></row><row><cell>models</cell><cell cols="3">WeakAlign res101 [42] 29.4 12.2</cell><cell>6.9</cell><cell cols="12">25.4 19.4 10.3 24.1 16.0 18.5 15.7 23.4 16.7 16.7 14.8 21.1</cell></row><row><cell cols="4">NC-Net res101 [43] 34.0 SPair-71k CNNGeo res101 [41] 28.8 12.0</cell><cell>6.4</cell><cell>24.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>trained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by Samsung Advanced Institute of Technology (SAIT) and Basic Science Research Program (NRF-2017R1E1A1A01077999), and also in part by the Inria/NYU collaboration and the Louis Vuitton/ENS chair on artificial intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilton</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning graphs to match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2414" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximate thin plate spline mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical metric learning and matching for 2d and 3d geometric correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Mohammed E Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">178</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">CalTech Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Proposal flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3475" to="3484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proposal flow: Semantic correspondences from object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pyramidal affine regression networks for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Sangryul Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Warpnet: Weakly supervised matching for singleview reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent transformer networks for semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6560" to="6569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dctm: Discrete-continuous transformation matching for semantic flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic matching by weakly supervised 2d point set registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakaria</forename><surname>Laskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kannala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08341</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jointly optimizing 3d model fitting and fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="466" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prime object proposals with randomized prim&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Manen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2536" to="2543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speech understanding systems: Report of a steering committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Medress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Forgie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Klatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>O&amp;apos;malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Neuburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ritea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Shoup-Hummel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">I have seen enough: Transferring parts across categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Anchornet: A weakly supervised network to learn geometrysensitive features for semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1656" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attentive semantic alignment with offset-aware correlation kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deunsol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint recovery of dense correspondence and cosegmentation in two images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep semantic feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Ufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01569</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">FlowWeb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1191" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
