<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Attention Mechanism in Graph Neural Networks via Cardinality Preservation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhang</surname></persName>
							<email>szhang4@gradcenter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ph.D. Program in Computer Science</orgName>
								<orgName type="department" key="dep2">The Graduate Center</orgName>
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<email>lei.xie@hunter.cuny.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ph.D. Program in Computer Science</orgName>
								<orgName type="department" key="dep2">The Graduate Center</orgName>
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Hunter College</orgName>
								<orgName type="institution" key="instit2">The City University of New York</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Helen and Robert Appel Alzheimer&apos;s Disease Research Institute</orgName>
								<orgName type="institution" key="instit2">Feil Family Brain and Mind Research Institute</orgName>
								<orgName type="institution" key="instit3">Weill Cornell Medicine</orgName>
								<orgName type="institution" key="instit4">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Attention Mechanism in Graph Neural Networks via Cardinality Preservation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) are powerful to learn the representation of graph-structured data. Most of the GNNs use the message-passing scheme, where the embedding of a node is iteratively updated by aggregating the information of its neighbors. To achieve a better expressive capability of node influences, attention mechanism has grown to be popular to assign trainable weights to the nodes in aggregation. Though the attention-based GNNs have achieved remarkable results in various tasks, a clear understanding of their discriminative capacities is missing. In this work, we present a theoretical analysis of the representational properties of the GNN that adopts the attention mechanism as an aggregator. Our analysis determines all cases when those attention-based GNNs can always fail to distinguish certain distinct structures. Those cases appear due to the ignorance of cardinality information in attention-based aggregation. To improve the performance of attention-based GNNs, we propose cardinality preserved attention (CPA) models that can be applied to any kind of attention mechanisms. Our experiments on node and graph classification confirm our theoretical analysis and show the competitive performance of our CPA models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graph, as a kind of powerful data structure in non-Euclidean domain, can represent a set of instances (nodes) and the relationships (edges) between them, thus has a broad application in various fields <ref type="bibr" target="#b21">(Zhou et al. 2018b)</ref>. Different from regular Euclidean data such as texts, images and videos, which have clear grid structures that are relatively easy to generalize fundamental mathematical operations <ref type="bibr" target="#b15">(Shuman et al. 2013)</ref>, graph structured data are irregular so it is not straightforward to apply important operations in deep learning (e.g. convolutions). Consequently, the analysis of graph-structured data remains a challenging and ubiquitous question.</p><p>In recent years, Graph Neural Networks (GNNs) have been proposed to learn the representations of graphstructured data and attract a growing interest <ref type="bibr" target="#b14">(Scarselli et al. 2009;</ref><ref type="bibr" target="#b11">Li et al. 2016;</ref><ref type="bibr" target="#b1">Duvenaud et al. 2015;</ref><ref type="bibr" target="#b14">Niepert, Ahmed, and Kutzkov 2016;</ref><ref type="bibr" target="#b7">Kipf and Welling 2017;</ref><ref type="bibr" target="#b2">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b19">Zhang et al. 2018;</ref><ref type="bibr" target="#b18">Ying et al. 2018;</ref><ref type="bibr" target="#b13">Morris et al. 2019a;</ref><ref type="bibr" target="#b18">Xu et al. 2019)</ref>. GNNs can iteratively update node embeddings by aggregating/passing node fea-tures and structural information in the graph. The generated node embeddings can be fed into an extra classification/prediction layer and the whole model is trained end-toend for different tasks.</p><p>Though many GNNs have been proposed, it is noted that when updating the embedding of a node v i by aggregating the embeddings of its neighbor nodes v j , most of the GNN variants will assign non-parametric weight between v i and v j in their aggregators <ref type="bibr" target="#b7">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b2">Hamilton, Ying, and Leskovec 2017;</ref><ref type="bibr" target="#b18">Xu et al. 2019)</ref>. However, such aggregators (e.g. sum or mean) fail to learn and distinguish the information between a target node and its neighbors during the training. Taking account of different contributions from the nodes in a graph is important in realworld data as not all edges have similar impacts. A natural alternative solution is making the edge weights trainable to have a better expressive capability.</p><p>To assign learnable weights in the aggregation, attention mechanism <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014;</ref><ref type="bibr" target="#b16">Vaswani et al. 2017</ref>) is incorporated in GNNs. Thus the weights can be directly represented by attention coefficients between nodes and give interpretability <ref type="bibr" target="#b16">(Veličković et al. 2018;</ref><ref type="bibr" target="#b16">Thekumparampil et al. 2018;</ref><ref type="bibr" target="#b20">Zhou et al. 2018a</ref>). Though GNNs with the attention-based aggregators achieve promising performance on various tasks empirically, a clear understanding of their discriminative power is missing for the designing of more powerful attention-based GNNs. Recent works <ref type="bibr" target="#b14">(Morris et al. 2019b;</ref><ref type="bibr" target="#b18">Xu et al. 2019;</ref><ref type="bibr" target="#b12">Maron et al. 2019</ref>) have theoretically analyzed the expressive power of GNNs. However, they are unaware of the attention mechanism in their analysis. So that it's unclear whether using attention mechanism in aggregation will constrain the expressive power of GNNs.</p><p>In this work, we make efforts to theoretically analyze the discriminative power of GNNs with attention-based aggregators. Our findings reveal that previous proposed attentionbased aggregators fail to distinguish certain distinct structures. By determining all such cases, we reveal the reason for those failures is the ignorance of cardinality information in aggregation. It inspires us to improve the attention mechanism via cardinality preservation. We propose models that can be applied to any kind of attention mechanisms and achieve the goal. In our experiments on node and graph classifications, we confirm our theoretical analysis and validate the power of our proposed models. The best-performed one can achieve competitive results comparing to other baselines. Specifically, our key contributions are summarized as follows:</p><p>• We show that previously proposed attention-based aggregators in message-passing GNNs always fail to distinguish certain distinct structures. We determine all of those cases and demonstrate the reason is the ignorance of the cardinality information in attention-based aggregation. • We propose Cardinality Preserved Attention (CPA) methods to improve the original attention-based aggregator.</p><p>With them, we can distinguish all cases that previously always fail an attention-based aggregator. • Experiments on node and graph classification validate our theoretical analysis and the power of our CPA models. Comparing to baselines, CPA models can reach state-ofthe-art level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries Notations</head><p>Let G = (V, E) be a graph with set of nodes V and set of edges E. The nearest neighbors of node i are defined as</p><formula xml:id="formula_0">N (i) = {j|d(i, j) = 1}, where d(i, j)</formula><p>is the shortest distance between node i and j. We denote the set of node i and its nearest neighbors asÑ (i) = N (i)∪{i}. For the nodes iñ N (i), their feature vectors form a multiset M (i) = (S i , µ i ), where S i = {s 1 , . . . , s n } is the ground set of M (i), and µ i : S i → N * is the multiplicity function that gives the multiplicity of each s ∈ S i . The cardinality |M | of a multiset is the number of elements (with multiplicity) in the multiset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks</head><p>General GNNs Graph Neural Networks (GNNs) adopt element (node or edge) features X and the graph structure A as input to learn the representation of each element, h i , or each graph, h G , for different tasks. In this work, we focus on the GNNs under massage-passing framework, which updates the node embeddings by aggregating its nearest neighbor node embeddings iteratively. In previous surveys, this type of GNNs is referred as Graph Convolutional Networks in <ref type="bibr" target="#b17">(Wu et al. 2019)</ref> or the GNNs with convolutional aggregator in <ref type="bibr" target="#b21">(Zhou et al. 2018b</ref>). Under the framework, a learned representation of the node after l aggregation layers can contain the features and the structural information within l-step neighborhoods of the node. The l-th layer of a GNN can be formally represented as:</p><formula xml:id="formula_1">h l i = φ l h l−1 i , h l−1 j , ∀j ∈ N (i) ,<label>(1)</label></formula><p>where the superscript l denotes the l-th layer and h 0 i is initialized as X i . The aggregation function φ in equation 1 propagates information between nodes and updates the hidden state of nodes.</p><p>In the final layer, since the node representation h L i after L iterations contains the L-step neighborhood information, it can be directly used for local/node-level tasks. While for global/graph-level tasks, the whole graph representation h G is needed, which requiring an extra readout function g to compute h G from all h L i :</p><formula xml:id="formula_2">h G = g h L i , ∀i ∈ G .<label>(2)</label></formula><p>Attention-Based GNNs In a GNN, when the aggregation function φ in equation 1 adopts attention mechanism, we consider it as an attention-based GNN. In previous survey (Section 6 of <ref type="bibr" target="#b9">(Lee et al. 2018)</ref>), this is referred to the first two types of attentions which have been applied to graph data. The attention-based aggregator in l-th layer can be formulated as follows:</p><formula xml:id="formula_3">e l−1 ij = Att h l−1 i , h l−1 j ,<label>(3)</label></formula><formula xml:id="formula_4">α l−1 ij = softmax e l−1 ij = exp(e l−1 ij ) k∈Ñ (i) exp e l−1 ik ,<label>(4)</label></formula><formula xml:id="formula_5">h l i = f l j∈Ñ (i) α l−1 ij h l−1 j ,<label>(5)</label></formula><p>where the superscript l denotes the l-th layer and e ij is the attention coefficient computed by an attention function Att to measure the relation between node i and node j. α ij is the attention weight calculated by the softmax function. Equation 5 is a weighted summation that uses all α as weights followed with a nonlinear function f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Since GNNs have achieved remarkable results in practice, a clear understanding of the power of GNNs in graph representational learning is needed to design better models and make further improvements. Recent works <ref type="bibr" target="#b14">(Morris et al. 2019b;</ref><ref type="bibr" target="#b18">Xu et al. 2019;</ref><ref type="bibr" target="#b12">Maron et al. 2019</ref>) focus on understanding the discriminative power of GNNs by comparing it to the Weisfeiler-Lehman (WL) test <ref type="bibr" target="#b17">(Weisfeiler and Leman 1968</ref>) when deciding the graph isomorphism. It is proved that massage-passing-based GNNs which aggregate the nearest neighbor node features of a node for embedding are at most as powerful as the 1-WL test <ref type="bibr" target="#b18">(Xu et al. 2019)</ref>. Inspired by the higher discriminative power of the k-WL test (k &gt; 2) <ref type="bibr" target="#b1">(Cai, Fürer, and Immerman 1992)</ref> than the 1-WL test, GNNs that have a theoretically higher discriminative power than the massage-passing-based GNNs have been proposed based on the k-WL test <ref type="bibr" target="#b14">(Morris et al. 2019b;</ref><ref type="bibr" target="#b12">Maron et al. 2019</ref>). However, the GNNs proposed in those works don't specifically contain the attention mechanism as the part of their analysis. So it's currently unknown whether the attention mechanism will constrain the discriminative power. Our work focuses on the massage-passing-based GNNs with attention mechanism, which are upper bounded by the 1-WL test. Another recent work <ref type="bibr" target="#b8">(Knyazev, Taylor, and Amer 2019)</ref> aims to understand the attention mechanism over nodes in GNNs with experiments in a controlled environment. However, the attention mechanism discussed in the work is used in the pooling layer for the pooling of nodes, while our work investigates the usage of attention mechanism in the aggregation layer for the updating of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitation of Attention-Based GNNs</head><p>In this section, we theoretically analyze the discriminative power of attention-based GNNs and show their limitations. The discriminative power means how well an attention-based GNN can distinguish different elements (local or global structures). We find that previously proposed attention-based GNNs can fail in certain cases and the discriminative power is limited. Besides, by theoretically finding out all cases that always fail an attention-based GNN, we reveal that those failures come from the lack of cardinality preservation in attention-based aggregators. The details of proofs are included in the Supplemental Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminative Power of Attention-based GNNs</head><p>We assume the node input feature space is countable. For any attention-based GNNs, we give the conditions in Lemma 1 to make them reach the upper bound of discriminative power when distinguishing different elements (local or global structures). In particular, each local structure belongs to a node and is the k-height subtree structure rooted at the node, which is naturally captured in the node feature h k i after k iterations in a GNN. The global structure contains the information of all such subtrees in a graph. Lemma 1. Let A : G → R g be a GNN following the neighborhood aggregation scheme with the attention-based aggregator (Equation 5). For global-level task, an extra readout function <ref type="formula" target="#formula_2">(Equation 2</ref>) is used in the final layer. A can reach its upper bound of discriminative power (can distinguish all distinct local structures or be as powerful as the 1-WL test when distinguishing distinct global structures) after sufficient iterations with the following conditions: With Lemma 1, we are interested in whether its conditions can always be satisfied, so as to reach the upper bound of discriminative capacity of an attention-based GNN. Since the function f and the global-level readout function can be predetermined to be injective, we focus on whether the weighted summation function in attention-based aggregator can be injective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Non-Injectivity of Attention-Based Aggregator</head><p>In this part, we aim to answer the following two questions: Q 1. Can the attention-based GNNs actually reach the upper bound of discriminative power? In other words, can the weighted summation function in an attention-based aggregator be injective? Q 2. If not, can we determine all of the cases that prevent any kind of weighted summation function being injective?</p><p>Given a countable feature space H, a weighted summation function is a mapping W : H → R n . The exact W is determined by the attention weights α computed from Att in Equation 3. Since Att is affected by stochastic optimization algorithms (e.g. SGD) which introduce stochasticity in W , we have to pay attention that W is not fixed when dealing with the two questions.</p><p>In Theorem 1, we answer Q1 with No by giving the cases that make W not to be injective. So that the attention-based GNNs can never meet their upper bound of discriminative power, which is stated in Corollary 1. Moreover, we answer Q2 with Yes in Theorem 1 by pointing out those cases are the only reason to always prevent W being injective. This alleviates the difficulty of summarizing the properties of those cases. Besides, we can specifically propose methods to avoid those cases so as to let W to be injective. Theorem 1. Assume the input feature space X is countable. Given a multiset X ⊂ X and the node feature c of the central node, the weighted summation function h(c, X) in aggregation is defined as . For all f and Att, h(c 1 , X 1 ) = h(c 2 , X 2 ) if and only if c 1 = c 2 , X 1 = (S, µ) and X 2 = (S, k · µ) for k ∈ N * . In other words, h will map different multisets to the same embedding if and only if the multisets have the same central node feature and the same distribution of node features. Corollary 1. Let A be the GNN defined in Lemma 1. A never reaches its upper bound of discriminative power:</p><formula xml:id="formula_6">h(c, X) = x∈X α cx f (x), where f : X → R n</formula><p>There exists two different subtrees S 1 and S 2 or two graphs G 1 and G 2 that the Weisfeiler-Lehman test decides as non-isomorphic, such that A always maps the two subtrees/graphs to the same embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Mechanism Fails to Preserve Cardinality</head><p>With Theorem 1, we are now interested in the properties of all cases that always prevent the weighted summation functions W being injective. Since the multisets that all W fail to distinguish share the same distribution of node features, we can say that W ignores the multiplicity information of each identical element in the multisets. Thus the cardinality of the multiset is not preserved: Corollary 2. Let A be the GNN defined in Lemma 1. The attention-based aggregator in A cannot preserve the cardinality information of the multiset of node features in aggregation.</p><p>In the next section, we aim to propose improved attentionbased models to preserve the cardinality in aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cardinality Preserved Attention (CPA) Model</head><p>Since the cardinality of the multiset is not preserved in attention-based aggregators, our goal is to propose modifications to any kind of attention mechanism to make them capture the cardinality information. So that all of the cases that always prevent attention-based aggregator being injective can be avoided.</p><p>To achieve our goal, we modify the weighted summation function in Equation 5 to incorporate the cardinality information and don't change the attention function in Equation 3 so as to keep its original expressive power. Two different </p><formula xml:id="formula_7">D D Ü Ü D D Ü Ü ð ð : : é é 0 0 5 5 E E ; ; D D Ü Ü 6 6 ñ ñ D D Ü Ü 5 5 ñ ñ ñ ñ ñ ñ D D Ü Ü 6 6 ñ ñ ñ ñ ñ ñ D D Ü Ü 6 6 ñ ñ ñ ñ D D Ü Ü 5 5 ñ ñ ñ ñ M M M M # # @ @ @ @ E E P P E E R R A A 5 5 ? ? = = H H A A @ @ D D 6 6 D D 5 5 D D 7 7 ® ® Ù Ù Ü Ü 5 5 Ù Ù Ü Ü 6 6 Ù Ù Ü Ü 7 7 Ù Ù Ü Ü Ü Ü t t Ú Ú L L : : á á AE AE ; ; D D 6 6 D D 8 8 D D 5 5 ® ® D D 7 7 Ù Ù Ü Ü Ü Ü Ù Ù Ü Ü 8 8 Ù Ù Ü Ü 7 7 Ù Ù Ü Ü 6 6 Ù Ù Ü Ü 5 5 t t Û Û L L : : á á AE AE ; ; D D Ü Ü 5 5 ñ ñ 1 1 N N E E C C E E J J = = H H 1 1 N N E E C C E E J J = = H H E E è è L L S S è è Ã Ã Ý Ý Ð Ð é éÇ Ç - -Ü Ü D D Ý Ý E E è è # # @ @ @ @ E E P P E E R R A A S S è è Ã Ã Ý Ý Ð Ð é éÇ Ç . . Ü Ü D D Ý Ý<label>ð</label></formula><formula xml:id="formula_8">h l i = f l j∈Ñ (i) α l−1 ij h l−1 j + w l j∈Ñ (i) h l−1 j ,<label>(6)</label></formula><p>Model 2. (Scaled)</p><formula xml:id="formula_9">h l i = f l ψ l Ñ (i) j∈Ñ (i) α l−1 ij h l−1 j ,<label>(7)</label></formula><p>where w is a non-zero vector ∈ R n , denotes the elementwise multiplication, |Ñ (i)| equals to the cardinality of the multisetÑ (i), ψ : Z + → R n is an injective function. In the Additive model, each element in the multiset will contribute to the term that we added to preserve the cardinality information. In the Scaled model, the original weighted summation is directly multiplied by a representational vector of the cardinality value. So with these models, distinct multisets with the same distribution will result in different embedding h. Note that both of our models don't change the Att function, such that they can keep the learning power of the original attention mechanism. We summarize the effect of our models in Corollary 3 and illustrate it in <ref type="figure">Figure 1</ref>.  <ref type="figure">6</ref> and 7, T s discriminative power is increased: T can now distinguish all different multisets in aggregation that it previously always fails to distinguish.</p><p>While the original attention-based aggregator is never injective as we mentioned in previous sections, our cardinality preserved attention-based aggregator can be injective with certain learned attention weights to reach its upper bound of discriminative power. We validate this in our experiments.</p><p>For the time and space complexity of our CPA models comparing to the original attention-based aggregator, it is obvious that the Model 1 and 2 take more time and space than the original one due to our introduced vectors w and ψ(|Ñ (i)|). Thus we further simplify our models by fixing the values in w and ψ(|Ñ (i)|) and define two CPA variants:</p><formula xml:id="formula_10">Model 3. (f-Additive) h l i = f l j∈Ñ (i) (α l−1 ij + 1)h l−1 j ,<label>(8)</label></formula><p>Model 4. (f-Scaled)</p><formula xml:id="formula_11">h l i = f l Ñ (i) · j∈Ñ (i) α l−1 ij h l−1 j .<label>(9)</label></formula><p>Model 3 and 4 still preserve the cardinality information and have reduced time and space complexity comparing to Model 1 and 2. Actually, since w and ψ(|Ñ (i)|) are degenerate into constants, Model 3 and 4 have the same time and space complexity as the original model in Equation 5. In our experiments, we will examine all 4 models together with the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In our experiments, we focus on the following questions: Q 3. Since attention-based GNNs (e.g. GAT) are originally proposed for local-level tasks like node classification, will those models fail or not meet the upper bound of discriminative power when solving certain node classification tasks? If so, can our proposed CPA models improve the original model? To answer Question 3, we design a node classification task which is to predict whether or not a node is included in a triangle as one vertex in a graph. To answer Question 4 and 5, we perform experiments on graph classification benchmarks and evaluate the performance of attention-based GNNs with CPA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>Datasets In our synthetic task (TRIANGLE-NODE) for predicting whether or not a node is included in a triangle, we generate a graph with different node features. In our experiment on graph classification, we use 6 benchmark datasets: 2 social network datasets (REDDIT-BINARY (RE-B), REDDIT-MULTI5K (RE-M5K)) and 4 bioinformatics datasets (MUTAG, PROTEINS, ENZYMES, NCI1). More details of the datasets are provided in Supplemental Material.  Models In our experiments, the Original model is the one that uses the original version of an attention mechanism. We apply each of our 4 CPA models (Additive, Scaled, f-Additive and f-Scaled) to the original attention mechanism for comparison. In the Additive and Scaled models, we take advantage of the approximation capability of multi-layer perceptron (MLP) <ref type="bibr" target="#b3">(Hornik, Stinchcombe, and White 1989;</ref><ref type="bibr" target="#b4">Hornik 1991</ref>) to model f and ψ.</p><p>For node classification, we use GAT (Veličković et al. 2018) as the Original model. For graph classification, we build a GNN (GAT-GC) based on GAT as the Original model: We adopt the attention mechanism in GAT to specify the form of Equation 3: e ij = LeakyReLU a [Wh i Wh j ] . For the readout function, a naive way is to only consider the node embeddings from the last iteration. Although a sufficient number of iterations can help to avoid the cases in Theorem 1 by aggregating more diverse node features, the features from the latter iterations may generalize worse and the GNNs usually have shallow structures <ref type="bibr" target="#b18">(Xu et al. 2019;</ref><ref type="bibr" target="#b21">Zhou et al. 2018b</ref>). So the GAT-GC adopts the same function as used in <ref type="bibr" target="#b18">(Xu et al. 2018;</ref><ref type="bibr" target="#b18">Xu et al. 2019;</ref><ref type="bibr" target="#b10">Lee, Lee, and Kang 2019;</ref>, which concatenates graph embeddings from all iterations:</p><formula xml:id="formula_12">h G = L k=0 Readout( h l i i ∈ G )</formula><p>, Readout function can be sum or mean. With CPA models, the cases in The-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Classification</head><p>For the TRIANGLE-NODE dataset, the proportion P of multisets that hold the properties in Theorem 1 is 29.2%, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Classification</head><p>In this section, we aim to answer Question 4 by evaluating the performance of variants of GAT-based GNN (GAT-GC) on graph classification benchmarks. Besides, we compare our best-performed CPA model with baseline models to answer Question 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Network Datasets</head><p>Since the RE-B and RE-M5K datasets don't have original node features and we assign all the node features to be the same, we have P = 100.0% in those datasets. Thus all multisets in aggregation will be mapped to the same embedding by the Original GAT-GC. After a mean readout function on all multisets, all graphs are finally mapped to the same embedding. The performance of the Original model is just random guessing of the graph labels as reported in <ref type="table" target="#tab_2">Table 2</ref>. While our CPA models can distinguish all different multisets and are confirmed to be significantly better than the Original one.</p><p>Here we examine a naive approach to incorporate the cardinality information in the Original model by assigning node degrees as input node labels. By doing this way, the node features are diverse and we get P = 0.0%, which means that the cases in Theorem 1 can be all avoided. However, the testing accuracies of Original can only reach 76.65 ± 9.87% on RE-B and 43.71 ± 9.05% on RE-M5K, which are significantly lower than the results of CPA models in <ref type="table" target="#tab_2">Table 2</ref>. Thus in practice, our proposed models exhibit good generalization power comparing to the naive approach.</p><p>Bioinformatics Datasets For bioinformatics datasets that contain diverse node labels, we also report the P values in <ref type="table" target="#tab_3">Table 3</ref>. The results reveal the existence (P ≥ 29.3%) of the cases in those datasets that can fool the Original model, thus the discriminative power of the Original model is theoretically constrained.</p><p>To empirically validate this, we compare the training accuracies of GAT-GC variants, since the discriminative power can be directly indicated by the accuracies on training sets. Higher training accuracy indicates a better fitting ability to distinguish different graphs. The training curves of GAT-GC variants are shown in <ref type="figure" target="#fig_4">Figure 2</ref>. From these curves, we can see even though the Original model has overfitted different datasets, the fitting accuracies that it converges to can never be higher than those of our CPA models. Compared to the WL kernel, CPA models can get training accuracies close to 100% on several datasets, which reach those obtained from the WL kernel (equal to 100% as shown in <ref type="bibr" target="#b18">(Xu et al. 2019)</ref>). These findings validate that the discriminative power of the Original model is constrained while our CPA models can approach the upper bound of discriminative power with certain learned weights.</p><p>In <ref type="table" target="#tab_3">Table 3</ref> we report the testing accuracies of GAT-GC variants on bioinformatics datasets. The Original model can get meaningful results. However, we find our proposed CPA models further improve the testing accuracies of the Original model on all datasets. This indicates that the preservation of cardinality can also benefit the generalization power of the model besides the discriminative power.</p><p>From previous results in <ref type="table" target="#tab_2">Table 2</ref> and 3, we find the f-Scaled model performs the best with an average ranking measure <ref type="bibr" target="#b16">(Taheri, Gimpel, and Berger-Wolf 2018)</ref>. The good performance of the fixed-weight models (f-Additive and f-Scaled) comparing to the full models (Additive and Scaled) demonstrates that the improvements achieved by CPA models are not simply due to the increased capacities given by the additional vectors embedded.</p><p>Comparison to Baselines We further compare the bestperformed GAT-GC variant (f-Scaled) with other baselines (WL kernel (WL) <ref type="bibr" target="#b15">(Shervashidze et al. 2011)</ref>, PATCHY-SAN (PSCN) (Niepert, Ahmed, and Kutzkov 2016), Deep Graph CNN (DGCNN) <ref type="bibr" target="#b19">(Zhang et al. 2018)</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b18">(Xu et al. 2019)</ref> and Capsule Graph Neural Network (CapsGNN) (Xinyi and Chen 2019)). In Table 4, we report the results. Our GAT-GC (f-Scaled) model achieves 4 top 1 and 2 top 2 on all 6 datasets. It is expected that even better performance can be achieved with certain choices of attention mechanism besides the GAT one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we theoretically analyze the representational power of GNNs with attention-based aggregators: We determine all cases when those GNNs always fail to distinguish distinct structures. The finding shows that the missing cardinality information in aggregation is the only reason to cause those failures. To improve, we propose cardinality preserved attention (CPA) models to solve this issue.</p><p>In our experiments, we validate our theoretical analysis that the performances of the original attention-based GNNs are limited. With our models, the original models can be improved. Compared to other baselines, our best-performed model achieves competitive performance. In future work, a challenging problem is to better learn the attention weights so as to guarantee the injectivity of our cardinality preserved attention models after the training. Besides, it would be interesting to analyze the effects of different attention mechanisms. From all equations above, we finally have</p><formula xml:id="formula_13">h(c 1 , X 1 ) = s∈S µ(s) exp(e cs1 ) x∈X1 exp(e cx1 ) f (s) = k · s∈S µ(s) exp(e cs2 ) x∈X2 exp(e cx2 ) f (s) = h(c 2 , X 2 ).</formula><p>(2) If given h(c 1 , X 1 ) = h(c 2 , X 2 ) for all f , Att, we have</p><formula xml:id="formula_14">x∈X1 α cx1 f (x) = x∈X2 α cx2 f (x), ∀f, Att,</formula><p>where α cxi is the attention weight belongs to X i , and be-</p><formula xml:id="formula_15">tween f (c i ) and f (x), x ∈ X i , i ∈ {1, 2}.</formula><p>We denote X 1 = (S 1 , µ 1 ) and X 2 = (S 2 , µ 2 ) and rewrite the equation:</p><formula xml:id="formula_16">s∈S1 µ 1 (s)α cs1 f (s) = s∈S2 µ 2 (s)α cs2 f (s), ∀f, Att,</formula><p>where µ i (s) is the multiplicity function of X i , i ∈ {1, 2}. Moreover, α csi is the attention weight belongs to X i , and between f (c i ) and f (s), s ∈ S i , i ∈ {1, 2}.</p><p>When considering the relations between S 1 and S 2 , we have:</p><formula xml:id="formula_17">s∈S1∩S2 µ 1 (s)α cs1 − µ 2 (s)α cs2 f (s)+ s∈S1\S2 µ 1 (s)α cs1 f (s) − s∈S2\S1 µ 2 (s)α cs2 f (s) = 0.<label>(10)</label></formula><p>If we assume the equality of Equation 10 is true for all f and S 1 = S 2 , we can define such two functions f 1 and f 2 :</p><formula xml:id="formula_18">f 1 (s) = f 2 (s), ∀s ∈ S 1 ∩ S 2 , f 1 (s) = f 2 (s) − 1, ∀s ∈ S 1 \ S 2 , f 1 (s) = f 2 (s) + 1, ∀s ∈ S 2 \ S 1 .</formula><p>If given the equality of Equation 10 is true for f 1 , we have:</p><formula xml:id="formula_19">s∈S1∩S2 µ 1 (s)α cs1 − µ 2 (s)α cs2 f 1 (s)+ s∈S1\S2 µ 1 (s)α cs1 f 1 (s) − s∈S2\S1 µ 2 (s)α cs2 f 1 (s) = 0.<label>(11)</label></formula><p>We can rewrite Equation 11 using f 2 :</p><formula xml:id="formula_20">s∈S1∩S2 µ 1 (s)α cs1 − µ 2 (s)α cs2 f 2 (s)+ s∈S1\S2 µ 1 (s)α cs1 (f 2 (s) − 1)− s∈S2\S1 µ 2 (s)α cs2 (f 2 (s) + 1) = 0. Thus we know s∈S1∩S2 µ 1 (s)α cs1 − µ 2 (s)α cs2 f 2 (s)+ s∈S1\S2 µ 1 (s)α cs1 f 2 (s) − s∈S2\S1 µ 2 (s)α cs2 f 2 (s) = s∈S1\S2 µ 1 (s)α cs1 + s∈S2\S1 µ 2 (s)α cs2<label>(12)</label></formula><p>Note that the LHS of Equation 12 is just the LHS of Equation 10 when f = f 2 . As µ i (s) ≥ 1 due to the definition of multiplicity, α csi &gt; 0 due to the softmax function, we have µ i (s)α csi &gt; 0, ∀s ∈ S i , i ∈ {1, 2}. Thus the RHS of Equation 12 &gt; 0 and we now know the equality in Equation 10 is not true for f 2 . So the assumption of S 1 = S 2 is false.</p><p>We denote S = S 1 = S 2 . To let the remaining summation term always equal to 0, we have µ 1 (s)α cs1 − µ 2 (s)α cs2 = 0, ∀Att.</p><p>Considering Equation 2 in our paper, we can rewrite the equation above:</p><formula xml:id="formula_21">µ 1 (s) µ 2 (s) = exp(e cs2 ) exp(e cs1 ) x∈X1 exp(e cx1 ) x∈X2 exp(e cx2 )</formula><p>, ∀Att, <ref type="formula" target="#formula_1">(13)</ref> where e csi is the attention coefficient belongs to X i , and between f (c i ) and f (s), s ∈ S. And e cxi is the attention coefficient belongs to X i , and between f (c i ) and f (x),</p><formula xml:id="formula_22">x ∈ X i , i ∈ {1, 2}.</formula><p>The LHS of Equation 13 is a rational number. However if c 1 = c 2 , the RHS of Equation 13 can be irrational: We assume S contains at least two elements s 0 and s = s 0 . If not, we can directly get c 1 = c 2 . We consider any attention mechanism that results in: e cs1 = 1, ∀s ∈ S, e cs2 = 1, fors = s 0 , 2, ∀s = s 0 ∈ S.</p><p>Thus when s = s 0 , the RHS of the equation become:</p><formula xml:id="formula_23">e e X 1 e ( X 2 − n)e 2 + ne = X 1 ( X 2 − n)e + n ,</formula><p>where n is the multiplicity of s 0 in X 2 . It is obvious that the value of RHS is irrational. So we have c 1 = c 2 to always hold the equality.</p><p>With c 1 = c 2 , we know e cs1 = e cs2 , ∀s ∈ S and e cx1 = e cx2 , ∀x ∈ X 1 , X 2 . We denote e cx = e cx1 = e cx2 , Equation 13 becomes µ 1 (s) µ 2 (s) = x∈X1 exp(e cx ) x∈X2 exp(e cx ) = const., ∀Att.</p><p>We further denote k = µ 1 (s)/µ 2 (s), ∀s ∈ S. So that µ 2 = k · µ 1 . Finally by denoting µ = µ 1 , we have X 1 = (S, µ), X 2 = (S, k · µ) and c 1 = c 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Corollary 1</head><p>Proof. For subtrees, if S 1 and S 2 are 1-height subtrees that have the same root node feature and the same distribution of node features, A will get the same embeddings for S 1 and S 2 according to Theorem 1.</p><p>For graphs, let G 1 be a fully connect graph with n nodes and G 2 be a ring-like graph with n nodes. All nodes in G 1 and G 2 have the same feature x. It is clear that the Weisfeiler-Lehman test of isomorphism decides G 1 and G 2 as non-isomorphic.</p><p>We denote {X i }, i ∈ G 1 as the set of multisets for aggregation in G 1 , and {X j }, j ∈ G 2 as the set of multisets for aggregation in G 2 . As G 1 is a fully connect graph, all multisets in G 1 contain 1 central node and n − 1 neighbors. As G 2 is a ring-like graph, all multisets in G 2 contain 1 central node and 2 neighbors. Thus we have</p><formula xml:id="formula_24">X i = ({x}, {µ 1 (x) = n}), ∀i ∈ G 1 , X j = ({x}, {µ 2 (x) = 3}), ∀j ∈ G 2 , where µ i (x) is the multiplicity function of the node with feature x in G i , i ∈ {1, 2}.</formula><p>From Theorem 1, we know that h(c, X i ) = h(c, X j ), ∀i ∈ G 1 , ∀j ∈ G 2 . Considering the Equation 3 of our paper, we have h l i = h l j , ∀i ∈ G 1 , ∀j ∈ G 2 in each iteration l. Besides, as the number of node in G 1 and G 2 are equals to n, A will always map G 1 and G 2 to the same set of multisets of node features {h l } in each iteration l and finally get the same embedding for each graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Corollary 2</head><p>Proof. Given two distinct multiset of node features X 1 and X 2 that have the same central node feature and the same distribution of node features: c 1 = c 2 , X 1 = (S, µ) and X 2 = (S, k · µ) for k ∈ N * , we know the cardinality of X 2 is k times of the cardinality of X 1 . Thus X 1 and X 2 can be distinguished by their cardinality.</p><p>However, the weighted summation function h in attention-based aggregator A will map them to the same embedding: h(c 1 , X 1 ) = h(c 2 , X 2 ) according to Theorem 1. Thus we cannot distinguish X 1 and X 2 via A. To conclude, A lost the cardinality information after aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Corollary 3</head><p>Proof. For any two distinct multisets X 1 and X 2 that T previously always fail to distinguish according to Theorem 1, we denote X 1 = (S, µ) and X 2 = (S, k · µ) ⊂ X for some k ∈ N * and c ∈ S. Thus x∈X1 α cx1 f (x) = x∈X2 α cx2 f (x), where α cxi is the attention weight belongs to X i , and between f (c) and f (x), x ∈ X i , i ∈ {1, 2}. We denote H = x∈X1 α cx1 f (x) = x∈X2 α cx2 f (x). When applying CPA models, the aggregation functions in T can be rewritten as:</p><formula xml:id="formula_25">h 1 (c, X i ) = H + w x∈Xi f (x), i ∈ {1, 2}, h 2 (c, X i ) = ψ( X i ) H, i ∈ {1, 2}.</formula><p>We consider the following example: All elements in w equal to 1. Function ψ maps X to a n-dimensional vector which all elements in it equal to X . And f (x) = N −Z(x) , where Z : X → N and N &gt; X . So that the aggregation functions become:</p><formula xml:id="formula_26">h 1 (c, X i ) = H + x∈Xi f (x), i ∈ {1, 2}, h 2 (c, X i ) = X i · H, i ∈ {1, 2}. For h 1 , we have h 1 (c, X 1 ) − h 1 (c, X 2 ) = x∈X1 f (x) − x∈X2 f (x).</formula><p>According to Lemma 5 of <ref type="bibr" target="#b18">(Xu et al. 2019)</ref>,</p><formula xml:id="formula_27">when X 1 = X 2 , x∈X1 f (x) = x∈X2 f (x). So h 1 (c, X 1 ) = h 1 (c, X 2 ).</formula><p>For h 2 , we have h 2 (c, X 1 ) − h 2 (c, X 2 ) = ( X 1 − X 2 ) · H. As α cx &gt; 0 due to the softmax function, and f (x) &gt; 0 in our example, we know H &gt; 0. Moreover as X 1 − X 2 = 0, we can get h 2 (c, X 1 ) = h 2 (c, X 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Datasets</head><p>For the node classification task, we generate a graph with 4800 nodes and 32400 edges. 40.58% of the nodes are included in triangles as vertices while 59.42% are not. There are 4000 nodes assigned with feature '0', 400 with feature '1' and 400 with feature '2'. The label of each node for prediction is whether or not it's included in a triangle. For the graph classification task, detailed statistics of the bioinformatics and social network datasets are listed in <ref type="table" target="#tab_6">Table  5</ref>. All of the datasets are available at https://ls11-www.cs.tudortmund.de/staff/morris/graphkerneldatasets.</p><p>In all datasets, if the original node features are provided, we use the one-hot encodings of them as input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Experiment Settings</head><p>For all experiments, we perform 10-fold cross-validation and repeat the experiments 10 times for each dataset and each model. To get a final accuracy for each run, we select the epoch with the best cross-validation accuracy averaged over all 10 folds. The average accuracies and their standard deviations are reported based on the results across the folds in all runs. In our Additive and Scaled models, all MLPs have 2 layers with ReLU activation.</p><p>In the GAT variants, we use 2 GNN layers and a hidden dimensionality of 32. The negative input slope of LeakyReLU in the GAT attention mechanism is 0.2. The number of heads in multi-head attention is 1.</p><p>In the GAT-GC variants, we use 4 GNN layers. For the Readout function in all models, we use sum for bioinformatics datasets and mean for social network datasets. We apply Batch normalization <ref type="bibr" target="#b5">(Ioffe and Szegedy 2015)</ref> after every hidden layers. The hidden dimensionality is set as 32 for bioinformatics datasets and 64 for social network datasets. The negative input slope of LeakyReLU in the GAT attention mechanism is 0.2. We use a single head in the multihead attention in all models.</p><p>All models are trained using the Adam optimizer (Kingma and Ba 2018) and the learning rate is dropped by a factor of 0.5 every 400 epochs in the node classification task and every 50 epochs in the graph classification task. We use an initial learning rate of 0.01 for the TRIANGLE-NODE and bioinformatics datasets and 0.0025 for the social network datasets. For the GAT variants, we use a dropout ratio of 0 and a weight decay value of 0. For the GAT-GC variants on each dataset, the following hyper-parameters are tuned: (1) Batch size in {32, 128}; (2) Dropout ratio in {0, 0.5} after dense layer; (3) L 2 regularization from 0 to 0.001. On each dataset, we use the same hyper-parameter configurations in all model variants for a fair comparison.</p><p>For the results of the baselines for comparison, we use the results reported in their original works by default. If results are not available, we use the best testing results reported in <ref type="bibr" target="#b17">(Xinyi and Chen 2019;</ref><ref type="bibr" target="#b6">Ivanov and Burnaev 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Local-level: Function f and the weighted summation in Equation 5 are injective. • Global-level: Besides the conditions for local-level, A's readout function (Equation 2) is injective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>is a mapping of input feature vector and α cx is the attention weight between f (c) and f (x) calculated by the attention function Att in Equation 3 and the softmax function in Equation 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Corollary 3. Let T be the original attention-based aggregator in Equation 5. With our proposed Cardinality Preserved Attention (CPA) models in Equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Q 4 .</head><label>4</label><figDesc>For global-level tasks like graph classification, how well can the original attention-based GNNs perform? Can our proposed CPA models improve the original model? Q 5. How the attention-based GNNs with our CPA models perform compared to baselines?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Training curves of GAT-GC variants on bioinformatics datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>s∈S µ(s) exp(e cs2 ) x∈X2 exp(e cx2 ) f (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An illustration of different attention-based aggregators on multiset of node features. Given two distinct multisets H 1 and H 2 that have the same central node feature h i and the same distribution of node features, aggregators will map h i to h i1 and h i2 for H 1 and H 2 . The Original model will get h i1 = h i2 and fail to distinguish H 1 and H 2 , while our Additive and Scaled models can always distinguish H 1 and H 2 with h i1 = h i2 and h i1 = h i2 .</figDesc><table><row><cell>ð : :</cell><cell>é é 0 0 6 6 E E ; ;</cell></row><row><cell cols="2">5 5 ? ? = = H H A A @ @</cell></row><row><cell>Figure 1:</cell><cell></cell></row></table><note>models named as Additive and Scaled are proposed to mod- ify the Original model in Equation 5: Model 1. (Additive)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Testing accuracies(%) of GAT variants (the original GAT and the GAT applied with each of our 4 CPA models) on TRIANGLE-NODE dataset for node classification. We highlight the result of the best performed model. The proportion P of multisets that hold the properties in Theorem 1 among all multisets is also reported.</figDesc><table><row><cell>Dataset</cell><cell>TRIANGLE-NODE</cell></row><row><cell>P (%)</cell><cell>29.2</cell></row><row><cell>Original</cell><cell>78.40 ± 7.65</cell></row><row><cell>Additive</cell><cell>91.31 ± 1.19</cell></row><row><cell>Scaled</cell><cell>91.38 ± 1.23</cell></row><row><cell>f-Additive</cell><cell>91.18 ± 1.24</cell></row><row><cell>f-Scaled</cell><cell>91.36 ± 1.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Testing accuracies(%) of GAT-GC variants (the</cell></row><row><cell cols="3">original one and the ones applied with each of our 4 CPA</cell></row><row><cell cols="3">models) on social network datasets. We highlight the result</cell></row><row><cell cols="3">of the best performed model per dataset. The proportion P</cell></row><row><cell cols="3">of multisets that hold the properties in Theorem 1 among all</cell></row><row><cell cols="3">multisets is also reported for each dataset.</cell></row><row><cell>Datasets</cell><cell>RE-B</cell><cell>RE-M5K</cell></row><row><cell>P (%)</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>Original</cell><cell cols="2">50.00 ± 0.00 20.00 ± 0.00</cell></row><row><cell>Additive</cell><cell cols="2">93.07 ± 1.82 57.39 ± 2.09</cell></row><row><cell>Scaled</cell><cell cols="2">92.36 ± 2.27 56.76 ± 2.26</cell></row><row><cell cols="3">f-Additive 93.05 ± 1.87 56.43 ± 2.38</cell></row><row><cell>f-Scaled</cell><cell cols="2">92.57 ± 2.06 57.22 ± 2.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Testing accuracies(%) of GAT-GC variants (the original one and the ones applied with each of our 4 CPA models) on bioinformatics datasets. We highlight the result of the best performed model per dataset. The highlighted results are significantly higher than those from the corresponding Original model under paired t-test at significance level 5%. The proportion P of multisets that hold the properties in Theorem 1 among all multisets is also reported for each dataset.</figDesc><table><row><cell>Datasets</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>ENZYMES</cell><cell>NCI1</cell></row><row><cell>P (%)</cell><cell>56.9</cell><cell>29.3</cell><cell>29.4</cell><cell>43.3</cell></row><row><cell>Original</cell><cell>84.96 ± 7.65</cell><cell>75.64 ± 3.96</cell><cell>58.08 ± 6.82</cell><cell>80.29 ± 1.89</cell></row><row><cell>Additive</cell><cell>89.75 ± 6.39</cell><cell>76.61 ± 3.80</cell><cell>58.90 ± 6.96</cell><cell>81.92 ± 1.89</cell></row><row><cell>Scaled</cell><cell>89.65 ± 7.47</cell><cell>76.44 ± 3.77</cell><cell>58.35 ± 6.97</cell><cell>82.18 ± 1.67</cell></row><row><cell>f-Additive</cell><cell>90.34 ± 6.05</cell><cell>76.60 ± 3.91</cell><cell>59.80 ± 6.18</cell><cell>81.96 ± 2.01</cell></row><row><cell>f-Scaled</cell><cell>90.44 ± 6.44</cell><cell>76.81 ± 3.77</cell><cell>58.45 ± 6.35</cell><cell>82.28 ± 1.81</cell></row><row><cell cols="5">orem 1 can be avoided in each iteration. Full experimental</cell></row><row><cell cols="4">settings are included in Supplemental Material.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>The classification accuracy of the Original model (GAT) is significantly lower than the CPA models. It supports the claim in Corollary 1: the Original model fails to distinguish all distinct multisets in the dataset and exhibits constrained discriminate power. On the contrary, CPA models can distinguish all different multisets in the graph as suggested in Corollary 3 and indeed significantly improve the accuracy of the Original model as shown inTable 1. This experiment thus well answers Question 3 that we raised.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Testing accuracies(%) for graph classification. We highlight the result of the best performed model for each dataset. Our GAT-GC (f-Scaled) model achieves the top 2 on all 6 datasets. ± 0.36 74.68 ± 0.49 52.22 ± 1.26 82.19 ± 0.18 81.10 ± 1.90 49.44 ± 2.36 PSCN 88.95 ± 4.37 75.00 ± 2.51 -76.34 ± 1.68 86.30 ± 1.58 49.10 ± 0.70 DGCNN 85.83 ± 1.66 75.54 ± 0.94 51.00 ± 7.29 74.44 ± 0.47 76.02 ± 1.73 48.70 ± 4.54 GIN 89.40 ± 5.60 76.20 ± 2.80 -82.70 ± 1.70 92.40 ± 2.50 57.50 ± 1.50 CapsGNN 86.67 ± 6.88 76.28 ± 3.63 54.67 ± 5.67 78.35 ± 1.55 -52.88 ± 1.48 GAT-GC (f-Scaled) 90.44 ± 6.44 76.81 ± 3.77 58.45 ± 6.35 82.28 ± 1.81 92.57 ± 2.06 57.22 ± 2.20</figDesc><table><row><cell></cell><cell>Datasets</cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>ENZYMES</cell><cell>NCI1</cell><cell>RE-B</cell><cell>RE-M5K</cell></row><row><cell>Baselines</cell><cell>WL</cell><cell>82.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Dataset Description</figDesc><table><row><cell>Datasets</cell><cell>Graphs</cell><cell>Classes</cell><cell>Features</cell><cell>Node Avg.</cell><cell>Edge Avg.</cell></row><row><cell>MUTAG</cell><cell>188</cell><cell>2</cell><cell>7</cell><cell>17.93</cell><cell>19.79</cell></row><row><cell>PROTEINS</cell><cell>1113</cell><cell>2</cell><cell>4</cell><cell>39.06</cell><cell>72.81</cell></row><row><cell>ENZYMES</cell><cell>600</cell><cell>6</cell><cell>6</cell><cell>32.63</cell><cell>62.14</cell></row><row><cell>NCI1</cell><cell>4110</cell><cell>2</cell><cell>23</cell><cell>29.87</cell><cell>32.30</cell></row><row><cell>RE-B</cell><cell>2000</cell><cell>2</cell><cell>-</cell><cell>429.63</cell><cell>995.51</cell></row><row><cell>RE-M5K</cell><cell>4999</cell><cell>5</cell><cell>-</cell><cell>508.52</cell><cell>1189.75</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Lemma 1</head><p>Proof. Local-level: For the aggregator in the first layer, it will map different 1-height subtree structures to different embeddings from the distinct input multisets of neighborhood node features, since it's injective. Iteratively, the aggregator in the l-th layer can distinguish different l-height subtree structures by mapping them to different embeddings from the distinct input multisets of l-1-height subtree features, since it's injective.</p><p>Global level: From Lemma 2 and Theorem 3 in <ref type="bibr" target="#b18">(Xu et al. 2019</ref>), we know: When all functions in A are injective, A can reach its upper bound of discriminative power, which is the same as the Weisfeiler-Lehman (WL) test (Weisfeiler and Leman 1968) when deciding the graph isomorphism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof for Theorem 1</head><p>Proof. To prove Theorem 1, we have consider both two directions in the iff statement:</p><p>where α cxi is the attention weight belongs to X i , and be-</p><p>We can rewrite the equations using S and µ:</p><p>where µ(s) is the multiplicity function, and α csi is the attention weight belongs to X i , and between f (c) and f (s), s ∈ S, i ∈ {1, 2}.</p><p>Considering the softmax function in Equation 2 of our paper, we can use attention coefficient e to rewrite the equations:</p><p>where e csi is the attention coefficient belongs to X i , and between f (c) and f (s), s ∈ S, i ∈ {1, 2}. Moreover, e cxi is the attention coefficient belongs to X i , and between f (c) and f (x), x ∈ X i , i ∈ {1, 2}.</p><p>As attention coefficient e is computed by function Att, which is regardless of X, thus e cs1 = e cs2 , ∀s ∈ S and e cx1 = e cx2 , ∀x ∈ X 1 , X 2 . We denote e cx = e cx1 = e cx2 , e cs = e cs1 = e cs2 . Remind that X 2 has k copies of the elements in X 1 , so that x∈X1 exp(e cx ) = 1 k x∈X2 exp(e cx ).</p><p>Using this equation, we can get s∈S µ(s) exp(e cs1 ) x∈X1 exp(e cx1 ) f (s) = s∈S µ(s) exp(e cs )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fürer</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
	<note>Convolutional networks on graphs for learning molecular fingerprints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stinchcombe</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>White ; Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:1807.07984</idno>
		<title level="m">Attention models in graphs: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Kang ; Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
	<note>The graph neural network model</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending highdimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shervashidze</surname></persName>
		</author>
		<idno>Shuman et al. 2013</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
	<note>IEEE Signal Processing Magazine</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning graph representations with recurrent neural network autoencoders. KDD Deep Learning Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gimpel</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berger-Wolf ;</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berger-Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J ;</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein. NTI, Series 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A comprehensive survey on graph neural networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
		<idno>Xu et al. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Inteligence</title>
		<meeting>AAAI Conference on Artificial Inteligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
