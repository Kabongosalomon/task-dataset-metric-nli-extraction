<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vehicle Re-identification Using Quadruple Directional Deep Learning Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqing</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanqiang</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canhui</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">Vehicle Re-identification Using Quadruple Directional Deep Learning Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to resist the adverse effect of viewpoint variations for improving vehicle re-identification performance, we design quadruple directional deep learning networks to extract quadruple directional deep learning features (QD-DLF) of vehicle images. The quadruple directional deep learning networks are with similar overall architecture, including the same basic deep learning architecture but different directional feature pooling layers. Specifically, the same basic deep learning architecture is a shortly and densely connected convolutional neural network to extract basic feature maps of an input square vehicle image in the first stage. Then, the quadruple directional deep learning networks utilize different directional pooling layers, i.e., horizontal average pooling (HAP) layer, vertical average pooling (VAP) layer, diagonal average pooling (DAP) layer and anti-diagonal average pooling (AAP) layer, to compress the basic feature maps into horizontal, vertical, diagonal and anti-diagonal directional feature maps, respectively. Finally, these directional feature maps are spatially normalized and concatenated together as a quadruple directional deep learning feature for vehicle re-identification. Extensive experiments on both VeRi and VehicleID databases show that the proposed QD-DLF approach outperforms multiple state-of-the-art vehicle re-identification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1811.05163v1 [cs.CV] 13 Nov 2018</head><p>Jianqing Zhu received the B.S. degree in communication engineering and the M.S. degree in communication and information system from the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Vehicle re-identification with the goal of matching the same vehicle image captured by different cameras plays an important role in video surveillance for public security, since vehicle has been an indispensable part of human daily life <ref type="bibr" target="#b0">[1]</ref>. In practical scenarios, vehicle re-identification is a very challenging computer vision problem, due to the fact that vehicle images usually contain a lot of adverse factors, such as   viewpoint variation, illumination change, blur, occlusion and low resolution, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Therefore, how to design an effective vehicle re-identification method has attracted more and more attentions.</p><p>To address the problem of vehicle re-identification, two large benchmark databases, VeRi <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and VehicleID <ref type="bibr" target="#b2">[3]</ref>, are released by the Institute of Digital Media, Peking University. Based on these two databases, multiple vehicle reidentification methods were developed and will be highlighted in the following Section II. Note that for vehicle reidentification, viewpoint variation is most crucial challenge and often encountered factor among those above-mentioned adverse ones, because vehicle images are usually captured under different camera viewpoints. Hence, this paper focuses on designing a method to resist adverse viewpoint variations so as to improve the vehicle re-identification performance. For that, we propose quadruple directional deep learning features (QD-DLF) to comprehensively describe vehicle images for improving the vehicle re-identification performance. The novelty and contribution of the proposed QD-DLF is that we make the first attempt to fuse quadruple directional deep features learned by using quadruple directional average pooling layers to improve the robustness of viewpoint variations. Experimental results have shown that the proposed approach is able to significantly improve the vehicle re-identification accuracy.</p><p>The rest of this paper is organized as follows. Section II introduces the related work. Section III describes the proposed quadruple directional deep learning features for vehicle reidentification. Section IV presents the experimental results to validate the superiority of the proposed method. Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we briefly review the existing vehicle reidentification works. Since feature representation and similarity metric are two key roles in vehicle re-identification, the existing vehicle re-identification works mainly focus on two aspects: (1) feature representation for vehicle re-identification, and (2) similarity metric for vehicle re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Representation for Vehicle Re-identification</head><p>Feature representation methods for vehicle re-identification can be mainly divided into two classes: hand crafted feature representations and deep learning features representations. For hand crafted feature representations, LOMO <ref type="bibr" target="#b3">[4]</ref> and BOW-CN <ref type="bibr" target="#b4">[5]</ref> features originally used in person re-identification are directly applied to vehicle re-identification. For deep learning feature representations, some well-known deep feature learning networks, such as AlexNet <ref type="bibr" target="#b5">[6]</ref>, VGGNet <ref type="bibr" target="#b6">[7]</ref> and GoogLeNet <ref type="bibr" target="#b7">[8]</ref>, are used as feature extractors for vehicle reidentification. For example, FACT <ref type="bibr" target="#b1">[2]</ref> uses AlexNet <ref type="bibr" target="#b5">[6]</ref> to extract features of vehicles. NuFACT <ref type="bibr" target="#b0">[1]</ref> takes GoogLeNet <ref type="bibr" target="#b7">[8]</ref> as a feature extractor. DRDL <ref type="bibr" target="#b2">[3]</ref> utilizes VGGNet <ref type="bibr" target="#b6">[7]</ref> to extract features of vehicles. It can be further found that the deep learning features obviously outperforms hand crafted features on VeRi and VehicleID databases as reported in <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>.</p><p>To effectively learn discriminative feature representations of vehicle images, multiple types of loss functions are applied to train deep learning based vehicle re-identification models. For example, the deep joint discriminative learning (DJDL) <ref type="bibr" target="#b8">[9]</ref> method trains a convolutional neural network to extract discriminative feature representations of vehicle images using identification, verification and triplet loss functions simultaneously. The improved triplet convolutional neural network <ref type="bibr" target="#b9">[10]</ref> using the classification-oriented loss function and the original triplet loss function is proposed for learning deep feature representations of vehicle images. Most of the abovementioned deep learning features <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> are holistical, which are learned by a deep learning network ended up with several fully connection layers. Although these deep learning methods make great progresses of vehicle re-identification, they do not design a specific solution for processing viewpoint variations, which is a crucial challenge for vehicle re-identification.</p><p>For better handling viewpoint variations, the adversarial bidirectional long short-term memory (LSTM) network (ABLN) is proposed in <ref type="bibr" target="#b10">[11]</ref>. ABLN uses LSTM to model transformations across continuous view variations of a vehicle and adopts the adversarial architecture to enhance training. Thus, a global vehicle representation containing all views' information can be inferred from only one visible view, and then used for learning to measure the distance between two vehicles with arbitrary views. Similar to ABLN, the spatially concatenated convolutional network (SCCN) and CNN-LSTM bi-directional loop (CLBL) are further proposed in <ref type="bibr" target="#b11">[12]</ref> to address the challenging of viewpoint variations. However, all of ABLN <ref type="bibr" target="#b10">[11]</ref>, SCCN <ref type="bibr" target="#b11">[12]</ref> and CNN-LSTM <ref type="bibr" target="#b11">[12]</ref> require a vehicle dataset, where each vehicle has images in densely sampled camera viewpoints. However, this is hard to acquire in practical video surveillance systems. Therefore, there is still ample room for vehicle re-identification by thoroughly considering viewpoint variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Metric for Vehicle Re-identification</head><p>For similarity metrics for vehicle re-identification, FACT <ref type="bibr" target="#b1">[2]</ref> applies the Euclidean or Cosine distance between a vehicle pair described with deep learning features to measure the similarity, as done in many face recognition algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Moreover, NuFACT <ref type="bibr" target="#b0">[1]</ref> calculates the similarity of the query and gallery vehicle images measured by the Euclidean distance in the discriminative null space <ref type="bibr" target="#b14">[15]</ref>. In addition, DRDL <ref type="bibr" target="#b2">[3]</ref> proposes a deep relative distance learning method. This method exploits a two-branch convolutional neural network to transform raw vehicle images into an Euclidean space, where the distance can be directly used to measure the similarity of arbitrary two vehicles.</p><p>Besides, multi-modal vehicle re-identification methods are also proposed to improve vehicle similarity metrics. For example, the progressive and multi-modal vehicle re-identification (PROVID) <ref type="bibr" target="#b0">[1]</ref> is presented to obtain a more accurate vehicle searching. The PROVID method first applies the NuFACT method to make a coarse searching. Then, it makes a fine searching based on a vehicle license plate verification model so that the the re-identification accuracy is improved. In addition, the two-stage framework (i.e., siamese convolutional neural network (Siamese-CNN) and path long short-term memory (Path-LSTM) network) <ref type="bibr" target="#b15">[16]</ref> incorporates complex spatial-temporal information for effectively regularizing the vehicle re-identification results. However, it is obvious that those multi-modal vehicle re-identification methods require the extra vehicle information (e.g., license plate, spatial-temporal information) and thus the additional computational load.</p><p>Based on the above analysis, vehicle re-identification method could improve the performance by resisting the viewpoint variation. Furthermore, from the viewpoint of practical applications, the vehicle re-identification method could not require extra vehicle information and introduce extra computational load. Motivated by these, the main objective of our proposed approach lies in: how to develop a more effective deep learning feature representation method for vehicle reidentification, without using extra vehicle information. For that, we propose quadruple directional deep learning features (QD-DLF) for vehicle re-identification in this paper. The most significant contribution of this paper is to fuse quadruple directional deep features learned by using quadruple directional average pooling layers, which constructs viewpoint robust features to significantly boost the vehicle re-identification performance. The details of the proposed QD-DLF will be described in the following Section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VEHICLE RE-IDENTIFICATION USING QUADRUPLE DIRECTIONAL DEEP LEARNING FEATURES A. Quadruple Deep Feature Learning Networks</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>  1) Basic Deep Feature Learning Architecture: It can be seen from <ref type="figure" target="#fig_2">Fig. 2</ref> that the basic deep feature learning architecture (BDFLA) is realized by a shortly and densely connected convolutional neural network, which is constructed by a list of short and dense units (SDUs) and max pooling layers. For convenient description, convolutional, batch normalization <ref type="bibr" target="#b17">[18]</ref> and Leaky ReLU <ref type="bibr" target="#b18">[19]</ref> layers are sequently packaged to construct a CBLR block, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (a). Then, three CBLR blocks are densely connected with two concatenation layers (i.e., CAT1 and CAT2) to build a short and dense unit (SDU), as shown in <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>. Each concatenation layer concatenates the input images according to the channel dimension. Finally, one convolutional layer (i.e., Conv0), a batch normalization, five SDUs (i.e., SDU1-SDU5), and five max pooling layers (i.e., MP1-MP5) are packaged in turn to obtain the basic deep feature learning architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Quadruple Directional Average Pooling Layers:</head><p>To comprehensively describe vehicle images from different directions, quadruple directional (i.e., horizontal, vertical, diagonal and anti-diagonal) average pooling layers are designed. Assume that the basic feature maps produced by the BDFLA is X ∈ d×d×c , where d and c represent the height/width, and channel sizes, respectively. The developed quadruple directional average pooling layers can be described as follows.</p><formula xml:id="formula_0">f 1 f 2 f 3 f 4 f 5 f 6 f 7 f 8 f 9 f 10 f 11 f 12 f 13 f 14 f 15 f 16 h 1 h 2 h 3 h 4 Vertical Average Pooling Deep Learning Feature Map v 1 v 2 v 3 v 4 Average of (f 1 , f 2 , f 3 , f 4 )</formula><p>Average of (f 9 , f 10 , f 11 , f 12 )</p><p>Horizontal Average Pooling i) Horizontal Average Pooling (HAP) Layer: The HAP layer averages each row of X into a single point to obtain the horizontal average pooling feature map P ∈ d×1×c , as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. For example, h 1 is equal to the aver-</p><formula xml:id="formula_1">age of f 1 , f 2 , f 3 and f 4 , that is, h 1 = 1 4 (f 1 +f 2 +f 3 +f 4 )</formula><p>. ii) Vertical Average Pooling (VAP) Layer: The VAP layer averages each column of X ∈ d×d×c into a single point to obtain the vertical average pooling feature Q ∈ 1×d×c , as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Note that Q is transposed into Q t ∈ d×1×c in a practical testing process to make the dimension of Q t be compatible with that of P (i.e., the output of the HAP layer). For instance, v 4 is equal to the average of <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><formula xml:id="formula_2">f 4 , f 8 , f 12 and f 16 , that is, v 1 = 1 4 (f 4 + f 8 + f 12 + f 16 ), as show in</formula><formula xml:id="formula_3">f 1 f 2 f 3 f 4 f 5 f 6 f 7 f 8 f 9 f 10 f 11 f 12 f 13 f 14 f 15 f 16 a 1 a 2 a 3 a 4 a 5 a 6 a 7 d 1 d 2 d 3 d 4 d 5 d 6 d 7 Diagonal Average Pooling Anti-diagonal Average Pooling Average of (f 4 , f 7 , f 10 , f 13 ) Average of (f 1 , f 6 , f 11 , f 16 )</formula><p>Average of (f 4 )</p><p>Average of (f 9 , f 14 )</p><p>Average of (f 2 , f 5 )</p><p>Average of (f 16 )</p><p>Deep Learning Feature Map iii) Diagonal Average Pooling (DAP) Layer: The DAP layer averages multiple elements of the feature map X ∈ d×d×c according to the diagonal direction, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. For example, d 6 is equal to the average of f 9 and f 14 , that is,</p><formula xml:id="formula_4">d 6 = 1 2 (f 9 + f 14 )</formula><p>. iv) Anti-diagonal Average Pooling (AAP) Layer: The AAP layer averages multiple elements of the feature map X ∈ d×d×c according to the anti-diagonal direction, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. For instance, a 4 is equal to the average of f 4 , f 7 , f 10 and f 13 , that is, a 4 = 1 4 (f 4 +f 7 +f 10 +f 13 ). Since all the above-mentioned HAP, VAP, DAP and AAP layers use a average pooling operation, the forward and backward propagations are briefly introduced as follows. Assume that the input feature map of an average pooling layer is X = [X 1 , X 2 , ..., X d ] ∈ d×d , the average pooling window size is d × 1, and the output feature map is Y ∈ 1×d = [y 1 , y 2 , ..., y d ]. Then, the forward propagation of this average pooling layer is calculated as follows:</p><formula xml:id="formula_5">y i = 1 d d j=1 X ij ,<label>(1)</label></formula><p>where y i is the i-th element of Y ; X ij is j-th element of i-th column vector of X. According to the chain rule, the backward propagation of this average pooling layer can be calculated as follows:</p><formula xml:id="formula_6">∂J ∂X i1 = ∂J ∂y i ∂y i ∂X i1 = 1 d ∂J ∂y i , ∂J ∂X i2 = ∂J ∂y i ∂y i ∂X i2 = 1 d ∂J ∂y i , ... ∂J ∂X id = ∂J ∂y i ∂y i ∂X id = 1 d ∂J ∂y i ,<label>(2)</label></formula><p>and the matrix form can be formulated as follows:</p><formula xml:id="formula_7">∂J ∂X i = 1 d ∂J ∂y i , ∂J ∂y i , ..., ∂J ∂y i T ,<label>(3)</label></formula><p>where X i = [X i1 , X i2 , ..., X id ] T is i-th column vector of X; J is the objective function (i.e., Eq. (6)) of the overall learning framework and will be discussed in the following subsection.</p><p>3) Spatial Normalization Layer: As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, a spatial normalization (SN) layer <ref type="bibr" target="#b16">[17]</ref> is exploited to follow each directional average pooling layer. It is to make each dimension of the directional average pooling feature maps unified distributing in [0, 1), which is beneficial to prevent a specific dimension whose value is too predominate. Assume that the input of a SN layer is P ∈ d×c . Then, the corresponding output Z of the SN layer can be calculated as follows:</p><formula xml:id="formula_8">Z k j = P k j 1 + l∈Nj (P k l ) 2 ,<label>(4)</label></formula><p>where Z k j is j-th element of the k-th feature map of Z, P k j represents the j-th element of the k-th feature map of P , and N j is the neighborhood size.</p><p>The backward propagation of the SN layer can be formulated as follows:</p><formula xml:id="formula_9">∂J ∂P k j = ∂J ∂Z k j − Z k j l∈N j ∂J ∂Z k l Z k l 1 + l∈N j (P k l ) 2 .<label>(5)</label></formula><p>Based on the above-mentioned basic deep feature learning network, quadruple directional average pooling layers, and SN layer, quadruple deep feature learning networks (QDFLNs) are constructed, consisting of horizontal deep feature learning network (HDFLN), vertical deep feature learning network (VDFLN), diagonal deep feature learning network (DDFLN), and anti-diagonal deep feature learning network (ADFLN), as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>The recommended parameter configuration of the proposed QD-DLF method is listed in <ref type="table" target="#tab_2">Table I</ref>. The channel numbers of Conv0, SDU1, SDU2, SDU3, SDU4 and SDU5 are 64, 64, 128, 192, 256 and 320, respectively. The scope of Leaky ReLU layer of SDU5 is 0, and that of the others is 0.15. The sub-window for Conv0 and SDU represents a filter size. Specifically, for pooling layers (i.e. MP1-MP5), it means a pooling window size, while for the spatial normalization (i.e. SN) layers, it denotes a normalization window size. Conv0 and five SDUs (i.e., SDU1-SDU5) apply 3 × 3 sized filters. Five max pooling layers use 3 × 3 sized pooling windows. Among the quadruple directional average pooling layers (i.e., HAP, VAP, DAP and AAP), the pooling window sizes for HAP and VAP layers are 1 × 4 and 4 × 1, respectively, while for DAP and AAP the maximum number of pooled elements is 4, as show in <ref type="figure" target="#fig_4">Fig. 4</ref>. The normalization neighborhood size (i.e., N j in Eq. (4)) in all spatial normalization layers is 4. Moreover, only those strides working on five MP layers are set as 2 pixels, and those on the others are set as 1 pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Objective Function</head><p>Similar to <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, the softmax function is utilized to build the objective function of the proposed method, as follows:</p><formula xml:id="formula_10">J(W ) = 1 K [ K k=1 C c=1 (y (k) = c) log e Wc T X (k) C p=1 e Wp T X (k) ]+ 1 2 α W 2 2 ,<label>(6)</label></formula><p>where W = [W 1 , W 2 , ..., W C ] ∈ d×C is the projection matrix used to predicate a vehicle's class label, X (k) is the deep learning feature of k-th training sample, y (k) ∈ {1, 2, 3, ..., C} is the corresponding class label, α is a constant used to control the contribution of the L 2 regularization item, K and C represent the numbers of the training samples and classes, respectively, and (·) is an indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT AND ANALYSIS</head><p>To validate the superiority of the proposed quadruple directional deep learning feature (QD-DLF) approach, the performance comparison with multiple state-of-the-art methods is conducted on two challenging databases, namely, VeRi <ref type="bibr" target="#b0">[1]</ref> and VehicleID <ref type="bibr" target="#b2">[3]</ref>. In our experiments, the Euclidean distance is employed to measure the similarity of a vehicle pair described with quadruple deep learning features. And two commonly used criteria in the re-identification field, i.e., cumulative match curve (CMC) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and mean average precision (MAP) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>, are used to evaluate the performance. The CMC shows the identification accuracy rates that a query identity appears in different sized candidate lists. The MAP is used to evaluate the overall performance. For each query, the area under the precision-recall curve is calculated, which is known as average precision (AP). Then, the mean value of APs of all queries is calculated as MAP, which considers both precision and recall of a re-identification method, and thus provides a more comprehensive performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Configuration</head><p>In our experiments, the software tools are Matconvnet <ref type="bibr" target="#b16">[17]</ref>, CUDA 8.0, CUDNN V5.1, MATLAB 2014 and Visual Studio 2012. The hardware device is workstation configured with a Intel Xeon E3-1505 M v5 CPU @2.80 GHz, a NVIDIA Titan X GPU and 128 GB DDR3 Memory. Moreover, the training settings similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> are adopted and summarized as follows. All images in these two databases are scaled to 128 × 128 pixels, and each image is further augmented by the horizontal mirror and randomly rotating operations. The randomly rotating operation is applied to randomly rotate an image in ranges [−3 • , 0 • ] and [0 • , 3 • ]. The weights in each layer are initialized based on a normal distribution N (0, 0.01), and the biases are initialized to 0. The L 2 regularization weights α in Eq. (6) is set as 0.005 on the VeRi <ref type="bibr" target="#b0">[1]</ref> database, while for the larger VehicleID <ref type="bibr" target="#b2">[3]</ref> database, it is set as 0.001. The size of mini-batch is 128 including 64 positive and 64 negative image pairs, and both positive and negative pairs are randomly selected from the whole database. The momentums are set to 0.9. The learning rates start with 0.01 and are gradually decreased along the training progress. That is, if the objective function is convergent at a stage, the learning rates are reduced to 1/10 of the current values, and the minimum learning rates are 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Databases</head><p>VeRi <ref type="bibr" target="#b0">[1]</ref> is captured by 20 cameras in unconstrained traffic scenarios and each vehicle is captured by 2-18 cameras under different viewpoints, illuminations, occlusions and resolutions. The VeRi dataset is divided into a training subset containing 37,781 images of 576 subjects and a testing subset with 13,257 images of 200 subjects. For the evaluation, one image of each vehicle captured from each camera is applied as a query, then a query set containing 1,678 images of 200 subjects and a gallery including 11,579 image of 200 subjects are finally obtained. Furthermore, only the cross-camera vehicle re-identification is evaluated, which means that if a probe image and a gallery image are captured under the same camera viewpoint, the corresponding matching result will be excluded in the final performance evaluation.</p><p>VehicleID <ref type="bibr" target="#b2">[3]</ref> is captured in daytime by multiple real-world surveillance cameras distributed in a small city of China. There are 221,763 images of 26,267 subjects in the entire database. Each vehicle is captured from either a front viewpoint or a back viewpoint. The training subset consists of 110,178 images of 13,134 subjects. In addition, VehicleID provides three testing subsets, Test800, Test1600 and Test2400, for evaluating the performance in different data scales. Specifically, Test800 includes 800 gallery images and 6,532 probe images of 800 subjects. Test1600 contains 1600 gallery images and 11,395 probe images of 1,600 subjects. Test2400 consists 2400 gallery images and 17,638 probe images of 2,400 subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation 1) Comparison on VeRi:</head><p>The performance comparison of the proposed QD-DLF and multiple state-of-art methods on the VeRi database is shown in <ref type="table" target="#tab_2">Table II</ref>. It can be found that the proposed QD-DLF acquires the highest MAP (i.e., 61.83%) and rank-1 identification rate (i.e., 88.50%) among all methods under comparisons. More details are analyzed as follows.</p><p>Firstly, compared with four multi-modal deep learning based vehicle re-identification methods (i.e., NuFACT + Plate-SNN <ref type="bibr" target="#b0">[1]</ref>, NuFACT + Plate-REC <ref type="bibr" target="#b0">[1]</ref>, PROVID <ref type="bibr" target="#b0">[1]</ref> and Siamese-CNN+Path-LSTM <ref type="bibr" target="#b15">[16]</ref>), the proposed QD-DLF method consistently defeats NuFACT + Plate-SNN <ref type="bibr" target="#b0">[1]</ref>, NuFACT +  Plate-REC <ref type="bibr" target="#b0">[1]</ref> and Siamese-CNN+Path-LSTM <ref type="bibr" target="#b15">[16]</ref> by higher MAPs, rank-1 identification rates and rank-5 identification rates. Although the rank-5 identification rate of the proposed QD-DLF method is a bit lower than that of PROVID <ref type="bibr" target="#b0">[1]</ref>, the proposed QD-DLF method achieves much higher MAP and rank-1 identification rate, and thus is still superior to PROVID <ref type="bibr" target="#b0">[1]</ref>. Secondly, compared with those single modal deep learning based vehicle re-identification methods (i.e., NuFACT <ref type="bibr" target="#b0">[1]</ref>, DenseNet121 <ref type="bibr" target="#b24">[25]</ref>, SCCN-Ft+CLBL-8-Ft <ref type="bibr" target="#b11">[12]</ref>, ABLN-Ft-16 <ref type="bibr" target="#b10">[11]</ref>, FACT <ref type="bibr" target="#b1">[2]</ref>, GoogLeNet <ref type="bibr" target="#b25">[26]</ref> and VGG-CNN-M-1024 <ref type="bibr" target="#b2">[3]</ref>), the proposed QD-DLF methods shows a larger accuracy improvement. Specifically, the best single modal deep learning based vehicle re-identification method, i.e., NuFACT <ref type="bibr" target="#b0">[1]</ref>, only obtains a 48.47% MAP, a 76.76% rank-1 identification rate and a 91.42% rank-5 identification rate, which are much lower than those of the proposed QD-DLF method. Moreover, it can be seen that SCCN-Ft+CLBL-8-Ft <ref type="bibr" target="#b11">[12]</ref> and ABLN-Ft-16 <ref type="bibr" target="#b10">[11]</ref> do not obviously show the superiority on the VeRi database, although they specially consider the viewpoint variation. This is because each vehicle in the VeRi database is not densely captured by different camera viewpoints, which does not fully meet the requirement of the training data for CCN-Ft+CLBL-8-Ft <ref type="bibr" target="#b11">[12]</ref> and ABLN-Ft-16 <ref type="bibr" target="#b10">[11]</ref>, limiting the performance improvement.</p><p>Thirdly, the proposed QD-DLF also obtains much better performance than those hand-crafted feature representation methods, i.e., BOW-CN <ref type="bibr" target="#b4">[5]</ref>, LOMO <ref type="bibr" target="#b3">[4]</ref> and BOW-SFIT <ref type="bibr" target="#b26">[27]</ref>.</p><p>2) Comparison on VehicleID: The performance comparisons of the proposed QD-DLF and multiple state-of-art methods on VehicleID database are shown in <ref type="figure" target="#fig_5">Fig. 5</ref> and <ref type="table" target="#tab_2">Table III</ref>. Firstly, it can be observed that deep learning based methods (i.e., DJDL <ref type="bibr" target="#b8">[9]</ref>, DenseNet121 <ref type="bibr" target="#b24">[25]</ref>, Improved Triplet CNN <ref type="bibr" target="#b9">[10]</ref>, DRDL <ref type="bibr" target="#b2">[3]</ref>, FACT <ref type="bibr" target="#b1">[2]</ref>, NuFACT <ref type="bibr" target="#b0">[1]</ref> and GoogLeNet <ref type="bibr" target="#b25">[26]</ref>) obviously defeat traditional methods (i.e., LOMO <ref type="bibr" target="#b3">[4]</ref>, BOW-CN <ref type="bibr" target="#b4">[5]</ref> and BOW-SIFT <ref type="bibr" target="#b26">[27]</ref>) on this large scale database. Secondly, the proposed QD-DLF method outperforms all deep learning based methods under comparison, including DJDL <ref type="bibr" target="#b8">[9]</ref>, DenseNet121 <ref type="bibr" target="#b24">[25]</ref>, Improved Triplet CNN <ref type="bibr" target="#b9">[10]</ref>, DRDL <ref type="bibr" target="#b2">[3]</ref>, FACT <ref type="bibr" target="#b1">[2]</ref>, NuFACT <ref type="bibr" target="#b0">[1]</ref> and GoogLeNet <ref type="bibr" target="#b25">[26]</ref>, on the Test800, Test1600 and Test2400 subsets of the VehicleID database.</p><p>3) Comparison on Each Directional Deep Learning Feature: In addition, we further comprehensively analyze the performance resulted from each directional deep learning feature. The corresponding results on VeRi and VehicleID databases are shown in <ref type="figure" target="#fig_6">Fig. 6, and</ref>       <ref type="table" target="#tab_7">Table V</ref>, one can find that among F-DLF-128, F-DLF-256, F-DLF-512 and F-DLF-1024, F-DLF-256 obtains the highest MAP, but is inferior to our weakest directional deep learning feature, V-DLF. Specifically, the MAP of V-DLF is 9.10% higher than that of F-DLF-256 and the rank-1 identification rate of V-DLF is 1.02% higher than that of F-DLF-256, respectively. Moreover, the proposed QD-DLF is significant better than F-DLF-256 (i.e., 20.84% higher MAP and 8.35% higher rank-1 identification rate). This study indicates that the proposed directional deep learning features are more suitable and effective than deep holistic learning features to represent vehicle images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Running Time Analysis:</head><p>In addition to the accuracy, the efficiency is also very important for vehicle re-identification methods. For that, the running time comparison is conducted in terms of the feature extraction time (FET) per image as suggested in the similar person re-identification task <ref type="bibr" target="#b21">[22]</ref>. The running time comparison of the proposed QD-DLF method and several state-of-the-art vehicle re-identification methods is shown in <ref type="table" target="#tab_2">Table VI</ref>, where all the methods are implemented in the GPU mode.</p><p>Firstly, one can see that the running time of each proposed directional deep learning feature (i.e., D-DLF, A-DLF, H-DLF, V-DLF) is similar to that of each other. And the FETs of each directional deep learning features are a bit slower than that of VGG-CNN-M-1024 <ref type="bibr" target="#b2">[3]</ref> and are comparable to that of GoogLeNet <ref type="bibr" target="#b25">[26]</ref>. Moreover, compared to the ultra-deep model DenseNet121 <ref type="bibr" target="#b24">[25]</ref>, the FET of each directional deep learning feature is about 17% of that of DenseNet121 <ref type="bibr" target="#b24">[25]</ref>, which demonstrates all of D-DLF, A-DLF, H-DLF, V-DLF are much faster than DenseNet121 <ref type="bibr" target="#b24">[25]</ref>. Secondly, the running time of the proposed QD-DLF is analyzed. Intuitively, the FET of QD-DLF would be 4 times of that of each proposed single directional deep learning feature. However, from Table VI, one can find that the FET of QD-DLF is about 5 times of that of each proposed directional deep learning feature. This is due to that the efficiency of simultaneously making the communalization between CPU and GPU for quadruple directional deep learning model is lower than that for each proposed directional deep learning model, since quadruple directional deep learning model is naturally larger than each single directional deep learning model. In addition, it should be pointed out that the FET of proposed QD-DLF is 2.448ms faster than that of DenseNet121 <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, quadruple directional deep learning networks are designed for vehicle re-identification. The quadruple directional deep learning networks are with the same basic deep learning architecture but different directional feature pooling layers. The same basic deep learning architecture is a shortly and densely connected convolutional neural network to extract basic feature maps of an input square vehicle image. After that, a horizontal average pooling (HAP) layer, a vertical average pooling (VAP) layer, a diagonal average pooling (DAP) layer and an anti-diagonal average pooling (AAP) layer are repetitively applied in the proposed quadruple directional deep learning network to compress the basic feature maps into horizontal, vertical, diagonal and anti-diagonal directional feature maps. Finally, these obtained directional feature maps are spatially normalized and concatenated together as a quadruple directional deep learning feature for vehicle re-identification. Through quadruple directional deep learning features learned by the proposed quadruple directional deep learning network, the adverse effect of viewpoint variations is effectively resisted and the performance of vehicle re-identification is thus significantly improved. Extensive experiments on both VeRi and VehicleID databases show that the proposed method is obviously superior to multiple state-of-the-art vehicle reidentification methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported in part by the National Natural Science Foundation of China under the Grants 61602191, 61401167, 61605048, 61473291 and 61372107, in part by the Natural Science Foundation of Fujian Province under the Grants 2018J01090 and 2016J01308, in part by High-level Talent Innovation Program of Quanzhou City under the Grant 2017G027, in part by the Promotion Program for Young and Middle aged Teacher in Science and Technology Research of Huaqiao University under the Grants ZQN-PY418 and ZQN-YX403, in part by the Scientific and Technology Founds of Xiamen under the Grant 3502Z20173045, and in part by the Scientific Research Funds of Huaqiao University under the Grant 16BS108, 14BS201 and 14BS204. (Corresponding author: Huanqiang Zeng</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Classical vehicle samples from the VeRi<ref type="bibr" target="#b0">[1]</ref> database. Each row denotes the same vehicle captured by cameras from different viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The diagrams of the proposed quadruple directional deep feature learning networks. Here, MP, HAP, VAP, DAP, AAP and SN represents max pooling, horizontal average pooling, vertical average pooling, diagonal average pooling, anti-diagonal average pooling and spatial normalization layers, respectively. diagonal and anti-diagonal) deep feature learning networks (i.e.,HDFLN, VDFLN, DDFLN, ADFLN). More specifically, each directional deep feature learning network is composed of a common basic deep feature learning architecture (BDFLA), the corresponding directional average pooling layer and a spatial normalization (SN) layer<ref type="bibr" target="#b16">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The schematic diagram of horizontal and vertical average pooling operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The schematic diagram of diagonal and anti-diagonal average pooling operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The CMC curve comparisons of the proposed QD-DLF method and multiple state-of-the-art methods on (a) Test800, (b) Test1600, and (c) Test2400 of VehicleID, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The performance (%) comparison of diagonal deep learning feature (D-DLF), anti-diagonal deep learning feature (A-DLF), horizonal deep learning feature (H-DLF) and veridical deep learning feature (V-DLF) on (a) VeRi, (b) Test2400 of VehicleID, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Jianqing Zhu, Canhui Cai and LiXin Zheng are with the College of Engineering, Huaqiao University, Quanzhou, 362021, China and Fujian Provincial Academic Engineering Research Centre in Industrial Intellectual Techniques and Systems (e-mail: {jqzhu, chcai, zlx}@hqu.edu.cn). Huanqiang Zeng is with the College of Information Science and Engineering, Huaqiao University, Xiamen, 361021, China (e-mail: zeng0043@hqu.edu.cn). Jingchang Huang is with IBM-Research China, Building 10/6F, 399 Ke-Yuan Road, Zhangjiang Hi-Tech Park, Pudong New District, Shanghai 201203, China (e-mail: hjingc@cn.ibm.com). Shengcai Liao and Zhen Lei are with the Center for Biometrics and Security Research and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China (e-mail: {scliao, zlei}@cbsr.ia.ac.cn).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>PARAMETER CONFIGURATION OF THE PROPOSED QDFLNS.</figDesc><table><row><cell cols="2">Name Channels</cell><cell>Scope of</cell><cell>Sub-window</cell><cell cols="2">Stride Output Size</cell></row><row><cell></cell><cell></cell><cell>Leaky ReLU</cell><cell>(h × w)</cell><cell></cell><cell></cell></row><row><cell>Conv0</cell><cell>64</cell><cell>0.15</cell><cell>3 × 3</cell><cell cols="2">1 128 × 128 × 64</cell></row><row><cell>SDU1</cell><cell>64</cell><cell>0.15</cell><cell>3 × 3</cell><cell cols="2">1 128 × 128 × 64</cell></row><row><cell>MP1</cell><cell>64</cell><cell>-</cell><cell>3 × 3</cell><cell>2</cell><cell>64 × 64 × 64</cell></row><row><cell>SDU2</cell><cell>128</cell><cell>0.15</cell><cell>3 × 3</cell><cell>1</cell><cell>64 × 64 × 128</cell></row><row><cell>MP2</cell><cell>128</cell><cell>-</cell><cell>3 × 3</cell><cell>2</cell><cell>32 × 32 × 128</cell></row><row><cell>SDU3</cell><cell>192</cell><cell>0.15</cell><cell>3 × 3</cell><cell>1</cell><cell>32 × 32 × 192</cell></row><row><cell>MP3</cell><cell>192</cell><cell>-</cell><cell>3 × 3</cell><cell>2</cell><cell>16 × 16 × 192</cell></row><row><cell>SDU4</cell><cell>256</cell><cell>0.15</cell><cell>3 × 3</cell><cell>1</cell><cell>16 × 16 × 256</cell></row><row><cell>MP4</cell><cell>256</cell><cell>-</cell><cell>3 × 3</cell><cell>2</cell><cell>8 × 8 × 256</cell></row><row><cell>SDU5</cell><cell>320</cell><cell>0</cell><cell>3 × 3</cell><cell>1</cell><cell>8 × 8 × 320</cell></row><row><cell>MP5</cell><cell>320</cell><cell>-</cell><cell>3 × 3</cell><cell>2</cell><cell>4 × 4 × 320</cell></row><row><cell>HAP</cell><cell>320</cell><cell>-</cell><cell>1 × 4</cell><cell>1</cell><cell>4 × 1 × 320</cell></row><row><cell>VAP</cell><cell>320</cell><cell>-</cell><cell>4 × 1</cell><cell>1</cell><cell>1 × 4 × 320</cell></row><row><cell>DAP</cell><cell>320</cell><cell>-</cell><cell>4</cell><cell>1</cell><cell>7 × 1 × 320</cell></row><row><cell>AAP</cell><cell>320</cell><cell>-</cell><cell>4</cell><cell>1</cell><cell>7 × 1 × 320</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THE</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">PERFORMANCE (%) COMPARISON OF THE PROPOSED QD-DLF AND</cell></row><row><cell cols="3">MULTIPLE STATE-OF-THE-ART METHODS ON VERI.</cell></row><row><cell>Methods</cell><cell cols="2">MAP Rank=1 Rank=5</cell></row><row><cell>Proposed QD-DLF</cell><cell cols="2">61.83 88.50 94.46</cell></row><row><cell cols="3">Siamese-CNN+Path-LSTM [16] 58.27 83.49 90.04</cell></row><row><cell>PROVID [1]</cell><cell cols="2">53.42 81.56 95.11</cell></row><row><cell cols="3">NuFACT + Plate-SNN [1] 50.87 81.11 92.79</cell></row><row><cell cols="3">NuFACT + Plate-REC [1] 48.55 76.88 91.42</cell></row><row><cell>NuFACT [1]</cell><cell cols="2">48.47 76.76 91.42</cell></row><row><cell>DenseNet121 [25]</cell><cell cols="2">45.06 80.27 91.12</cell></row><row><cell cols="3">SCCN-Ft+CLBL-8-Ft [12] 25.12 60.83 78.55</cell></row><row><cell>ABLN-Ft-16 [11]</cell><cell cols="2">24.92 60.49 77.33</cell></row><row><cell>FACT [2]</cell><cell cols="2">18.75 52.21 72.88</cell></row><row><cell>GoogLeNet [26]</cell><cell cols="2">17.89 52.32 72.17</cell></row><row><cell>VGG-CNN-M-1024 [3]</cell><cell cols="2">12.76 44.10 62.63</cell></row><row><cell>BOW-CN [5]</cell><cell cols="2">12.20 33.91 53.69</cell></row><row><cell>LOMO [4]</cell><cell cols="2">9.64 25.33 46.48</cell></row><row><cell>BOW-SFIT [27]</cell><cell>1.51 1.91</cell><cell>4.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell>, where horizontal,</cell></row><row><cell>vertical, diagonal and anti-diagonal deep learning features are</cell></row><row><cell>denoted as H-DLF, V-DLF, D-DLF and A-DLF, respectively.</cell></row><row><cell>The analyses are described in detailed as follows.</cell></row><row><cell>i) Which directional deep learning feature is better for</cell></row><row><cell>vehicle re-identification?</cell></row><row><cell>From Fig. 6, it can be found that the D-DLF/A-DLF</cell></row><row><cell>defeats H-DLF/V-DLF. It means that the diagonal/anti-</cell></row><row><cell>diagonal directional deep learning feature is more ap-</cell></row></table><note>propriate than the horizontal/vertical directional deep learning feature for vehicle re-identification. This may be</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III THE</head><label>III</label><figDesc>PERFORMANCE (%) COMPARISON OF THE PROPOSED QD-DLF AND MULTIPLE STATE-OF-THE-ART METHODS ON VEHICLEID. DLF 76.54 72.32 92.48 74.63 70.66 88.90 68.41 64.14 83.37 73.19 69.04 88.25</figDesc><table><row><cell>Method</cell><cell>Test800</cell><cell></cell><cell>Test1600</cell><cell></cell><cell>Test2400</cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell></cell><cell cols="8">MAP Rank=1 Rank=5 MAP Rank=1 Rank=5 MAP Rank=1 Rank=5 MAP Rank=1 Rank=5</cell></row><row><cell>Proposed QD-DJDL [9]</cell><cell>N/A 72.3</cell><cell>85.7</cell><cell>N/A 70.8</cell><cell>81.8</cell><cell>N/A 68.0</cell><cell>78.9</cell><cell>N/A 70.4</cell><cell>82.1</cell></row><row><cell>DenseNet121 [25]</cell><cell cols="8">68.85 66.10 77.87 69.45 67.39 75.49 65.37 63.07 72.57 67.89 65.52 75.31</cell></row><row><cell cols="2">Improved Triplet CNN [10] N/A 69.9</cell><cell>87.3</cell><cell>N/A 66.2</cell><cell>82.3</cell><cell>N/A 63.2</cell><cell>79.4</cell><cell>N/A 66.4</cell><cell>83.0</cell></row><row><cell>DRDL [3]</cell><cell cols="8">N/A 48.91 66.71 N/A 46.36 64.38 N/A 40.97 60.02 N/A 45.41 63.70</cell></row><row><cell>FACT [2]</cell><cell cols="8">N/A 49.53 67.96 N/A 44.63 64.19 N/A 39.91 60.49 N/A 44.69 64.21</cell></row><row><cell>NuFACT [1]</cell><cell cols="8">N/A 48.90 69.51 N/A 43.64 65.34 N/A 38.63 60.72 N/A 43.72 65.19</cell></row><row><cell>GoogLeNet [26]</cell><cell cols="8">N/A 47.90 67.43 N/A 43.45 63.53 N/A 38.24 59.51 N/A 43.20 60.04</cell></row><row><cell>LOMO [4]</cell><cell cols="8">N/A 19.74 32.14 N/A 18.95 29.46 N/A 15.26 25.63 N/A 17.98 3.76</cell></row><row><cell>BOW-CN [5]</cell><cell cols="8">N/A 13.14 22.69 N/A 12.94 21.09 N/A 10.20 17.89 N/A 12.09 20.56</cell></row><row><cell>BOW-SIFT [27]</cell><cell>N/A 2.81</cell><cell>4.23</cell><cell>N/A 3.11</cell><cell>5.22</cell><cell>N/A 2.11</cell><cell>3.76</cell><cell>N/A 2.68</cell><cell>3.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV THE</head><label>IV</label><figDesc>PERFORMANCE (%) COMPARISON OF QD-DLF, DAH-DLF, DA-DLF AND D-DLF ON VERI AND TEST2400 OF VEHICLEID. Is there contribution from the fusion of various directional deep learning features? By starting from the D-DLF as an example, we gradually fuse more directional deep learning features to see their performance. Here, DA-DLF means the fusion of D-DLF and A-DLF, DAH-DLF means the fusion of D-DLF, A-DLF and H-DLF. Note that the proposed QD-DLF means the fusion of all four directional deep learning features. From Table IV, it can be observed that QD-DLF fusing quadruple directional deep learning features obtains the best performance, DAH-DLF defeats DA-DLF and D-DLF, and DA-DLF outperforms D-DLF. It can be found that the more directional deep learning features are, the better the performance is. This study shows that the fusion of various directional deep learning features can indeed contribute to the improvement of the performance. 4) Comparison on Directional and Deep Holistic Learning Features: In this work, we also compare the performance resulted from the proposed method with directional deep learn-</figDesc><table><row><cell>Methods</cell><cell>VeRi</cell><cell>Test2400 of VehicleID</cell></row><row><cell></cell><cell cols="2">MAP Rank=1 Rank=5 MAP Rank=1 Rank=5</cell></row><row><cell cols="3">QD-DLF 61.83 88.50 94.46 68.41 64.14 83.37</cell></row><row><cell cols="3">DAH-DLF 60.31 88.62 94.34 68.24 63.88 83.72</cell></row><row><cell cols="3">DA-DLF 58.16 87.19 94.46 66.90 62.53 82.07</cell></row><row><cell cols="3">D-DLF 53.26 84.92 93.03 64.25 59.64 80.11</cell></row><row><cell cols="3">attributed to two aspects. Firstly, although vehicle images</cell></row><row><cell cols="3">are captured from different camera viewpoints, most vehi-</cell></row><row><cell cols="3">cle images still have a prominent symmetry. Secondly, for</cell></row><row><cell cols="3">the vehicle images with symmetry, the average pooling</cell></row><row><cell cols="3">corresponding to the diagonal or anti-diagonal direction</cell></row><row><cell cols="3">is able to involve more diverse local feature regions to</cell></row><row><cell cols="3">construct more effective features so as to obtain a better</cell></row><row><cell cols="3">performance of vehicle re-identification.</cell></row><row><cell>ii)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V THE</head><label>V</label><figDesc>PERFORMANCE (%) COMPARISON OF QD-DLF, D-DLF, A-DLF, H-DLF, V-DLF, AND F-DLFS ON VERI.</figDesc><table><row><cell>Methods MAP Rank=1 Rank=5</cell></row><row><cell>QD-DLF 61.83 88.50 94.46</cell></row><row><cell>D-DLF 53.26 84.92 93.03</cell></row><row><cell>A-DLF 53.12 84.56 92.61</cell></row><row><cell>H-DLF 50.40 83.13 92.37</cell></row><row><cell>V-DLF 50.09 81.17 91.12</cell></row><row><cell>F-DLF-256 40.99 80.15 90.64</cell></row><row><cell>F-DLF-512 39.68 80.21 89.87</cell></row><row><cell>F-DLF-128 39.39 79.68 91.12</cell></row><row><cell>F-DLF-1024 39.10 79.86 89.33</cell></row><row><cell>ing feature and a deep holistic feature learning configuration.</cell></row><row><cell>This deep holistic feature learning configuration is built by</cell></row><row><cell>keeping the basic deep feature learning architecture (BDLFA)</cell></row><row><cell>unchanged and using a Full connection layer instead of one</cell></row><row><cell>directional pooling layer. The corresponding learned feature</cell></row><row><cell>is denoted as F-DLF. Since the F-DLF is able to produce</cell></row><row><cell>holistical features with different dimensions and we do not</cell></row><row><cell>know the suitable feature dimension in advance, the perfor-</cell></row><row><cell>mances resulted from the holistical features with different</cell></row><row><cell>dimensions produced by the full connection layer are all</cell></row><row><cell>obtained. The F-DLF-128, F-DLF-256, F-DLF-512 and F-</cell></row><row><cell>DLF-1024 represents the corresponding learning configuration</cell></row><row><cell>that produces 128, 256, 512 and 1024 dimensional holistical</cell></row><row><cell>features, respectively.</cell></row><row><cell>From the results shown in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI THE</head><label>VI</label><figDesc>RUNNING TIME COMPARISON OF THE PROPOSED QD-DLF METHOD AND MULTIPLE STATE-OF-THE-ART VEHICLE RE-IDENTIFICATION METHODS. FET REPRESENTS THE FEATURE EXTRACTION TIME.</figDesc><table><row><cell>Methods</cell><cell>FET (msec/image)</cell></row><row><cell>D-DLF</cell><cell>2.321</cell></row><row><cell>A-DLF</cell><cell>2.304</cell></row><row><cell>H-DLF</cell><cell>2.296</cell></row><row><cell>V-DLF</cell><cell>2.312</cell></row><row><cell>QD-DLF</cell><cell>11.199</cell></row><row><cell>DenseNet121 [25]</cell><cell>13.647</cell></row><row><cell>GoogLeNet [26]</cell><cell>2.345</cell></row><row><cell>VGG-CNN-M-1024 [3]</cell><cell>1.872</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Provid: Progressive and multi-modal vehicle re-identification for large-scale urban surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep relative distance learning: Tell the difference between similar vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2167" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person reidentification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep joint discriminative learning for vehicle re-identification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="395" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving tripletwise training of convolutional neural network for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1386" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vehicle re-identification by adversarial bi-directional lstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="653" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vehicle re-identification by deep hidden multi-view inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3275" to="3287" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning deep neural networks for vehicle re-id with visualspatio-temporal path proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1918" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hog-assisted deep feature learning for pedestrian gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Franklin Institute</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1991" to="2008" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep hybrid similarity learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person re-identification based on novel triplet convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronics &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1012" to="1016" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A largescale car dataset for fine-grained categorization and verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayes merging of multiple vocabularies for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1963" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
