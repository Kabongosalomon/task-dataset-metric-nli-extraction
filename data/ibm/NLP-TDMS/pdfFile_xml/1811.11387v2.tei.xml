<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longlong</forename><surname>Jing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinggen</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">JD AI Reserach</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The City University of New York</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Spatiotemporal Feature Learning via Video Rotation Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of deep neural networks generally requires a vast amount of training data to be labeled, which is expensive and unfeasible in scale, especially for video collections. To alleviate this problem, in this paper, we propose 3DRotNet: a fully self-supervised approach to learn spatiotemporal features from unlabeled videos. A set of rotations are applied to all videos, and a pretext task is defined as prediction of these rotations. When accomplishing this task, 3DRotNet is actually trained to understand the semantic concepts and motions in videos. In other words, it learns a spatiotemporal video representation, which can be transferred to improve video understanding tasks in small datasets. Our extensive experiments successfully demonstrate the effectiveness of the proposed framework on action recognition, leading to significant improvements over the state-of-the-art self-supervised methods. With the selfsupervised pre-trained 3DRotNet from large datasets, the recognition accuracy is boosted up by 20.4% on UCF101 and 16.7% on HMDB51 respectively, compared to the models trained from scratch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With more videos flourishing on the internet, recognizing human actions from videos <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> has drawn increasing attention in computer vision community. Recently, thanks to the strong capability of simultaneously capturing both spatial and temporal representations, 3DC-NNs have been widely and successfully explored in many video understanding tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>To achieve a good performance, 3DCNN-based supervised feature learning approaches require millions of video and label pairs for training. For instance, at the time C3D <ref type="bibr" target="#b41">[42]</ref> is proposed for action recognition on UCF101, a relatively small dataset, its performance is not comparable to that of hand-crafted features like dense trajectories <ref type="bibr" target="#b44">[45]</ref>. Until 3DCNNs are pre-trained on large-scale video datasets such as Sports-1M <ref type="bibr" target="#b18">[19]</ref> and Kinetics <ref type="bibr" target="#b19">[20]</ref>, the performance * Corresponding author.  <ref type="figure">Figure 1</ref>. Video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet at each rotation. Note that both spatial (e.g. locations and shapes of different persons) and temporal features (e.g. motions and location changes of persons) are effectively captured. The hottest areas in attention maps indicate the person with the most significant motion (corresponding to the red bounding boxes in images). The attention map is computed by averaging the activations in each pixel which reflects the importance of that pixel.</p><p>has been largely improved. In fact, Kinetics consists of approximately 500, 000 videos of 600 human actions. However, collecting such large-scale annotated video datasets is laborious and expensive in practice for new video understanding tasks. Therefore, here we attempt to learn spatiotemporal features directly from numerous unlabeled videos.</p><p>To mitigate the aforementioned problem, in this paper, we propose 3DRotNet, a simple yet effective 3DCNNbased self-supervised spatiotemporal feature learning framework, which eliminates the requirement of human annotations.</p><p>Our self-supervised learning defines an annotation-free pretext task to identify and provide super-visory signals solely from the visual information present in videos. This paradigm has been widely and successfully applied in image domain to learn image features for various image understanding tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. As an example, in the pretext task of image inpainting, Pathak et al. design a self-supervised 2DCNN to predict the missing regions in an image by learning the concept and the structure of the image <ref type="bibr" target="#b35">[36]</ref>. Overall, the rationale behind the self-supervised approaches is that networks are enforced to learn high-level semantic features during accomplishing the pretext tasks.</p><p>Following this learning strategy, our work is in particular designed to achieve a pretext task of recognizing the rotation transformation that is applied to videos, and as a result, it simultaneously learns a network capturing high-level semantic and motion features. As examples shown in <ref type="figure">Fig. 1</ref>, in order to recognize how the video is rotated, a semantic sense to persons, objects as well as their locations and motions are needed. It is difficult to accomplish the pretext task without the knowledge of these semantic concepts. This is the underlying rationale behind our approach.</p><p>Specifically, we first apply a set of rotations (e.g. 0 • , 90 • , 180 • and 270 • ) to videos as shown in <ref type="figure">Fig. 1</ref>, and then define a pretext task as recognizing the set of rotations. To this end, the 3DRotNet is trained to recognize how many degrees each input video is rotated given raw frames (RGB) or difference of frames (DIF) of videos as inputs, where the latter can be treated as a light version of optical flow. During the training process, 3DRotNet attempts to learn a semantic video representation, which is able to capture spatial appearance cues (e.g. location and shape) as well as temporal information (e.g. motion and evolution). <ref type="figure">Fig. 1</ref> illustrates the video frames and their corresponding attention maps generated by 3DRotNet at each rotation. It demonstrates the effectiveness of our approach to capture spatiotemporal video representations.</p><p>For quantitative evaluation, we pre-train 3DRotNet on Kinetics using the proposed self-supervised feature learning framework without annotations, and then transfer the learned features for action recognition tasks on UCF101 and HMDB51. The performance gap between our selfsupervised feature learning and the supervised pre-trained models is getting small. In addition, our approach substantially outperforms other alternative self-supervised methods. A variety of ablation studies have been conducted to further analyze our models.</p><p>Our work is inspired by some recent image-based selfsupervised feature learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>, which also involve geometric transformations. The feature representations are yielded during the processes of learning to predict camera transformation using ego-motion <ref type="bibr" target="#b0">[1]</ref>, learning to be discriminative <ref type="bibr" target="#b7">[8]</ref>, or learning to tell image rotation <ref type="bibr" target="#b11">[12]</ref>. Instead, our work focuses on self-supervised video repre-sentation learning through simultaneous spatial and temporal feature modeling.</p><p>In summary, our main contributions in this paper are as follows. First, we propose 3DRotNet, a simple yet effective fully self-supervised approach for spatiotemporal feature learning. By only using the video rotation transformation without requiring any annotations, 3DRotNet is capable of capturing both spatial and temporal information. Second, extensive experiments find our approach to produce significantly better results than the state-of-the-art self-supervised methods. Third, 3DRotNet learned in the unsupervised manner can be served as a pre-trained model to be transferred to other video understanding tasks when only small datasets are available. With 3DRotNet pre-trained on Kinetics dataset, the performance of action recognition is remarkably boosted up by 20.4% on UCF101 and 16.7% on HMDB51 compared to that from the models trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, a number of self-supervised learning methods have been proposed to for representation learning from images and videos <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17]</ref>. Based on the pretext tasks, these methods mainly fall into two categories: one is the texture based methods, which utilize texture information of images as supervision, such as the boundary of the objects <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref>, the context of images <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref>, and the similarity of two patches from an image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>, and the other is the temporal based methods, which exploit temporal connection between frames such as the temporal order of frames <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> and cross-modal correspondence <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Self-Supervised Learning from Images. The similarity between two patches from the same image is often used as a supervision signal for the self-supervised image feature learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. Noroozi and Favaro propose an approach to learn visual representations by solving Jiasaw puzzles with 9 patches from same image <ref type="bibr" target="#b32">[33]</ref>. Doersch et al. introduce to learn visual features by predicting the relative positions of two patches from the same image <ref type="bibr" target="#b5">[6]</ref>. Li et al. propose to mine the positive and negative image pairs with graph constraints in the feature space and the mined pairs are used to train the network to learn visual features <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr">Caron et al. present</ref> DeepCluster to iteratively train the network with categories that are generated by clustering <ref type="bibr" target="#b2">[3]</ref>. The context information of images such as structure <ref type="bibr" target="#b35">[36]</ref>, color <ref type="bibr" target="#b24">[25]</ref> and relations of objects is another type of supervision for self-supervised image feature learning. Gidaris et al. propose to learn visual features by training a 2DCNN to recognize 2D image rotations which is proved to be helpful for image feature learning <ref type="bibr" target="#b11">[12]</ref>. Larsson et al. employ image colorization as the pretext to learn semantic features of images <ref type="bibr" target="#b24">[25]</ref>. Zhang et al. present the split-brain autoencoder to predict a subset of image channels from other channels <ref type="bibr" target="#b52">[53]</ref>. Ren and Lee propose to learn image features from synthetic images generated by a game engine based on a generative adversarial network <ref type="bibr" target="#b37">[38]</ref>.</p><p>Self-Supervised Learning from Videos. Although there are some prior work about self-supervised learning from videos, most of them still apply 2DCNNs to essentially learn image representations by using temporal information in videos as supervision. Pathak et al. use a 2DCNN to segment moving objects that are unsupervised segmented from videos <ref type="bibr" target="#b34">[35]</ref>. Misra et al. adopt a 2DCNN to verify whether a sequence of frames is in correct temporal order <ref type="bibr" target="#b28">[29]</ref>. Wang and Gupta propose a Siamese-triplet network with a ranking loss to train a 2DCNN with the patches from a video sequence <ref type="bibr" target="#b46">[47]</ref>. Fernando et al. propose to learn video representations by odd-one-out networks to identify the odd element from a set of related elements with a 2DCNN <ref type="bibr" target="#b9">[10]</ref>. Lee et al. take shuffled video frames as input to a 2DCNN to sort the sequence <ref type="bibr" target="#b25">[26]</ref>.</p><p>3DCNN has been widely used to simultaneously model both spatial and temporal information in videos <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18]</ref>, however, only a few attempts exploit it for self-supervised learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b21">22]</ref> and its performance is much lower than that of the supervised methods. Vondrick et al. use a generative adversarial network for video generation and feature learning with a 3DCNN <ref type="bibr" target="#b42">[43]</ref>. They also propose to learn spatiotemporal features by colorizing videos with 3DCNN <ref type="bibr" target="#b43">[44]</ref>. Compared to the image based self-supervised learning, the spatiotemporal feature learning with 3DCNN is overall much less explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Parametrization</head><p>In this paper, we adopt a 3DCNN F (x|θ) to learn the spatiotemporal features from predicting a set of pre-applied video rotation transformations G(x|y) = Rot(x, y), while x denotes the input video, y indicates the parameter of the rotation transformation, the Rot(x, y) is the rotation operation that rotates all the frames in a video with y degrees and G(x, y) is the result of transformation.</p><p>The video rotation prediction can be implemented in two ways: regression and classification. For the regression pretext task, the network predicts y of the rotation transformation as a continuous variable. For the classification pretext task, a set of discrete rotations are pre-defined, then the network is trained to recognize the rotation category.</p><p>Given a video x i , the regression implementation is straightforward and can be formulated as:</p><formula xml:id="formula_0">loss(x i |θ) = (F (G(x i |y)|θ) − y) 2 ,<label>(1)</label></formula><p>The network F (x|θ) is trained to predict the parameter y of the rotation transformation, while usually 1 loss or 2 loss is computed as the regression loss to optimize the network. When formulate the problem as a classification task, a set of K discrete rotations is defined, and the network F (.) is optimized by minimizing the cross entropy loss between the predicted probability distribution over K and the rotation y. The loss function is:</p><formula xml:id="formula_1">loss(x i |θ) = − 1 K K y=1 log(F (G(x i , y)|θ)),<label>(2)</label></formula><p>In both scenarios, given a set of N training videos D = {x i } N i=0 , the overall training loss function is defined as:</p><formula xml:id="formula_2">loss(D) = min θ 1 N N i=1 loss(x i |θ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Rotation Transformation Design</head><p>Different types of image transformations are designed as supervision information to train 2DCNNs for image representation learning including image colorization <ref type="bibr" target="#b24">[25]</ref>, image rotation <ref type="bibr" target="#b11">[12]</ref>, and image denoise <ref type="bibr" target="#b1">[2]</ref>. In this paper, we propose to use video rotation as the supervision signal to learn video features. Specifically, 3DCNNs are trained to model both the spatial and temporal features which are representative for the semantic context of videos. Inspired by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>, we formulate the problem as a classification task in which the network is to recognize K types of discrete rotations that are applied to videos.</p><p>Choosing rotations as the geometric transformations for learning video features has the following advantages: <ref type="bibr" target="#b0">(1)</ref> The problem is well-defined. Most of the videos in realworld environments are filmed in an upright way that the objects in the videos tend to be upright. (2) Compared to other pretext tasks, the rotation is easy to implement by the flip and transpose operations without adding much time complexity to the network. (3) Unlike other self-supervised learning methods need to take a lot of efforts to avoid the network to learn trivial solutions <ref type="bibr" target="#b5">[6]</ref>, the rotation operation leaves no artifacts in an image which can ensure the network learn meaningful semantic features through the process of accomplishing this pretext task. Following <ref type="bibr" target="#b11">[12]</ref>, we design four types of rotations at 0 • , 90 • , 180 • and 270 • . Therefore, for each video x with the type of rotation y, the output video after rotation transformation is 3DRotNet which predict the probability over all possible rotations for each video. The cross entropy loss is computed between the predicted probability distribution F (X) and the rotation categories y and is minimized to update the weights of the network. As for the network architecture, we follow the 3D ResNet18 since it has relatively fewer parameters and is capable to learn spatiotemporal features from large-scale datasets <ref type="bibr" target="#b13">[14]</ref>. There are five convolution blocks, while the first one consists of one convolution layer, one batch normalization layer, one ReLU layer, and followed by one max-pooling layer, and the rest four convolution blocks are 3D residual blocks with skip connection. The number of kernels in each convolution block is shown in <ref type="figure">Fig. 2</ref>. After the five convolution blocks, the global average pooling is applied to obtain a 512-dimensional feature vector. For rotation prediction, this 512-dimensional vector is followed by two fully connected layers with the dimensions of 64 and 4 to generate the prediction probability, while in the fine-tuning on action recognition task, this 512-dimensional vector is followed by only one fully connected layer of size equals to the number of action classes in the target action recognition dataset.</p><formula xml:id="formula_3">G = {G(x|y)} 4 y=1 , where G(x|y) = Rot(x, (y − 1) × 90).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proposed Framework</head><p>Unlike other self-supervised learning methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref> that usually involve massive data preparation, our approach is straightforward to implement without complicated data preprocessing. Additionally, there is no extra effort needed to avoid trivial solutions since the rotation operations do not generate image artifacts. As a comparison in <ref type="bibr" target="#b34">[35]</ref>, masks of moving objects need to be generated in advance, and heavy data augmentation is applied to avoid the network to learn a trivial solution <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation Metrics</head><p>To evaluate the quality of the learned features, previous self-supervised learning methods usually use the learned parameters as a start point and fine-tune on other high-level visual tasks such as action recognition. The performance of transfer learning on these tasks are compared to evaluate the generalization ability of the self-supervised learned features. If the self-supervised learning model can learn representative semantic features, then it can be served as a good start point and leads to better performance on these high-level visual tasks. In addition to the quantitative evaluation, previous methods also analyze network kernels and activation maps to provide qualitative visualization results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Following other self-supervised spatiotemporal feature learning methods such as <ref type="bibr" target="#b21">[22]</ref>, our proposed approach is evaluated in the following ways.</p><p>• Qualitatively analyze the kernels of the first convolution layer in 3DRotNet learned with the proposed approach and compare the kernels with that of the stateof-the-art supervised models. • Analyze the feature activation maps generated by 3DRotNet and compare them with that of the state-ofthe-art supervised models. • Transfer the pre-trained 3DRotNet to action recognition task and compare the performance with the state-of-the-art self-supervised methods on two public benchmarks. • Perform ablation studies to evaluate the impact of different configurations of the rotation transformation to the quality of the features learned by 3DRotNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we conduct extensive experiments to evaluate the proposed approach and the quality of the learned spatiotemporal features for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Our self-supervised video representation learning network 3DRotNet is trained on two large-scale datasets Moment in Time <ref type="bibr" target="#b30">[31]</ref> and Kinetics <ref type="bibr" target="#b19">[20]</ref>. No action labels are used during the training. Following the standard evaluation protocol, the pre-trained 3DRotNet is then supervised fine-tuned for action recognition on two relatively small datasets: UCF101 <ref type="bibr" target="#b39">[40]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>, respectively.</p><p>Moment in Time (MT) is a large-scale balanced and diverse dataset for video understanding <ref type="bibr" target="#b30">[31]</ref>. MT consists of around 1 million videos covering 339 action classes, and each video lasts around 3 seconds. The average number of videos for each class is 1, 757 with a median of 2, 775. The training set of the dataset is used for the self-supervised learning without using video labels.</p><p>Kinetics is a large-scale and high-quality video dataset collected from YouTube <ref type="bibr" target="#b19">[20]</ref>. The dataset consists of around 500, 000 videos belonging to 600 action classes with at least 600 videos for each class. Each video is around 10 second. We download about 480, 000 videos and all of them are used to train our self-supervised model without knowing video labels.</p><p>UCF101 is a widely used benchmark for action recognition. It consists of 13, 320 videos that cover 101 human action classes. Due to the small size of the dataset, 3DCNNs often suffer from over-fitting on this dataset when trained from scratch. Pre-trained models (either supervised or selfsupervised) from other large-scale datasets are needed to overcome over-fitting.</p><p>HMDB51 is another widely used benchmark for action recognition. It consists of 6, 770 videos in 51 actions. Similar as UCF101, pre-trained models are also needed to alleviate over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Self-Supervised Learning. The videos in Kinetics and MT are evenly downsampled into 160 and 90 frames respectively and then are resized to a spatial resolution at 136 × 136. During training, 16 (default value) consecutive frames are randomly selected from each video as a training clip, and a patch with 112 × 112 pixels is randomly cropped from each frame to form a clip of size 3 channels × 16 frames ×112 × 112 pixels. Each video is horizontally flipped with 50% probability to augment the dataset. For each video, all the frames are rotated with four different degrees, and the four rotated videos are simultaneously fed into the network. The training is optimized by stochastic gradient descent (SGD) using 10, 4000 iterations and with a batch size of 32. The initial learning rate is set to 0.1 and is decayed by 0.1 in every 2, 4000 iterations.</p><p>Transfer Learning. To evaluate the learned features, we fine-tune the pre-trained model to perform action recognition on the two public datasets: UCF101 <ref type="bibr" target="#b39">[40]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>. During training, 16 consecutive frames are randomly selected from a video and resized to a spatial size of 136 × 136 pixels, then a 112 × 112 patch is cropped from each frame within the clip to form a tensor of size 3 channels × 16 frames × 112 × 112 pixels. The cross entropy loss is computed and optimized by SGD with 100 epochs. The initial learning rate is set to 0.008 and is multiplied by 0.1 in every 4, 000 iterations. The top-1 classification accuracy on UCF101 and HMDB51 datasets are reported and compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Can 3DRotNet Recognize Video Rotations?</head><p>The hypothesis of our idea is that a network should be able to capture the semantic information in videos through recognizing video rotations, and the learned semantic information can be further transferred to other video understanding tasks such as action recognition. Therefore, we first test the performance of 3DRotNet to recognize the four rotations (0 • , 90 • , 180 • and 270 • ). During training, the class labels of the videos in the two datasets (Kinetics <ref type="bibr" target="#b19">[20]</ref> and MT <ref type="bibr" target="#b30">[31]</ref>) are excluded and videos applied with the four rotations are used to train the 3DRotNet.</p><p>After trained on the two large-scale video datasets, the network is cross-domain tested on UCF101 and HMDB51. For testing, all videos in the two datasets are first applied with the four rotations, then the rotated videos are input to the 3DRotNet to predict the rotation angles. The average classification accuracy of the four rotations is shown in <ref type="table">Table 1</ref>. The accuracy of video rotation recognition on UCF101 and HMDB51 are all higher than 89%, demonstrating that the proposed 3DRotNet is able to capture representative appearance cues in videos to recognize their rotations. However, it is still unclear whether the 3DRotNet can effectively capture the spatiotemporal information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Can 3DRotNet Learn Spatiotemporal Video Features?</head><p>In order to verify whether the 3DRotNet learned from video rotations can capture both spatial and temporal fea- tures from videos such as moving objects, or whether the 3DRotNet solely rely a trivial solution such as using lines in videos to determine their rotations, we visualize the attention maps of the learned 3DRotNet models by averaging the activation maps of the first convolution layer, which can be used to reflect the importance of each pixel.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the attention maps show that the 3DRotNet mainly focuses on the important foreground persons in videos and capture moving objects well. As shown in the images of the baby crawling video (right-bottom in <ref type="figure" target="#fig_2">Fig. 3)</ref>, the 3DRotNet can capture the moving baby on the ground. This confirms that our 3DRotNet can capture the spatiotemporal information within videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Transfer to Action Recognition Task</head><p>In order to evaluate the generalization capability of the learned video features from our self-supervised models, we further conduct action recognition task on two different datasets (UCF101 and HMDB51) by using the learned video features (i.e., pre-trained 3DRotNet) as a start point and then finetuned on the action recognition datasets. The experimental results on the first split of UCF101 and HMDB51 datasets are shown in <ref type="table">Table.</ref>   <ref type="table">Table 2</ref>. Results of transfer learning of the self-supervised model on action recognition task on UCF101 and HMDB51 datasets.</p><p>As shown in <ref type="table">Table.</ref> 2, when the 3DResNet is trained from scratch on two action datasets it only achieves 42.5% on UCF101 and 17.0% on HMDB51 due to over-fitting. However, when fine-tune our self-supervised models on each dataset using the pre-trained models, the performance has a significant improvement of 20.4% (achieves 62.9%) on UCF101 and 16.7% (33.7%) on HMDB51 which proves that the proposed self-supervised learning method is effective and indeed can provide a good start point for training a discriminative 3DResNets on the small datasets. Since MT dataset is very different to UCF101 dataset, the result on MT is more convincing.</p><p>Following other self-supervised methods <ref type="bibr" target="#b34">[35]</ref>, the performance of CNNs layers frozen with different extent are compared and shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>. The model pre-trained on Kinetics dataset is finetuned on HMDB51 and UCF101 datasets. For UCF101 dataset, the network has the best performance when the first convolution block is frozen, and has the worst performance when all the convolution blocks are frozen during training. For HMDB51 dataset, the network has the best performance when the first two convolution blocks are frozen, and has the worst performance when all the convolution blocks are frozen. This probably is because the lower layers learn the general low-level feature, while deeper layers learn the high-level task-specific features. When fine-tuned on the small dataset, the parameters of lower layers need to be preserved and deeper layers need to be further tuned for specific tasks.</p><p>We also study the relationship between the accuracy of rotation recognition and the accuracy of action recognition on UCF101 dataset. The results are shown in <ref type="figure" target="#fig_3">Fig. 4 (b)</ref>. The performance of action recognition increases along with the improvement of the accuracy of rotation recognition which validates that the proposed 3DRotNet can learn meaningful features for high-level video tasks through simple recognition of rotation geometric transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study of Impact of Rotations</head><p>We further conduct experiments to evaluate the impact of the combination of different rotation degrees to the accuracy of action recognition task under four situations: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotations</head><p>Combination</p><formula xml:id="formula_4">0 • rotation √ √ √ 90 • rotation √ √ √ √ 180 • rotation √ √ √ 270 • rotation √ √</formula><p>Performance 50.94% 58.79% 59.24% 62.90% <ref type="table">Table 3</ref>. The comparison of the performance of networks to recognize different number of rotations on UCF101 dataset. The network that recognizes 4 rotation degrees has the best performance among all the networks. <ref type="table">Table 3</ref> illustrates the effects of the number of rotations to the transfer learning. The network trained for four rotations has the best performance on the transfer learning, and the network based only two rotations has the worst performance. When only two kinds of rotations are available, the finetune performance on the UCF101 dataset is only 50.94% which is 11.96% lower than the performance of the network pre-trained with four rotations. This is probably because the network trained to recognize 4 rotations received more supervision signal than the network trained to recognize 2 rotations.</p><p>In addition to the 4-rotation recognition, we also train the 3DRotNet to recognize 8 rotation degrees {0 • , 45 • , 90 • , 135 • , 180 • , 225 • , 270 • , and 315 • } and to recognize 360 rotation degrees. The 360-rotation network is optimized with regression loss. When fine-tuned on UCF101 dataset for action recognition, the 8-rotation network achieves only 57.0% performance which is 5.9% lower than that of the 4rotation network, while the 360-rotation network achieves only 60.9% performance which is 2.0% lower than that of the 4-rotation network. The performance degradation probably comes from the context lost since only the center patch is cropped to avoid the empty image areas introduced by the rotation transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Ablation Study of Impact of Data Amount</head><p>In this section, we evaluate the impact of the amount of training videos on the quality of features. We vary the amount of training data with Kinetics dataset for selfsupervised learning and observe the action recognition performance of the transfer learning on action recognition task on UCF101 and HMDB51 datasets. The results are demonstrated in <ref type="table">Table 4</ref> As shown in <ref type="table">Table 4</ref>, the performance of the transfer learning increase as more training data is available which indicates that large-scale data is needed for self-supervised learning. The table also shows that the action recognition performance can be further improved by utilizing more training data for self-supervised pre-training.  <ref type="table">Table 5</ref>. Action recognition performance of few-shot learning on UCF101 dataset with and without using pre-trained models. When training dataset is extremely limited, the self-supervised pre-trained model can significantly improve the performance.</p><p>In addition to the ablation study for the amount of training data for self-supervised pre-training in <ref type="table">Table 5</ref>, we conduct experiments to evaluate the performance of 3DRotNet when the training data for the target task is extremely small by few-shot learning on UCF101. Our self-supervised pretrained model can significantly improve the performance when there are only a few samples are available. With 3DRotNet pre-trained model, even when only 20 labeled videos are available, the performance of 3DRotNet for action recognition on UCF101 is comparable with the model trained with nearly 10, 000 labeled videos from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Learning Long-Term Temporal Information</head><p>In this section, we evaluate the impact of the length of input video clips as well as the quality of the features on RGB channel and the difference between frames respectively. We vary the length of input video clips for the self-supervised learning and observe the performance of the transfer learning on the action recognition task on the UCF101 dataset. The results are demonstrated in <ref type="table">Table 6</ref>.  <ref type="table">Table 6</ref>. The comparison of the action recognition performance of networks with different length of input clips for both RGB and difference of frames (DIF). The networks with longer input clip achieve better performance for action recognition since long-term temporal information is provided by the video clip.</p><p>As shown in <ref type="table">Table 6</ref>, the performance of the transfer learning increase as long input video clips are used. Simply by increasing the length of clips from 16 to 64, the performance increases 3.5% on UCF101 dataset. The difference of frames (DIF) captures motion within video clips and invariant to appearance which probably leads to much higher performance than models trained with RGB clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Compare with Other Self-supervised Methods</head><p>In this section, we compare our 3DRotNet with other self-supervised methods on action recognition task including the 2DCNN-based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref> and the 3DCNNbased methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b21">22]</ref>. The 2DCNN-based methods mainly use the temporal information between frames as the supervision signal to train the 2DCNN. The features learned in most of these models are still focusing on the image features of every single frame <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29]</ref>. However, the 3DCNNbased methods can simultaneously learn both spatial and temporal information in videos. <ref type="table">Table 7</ref> shows the action recognition accuracy on UCF101 and HMDB51. The supervised models of 2DCNN and 3DCNN based methods have the state-of-the-art performance of over 80% on the UCF101 dataset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref>. These models usually require pre-trained models from large-scale labeled datasets and involve fusion of different modalities such as the Optical Flow, RGB, and DIF.</p><p>Our 3DRotNet-RGB outperforms all the 2DCNN-based and 3DCNN-based self-supervised learning methods and achieves 0.2% and 3.7% higher on UCF101 and HMDB51 dataset respectively than the state-of-the-art self-supervised method <ref type="bibr" target="#b21">[22]</ref>. The F usion indicates the geometric mean of RGB network and DIF network is computed to obtain the final score. The fusion boosts the performance by 2.3% and by 4.5% on the UCF101 and HMDB51 dataset. The gap between the fusion result and the supervised result <ref type="bibr" target="#b13">[14]</ref> is only 7.8% on UCF101 dataset.  <ref type="table">Table 7</ref>. Comparison with other self-supervised methods on action recognition task. Our proposed method outperforms all other selfsupervised methods on both UCF101 and HMDB51 datasets. All the 3DCNN-based self-supervised methods use the same architecture, and all the accuracies are averaged over three splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Kernel Comparison between Supervised and Self-supervised Models</head><p>Here, we visualize all the kernels of the first convolution layer of the proposed self-supervised 3DRotNet and the kernels of the fully supervised model in <ref type="figure">Fig. 5</ref>. Both the proposed 3DRotNet and the supervised 3DConvNet are trained on the Kinetics dataset. The only difference is that the 3DRotNet is trained without the human-annotated category labels. As shown in <ref type="figure">Fig 5,</ref> the self-supervised model learned the similar kernels as the supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a straightforward and effective 3DCNN-based approach for self-supervised learning of spatiotemporal features from videos. The experiment results demonstrate that video rotation transformations are able to provide essential information for networks to learn both spatial and temporal features for videos. The effectiveness of the learned video features has been evaluated on action recognition task, and the proposed framework has achieved the state-of-the-art performance on two benchmarks among all existing self-supervised methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Figure 2 .</head><label>22</label><figDesc>illustrates the pipeline of the proposed 3DRotNet to learn spatiotemporal features by predicting the video rotation transformation. In our implementation, four kinds of rotations G = {Rot(X, (y − 1) × 90)} 4 y=1 are applied to each video respectively. Then these four types of videos along with their rotation categories y are used to train the The pipeline of the proposed self-supervised spatiotemporal representation learning. Each video is rotated with four different degrees (0 • , 90 • , 180 • , 270 • ), and 3DRotNet is trained to recognize the rotations that applied to input videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Sampled video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet and by supervised model. The attention maps show that our model can capture both spatial and temporal information within videos. Moreover, the proposed self-supervised model can capture the main objects and their motions in a video as the supervised model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) Finetune results on UCF101 and HMDB51 datasets. Cn means the n-th convolution block. &gt;Cn means the blocks before the n-th convolution block are frozen during fine-tune. (b) Relation of the rotation recognition accuracy and the action recognition accuracy. The performance of action recognition increases along with the improvement of the accuracy of rotation recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Combining 0 • and 90 • rotations, (b) Combining 0 • , 90 • , and 180 • rotations, (c) Combining 0 • , 90 • , 180 • , and 270 • rotations, and (d) Combining 90 • , 180 • , and 270 • rotations. These networks are trained on Kinetics dataset and finetuned on UCF101 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Sampled video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet model at each rotation angle. The network focuses on the moving baby at all rotations. Sampled video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet model at each rotation angle. The network focuses on the moving person in this video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Sampled video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet model at each rotation angle. The network can capture the multiple persons at the same time among all the frames. Sampled video frames and their corresponding attention maps generated by our proposed self-supervised 3DRotNet model at each rotation angle. The network can capture the two persons at the same time and focuses on the person with the most significant movement.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This material is based upon the work supported by National Science Foundation (NSF) under award number IIS-1400802.  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05310</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Karami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Videocapsulenet: A simplified network for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7621" to="7630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning through spatial contrasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00243</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06162</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video you only look once: Overall temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JV-CIR</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised meta-learning for few-shot image and video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bölöni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09795</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7774" to="7785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCSE</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by graph-based consistent constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1619" to="1627" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representions by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00385</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multitask feature learning using synthetic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cross and learn: Cross-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03879</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CRCV-TR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09594</idno>
		<title level="m">Tracking emerges by colorizing videos</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Transitive invariance for selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multilayer and multimodal fusion of deep neural networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Making convolutional networks recurrent for visual sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
