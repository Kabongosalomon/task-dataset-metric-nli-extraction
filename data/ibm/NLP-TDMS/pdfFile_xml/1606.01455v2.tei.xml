<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Residual Learning for Visual QA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
							<email>jhkim@bi.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
							<email>slee@bi.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Kwak</surname></persName>
							<email>dhkwak@bi.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
							<email>moheo@bi.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
							<email>jeonghee.kim@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Naver Labs, Naver Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
							<email>jungwoo.ha@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Naver Labs, Naver Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
							<email>btzhang@bi.snu.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Residual Learning for Visual QA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual question-answering tasks provide a testbed to cultivate the synergistic proposals which handle multidisciplinary problems of vision, language and integrated reasoning. So, the visual questionanswering tasks let the studies in artificial intelligence go beyond narrow tasks. Furthermore, it may help to solve the real world problems which need the integrated reasoning of vision and language.</p><p>Deep residual learning <ref type="bibr" target="#b5">[6]</ref> not only advances the studies in object recognition problems, but also gives a general framework for deep neural networks. The existing non-linear layers of neural networks serve to fit another mapping of F(x), which is the residual of identity mapping x. So, with the shortcut connection of identity mapping x, the whole module of layers fit F(x) + x for the desired underlying mapping H(x). In other words, the only residual mapping F(x), defined by H(x) − x, is learned with non-linear layers. In this way, very deep neural networks effectively learn representations in an efficient manner.</p><p>Many attentional models utilize the residual learning to deal with various tasks, including textual reasoning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21]</ref> and visual question-answering <ref type="bibr" target="#b28">[29]</ref>. They use an attentional mechanism to handle two different information sources, a query and the context of the query (e.g. contextual sentences or an image). The query is added to the output of the attentional module, that makes the attentional module learn the residual of query mapping as in deep residual learning.</p><p>In this paper, we propose Multimodal Residual Networks (MRN) to learn multimodality of visual question-answering tasks exploiting the excellence of deep residual learning <ref type="bibr" target="#b5">[6]</ref>. MRN inherently uses shortcuts and residual mappings for multimodality. We explore various models upon the  choice of the shortcuts for each modality, and the joint residual mappings based on element-wise multiplication, which effectively learn the multimodal representations not using explicit attention parameters. <ref type="figure" target="#fig_0">Figure 1</ref> shows inference flow of the proposed MRN.</p><p>Additionally, we propose a novel method to visualize the attention effects of each joint residual mapping. The visualization method uses back-propagation algorithm <ref type="bibr" target="#b21">[22]</ref> for the difference between the visual input and the output of the joint residual mapping. The difference is back-propagated up to an input image. Since we use the pretrained visual features, the pretrained CNN is augmented for visualization. Based on this, we argue that MRN is an implicit attention model without explicit attention parameters.</p><p>Our contribution is three-fold: 1) extending the deep residual learning for visual question-answering tasks. This method utilizes multimodal inputs, and allows a deeper network structure, 2) achieving the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks, and finally, 3) introducing a novel method to visualize spatial attention effect of joint residual mappings from the collapsed visual feature using back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Residual Learning</head><p>Deep residual learning <ref type="bibr" target="#b5">[6]</ref> allows neural networks to have a deeper structure of over-100 layers. The very deep neural networks are usually hard to be optimized even though the well-known activation functions and regularization techniques are applied <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. This method consistently shows state-of-the-art results across multiple visual tasks including image classification, object detection, localization and segmentation.</p><p>This idea assumes that a block of deep neural networks forming a non-linear mapping F(x) may paradoxically fail to fit into an identity mapping. To resolve this, the deep residual learning adds x to F(x) as a shortcut connection. With this idea, the non-linear mapping F(x) can focus on the residual of the shortcut mapping x. Therefore, a learning block is defined as:</p><formula xml:id="formula_0">y = F(x) + x<label>(1)</label></formula><p>where x and y are the input and output of the learning block, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stacked Attention Networks</head><p>Stacked Attention Networks (SAN) <ref type="bibr" target="#b28">[29]</ref> explicitly learns the weights of visual feature vectors to select a small portion of visual information for a given question vector. Furthermore, this model stacks the attention networks for multi-step reasoning narrowing down the selection of visual information. For example, if the attention networks are asked to find a pink handbag in a scene, they try to find pink objects first, and then, narrow down to the pink handbag.</p><p>For the attention networks, the weights are learned by a question vector and the corresponding visual feature vectors. These weights are used for the linear combination of multiple visual feature vectors indexing spatial information. Through this, SAN successfully selects a portion of visual information. Finally, an addition of the combined visual feature vector and the previous question vector is transferred as a new input question vector to next learning block.</p><formula xml:id="formula_1">q k = F(q k−1 , V) + q k−1<label>(2)</label></formula><p>Here, q l is a question vector for l-th learning block and V is a visual feature matrix, whose columns indicate the specific spatial indexes. F(q, V) is the attention networks of SAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal Residual Networks</head><p>Deep residual learning emphasizes the importance of identity (or linear) shortcuts to have the nonlinear mappings efficiently learn only residuals <ref type="bibr" target="#b5">[6]</ref>. In multimodal learning, this idea may not be readily applied. Since the modalities may have correlations, we need to carefully define joint residual functions as the non-linear mappings. Moreover, the shortcuts are undetermined due to its multimodality. Therefore, the characteristics of a given task ought to be considered to determine the model structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>We infer a residual learning in the attention networks of SAN. Since Equation 18 in <ref type="bibr" target="#b28">[29]</ref> shows a question vector transferred directly through successive layers of the attention networks. In the case of SAN, the shortcut mapping is for the question vector, and the non-linear mapping is the attention networks.</p><p>In the attention networks, Yang et al. <ref type="bibr" target="#b28">[29]</ref> assume that an appropriate choice of weights on visual feature vectors for a given question vector sufficiently captures the joint representation for answering. However, question information weakly contributes to the joint representation only through coefficients p, which may cause a bottleneck to learn the joint representation.</p><formula xml:id="formula_2">F(q, V) = i p i V i<label>(3)</label></formula><p>The coefficients p are the output of a nonlinear function of a question vector q and a visual feature matrix V (see Equation 15-16 in Yang et al. <ref type="bibr" target="#b28">[29]</ref>). The V i is a visual feature vector of spatial index i in 14 × 14 grids.</p><p>Lu et al. <ref type="bibr" target="#b14">[15]</ref> propose an element-wise multiplication of a question vector and a visual feature vector after appropriate embeddings for a joint model. This makes a strong baseline outperforming some of the recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>. We firstly take this approach as a candidate for the joint residual function, since it is simple yet successful for visual question-answering. In this context, we take the global visual feature approach for the element-wise multiplication, instead of the multiple (spatial) visual features approach for the explicit attention mechanism of SAN. (We present a visualization technique exploiting the element-wise multiplication in Section 5.2.)</p><p>Based on these observations, we follow the shortcut mapping and the stacking architecture of SAN <ref type="bibr" target="#b28">[29]</ref>; however, the element-wise multiplication is used for the joint residual function F. These updates effectively learn the joint representation of given vision and language information addressing the bottleneck issue of the attention networks of SAN. Eventually, we chose (b) as the best performance and relative simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Residual Networks</head><p>MRN consists of multiple learning blocks, which are stacked for deep residual learning. Denoting an optimal mapping by H(q, v), we approximate it using</p><formula xml:id="formula_3">H 1 (q, v) = W (1) q q + F (1) (q, v).<label>(4)</label></formula><p>The first (linear) approximation term is W</p><p>q q and the first joint residual function is given by F (1) (q, v). The linear mapping W q is used for matching a feature dimension. We define the joint residual function as</p><formula xml:id="formula_5">F (k) (q, v) = σ(W (k) q q) σ(W (k) 2 σ(W (k) 1 v))<label>(5)</label></formula><p>where σ is tanh, and is element-wise multiplication. The question vector and the visual feature vector directly contribute to the joint representation. We justify this choice in Sections 4 and 5.</p><p>For a deeper residual learning, we replace q with H 1 (q, v) in the next layer. In more general terms, Equations 4 and 5 can be rewritten as</p><formula xml:id="formula_6">H L (q, v) = W q q + L l=1 W F (l) F (l) (H l−1 , v)<label>(6)</label></formula><p>where L is the number of learning blocks,</p><formula xml:id="formula_7">H 0 = q, W q = Π L l=1 W (l) q , and W F (l) = Π L m=l+1 W (m)</formula><p>q . The cascading in Equation 6 can intuitively be represented as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Notice that the shortcuts for a visual part are identity mappings to transfer the input visual feature vector to each layer (dashed line). At the end of each block, we denote H l as the output of the l-th learning block, and ⊕ is element-wise addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual QA Dataset</head><p>We choose the Visual QA (VQA) dataset <ref type="bibr" target="#b1">[2]</ref> for the evaluation of our models. Other datasets may not be ideal, since they have limited number of examples to train and test <ref type="bibr" target="#b15">[16]</ref>, or have synthesized questions from the image captions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref>.  min # of humans that provided that answer 3 , 1 .</p><p>The questions are answered in two ways: Open-Ended and Multiple-Choice. Unlike Open-Ended, Multiple-Choice allows additional information of eighteen candidate answers for each question. There are three types of answers: yes/no (Y/N), numbers (Num.) and others (Other). <ref type="table" target="#tab_2">Table 3</ref> shows that Other type has the most benefit from Multiple-Choice.</p><p>The images come from the MS-COCO dataset, 123,287 of them for training and validation, and 81,434 for test. The images are carefully collected to contain multiple objects and natural situations, which is also valid for visual question-answering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>Torch framework and rnn package <ref type="bibr" target="#b12">[13]</ref> are used to build our models. For efficient computation of variable-length questions, TrimZero is used to trim out zero vectors <ref type="bibr" target="#b10">[11]</ref>. TrimZero eliminates zero computations at every time-step in mini-batch learning. Its efficiency is affected by a batch size, RNN model size, and the number of zeros in inputs. We found out that TrimZero was suitable for VQA tasks. Approximately, 37.5% of training time is reduced in our experiments using this technique.</p><p>Preprocessing We follow the same preprocessing procedure of DeeperLSTM+NormalizedCNN <ref type="bibr" target="#b14">[15]</ref> (Deep Q+I) by default. The number of answers is 1k, 2k, or 3k using the most frequent answers, which covers 86.52%, 90.45% and 92.42% of questions, respectively. The questions are tokenized using Python Natural Language Toolkit (nltk) <ref type="bibr" target="#b2">[3]</ref>. Subsequently, the vocabulary sizes are 14,770, 15,031 and 15,169, respectively.</p><p>Pretrained Models A question vector q ∈ R 2,400 is the last output vector of GRU <ref type="bibr" target="#b3">[4]</ref>, initialized with the parameters of Skip-Thought Vectors <ref type="bibr" target="#b11">[12]</ref>. Based on the study of Noh et al. <ref type="bibr" target="#b18">[19]</ref>, this method shows effectiveness of question embedding in visual question-answering tasks. A visual feature vector v is an output of the first fully-connected layer of VGG-19 networks <ref type="bibr" target="#b22">[23]</ref>, whose dimension is 4,096.</p><p>Alternatively, ResNet-152 <ref type="bibr" target="#b5">[6]</ref> is used, whose dimension is of 2,048. The error is back-propagated to the input question for fine-tuning, yet, not for the visual part v due to the heavy computational cost of training.</p><p>Postprocessing Image captioning model <ref type="bibr" target="#b9">[10]</ref> is used to improve the accuracy of Other type. Let the intermediate representation v ∈ R |Ω| which is right before applying softmax. |Ω| is the vocabulary size of answers, and v i is corresponding to answer a i . If a i is not a number or yes or no, and appeared at least once in the generated caption, then update v i ← v i + 1. Notice that the pretrained image captioning model is not part of training. This simple procedure improves around 0.1% of the test-dev . We attribute this improvement to "tie break" in Other type. For the Multiple-Choice task, we mask the output of softmax layer with the given candidate answers.</p><p>Hyperparameters By default, we follow Deep Q+I. The common embedding size of the joint representation is 1,200. The learnable parameters are initialized using a uniform distribution from −0.08 to 0.08 except for the pretrained models. The batch size is 200, and the number of iterations is fixed to 250k. The RMSProp <ref type="bibr" target="#b25">[26]</ref> is used for optimization, and dropouts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> are used for regularization. The hyperparameters are fixed using test-dev results. We compare our method to state-of-the-arts using test-standard results. <ref type="figure" target="#fig_2">Figure 3</ref> shows alternative models we explored, based on the observations in Section 3. We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, (d) for the effectiveness of identity mapping as reported by <ref type="bibr" target="#b5">[6]</ref>, and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in <ref type="bibr" target="#b28">[29]</ref>. For comparison, all models have three-block layers (selected after a pilot test), using VGG-19 features and 1k answers, then, the number of learning blocks is explored to confirm the pilot test. The effect of the pretrained visual feature models and the number of answers are also explored. All validation is performed on the test-dev split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Exploring Alternative Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Analysis</head><p>The VQA Challenge, which released the VQA dataset, provides evaluation servers for test-dev and test-standard test splits. For the test-dev, the evaluation server permits unlimited submissions for validation, while the test-standard permits limited submissions for the competition. We report accuracies in percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternative Models</head><p>The test-dev results of the alternative models for the Open-Ended task are shown in <ref type="table" target="#tab_0">Table 1</ref>. (a) shows a significant improvement over SAN. However, (b) is marginally better than (a). As compared to (b), (c) deteriorates the performance. An extra embedding for a question vector may easily cause overfitting leading to the overall degradation. And, the identity shortcuts in (d) cause the degradation problem, too. Extra parameters of the linear mappings may effectively support to do the task.</p><p>(e) shows a reasonable performance, however, the extra shortcut is not essential. The empirical results seem to support this idea. Since the question-only model (50.39%) achieves a competitive result to the joint model (57.75%), while the image-only model gets a poor accuracy (28.13%) (see <ref type="table" target="#tab_1">Table 2</ref> in <ref type="bibr" target="#b1">[2]</ref>  The effects of other various options, Skip-Thought Vectors <ref type="bibr" target="#b11">[12]</ref> for parameter initialization, Bayesian Dropout <ref type="bibr" target="#b4">[5]</ref> for regularization, image captioning model <ref type="bibr" target="#b9">[10]</ref> for postprocessing, and the usage of shortcut connections, are explored in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Learning Blocks</head><p>To confirm the effectiveness of the number of learning blocks selected via a pilot test (L = 3), we explore this on the chosen model (b), again. As the depth increases, the overall accuracies are 58.85% (L = 1), 59.44% (L = 2), 60.53% (L = 3) and 60.42% (L = 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Features</head><p>The ResNet-152 visual features are significantly better than VGG-19 features for Other type in <ref type="table" target="#tab_1">Table 2</ref>, even if the dimension of the ResNet features (2,048) is a half of VGG features' (4,096). The ResNet visual features are also used in the previous work <ref type="bibr" target="#b7">[8]</ref>; however, our model achieves a remarkably better performance with a large margin (see <ref type="table" target="#tab_2">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Target Answers</head><p>The number of target answers slightly affects the overall accuracies with the trade-off among answer types. So, the decision on the number of target answers is difficult to be made. We chose Res, 2k in <ref type="table" target="#tab_1">Table 2</ref> based on the overall accuracy (for Multiple-Choice task, see Appendix A.1).</p><p>Comparisons with State-of-the-arts Our chosen model significantly outperforms other state-ofthe-art methods for both Open-Ended and Multiple-Choice tasks in <ref type="table" target="#tab_2">Table 3</ref>. However, the performance of Number and Other types are still not satisfactory compared to Human performance, though the advances in the recent works were mainly for Other-type answers. This fact motivates to study on a counting mechanism in future work. The model comparison is performed on the test-standard results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Analysis</head><p>In Equation 5, the left term σ(W q q) can be seen as a masking (attention) vector to select a part of visual information. We assume that the difference between the right term V := σ(W 2 σ(W 1 v)) and the masked vector F(q, v) indicates an attention effect caused by the masking vector. Then, the attention effect L att = 1 2 V − F 2 is visualized on the image by calculating the gradient of L att with respect to a given image I, while treating F as a constant.</p><formula xml:id="formula_9">∂L att ∂I = ∂V ∂I (V − F)<label>(8)</label></formula><p>This technique can be applied to each learning block in a similar way.</p><p>Since we use the preprocessed visual features, the pretrained CNN is augmented only for this visualization. Note that model (b) in <ref type="table" target="#tab_0">Table 1</ref> is used for this visualization, and the pretrained VGG-19 is used for preprocessing and augmentation. The model is trained using the training set of the VQA dataset, and visualized using the validation set. Examples are shown in <ref type="figure" target="#fig_3">Figure 4</ref> (more examples in Appendix A. <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>.</p><p>Unlike the other works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref> that use explicit attention parameters, MRN does not use any explicit attentional mechanism. However, we observe the interpretability of element-wise multiplication as an information masking, which yields a novel method for visualizing the attention effect from this operation. Since MRN does not depend on a few attention parameters (e.g. 14 × 14), our visualization method shows a higher resolution than others <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28]</ref>. Based on this, we argue that MRN is an implicit attention model without explicit attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>The idea of deep residual learning is applied to visual question-answering tasks. Based on the two observations of the previous works, various alternative models are suggested and validated to propose the three-block layered MRN. Our model achieves the state-of-the-art results on the VQA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we have introduced a novel method to visualize the spatial attention from the collapsed visual features using back-propagation.</p><p>We believe our visualization method brings implicit attention mechanism to research of attentional models. Using back-propagation of attention effect, extensive research in object detection, segmentation and tracking are worth further investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 VQA test-dev Results  <ref type="figure" target="#fig_2">Figure 3a</ref> is used, since these experiments are preliminarily conducted. VGG-19 features and 1k target answers are used. s stands for the usage of Skip-Thought Vectors <ref type="bibr" target="#b11">[12]</ref> to initialize the question embedding model of GRU, b stands for the usage of Bayesian Dropout <ref type="bibr" target="#b4">[5]</ref>, and c stands for the usage of postprocessing using image captioning model <ref type="bibr" target="#b9">[10]</ref>.     (a1) and (a2) depict a giraffe (left) and a man pointing at the giraffe. MRN consistently highlights on the giraffe in (a1). However, the other question "Can you see trees?" makes MRN less attentive to the giraffe, while a tree in the right of background is more focused in (a2). Similarily, the attention effect of (b2) is widely dispersed on background than (b1) in the middle of sequences, may be to recognize the site. However, the subtlety in comparative study is insufficient to objectively assess the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-Ended</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>arXiv:1606.01455v2 [cs.CV] 31 Aug 2016 Inference flow of Multimodal Residual Networks (MRN). Using our visualization method, the attention effects are shown as a sequence of three images. More examples are shown in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A schematic diagram of Multimodal Residual Networks with three-block layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Alternative models are explored to justify our proposed model. The base model (a) has a shortcut for a question vector as SAN does<ref type="bibr" target="#b28">[29]</ref>, and the joint residual function takes the form of the Deep Q+I model's joint function<ref type="bibr" target="#b14">[15]</ref>.(b) extra embedding for visual modality. (c) extra embeddings for both modalities. (d) identity mappings for shortcuts. In the first learning block, use a linear mapping for matching a dimension with the joint dimension. (e) two shortcuts for both modalities. For simplicity, the linear mapping of visual shortcut only appears in the first learning block. Notice that (d) and (e) are compared to (b) after the model selection of (b) among (a)-(c) on test-dev results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples for visualization of the three-block layered MRN. The original images are shown in the first of each group. The next three images show the input gradients of the attention effect for each learning block as described in Section 5.2. The gradients of color channels for each pixel are summed up after taking absolute values of these gradients. Then, these summed absolute values which are greater than the summation of the mean and the standard deviation of these values are visualized as the attention effect (bright color) on the images. The answers (blue) are predicted by MRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>More examples of Figure 4 in Section 5.2. A.3 Comparative Analysis (a1) What is the animal on the left ? giraffe (a2) Can you see trees ? yes (b1) What is the lady riding ? motorcycle (b2) Is she riding the motorcycle on the street ? no</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparative examples on the same image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The results of alternative models (a)-(e) on the test-dev.</figDesc><table><row><cell></cell><cell>Open-Ended</cell></row><row><cell>All</cell><cell>Y/N Num. Other</cell></row><row><cell cols="2">(a) 60.17 81.83 38.32 46.61</cell></row><row><cell cols="2">(b) 60.53 82.53 38.34 46.78</cell></row><row><cell cols="2">(c) 60.19 81.91 37.87 46.70</cell></row><row><cell cols="2">(d) 59.69 81.67 37.23 46.00</cell></row><row><cell cols="2">(e) 60.20 81.98 38.25 46.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The effect of the visual features and # of target answers on the test-dev results. Vgg for VGG-19, and Res for ResNet-152 features described in Section 4. Vgg, 1k 60.53 82.53 38.34 46.78 Vgg, 2k 60.77 82.10 39.11 47.46 Vgg, 3k 60.68 82.40 38.69 47.10 Res, 1k 61.45 82.36 38.40 48.81 Res, 2k 61.68 82.28 38.82 49.25 Res, 3k 61.47 82.28 39.09 48.76 The questions and answers of the VQA dataset are collected via Amazon Mechanical Turk from human subjects, who satisfy the experimental requirement. The dataset includes 614,163 questions and</figDesc><table><row><cell></cell><cell>Open-Ended</cell></row><row><cell>All</cell><cell>Y/N Num. Other</cell></row></table><note>7,984,119 answers, since ten answers are gathered for each question from unique human subjects. Therefore, Antol et al. [2] proposed a new accuracy metric as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The VQA test-standard results. The precision of some accuracies<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref> are one less than others, so, zero-filled to match others.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell></cell><cell cols="2">Multiple-Choice</cell><cell></cell></row><row><cell></cell><cell>All</cell><cell cols="3">Y/N Num. Other</cell><cell>All</cell><cell cols="3">Y/N Num. Other</cell></row><row><cell>DPPnet [19]</cell><cell cols="8">57.36 80.28 36.92 42.24 62.69 80.35 38.79 52.79</cell></row><row><cell>D-NMN [1]</cell><cell>58.00</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep Q+I [15]</cell><cell cols="8">58.16 80.56 36.53 43.73 63.09 80.59 37.70 53.64</cell></row><row><cell>SAN [29]</cell><cell>58.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ACK [27]</cell><cell cols="4">59.44 81.07 37.12 45.83</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FDA [8]</cell><cell cols="8">59.54 81.34 35.67 46.10 64.18 81.25 38.30 55.20</cell></row><row><cell>DMN+ [28]</cell><cell cols="4">60.36 80.43 36.82 48.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MRN</cell><cell cols="8">61.84 82.39 38.23 49.41 66.33 82.41 39.57 58.40</cell></row><row><cell>Human [2]</cell><cell cols="4">83.30 95.77 83.39 72.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>overall accuracy (0.3% for Other type)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The effects of various options for VQA test-dev. Here, the model of</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Other baseline 58.97 81.11 37.63 44.90 63.53 81.13 38.91 54.06 s 59.38 80.65 38.30 45.98 63.71 80.68 39.73 54.65 s,b 59.74 81.75 38.13 45.84 64.15 81.77 39.54 54.67 s,b,c 59.91 81.75 38.13 46.19 64.18 81.77 39.51 54.72</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Multiple-Choice</cell></row><row><cell>All</cell><cell>Y/N Num. Other</cell><cell>All</cell><cell>Y/N Num.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The results for VQA test-dev. The precision of some accuracies<ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28]</ref> are one less than others, so, zero-filled to match others. 78.20 35.68 26.59 54.75 78.22 36.82 38.78 LSTM Q+I [2] 53.74 78.94 35.24 36.42 57.17 78.95 35.80 43.41 Deep Q+I [15] 58.02 80.87 36.46 43.40 62.86 80.88 37.78 53.14 82.53 38.34 46.78 64.79 82.55 39.93 55.23 Vgg, 2k 60.77 82.10 39.11 47.46 65.27 82.12 40.84 56.39 Vgg, 3k 60.68 82.40 38.69 47.10 65.09 82.42 40.13 55.93 Res, 1k 61.45 82.36 38.40 48.81 65.62 82.39 39.65 57.15 Res, 2k 61.68 82.28 38.82 49.25 66.15 82.30 40.45 58.16 Res, 3k 61.47 82.28 39.09 48.76 66.33 82.41 39.57 58.40</figDesc><table><row><cell></cell><cell></cell><cell>Open-Ended</cell><cell></cell><cell cols="2">Multiple-Choice</cell></row><row><cell></cell><cell>All</cell><cell>Y/N Num. Other</cell><cell>All</cell><cell cols="3">Y/N Num. Other</cell></row><row><cell>Question [2]</cell><cell cols="6">48.09 75.66 36.70 27.14 53.68 75.71 37.05 38.64</cell></row><row><cell>Image [2]</cell><cell cols="6">28.13 64.01 00.42 03.77 30.53 69.87 00.45 03.76</cell></row><row><cell>Q+I [2]</cell><cell cols="6">52.64 75.55 33.67 37.37 58.97 75.59 34.35 50.33</cell></row><row><cell cols="7">LSTM Q [2] 48.76 DPPnet [19] 57.22 80.71 37.24 41.69 62.48 80.79 38.94 52.16</cell></row><row><cell>D-NMN [1]</cell><cell cols="2">57.90 80.50 37.40 43.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAN [29]</cell><cell cols="2">58.70 79.30 36.60 46.10</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ACK [27]</cell><cell cols="2">59.17 81.01 38.42 45.23</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FDA [8]</cell><cell cols="6">59.24 81.14 36.16 45.77 64.01 81.50 39.00 54.72</cell></row><row><cell>DMN+ [28]</cell><cell cols="2">60.30 80.50 36.80 48.30</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Vgg, 1k</cell><cell>60.53</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The effects of shortcut connections of MRN for VQA test-dev. ResNet-152 features and 2k target answers are used. MN stands for Multimodal Networks without residual learning, which does not have any shortcut connections. Dim. stands for common embedding vector's dimension. The number of parameters for word embedding (9.3M) and question embedding (21.8M) is subtracted from the total number of parameters in this table. Are all the cows the same color ? no (j) What is the reflection of in the mirror ? dog</figDesc><table><row><cell>Open-Ended</cell></row></table><note>(a) Does the man have good posture ? no (b) Did he fall down ? yes(c) Are there two cats in the picture ? no (d) What color are the bears ? brown(e) What are many of the people carrying ? umbrellas (f) What color is the dog ? black(g) Are these animals tall ? yes (h) What animal is that ? sheep(i)(k) What are the giraffe in the foreground doing ? eating(l) What animal is standing in the water other than birds ? bear</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Patrick Emaase for helpful comments and editing. This work was supported by Naver Corp. and partly by the Korea government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. <ref type="bibr" target="#b3">4</ref>   <ref type="figure">Figure 7</ref>: Failure Examples. Each question is followed by model prediction (blue) and answer (red). As mentioned in Section 5, MRN shows the weakness of counting in (d) and (k). Sometimes, the model finds objects regardless of the given question. In (j), even if a word cat does not appear in the question, the cat in the image is surely attended. (i) shows the limitation of attentional mechanism, which needs an inference using world knowledge.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to Compose Neural Networks for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01705</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="arXiv">arXiv:1512.05287</idno>
		<title level="m">Yarin Gal. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Focused Dynamic Attention Model for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KIIS Spring Conference</title>
		<meeting>KIIS Spring Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="165" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Skip-Thought Vectors. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Waghmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07889</idno>
		<title level="m">rnn : Recurrent Library for Torch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeper LSTM and normalized CNN Visual Question Answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01121</idno>
		<title level="m">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th International Conference on Machine Learning</title>
		<meeting>The 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">9781450306195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05756</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring Models and Data for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning about Entailment with Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal Learning with Deep Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dynamic Memory Networks for Visual and Textual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01417</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
