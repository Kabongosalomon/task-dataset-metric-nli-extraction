<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
							<email>hqchao16@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
							<email>jpzhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
							<email>jffeng@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Science and Technology for Brain-inspired Intelligence</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a unique biometric feature that can be recognized at a distance, gait has broad applications in crime prevention, forensic identification and social security. To portray a gait, existing gait recognition methods utilize either a gait template, where temporal information is hard to preserve, or a gait sequence, which must keep unnecessary sequential constraints and thus loses the flexibility of gait recognition. In this paper we present a novel perspective, where a gait is regarded as a set consisting of independent frames. We propose a new network named GaitSet to learn identity information from the set. Based on the set perspective, our method is immune to permutation of frames, and can naturally integrate frames from different videos which have been filmed under different scenarios, such as diverse viewing angles, different clothes/carrying conditions. Experiments show that under normal walking conditions, our single-model method achieves an average rank-1 accuracy of 95.0% on the CASIA-B gait dataset and an 87.1% accuracy on the OU-MVLP gait dataset. These results represent new state-of-the-art recognition accuracy. On various complex scenarios, our model exhibits a significant level of robustness. It achieves accuracies of 87.2% and 70.4% on CASIA-B under bag-carrying and coat-wearing walking conditions, respectively. These outperform the existing best methods by a large margin. The method presented can also achieve a satisfactory accuracy with a small number of frames in a test sample, e.g., 82.5% on CASIA-B with only 7 frames. The source code has been released at https://github.com/AbnerHqC/GaitSet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike other biometrics such as face, fingerprint and iris, gait is a unique biometric feature that can be recognized at a distance without the cooperation of subjects and intrusion to them. Therefore, it has broad applications in crime prevention, forensic identification and social security.</p><p>However, gait recognition suffers from exterior factors such as the subject's walking speed, dressing and carrying condition, and the camera's viewpoint and frame rate. There are two main ways to identify gait in literature, i.e., regarding gait as an image and regarding gait as a video sequence. The first category compresses all gait silhouettes into one image, or gait template for gait recognition <ref type="bibr" target="#b3">(He et al. 2019;</ref><ref type="bibr" target="#b7">Takemura et al. 2018a;</ref><ref type="bibr" target="#b4">Wu et al. 2017;</ref><ref type="bibr" target="#b4">Hu et al. 2013</ref>). Simple and easy to implement, gait template easily loses temporal and fine-grained spatial information. Differently, the second category extracts features directly from the original gait silhouette sequences in recent years <ref type="bibr" target="#b5">(Liao et al. 2017;</ref><ref type="bibr" target="#b7">Wolf, Babaee, and Rigoll 2016)</ref>. However, these methods are vulnerable to exterior factors. Further, deep neural networks like 3D-CNN for extracting sequential information are harder to train than those using a single template like Gait Energy Image (GEI) <ref type="bibr" target="#b2">(Han and Bhanu 2006)</ref>.</p><p>To solve these problems, we present a novel perspective which regards gait as a set of gait silhouettes. As a periodic motion, gait can be represented by a single period. In a silhouette sequence containing one gait period, it was observed that the silhouette in each position has unique appearance, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Even if these silhouettes are shuffled, it is not difficult to rearrange them into correct order only by observing the appearance of them. Thus, we assume the appearance of a silhouette has contained its position information. With this assumption, order information of gait sequence is not necessary and we can directly regard gait as a set to extract temporal information. We propose an end-to-end deep learning model called GaitSet whose scheme is shown in <ref type="figure">Fig. 2</ref>. The input of our model is a set of gait silhouettes. First, a CNN is used to extract frame-level features from each silhouette independently. Second, an operation called Set Pooling is used to aggregate frame-level features into a single set-level feature. Since this operation is applied on high-level feature maps instead of the original silhouettes, it can preserve spatial and temporal information better than gait template. This will be justified by the experiment in Sec. 4.3. Third, a structure called Horizontal Pyramid Mapping is used to map the set-level feature into a more discriminative space to obtain the final representation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will give a brief survey on gait recognition and set-based deep learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gait Recognition</head><p>Gait recognition can be grouped into template-based and sequence-based categories. Approaches in the former category first obtain human silhouettes of each frame by background subtraction. Second, they generate a gait template by rendering pixel level operators on the aligned silhouettes <ref type="bibr" target="#b2">(Han and Bhanu 2006;</ref><ref type="bibr" target="#b7">Wang et al. 2012)</ref>. Third, they extract the representation of the gait by machine learning approaches such as Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b8">(Xing et al. 2016)</ref>, Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b0">(Bashir, Xiang, and Gong 2010)</ref> and deep learning <ref type="bibr" target="#b6">(Shiraga et al. 2016)</ref>. Fourth, they measure the similarity between pairs of representations by Euclidean distance or some metric learning approaches <ref type="bibr" target="#b4">(Wu et al. 2017;</ref><ref type="bibr" target="#b7">Takemura et al. 2018a</ref>). Finally, they assign a label to the template by some classifier, e.g., nearest neighbor classifier. Previous works generally divides this pipeline into two parts, template generation and matching. The goal of generation is to compress gait information into a single image, e.g., Gait Energy Image (GEI) <ref type="bibr" target="#b2">(Han and Bhanu 2006)</ref> and Chrono-Gait Image (CGI) <ref type="bibr" target="#b7">(Wang et al. 2012)</ref>. In template matching approaches, View Transformation Model (VTM) learns a projection between different views <ref type="bibr" target="#b5">(Makihara et al. 2006)</ref>. <ref type="bibr" target="#b4">(Hu et al. 2013)</ref> proposed View-invariant Discriminative Projection (ViDP) to project the templates into a latent space to learn a view-invariance representation. Recently, as deep learning performs well on various generation tasks, it has been employed on gait recognition task <ref type="bibr" target="#b9">(Yu et al. 2017a;</ref><ref type="bibr" target="#b3">He et al. 2019;</ref><ref type="bibr" target="#b7">Takemura et al. 2018a;</ref><ref type="bibr" target="#b6">Shiraga et al. 2016;</ref><ref type="bibr" target="#b10">Yu et al. 2017b;</ref><ref type="bibr" target="#b4">Wu et al. 2017)</ref>.</p><p>As the second category, video-based approaches directly take a sequence of silhouettes as input. Based on the way of extracting temporal information, they can be classified into LSTM-based approaches <ref type="bibr" target="#b5">(Liao et al. 2017</ref>) and 3D CNN-based approaches <ref type="bibr" target="#b7">(Wolf, Babaee, and Rigoll 2016;</ref><ref type="bibr" target="#b4">Wu et al. 2017)</ref>. The advantages of these approaches are that 1) focusing on each silhouette, they can obtain more comprehensive spatial information. 2) They can gather more temporal information because specialized structures are utilized to extract sequential information. However, The price to pay for these advantages is high computational cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning on Unordered set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GaitSet</head><p>In this section, we describe our method for learning discriminative information from a set of gait silhouettes. The overall pipeline is illustrated in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We begin with formulating our concept of regarding gait as a set. Given a dataset of N people with identities y i , i ∈ 1, 2, ..., N , we assume the gait silhouettes of a certain person subject to a distribution P i which is only related to its identity. Therefore, all silhouettes in one or more sequences of a person can be regarded as a set of n silhouettes X i = {x j i |j = 1, 2, ..., n}, where x j i ∼ P i . Under this assumption, we tackle the gait recognition task through 3 steps, formulated as</p><formula xml:id="formula_0">f i = H(G(F (X i )))</formula><p>(1) where F is a convolutional network aims to extract framelevel features from each gait silhouette. The function G is a permutation invariant function used to map a set of framelevel feature to a set-level feature <ref type="bibr" target="#b12">(Zaheer et al. 2017)</ref>. It is implemented by an operation called Set Pooling (SP) which will be introduced in Sec. 3.2. The function H is used to learn the discriminative representation of P i from the setlevel feature. This function is implemented by a structure called Horizontal Pyramid Mapping (HMP) which will be discussed in Sec. 3.3. The input X i is a tensor with four dimensions, i.e. set dimension, image channel dimension, image hight dimension, and image width dimension.  <ref type="figure">Figure 2</ref>: The framework of GaitSet. 'SP' represents Set Pooling. Trapezoids represent convolution and pooling blocks and those in the same column have the same configurations which are shown by rectangles with capital letters. Note that although blocks in MGP have same configurations with those in the main pipeline, parameters are only shared across blocks in the main pipeline but not with those in MGP. HPP represents horizontal pyramid pooling <ref type="bibr" target="#b1">(Fu et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Set Pooling</head><p>The goal of Set Pooling (SP) is to aggregate gait information of elements in a set, formulated as z = G(V ), where z denotes the set-level feature and V = {v j |j = 1, 2, ..., n} denotes the frame-level features. There are two constraints in this operation. First, to take set as an input, it should be a permutation invariant function which is formulated as:</p><formula xml:id="formula_1">G({v j |j = 1, 2, ..., n}) = G({v π(j) |j = 1, 2, ..., n}) (2)</formula><p>where π is any permutation <ref type="bibr" target="#b12">(Zaheer et al. 2017)</ref>. Second, since in real-life scenario the number of a person's gait silhouettes can be arbitrary, the function G should be able to take a set with arbitrary cardinality. Next, we describe several instantiations of G. It will be shown in the experiments that although different instantiations of SP do have sort of influence on the performances, they do not differ greatly and all of them exceed GEI-based methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical Functions</head><p>To meet the requirement of invariant constraint in Equ. 2, a natural choice of SP is to apply statistical functions on the set dimension. Considering the representativeness and the computational cost, we studied three statistical functions: max(·), mean(·) and median(·). The comparison will be shown in Sec. 4.3.</p><p>Joint Functions We also studied two ways to join 3 statistical functions mentioned above:</p><formula xml:id="formula_2">G(·) = max(·) + mean(·) + median(·) (3) G(·) = 1 1C(cat(max(·), mean(·), median(·)))<label>(4)</label></formula><p>where cat means concatenate on the channel dimension, 1 1C means 1 × 1 convolutional layer, and max, mean and median are applied on set dimension. Equ. 4 is an enhanced version of Equ. 3 where the 1 × 1 convolutional layer can learn a proper weight to combine information extracted by different statistical functions. Attention Since visual attention was successfully applied in lots of tasks <ref type="bibr" target="#b7">(Wang et al. 2018b;</ref><ref type="bibr" target="#b8">Xu et al. 2015;</ref><ref type="bibr" target="#b5">Li, Zhu, and Gong 2018)</ref>, we use it to improve the performance of SP. Its structure is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The main idea is to utilize the global information to learn an element-wise attention map for each frame-level feature map to refine it. Global information is first collected by the statistical functions in the left. Then it is fed into a 1 × 1 convolutional layer along with the original feature map to calculate an attention for the refinement. The final set-level feature z will be extracted by employing MAX on the set of the refined frame-level feature maps. The residual structure can accelerate and stabilize the convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Horizontal Pyramid Mapping</head><p>In literature, splitting feature map into strips is commonly used in person re-identification task <ref type="bibr" target="#b7">(Wang et al. 2018a;</ref><ref type="bibr" target="#b1">Fu et al. 2018</ref>). The images are cropped and resized into uniform size according to pedestrian size whereas the discriminative parts vary from image to image. <ref type="bibr" target="#b1">(Fu et al. 2018)</ref> proposed Horizontal Pyramid Pooling (HPP) to deal with it. HPP has 4 scales and thus can help the deep network focus on features with different sizes to gather both local and global information. We improve HPP to make it adapt better for gait recognition task. Instead of applying a 1 × 1 convolutional layer after the pooling, we use independent fully connect layers (FC) for each pooled feature to map it into the discriminative space, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We call it Horizontal Pyramid Mapping (HPM). Specifically, HPM has S scales. On scale s ∈ 1, 2, ..., S, the feature map extracted by SP is split into 2 s−1 strips on height dimension, i.e. S s=1 2 s−1 strips in total. Then a Global Pooling is applied to the 3-D strips to get 1-D features. For a strip z s,t where t ∈ 1, 2, ..., 2 s−1 stands index of the strip in the scale, the Global Pooling is formulated as f s,t = maxpool(z s,t ) + avgpool(z s,t ), where maxpool and avgpool denote Global Max Pooling and Global Average Pooling respectively. Note that the functions maxpool and avgpool are used at the same time because it outperforms applying anyone of them alone. The final step is to employ FCs to map the features f into a discriminative space. Since strips in different scales depict features of different receptive fields, and different strips in each scales depict features of different spatial positions, it comes naturally to use independent FCs, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multilayer Global Pipeline</head><p>Different layers of a convolutional network have different receptive fields. The deeper the layer is, the larger the receptive field will be. Thus, pixels in feature maps of a shallow layer focus on local and fine-grained information while those in a deeper layer focus on more global and coarse-grained information. The set-level features extracted by applying SP on different layers have analogical property. As shown in the main pipeline of <ref type="figure">Fig. 2</ref>, there is only one SP on the last layer of the convolutional network. To collect various-level set information, Multilayer Global Pipeline (MGP) is proposed. It has a similar structure with the convolutional network in the main pipeline and the set-level features extracted in different layers are added to MGP. The final feature map generated by MGP will also be mapped into S s=1 2 s−1 features by HPM. Note that the HPM after MGP does not share parameters with the HPM after the main pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training And Testing</head><p>Training Loss As aforementioned, the output of the network is 2 × S s=1 2 s−1 features with dimension d. The corresponding features among different samples will be used to compute the loss. In this paper, Batch All (BA + ) triplet loss is employed to train the network <ref type="bibr" target="#b4">(Hermans, Beyer, and Leibe 2017)</ref>. A batch with size of p × k is sampled from the training set where p denotes the number of persons and k denotes the number of training samples each person has in the batch. Note that although the experiment shows that our model performs well when it is fed with the set composed by silhouettes gathered from arbitrary sequences, a sample used for training is actually composed by silhouettes sampled in one sequence.</p><p>Testing Given a query Q, the goal is to retrieve all the sets with the same identity in gallery set G. Denote the sample in G as G. The Q is first put into GaitSet net to generate multiscale features, followed by concatenating all these features into a final representations F Q as shown in <ref type="figure">Fig. 2</ref>. The same process is applied on each G to get F G . Finally, F Q is compared with every F G using Euclidean distance to calculate Rank 1 recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our empirical experiments mainly contain three parts. The first part compares GaitSet with other state-of-the-art methods on two public gait datasets: CASIA-B <ref type="bibr" target="#b11">(Yu, Tan, and Tan 2006)</ref> and OU-MVLP <ref type="bibr" target="#b7">(Takemura et al. 2018b</ref>). The Second part is ablation experiments conducted on CASIA-B. In the third part, we investigated the practicality of GaitSet in three aspects: the performance on limited silhouettes, multiple views and multiple walking conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Training Details</head><p>CASIA-B dataset <ref type="bibr" target="#b11">(Yu, Tan, and Tan 2006</ref>) is a popular gait dataset. It contains 124 subjects (labeled in 001-124), 3 walking conditions and 11 views (0 • , 18 • , ..., 180 • ). The walking condition contains normal (NM) (6 sequences per subject), walking with bag (BG) (2 sequences per subject) and wearing coat or jacket (CL) (2 sequences per subject). Namely, each subject has 11 × (6 + 2 + 2) = 110 sequences. As there is no official partition of training and test sets of this dataset, we conduct experiments on three settings which are popular in current literatures. We name these three settings as small-sample training (ST), medium-sample training (MT) and large-sample training (LT). In ST, the first 24 subjects (labeled in 001-024) are used for training and the rest 100 subjects are leaved for test. In MT, the first 62 subjects are used for training and the rest 62 subjects are leaved for test. In LT, the first 74 subjects are used for training and the rest 50 subjects are leaved for test. In the test sets of all three settings, the first 4 sequences of the NM condition (NM #1-4) are kept in gallery, and the rest 6 sequences are divided into 3 probe subsets, i.e. NM subsets containing NM #5-6, BG subsets containing BG #1-2 and CL subsets containing CL #1-2.</p><p>OU-MVLP dataset <ref type="bibr" target="#b7">(Takemura et al. 2018b</ref>) is so far the world's largest public gait dataset. It contains 10,307 subjects, 14 views (0 • , 15 • , ..., 90 • ; 180 • , 195 • , ..., 270 • ) per subject and 2 sequences (#00-01) per view. The sequences are divided into training and test set by subjects (5153 subjects for training and 5154 subjects for test). In the test set, sequences with index #01 are kept in gallery and those with index #00 are used as probes. Training Details In all the experiments, the input is a set of aligned silhouettes in size of 64 × 44. The silhouettes are directly provided by the datasets and are aligned based on methods in <ref type="bibr" target="#b7">(Takemura et al. 2018b</ref>). The set cardinality in the training is set to be 30. Adam is chosen as an optimizer <ref type="bibr" target="#b4">(Kingma and Ba 2015)</ref>. The number of scales S in HPM is set as 5. The margin in BA + triplet loss is set as 0.2. The models are trained with 8 NVIDIA 1080TI GPUs. 1) In CASIA-B, the mini-batch is composed by the manner introduced in Sec. 3.5 with p = 8 and k = 16. We set the number of channels in C1 and C2 as 32, in C3 and C4 as 64 and in C5 and C6 as 128. Under this setting, the average computational complexity of our model is 8.6GFLOPs. The learning rate is set to be 1e − 4. For ST, we train our model for 50K iterations. For MT, we train it for 60K iterations. For LT, we train it for 80K iterations.</p><p>2) In OU-MVLP, since it contains 20 times more sequences than CASIA-B, we use convolutional layers with more channels (C1 = C2 = 64, C3 = C4 = 128, C5 = C6 = 256) and train it with larger batch size (p = 32, k = 16). The learning rate is 1e − 4 in the first 150K iterations, and then is changed into 1e − 5 for the rest of 100K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>CASIA-B Tab. 1 shows the comparison between the stateof-the-art methods 1 and our GaitSet. Except of ours, other results are directly taken from their original papers. All the results are averaged on the 11 gallery views and the identical views are excluded. For example, the accuracy of probe view 36 • is averaged on 10 gallery views, excluding gallery view 36 • . An interesting pattern between views and accuracies can be observed in Tab. 1. Besides 0 • and 180 • , the accuracy of 90 • is a local minimum value. It is always worse than that of 72 • or 108 • . The possible reason is that gait information contains not only those parallel to the walking direction like stride which can be observed most clearly at 90 • , but also those vertical to the walking direction like a left-right swinging of body or arms which can be observed most clearly at 0 • or 180 • . So, both parallel and vertical perspectives lose some part of gait information while views like 36 • or 144 • can obtain most of it.</p><p>Small-Sample Training (ST) Our method achieves a high performance even with only 24 subjects in the training set and exceed the best performance reported so far (Wu et al. 2017) over 10 percent on the views they reported. There are mainly two reasons. 1) As our model regards the input as a set, images used to train the convolution network in the main pipeline are dozens of times more than those models based on gait templates. Taking a mini-batch for an example, our model is fed with 30 × 128 = 3840 silhouettes while under the same batch size models using gait templates can only get 128 templates. 2) Since the sample sets used in training phase are composed by frames selected randomly from the sequence, each sequence in the training set can generate multiple different sets. Thus any units related to set feature learning like MGP and HPM can also be trained well.</p><p>Medium-Sample Training (MT) &amp; Large-Sample Training (LT) Tab. 1 shows that our model obtains very nice results on the NM subset, especially on LT where results of all views except 180 • are over 90%. On the BG and CL subsets, although the accuracies of some views like 0 • and 180 • are still not high, the mean accuracies of our model exceed those of other models for at least 18.8%.</p><p>OU-MVLP Tab. 3 shows our results. As some of the previous works did not conduct experiments on all 14 views, we list our results on two kinds of gallery sets, i.e. all 14  It is note worthy that since some subjects miss several gait sequences and we did not remove them from the probe, the maximum of rank-1 accuracy cannot reach 100%. If we ignore the cases which have no corresponding samples in the gallery, the average rank-1 accuracy of all probe views is 93.3% rather than 87.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments</head><p>Tab. 2 shows the thorough results of ablation experiments. The effectiveness of every innovation in Sec. 3 is studied. Set VS. GEI The first two lines of Tab. 2 show the effectiveness of regarding gait as a set. With fully identical networks, the result of using set exceeds that of using GEI by more than 10% on NM subset and more than 25% on CL subset. The only difference is that in GEI experiment, gait silhouettes are averaged into a single GEI before being fed into the network. There are mainly two reasons for this phenom-enal improvement. 1) Our SP extracts the set-level feature based on high-level feature map where temporal information can be well preserved and spatial information has been sufficiently processed. 2) As mentioned in Sec. 4.2, regarding gait as a set enlarges the volume of training data.</p><p>Impact of SP In Tab. 2, the results from the third line to the eighth line show the impact of different SP strategies. SP with attention, 1 × 1 convolution (1 1C) joint function and max(·) obtain the highest accuracy on the NM, BG, and CL subsets respectively. Considering SP with max(·) also achieved the second best performance on the NM and BG subset and has the most concise structure, we choose it as SP in the final version of GaitSet.</p><p>Impact of HPM and MGP The second and the third lines of Tab. 2 compare the impact of independent weight in HPM. It can be seen that using independent weight improves the accuracy by about 2% on each subset. In the experiments, we also find out that the introduction of independent weight helps the network converge faster. The last two lines of Tab. 2 show that MGP can bring improvement on all three test subsets. This result is consistent the theory mentioned in Sec. 3.4 that set-level features extracted from different layers of the main pipeline contain different valuable information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Practicality</head><p>Due to the flexibility of set, GaitSet has great potential in more complicated practical conditions. In this section, we investigate the practicality of GaitSet through three novel scenarios. 1) How will it perform when the input set only contains a few silhouettes? 2) Can silhouettes with different views enhance the identification accuracy? 3) Whether can the model effectively extract discriminative representation from a set containing silhouettes shot under different walking conditions. It is worth noting that we did not retrain our model in these experiments. It is fully identical to that in Sec. 4.2 with setting LT. Note that, all the experiments containing random selection in this section are ran for 10 times and the average accuracies are reported.</p><p>Limited Silhouettes In real forensic identification scenarios, there are cases that we do not have a continuous sequence of a subject's gait but only some fitful and sporadic silhouettes. We simulate such a circumstance by randomly selecting a certain number of frames from sequences to compose each sample in both gallery and probe. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the relationship between the number of silhouettes in each input set and the rank-1 accuracy averaged on all 11 probe views. Our method attains an 82% accuracy with only 7 silhouettes. The result also indicates that our model makes full use of the temporal information of gait. Since 1) the accuracy rises monotonically with the increase of the number of silhouettes.</p><p>2) The accuracy is close to the best performance when the samples contain more than 25 silhouettes. This number is consistent with the number of frames that one gait period contains.</p><p>Multiple Views There are conditions that different views of one person's gait can be gathered. We simulate these scenarios by constructing each sample with silhouettes selected from two sequences with the same walking condition but different views. To eliminate the effects of silhouette number, we also conduct an experiment in which the silhouette number is limited to 10. Specifically, in the contrast experiments of single view, an input set is composed by 10 silhouettes from one sequence. In the two-view experiment, an input set is composed by 5 silhouettes from each of two sequences. Note that in this experiment, only probe samples are composed by the way discussed above, whereas sample in the gallery is composed by all silhouettes from one sequence. Tab. 4 shows the results. As there are too many view pairs to be shown, we summarize the results by averaging accuracies of each possible view difference. For example, the result of 90 • difference is averaged by accuracies of 6 view pairs (0 • &amp;90 • , 18 • &amp;108 • , ..., 90 • &amp;180 • ). Further, the 9 view differences are folded at 90 • and those larger than 90 • are averaged with the corresponding view differences less than 90 • . For example, the results of 18 • view difference are averaged with those of 162 • view difference. It can be seen that our model can aggregate information from different views and boost the performance. This can be explained by the pattern between views and accuracies that we have discussed in Sec. 4.2. Containing multiple views in the input set can let the model gather both parallel and vertical information, resulting in performance improvement.</p><p>Multiple Walking Conditions In real life, it is highly possible that gait sequences of the same person are under different walking conditions. We simulate such a condition by forming input set with silhouettes from two sequences with same view but different walking conditions. We conduct experiments with different silhouette number constraints. Note that in this experiment, only probe samples are composed by the way discussed above. Any sample in the gallery is constituted by all silhouettes from one sequence. What's more, the probe-gallery division of this experiment is different. For each subject, sequences NM #02, BG #02 and CL #02 are kept in the gallery and sequences NM #01, BG #01 and CL #01 are used as probe. Tab. 5 shows the results. First, the accuracies will still be boosted with the increase of silhouette number. Second, when the number of silhouettes are fixed, the results reveal relationships between different walking conditions. Silhouettes of BG and CL contain massive but different noises, which makes them complementary with each other. Thus, their combination can improve the accuracy. However, silhouettes of NM contain few noises, so substituting some of them with silhouettes of other two conditions cannot bring extra information but only noises and can decrease the accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a novel perspective that regards gait as a set and thus proposed a GaitSet approach. The GaitSet can extract both spatial and temporal information more effectively and efficiently than those existing methods regarding gait as a template or sequence. It also provide a novel way to aggregate valuable information from different sequences to enhance the recognition accuracy. Experiments on two benchmark gait datasets has indicated that compared with other state-of-the-art algorithms, GaitSet achieves the highest recognition accuracy, and reveals a wide range of flexibility on various complex environments, showing a great potential in practical applications. In the future, we will investigate a more effective instantiation for Set Pooling (SP) and further improve the performance in complex scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>From top-left to bottom-right are silhouettes of a completed period of a subject in CASIA-B gait dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Most works in deep learning focus on regular input representations like sequence and images. The concept of unordered set is first introduced into computer vision by (Charles et al. 2017) (PointNet) to tackle point cloud tasks. Using unordered set, PointNet can avoid the noise and the extension of data caused by quantization, and obtain a high performance. Since then, set-based methods have been wildly used in point cloud field (Wang et al. 2018c; Zhou and Tuzel 2018; Qi et al. 2017). Recently, such methods are introduced into computer vision domains like content recommendation (Hamilton, Ying, and Leskovec 2017) and image captioning (Krause et al. 2017) to aggregate features in a form of a set. (Zaheer et al. 2017) further formalized the deep learning tasks defined on sets and characterizes the permutation invariant functions. To the best of our knowledge, it has not been employed in gait recognition domain up to now.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The structure of Set Pooling (SP) using attention. 1 1C and cat represents 1 × 1 convolutional layer and concatenate respectively. The multiplication and the addition are both pointwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The structure of Horizontal Pyramid Mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Average rank-1 accuracies with constraints of silhouette volume on CASIA-B using setting LT. Accuracies are averaged on all 11 views excluding identical-view cases, and the final reported results are averaged across 10 times experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The superiorities of the proposed method are summarized as follows: • Flexible Our model is pretty flexible since there are no any constraints on the input of our model except the size of the silhouette. It means that the input set can contain any number of non-consecutive silhouettes filmed under different viewpoints with different walking conditions.</figDesc><table><row><cell>Related experiments are shown in Sec. 4.4</cell></row><row><cell>• Fast Our model directly learns the representation of gait</cell></row><row><cell>instead of measuring the similarity between a pair of gait</cell></row><row><cell>templates or sequences. Thus, the representation of each</cell></row><row><cell>sample needs to be calculated only once, then the recog-</cell></row><row><cell>nition can be completed by calculating the Euclidean dis-</cell></row><row><cell>tance between representations of different samples.</cell></row></table><note>• Effective Our model greatly improves the performance on the CASIA-B (Yu, Tan, and Tan 2006) and the OU- MVLP (Takemura et al. 2018b) datasets, showing its strong robustness to view and walking condition variations and high generalization ability to large datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Averaged rank-1 accuracies on CASIA-B under three different experimental settings, excluding identical-view cases.</figDesc><table><row><cell></cell><cell>Probe</cell><cell>Gallery NM#1-4</cell><cell cols="8">0°-180°m ean 0°18°36°54°72°90°108°126°144°162°180°S</cell></row><row><cell></cell><cell></cell><cell>ViDP (Hu et al. 2013)</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>59.1 −</cell><cell>50.2 −</cell><cell>57.5 −</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell></cell><cell></cell><cell>CMCC (Kusakunniran et al. 2014)</cell><cell cols="2">46.3 −</cell><cell>−</cell><cell>52.4 −</cell><cell>48.3 −</cell><cell>56.9 −</cell><cell>−</cell><cell>−</cell><cell>−</cell></row><row><cell>T</cell><cell>NM#5-6</cell><cell>CNN-LB (Wu et al. 2017) GaitSet(ours)</cell><cell cols="8">54.8 − 64.6 83.3 90.4 86.5 80.2 75.5 80.3 86.0 87.1 81.4 59.6 − 77.8 − 64.9 − 76.1 − − −</cell><cell>− 79.5</cell></row><row><cell>(24)</cell><cell>BG#1-2</cell><cell>GaitSet(ours)</cell><cell cols="8">55.8 70.5 76.9 75.5 69.7 63.4 68.0 75.8 76.2 70.7 52.5</cell><cell>68.6</cell></row><row><cell></cell><cell>CL#1-2</cell><cell>GaitSet(ours)</cell><cell cols="8">29.4 43.1 49.5 48.7 42.3 40.3 44.9 47.4 43.0 35.7 25.6</cell><cell>40.9</cell></row><row><cell></cell><cell></cell><cell>AE (Yu et al. 2017b)</cell><cell cols="8">49.3 61.5 64.4 63.6 63.7 58.1 59.9 66.5 64.8 56.9 44.0</cell><cell>59.3</cell></row><row><cell></cell><cell>NM#5-6</cell><cell>MGAN (He et al. 2019) GaitSet(ours)</cell><cell cols="8">54.9 65.9 72.1 74.8 71.1 65.7 70.0 75.6 76.2 68.6 53.8 86.8 95.2 98.0 94.5 91.5 89.1 91.1 95.0 97.4 93.7 80.2</cell><cell>68.1 92.0</cell></row><row><cell></cell><cell></cell><cell>AE (Yu et al. 2017b)</cell><cell cols="8">29.8 37.7 39.2 40.5 43.8 37.5 43.0 42.7 36.3 30.6 28.5</cell><cell>37.2</cell></row><row><cell>MT</cell><cell>BG#1-2</cell><cell>MGAN (He et al. 2019) GaitSet(ours)</cell><cell cols="8">48.5 58.5 59.7 58.0 53.7 49.8 54.0 61.3 59.5 55.9 43.1 79.9 89.8 91.2 86.7 81.6 76.7 81.0 88.2 90.3 88.5 73.0</cell><cell>54.7 84.3</cell></row><row><cell>(62)</cell><cell></cell><cell>AE (Yu et al. 2017b)</cell><cell cols="8">18.7 21.0 25.0 25.1 25.0 26.3 28.7 30.0 23.6 23.4 19.0</cell><cell>24.2</cell></row><row><cell></cell><cell>CL#1-2</cell><cell>MGAN (He et al. 2019) GaitSet(ours)</cell><cell cols="8">23.1 34.5 36.3 33.3 32.9 32.7 34.2 37.6 33.7 26.7 21.0 52.0 66.0 72.8 69.3 63.1 61.2 63.5 66.5 67.5 60.0 45.9</cell><cell>31.5 62.5</cell></row><row><cell></cell><cell></cell><cell>CNN-3D (Wu et al. 2017)</cell><cell cols="8">87.1 93.2 97.0 94.6 90.2 88.3 91.1 93.8 96.5 96.0 85.7</cell><cell>92.1</cell></row><row><cell></cell><cell>NM#5-6</cell><cell>CNN-Ensemble (Wu et al. 2017)</cell><cell cols="8">88.7 95.1 98.2 96.4 94.1 91.5 93.9 97.5 98.4 95.8 85.6</cell><cell>94.1</cell></row><row><cell>LT (74)</cell><cell>BG#1-2</cell><cell>GaitSet(ours) CNN-LB (Wu et al. 2017) GaitSet(ours)</cell><cell cols="8">90.8 97.9 99.4 96.9 93.6 91.7 95.0 97.8 98.9 96.8 85.8 64.2 80.6 82.7 76.9 64.8 63.1 68.0 76.9 82.2 75.4 61.3 83.8 91.2 91.8 88.8 83.3 81.0 84.1 90.0 92.2 94.4 79.0</cell><cell>95.0 72.4 87.2</cell></row><row><cell></cell><cell>CL#1-2</cell><cell>CNN-LB (Wu et al. 2017) GaitSet(ours)</cell><cell cols="8">37.7 57.2 66.6 61.1 55.2 54.6 55.2 59.1 58.9 48.8 39.4 61.4 75.4 80.7 77.3 72.1 70.1 71.5 73.5 73.5 68.4 50.0</cell><cell>54.0 70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation experiments conducted on CASIA-B using setting LT. Results are rank-1 accuracies averaged on all 11 views, excluding identical-view cases. The numbers in brackets indicate the second highest results in each column.</figDesc><table><row><cell>GEI √</cell><cell>Set √ √ √ √ √ √ √ √</cell><cell>Max √ √ √</cell><cell>Mean √</cell><cell>Median √</cell><cell>Set Pooling Joint sum 3 √</cell><cell>Joint 1 1C 4 √</cell><cell>Attention √</cell><cell>HPM weight Shared Independent √ √ √ √ √ √ √ √ √</cell><cell>MGP √</cell><cell>NM 80.4 91.3 93.2 90.0 89.5 92.4 93.3 (93.7) 95.0</cell><cell>BG 68.1 82.3 84.7 79.5 78.1 82.8 (85.7) 84.2 87.2</cell><cell>CL 40.8 67.1 (70.2) 57.1 53.5 63.4 66.3 69.4 70.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">: Averaged rank-1 accuracies on OU-MVLP,</cell></row><row><cell cols="7">excluding identical-view cases. GEINet: (Shiraga et al. 2016).</cell></row><row><cell cols="5">3in+2diff: (Takemura et al. 2018a)</cell><cell></cell></row><row><cell>Probe</cell><cell cols="2">Gallery All 14 Views GEINet Ours</cell><cell>.</cell><cell cols="3">Gallery 0 • , 30 • , 60 • , 90 • GEINet 3in+2diff Ours</cell></row><row><cell>0 •</cell><cell>11.4</cell><cell>79.5</cell><cell></cell><cell>8.2</cell><cell>25.5</cell><cell>77.7</cell></row><row><cell>15 •</cell><cell>29.1</cell><cell>87.9</cell><cell></cell><cell>-</cell><cell>-</cell><cell>86.3</cell></row><row><cell>30 •</cell><cell>41.5</cell><cell>89.9</cell><cell></cell><cell>32.3</cell><cell>50.0</cell><cell>86.9</cell></row><row><cell>45 •</cell><cell>45.5</cell><cell>90.2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>89.1</cell></row><row><cell>60 •</cell><cell>39.5</cell><cell>88.1</cell><cell></cell><cell>33.6</cell><cell>45.3</cell><cell>85.3</cell></row><row><cell>75 •</cell><cell>41.8</cell><cell>88.7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>87.6</cell></row><row><cell>90 •</cell><cell>38.9</cell><cell>87.8</cell><cell></cell><cell>28.5</cell><cell>40.6</cell><cell>83.5</cell></row><row><cell>180 •</cell><cell>14.9</cell><cell>81.7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>80.5</cell></row><row><cell>195 •</cell><cell>33.1</cell><cell>86.7</cell><cell></cell><cell>-</cell><cell>-</cell><cell>82.8</cell></row><row><cell>210 •</cell><cell>43.2</cell><cell>89.0</cell><cell></cell><cell>-</cell><cell>-</cell><cell>87.2</cell></row><row><cell>225 •</cell><cell>45.6</cell><cell>89.3</cell><cell></cell><cell>-</cell><cell>-</cell><cell>86.8</cell></row><row><cell>240 •</cell><cell>39.4</cell><cell>87.2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>85.4</cell></row><row><cell>255 •</cell><cell>40.5</cell><cell>87.8</cell><cell></cell><cell>-</cell><cell>-</cell><cell>85.7</cell></row><row><cell>270 •</cell><cell>36.3</cell><cell>86.2</cell><cell></cell><cell>-</cell><cell>-</cell><cell>85.0</cell></row><row><cell>mean</cell><cell>35.8</cell><cell>87.1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>85.0</cell></row><row><cell cols="7">views and 4 typical views (0 • , 30 • 60 • 90 • ). All the results</cell></row><row><cell cols="7">are averaged on the gallery views and the identical views</cell></row><row><cell cols="7">are excluded. The results show that our methods can gener-</cell></row><row><cell cols="7">alize well on the dataset with such a large scale and wide</cell></row><row><cell cols="7">view variation. Further, since representation for each sample</cell></row><row><cell cols="7">only needs to be calculated once, our model can complete</cell></row><row><cell cols="7">the test (containing 133780 sequences) in only 7 minutes</cell></row><row><cell cols="4">with 8 NVIDIA 1080TI GPUs.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Multi-view experiments conducted on CASIA-B using setting LT. Cases where the probe contains the view of the gallery are excluded.</figDesc><table><row><cell>View difference</cell><cell>18 • / 162 •</cell><cell>36 • / 144 •</cell><cell>54 • / 126 •</cell><cell>72 • / 108 •</cell><cell>90 •</cell><cell>Single view</cell></row><row><cell>All silhouettes</cell><cell>97.0</cell><cell>97.9</cell><cell>98.7</cell><cell>99.1</cell><cell>99.0</cell><cell>95.0</cell></row><row><cell>10 silhouettes</cell><cell>87.9</cell><cell>90.6</cell><cell>92.7</cell><cell>93.7</cell><cell>93.7</cell><cell>87.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Multiple walking condition experiments conducted on CASIA-B using setting LT. Results are rank-1 accuracies averaged on all 11 views, excluding identical-view cases. The numbers in brackets indicate the constraints of silhouette number in each input set.</figDesc><table><row><cell>NM(10)</cell><cell>81.5</cell><cell>NM(10)+BG(10)</cell><cell>87.9</cell><cell>NM(20)</cell><cell>89.8</cell></row><row><cell>BG(10)</cell><cell>77.1</cell><cell>NM(10)+CL(10)</cell><cell>85.8</cell><cell>BG(20)</cell><cell>84.1</cell></row><row><cell>CL(10)</cell><cell>74.4</cell><cell>BG(10)+CL(10)</cell><cell>84.6</cell><cell>CL(20)</cell><cell>82.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> Since (Wu et al. 2017)  proposed more than one model, the most competitive results under different experimental settings are cited.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
	<note>Gait recognition without subject cooperation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fu</surname></persName>
		</author>
		<idno>ArXiv:1804.05275</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Inductive representation learning on large graphs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>and Bhanu</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task GANs for view-specific feature learning in gait recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="113" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">View-invariant discriminative projection for multi-view gait-based human identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beyer</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<idno>ArXiv:1703.07737</idno>
	</analytic>
	<monogr>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="696" to="709" />
		</imprint>
	</monogr>
	<note>IEEE TIP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pose-based temporal-spatial network (ptsn) for gait recognition with carrying and clothing variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mukaigawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Echigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GEINet: View-invariant gait recognition using a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shiraga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICB</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Takemura</surname></persName>
		</author>
		<idno>ArXiv:1801.07829</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
	<note>IEEE TPAMI</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Complete canonical correlation analysis with application to multi-view gait recognition</title>
		<idno>Xu et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Show, attend and tell: Neural image caption generation with visual attention</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GaitGAN: Invariant gait feature extraction using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Invariant feature extraction for gait recognition using only one uniform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan ;</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Voxelnet: End-to-end learning for point cloud based 3D object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
