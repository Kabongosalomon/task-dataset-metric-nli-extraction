<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-21">21 Jul 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen-Smith</surname></persName>
							<email>lachlan.tychsen-smith@data61.csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO</orgName>
								<address>
									<addrLine>Data61) 7 London Circuit</addrLine>
									<postCode>2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
							<email>lars.petersson@data61.csiro.au</email>
							<affiliation key="aff0">
								<orgName type="institution">CSIRO</orgName>
								<address>
									<addrLine>Data61) 7 London Circuit</addrLine>
									<postCode>2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-21">21 Jul 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We define the object detection from imagery problem as estimating a very large but extremely sparse bounding box dependent probability distribution. Subsequently we identify a sparse distribution estimation scheme, Directed Sparse Sampling, and employ it in a single end-to-end CNN based detection model. This methodology extends and formalizes previous state-of-the-art detection models with an additional emphasis on high evaluation rates and reduced manual engineering. We introduce two novelties, a corner based region-of-interest estimator and a deconvolution based CNN model. The resulting model is scene adaptive, does not require manually defined reference bounding boxes and produces highly competitive results on MSCOCO, Pascal VOC 2007 and Pascal VOC 2012 with real-time evaluation rates.  Further analysis suggests our model performs particularly well when finegrained object localization is desirable. We argue that this advantage stems from the significantly larger set of available regions-of-interest relative to other methods. Source-code is available from: https://github.com/lachlants/denet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feed-forward neural networks exhibit good convergence properties given a random initialization under stochastic gradient descent (SGD) and, given an appropriate network design and training regime, can generalize well to previously unseen data <ref type="bibr" target="#b7">[8]</ref>. In particular, convolutional neural networks (CNNs) built from interleaved convolution and pooling layers with ReLU activation functions have set numerous benchmarks in computer vision tasks <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b19">[20]</ref>. A number of methodologies have been developed to map their state-of-the-art dense regression and classification capabilities to the problem of identifying axis aligned bounding boxes of object instances in images. In particular we highlight the relatively slow region based CNN approaches (R-CNN <ref type="bibr" target="#b3">[4]</ref>, Faster R-CNN <ref type="bibr" target="#b14">[15]</ref>) and the more recent work on real-time detection (YOLO <ref type="bibr" target="#b13">[14]</ref>, SSD <ref type="bibr" target="#b11">[12]</ref>  Rather than focusing on obtaining state-of-the-art accuracy in a competition environment (i.e. computationally unconstrained) in this paper we emphasis the dual task of obtaining the best detection performance at a predefined evaluation rate i.e. 60 Hz and 30 Hz. The primary contributions made within this paper include:</p><p>• An improved theoretical understanding of modern detection methods and a generic framework in which to describe them i.e. Directed Sparse Sampling.</p><p>• A novel, fast, region-of-interest estimator which doesn't require manually defined reference bounding boxes.</p><p>• A novel application of deconvolution layers which greatly improves evaluation rates.</p><p>• Six implementations of our method demonstrating competitive detection performance on a range of benchmarks.</p><p>• An easily extended Theano based code release to facilitate the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>In region based CNN detection (R-CNN) <ref type="bibr" target="#b3">[4]</ref> the image is first preprocessed with a region proposal algorithm e.g. selective search <ref type="bibr" target="#b20">[21]</ref>, region proposal network (RPN) <ref type="bibr" target="#b14">[15]</ref>, etc. This algorithm identifies image regions (i.e. bounding boxes) of interest (RoIs) which are then rescaled to fixed dimensions (normalizing scale and aspect ratio) and fed into a CNN based classifier. The CNN assigns a probability that the region bounds an object of interest or the null class and, via linear regression, identifies an improved bounding box. This approach has demonstrated state-of-the-art results, however, it is very expensive to train and evaluate, requiring multiple full CNN evaluations (one per region proposal) and an often expensive pre-processing step. Since the majority of CNN computation occurs in the first few layers, Fast R-CNN <ref type="bibr" target="#b2">[3]</ref> addressed these issues by applying a shallow CNN to the image and then, for each region, extracting fixed sized features from the generated feature map for the final classification. In Faster R-CNN <ref type="bibr" target="#b14">[15]</ref> the region proposal algorithm was integrated into the CNN providing an end-to-end solution, improved timings and demonstrating that both tasks (region proposal and classification) shared similar underlying features. Despite these improvements, to our knowledge, region based CNN's have not been demonstrated operating near real-time frequencies.</p><p>In You Only Look Once (YOLO) <ref type="bibr" target="#b13">[14]</ref> they depart from the algorithmically defined region based approaches described above, opting instead for a predefined, regular grid of detectors. In effect they merged the region classification problem into the region proposal network (RPN) first proposed in Faster R-CNN. With this approach the CNN is only evaluated once to produce the outcomes for all detectors resulting in significantly reduced training and evaluation times. In Single Shot Detector (SSD) <ref type="bibr" target="#b11">[12]</ref> this approach was further refined with an improved network design and training methodology to demonstrate comparable results to the region based methods. We note that the considerable improvements achieved with SSD required scene dependent engineering to manually predefine the most likely set of regions within the image to contain an object, a flaw shared with the Faster R-CNN region proposal network. In particular, SSD demonstrated an improvement of 2.7% MAP <ref type="bibr" target="#b11">[12]</ref> by the addition of four aspect ratios to the predefined regions on the Pascal VOC2007 <ref type="bibr" target="#b0">[1]</ref> dataset, highlighting the importance of manual engineering in modern state-of-theart detector designs. Without going into too much detail, we note that in practice manually engineered solutions typically limit scalability and adaptiveness to different problem sets (without an expensive re-engineering process).</p><p>The primary differentiator between these methods lies in how each method identifies and treats the regions to be classified. R-CNN based methods sample regions sparsely based on an algorithmic preprocessing step and normalize the region of interest while YOLO based approaches perform dense sampling with a manually defined grid of detectors without image normalization. Often dense methods are well suited to current implementations and, therefore offer a significant timing advantage over sparse methods. However, in this work, we demonstrate a novel model design which combines the ease of training, scene adaptability and classification accuracy of the sparse region-based approaches with the fast training and evaluation of the dense non-region based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Probabilistic Object Detection</head><p>We formulate the probabilistic multiclass detection problem as first estimating the distribution Pr(s|B, I) where s ∈ C ∩ {null} is a random variable indicating the presence of an instance of class c ∈ C or the null class (indicating no instances) which is sufficiently bounded by the box B = {x, y, w, h} and I is the input image (omitted in subsequent derivations). This formulation incorporates the assumption that only a single instance of a class can occupy each bounding box. We note that this definition does not seek to perform instance assignment, but can be used as an input to an algorithm that does e.g. Non-Max Suppression.</p><p>Given a suitable neural network design we assert that Pr(s|B) can be estimated from training data with class bounding box annotations. However, since the number of unique bounding boxes is given by |B| ∝ XY W H where (X, Y ) are the number of image positions and (W, H) the range of bounding box dimensions the naive solution quickly becomes intractable. For instance, assuming the most common settings for the ImageNet dataset, 1000 classes and 224 × 224 images, and considering all valid bounding boxes within the image, expressing this distribution requires approximately 629 × 10 9 values or 2.5TB in 32bit float format. Clearly this is an intractable problem with current hardware.</p><p>At the cost of localization accuracy, subsampling the output bounding boxes is a valid approach. For instance, by careful dataset dependent manual engineering, Faster R-CNN and YOLO based approaches subsample the distribution to the order of 10 4 to 10 5 bounding boxs <ref type="bibr" target="#b13">[14]</ref>  <ref type="bibr" target="#b14">[15]</ref>. These boxes are then refined by estimating only the most likely bounding box in a local region via linear regression.</p><p>As an alternative to large scale subsampling, we sought to exploit the fact that, due to occlusion and other factors, we expect a very small subset of bounding boxes to contain class instances other than the null class. Subsequently, we have developed a solution based on the state-of-the-art regression capabilities of a single end-to-end CNN which estimates the highly sparse distribution Pr(s|B) in a real-time (or computationally constrained) operational environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Directed Sparse Sampling (DSS)</head><p>We use the term Directed Sparse Sampling to refer to the method of a applying a jointly optimized two stage CNN where one stage estimates the likely locations where userdefined interesting values occur and the other sparsely classifies the identified values e.g. in R-CNN based models (including R-FCN and DeNet) we estimate the bounding boxes which are most likely to include a non-null class assignment, then run a classifer over these bounding boxs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Corner-based RoI Detector</head><p>Here we introduce the concept of bounding box corner estimation for efficient region-of-interest (RoI) estimation. In our methodology, this task is performed by estimating the likelihood that each position in the image contains an instance of one of 4 corner types i.e. Pr(t|k, y, x) where t is a binary variable indicating the presence of a corner of type k ∈ {top left, top right, bottom left, bottom right} at position (x, y) in the input image. We assert that due to the natural translation invariance of the problem, estimating the corner distribution can be efficiently performed with a standard CNN design trained on bounding box annotated image data (e.g. MSCOCO <ref type="bibr" target="#b10">[11]</ref>, Pascal VOC <ref type="bibr" target="#b0">[1]</ref>, etc).</p><p>With the corner distribution defined we estimate the likelihood that a bounding box B contains an instance by applying a Naive Bayesian Classifier to each corner of the bounding box:</p><formula xml:id="formula_0">Pr(s = null|B) ∝ k Pr(t|k, y k , x k )<label>(1)</label></formula><p>where (x k , y k ) = f k (B) indicates the bounding box position associated with each corner type k. For ease of implementation we define the N × N bounding boxes with the largest non-null probability Pr(s = null|B) as the sampling bounding boxes B S . The user defined variable N balances the maximum number of detections the model can handle with the computational and memory requirements.</p><p>With the potentially non-null bounding boxes estimated, we pass a feature vector of predefined length from the corner detector model to the final classification stages. Therefore, the final classification stage is a function of the form f :ᾱ B → Pr(s|B) whereᾱ B is a feature vector uniquely identified by the sampling bounding box B ∈ B S . It is important that the feature is uniquely associated with each bounding box, otherwise the classifier will have no information to distinguish between bounding boxes with the samē α B . Exactly how to construct the feature vector is still a matter of debate <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> however we constructᾱ B by concatenating together the nearest neighbour sampling features at predefined locations relative to each sampling bounding box (e.g. bounding box corners, center, etc) in addition to the bounding box width and height. The bounding box center position was omitted from the feature vector such that the classifier would be agnostic to image offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training</head><p>During training, the model is initially forward propagated to generate the sampling bounding boxes B S as described in the previous subsection. In addition, we augment the sampling bounding boxes with the ground truth bounding boxes and randomly generated samples. We then propagate the activationsᾱ B associated with the augmented set of sampling bounding boxes through the rest of the model to produce the final classification distribution Pr(s|B S ) and updated bounding box parameters. The set of sampling bounding boxes B S is held constant during gradient estimation to enable end-to-end training, therefore the corner detector network is optimized in conjunction with the bounding box classification and estimation task. Since forward propagation is a necessary preprocessing step in the back propagation based SGD policy typically used to optimize neural networks, the DeNet method introduces no penalty to training time over a standard dense network.</p><p>The DeNet model jointly optimizes over the corner probability distribution, final classification distribution and bounding box regression cost, i.e.</p><formula xml:id="formula_1">Cost = λ t Λ t k,y,x φ(t|k, y, x) ln(Pr(t|k, y, x))+ λ s Λ s B∈BS φ(s|B) ln(Pr(s|B))+ λ b Λ b i SoftL 1 (φ B,i − β i )<label>(2)</label></formula><p>where φ(...) are the ground truth corner and classification distributions, φ B,i = {x i , y i , w i , h i } the ground truth bounding boxes, (λ s , λ t , λ b ) are user defined constants indicating the relative strength of each component, (Λ s , Λ t , Λ b ) are constants normalizing each component to 1 given the model initialization and SoftL 1 (x) is defined in <ref type="bibr" target="#b2">[3]</ref>. The corner distribution φ(t|k, y, x) is identified by mapping each groundtruth instance's corners to a single position in the corner map, corners out of bounds are simply discarded. The detection distribution φ(s|B) is identified by calculating the intersection over union (IoU) overlap between the groundtruth bounding boxes and the sampling bounding boxes B S . Following standard practice, the regression target bounding box φ B is identified by selecting the ground truth bounding box with the largest IoU overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Detection Model</head><p>Residual neural networks <ref type="bibr" target="#b5">[6]</ref> have demonstrated impressive regression capabilities on a number of large scale datasets. In particular the 101 layer Residual Network model (ResNet-101) achieved state of the art performance on the ILSVRC2015 <ref type="bibr" target="#b15">[16]</ref> and MSCOCO <ref type="bibr" target="#b10">[11]</ref> datasets when combined with Faster R-CNN. As the base model to our networks we selected the 34 layer, 21M parameter ResNet-34 model (DeNet-34) and the 101 layer, 45M parameter ResNet-101 model (DeNet-101).</p><p>To each base model we modified the input size to 512 × 512 pixels, removed the final mean pooling and fully connected layers and appended two deconvolution <ref type="bibr" target="#b12">[13]</ref> layers followed by a corner detector. The corner detector is responsible for generating the corner distribution and produces a feature sampling map via a learnt linear projection with F s features at each spatial position. The deconvolution <ref type="bibr" target="#b12">[13]</ref> layers efficiently reintroduce spatial information that was lost in the base model such that the feature map and corner probability distribution can be defined at a greater spatial resolution i.e. 64 × 64 compared to 16 × 16 without. This results in a 16 × 16 pixel minimum size for each sampling bounding box.</p><p>Following the corner detector is the sparse layer which observes the corners identified by the corner detector and generates a set of sampling bounding boxes (RoIs). The RoIs are used to extract a set of N × N feature vectors from the feature sampling maps. In this case, we are sparsely sampling N 2 bounding boxes from a set of 4.2M valid bounding boxes. A feature vector is constructed by extracting the nearest neighbour sampling features associated with a 7 × 7 grid plus the bounding box width and height. This produces a feature with 7 × 7 × F s + 2 values. We found that nearest neighbour sampling was sufficient because the feature sampling maps have the same, relatively high, spatial resolution as the bounding box corners. Finally, the feature vectors are propagated through a relatively shallow fully connected network to generate the final classification and fine tuned bounding box for each sampling RoI.</p><p>In <ref type="table">Table 1</ref> and 2 we describe the additional layers appended to the base models with the following definitions:</p><p>• Conv: Convolves a series of 2D filters over the input activations. Filter weights were initialized via the normal distribution N (0, σ) with σ 2 = 2/(n f n x n y ) where n f is the number of filters and (n x , n y ) their spatial shape <ref type="bibr" target="#b4">[5]</ref>. Following each convolution is batch normalization <ref type="bibr" target="#b6">[7]</ref> then the ReLU activation function.</p><p>• Deconv: Applies a learnt deconvolution <ref type="bibr" target="#b12">[13]</ref> (upsampling) operation followed by ReLU activation. In this case it is equivalent to upscaling both spatial dimensions then applying a Conv layer. <ref type="table" target="#tab_1">-34  512 256 128 4706 1536 1024  768 512  DeNet-101 2048 384 192 6274 2048 1536 1024 768   Table 1</ref>. Filter parameters used for DeNet models. See <ref type="table" target="#tab_1">Table 2</ref> Layer</p><formula xml:id="formula_2">Model F 0 F 1 F 2 F 3 F 4 F 5 F 6 F 7 DeNet</formula><p>Input Shape Filters Shape Stride ResNet-34 or ResNet-101 <ref type="bibr" target="#b5">[6]</ref> base model. • Sparse: Identifies sampling bounding boxes from corner distribution and produces a fixed size sampling feature from the sampling feature maps.</p><formula xml:id="formula_3">Deconv 16 × 16 × F 0 F 1 3 × 3 2 × 2 Deconv 32 × 32 × F 1 F 2 3 × 3 2 × 2 Corner 64 × 64 × F 2 - - - Sparse - - - - Conv N × N × F 3 F 4 1 × 1 1 × 1 Conv N × N × F 4 F 5 1 × 1 1 × 1 Conv N × N × F 5 F 6 1 × 1 1 × 1 Conv N × N × F 6 F 7 1 × 1 1 × 1 Classifier N × N × F 7 - - -</formula><p>• Classifier: Maps activations to the desired probability distribution via the softmax function and generates bounding box targets.</p><p>For DeNet-34 we use a ResNet-34 base model and F s = 96 to produce a feature vector of 4706 values and a total of 32M parameters. The DeNet-101 model uses a ResNet-101 base model and increased the number of filters by approximately 1.5× for the appended layers (See <ref type="table">Table 1</ref>). These changes produce a sparse feature vector of 6274 values and a total of 69M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Skip Layer Variant</head><p>As an extension we considered augmenting the DeNet models with skip layers. In recent work, skip layers have demonstrated consistent improvements in classification <ref type="bibr" target="#b5">[6]</ref>, detection <ref type="bibr" target="#b9">[10]</ref> and semantic segmentation <ref type="bibr" target="#b1">[2]</ref> and, more generally, are an integral component to highway <ref type="bibr" target="#b17">[18]</ref> and residual networks <ref type="bibr" target="#b5">[6]</ref>. In this case, these layers connect the Deconv layer with the final layer in the base model which has the same spatial dimensions. Our implementation follows <ref type="bibr" target="#b9">[10]</ref>, each skip layer performs a linear projection of the source features to the destination feature dimensions and simply adds the resulting feature maps (before activation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Wide Variant</head><p>In this model we modified the skip model variants to use a 128 × 128 spatial resolution for the corner and feature sampling maps by the addition of another Deconv and skip layer. We also increased N to 48 to produce 2304 RoIs. In the current implementation, this approach comes with a considerable timing cost due to the increased classification burden and the CPU bound algorithm for identifying RoIs. With further engineering (e.g. deduplication) we believe these costs could be reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation Details</head><p>Our models are implemented within our Theano based CNN library called DeNet. The source-code is available from: https://github.com/lachlants/denet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training methodology</head><p>In all experiments we used Nesterov style SGD <ref type="bibr" target="#b18">[19]</ref> with an initial learning rate of 0.1, momentum of 0.9 and weight decay of 0.0001 (only applied to weights). A batch size of 128 was employed for both models with 32 samples per GPU iteration. The learning rate was divided by 10 at epoch 30 and epoch 60 and a total of 90 training epochs were performed. Note that, apart from the batch size changes, these hyperparameters are identical to those used when training the original residual networks for classification <ref type="bibr" target="#b5">[6]</ref>. No online hard negative mining <ref type="bibr" target="#b16">[17]</ref> or other gradient optimization techniques were applied, however, we observed some instances of overtraining on Pascal VOC. In response, to increase exposure to negative samples, we introduced 10% randomly generated bounding box samples during training.</p><p>An augmentation strategy very similar to GoogLeNet <ref type="bibr" target="#b19">[20]</ref> was employed to improve model generalization to different scales and translations. For each sample, a black border was added to the smallest dimension to produce a square image. At test time, this image was scaled to 512 × 512 pixels using bilinear sampling, during training a random crop was selected with an area between (0.08, 1.0) relative to the border image and an aspect ratio between (3/4, 4/3). The random crop was discarded and a new one generated if no ground truth objects overlapped with the crop by at least 50%. This process was repeated up to 10 times and, as a fallback, the entire bordered image was returned. As in testing, the resulting crop was scaled to 512 × 512 pixels. Random photometric (contrast, saturation and brightness) and mirror augmentation was also employed <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Identifying Sampling Bounding Boxes (RoIs)</head><p>A simple algorithm was developed to quickly search the corner distribution for non-null bounding boxes: Since the vast majority of corners are culled in step 1 this method obtains a significant speed up beyond the naive brute force method i.e. testing every possible bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Analysis</head><p>In this section we compare our design with previously published models. We note that in some cases, an applesto-apples comparison is difficult due to the wide range of base models, data augmentation schemes and dataset merging. In particular, we note that SSD utilize larger batch sizes while R-CNN models have larger input resolutions (on average). All our DeNet timing results are provided for a single Titan X GPU (CuDNN 5110) with a batch size of 8x, the same settings used in SSD. For brevity we include only three flavours of the non real-time Faster R-CNN model, the original RPN (VGG), the ResNet-101 extension RPN+ (ResNet-101) and R-FCN for comparison (highlighted in grey in the tables). We note that due to implementation restrictions RPN based models are tested with a single image per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Max. Input BS L Param. RPN (VGG) <ref type="bibr" target="#b14">[15]</ref> 1000 × 600 1 16 137M RPN+ (ResNet) <ref type="bibr" target="#b5">[6]</ref> 1000 × 600 1 100 45M R-FCN <ref type="bibr" target="#b8">[9]</ref> 1000 × 600 1 100 45M Fast YOLO <ref type="bibr" target="#b13">[14]</ref> 448 × 448 1 9 9M YOLO <ref type="bibr" target="#b13">[14]</ref> 448 × 448 1 26 60M SSD300 <ref type="bibr" target="#b11">[12]</ref> 300 × 300 8 25 27M SSD512 <ref type="bibr" target="#b11">[12]</ref> 512  In <ref type="table" target="#tab_3">Table 3</ref> we provide a broad overview of the baseline models. We note that despite an increased number of layers and parameters the DeNet models obtain improved evaluation rates (See Section 4.2). BBox=144 BBox=256</p><p>BBox=576 BBox=1024 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hyperparameter Optimization</head><p>For the following we used the DeNet-34 model and trained it on Pascal VOC 2007 train and Pascal VOC 2012 trainval (14,041 images), for testing we used Pascal VOC 2007 val (2,510 images). The same DeNet-34 model initialization was used for all experiments. We applied the training procedure described in Section 3.1 except with a batch size of 96.</p><p>In <ref type="table" target="#tab_4">Table 4</ref> we performed a coarse search over the corner and bounding box regression cost parameters λ t and λ b . Best results were achieved by setting λ s = 1, λ t = 100 and λ b = 1, these were applied in all subsequent experiments. Next, we investigated the model behaviour with varying numbers of sampling bounding boxes. In particular, we trained a set of models with N = {8, 12, 16, 24, 32}. At test time, we took each of these models and varied N from 8 to 32 to produce <ref type="figure" target="#fig_2">Figure 2</ref>. In <ref type="table">Table 5</ref>, we provide the model evaluation rate and coverage (percentage of ground truth with a sampling bounding box with IoU &gt; 0.5) over the training set that was obtained by the RoI estimator described in Section 2. As expected, we observed a consistently improving MAP with diminishing returns above 576 when training with a larger number of sampling bounding boxes. In general we observed an improved MAP with increased testing bounding boxes at the cost of evaluation rates. For subsequent experiments we set N = 24 for both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Timing Breakdown and Evaluation Rates</head><p>In <ref type="table">Table 6</ref> we present a coarse analysis of the timing for both DeNet models. We broke the timing into 4 sequentially executed stages:</p><p>1. Estimate corners: Images are uploaded to the GPU and fed through the base network generating the corner distribution and sampling feature maps. The corner distribution is transferred from GPU to CPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Generate RoI:</head><p>The sampling bounding boxes (RoIs) are generated from the corner distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Classify RoI:</head><p>The final classification CNN is executed, the classification distribution and bounding box regression outputs are transferred from GPU to CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Estimate instances:</head><p>Non-Max Supression is run over the resulting detection hits producing a de-duplicated list of detections for each image.</p><p>We observe that the vast majority of time is spent evaluating the base network to generate corners. Also, note that the CPU bound Generate RoI stage timing can vary substantially between different samples and may require additional tuning depending on application. Furthermore, we wish to emphasize a number of important features of the DeNet model which makes it significantly faster than most other baseline models:</p><p>• Deconvolution: Spatial information is increased via deconvolution layers as opposed to the atrous modified models used in R-FCN and SSD. This method introduces spatial information significantly later in the model, greatly improving evaluation rates.</p><p>• Fast RoI Features: Features are extracted via a simple nearest neighbour sampling method, limiting the the number of feature reads to 49 per RoI. Some RPN variants use pooling which varies from 49-580 per RoI.</p><p>• Input Image Dimensions: DeNet scales all images to 512x512 pixels, whereas RPN based methods use a varying input size up to 1000x600 pixels.</p><p>• Batching: Our models are tested with 8x samples per batch (same as SSD). This improves GPU utilization.</p><p>With these improvements in timing we are able to use a more expressive base model for the same evaluation rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RoI Coverage Comparison</head><p>In <ref type="table">Table 7</ref>. we provide the coverage obtained by by the top 300 RoIs for RPN, R-FCN and DeNet methods. We observe that given a relatively low number of RoIs, RPN (VGG) and R-FCN provide better coverage at low IoU thresholds, however with increasing IoU the DeNet models provide significantly improved coverage. We note that RPN / R-FCN utilize bounding box regression and deduplication methods in their RoI proposal networks, these factors improve coverage with low numbers of proposals. As demonstrated in the following sections, the DeNet RoI coverage results do not necessarily translate to a reduced MAP for the full model, which includes NMS and bounding box regression, at lower IoU thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MSCOCO</head><p>The Microsoft Common Object in Context <ref type="bibr" target="#b10">[11]</ref> dataset consists of 82K training and 40K validation images distributed across 80 classes. For testing, the dataset includes an 80K test dataset from which a user known subset of 20K images forms the test-dev2015 set and an unknown subset of 20K images forms test2015 allowing only 5 evaluations. Due to the dataset size, number of classes and relatively small size of the object instances within the images, MSCOCO is a considerably more difficult dataset compared to the Pascal VOC challenges. The primary evaluation metric for MSCOCO is the integral of the MAP over the detection matching parameter IoU=0.5 to IoU=0.95. This metric places a greater emphasis on localization performance compared to the Pascal datasets. We found that setting λ t = 50 for DeNet-101 was necessary for convergence, this is likely due to the greater number of corners present within each image on average compared to the validation experiments. Training took ∼4 days with 2× Tesla P100 GPUs for DeNet-34 and ∼6.5 days with 4× Tesla P100 GPUs for DeNet-101.</p><p>In <ref type="table">Table 8</ref> we provide the precision and recall results for our models on test-dev2015. The DeNet models demonstrate a clear advantage over other high evaluation rate implementations e.g. our real-time DeNet-34 model beats SSD300 by 6.2% MAP at the same evaluation rate and SSD512 by 2.6% at more than twice the evaluation rate. The DeNet-101 model furthers this advantage and is only beaten by the very slow competition style RPN+ model utilizing multi-scale evaluation and bounding box refinement. At the time of writing, the DeNet-101 model obtains a result good enough to be in the top-10 on the MSCOCO competition leaderboard which doesn't consider evaluation time. The skip model variants consistently improved performance on small and medium sized objects (see AR and AP for small and medium area objects in table) with a minor cost to large objects and evaluation rate. The wide variants further improved small object detection and fine object localization at the cost of evaluation rate. Near identical results were obtained on MSCOCO test-std2015 e.g. we obtained a MAP@[0.5:0.95] of 29.3% and 31.7% for DeNet-34 and DeNet-101 respectively. Analysis suggests our advantage stems from improved large object detection and finegrain object localization performance, as reflected in the MAP@IoU=0.75 result. We argue this is an outcome of the much larger range of candidate RoI's our method produces e.g. the vanilla DeNet models can select from a possible set of 4.2 × 10 6 bounding boxes while SSD utilizes 2.5 × 10 4 . Utilizing such a large set of candidate bounding boxes would likely be intractable with the dense evaluation methods used in the YOLO and RPN derived models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pascal VOC 2007</head><p>We combined the trainval samples from Pascal VOC 2007 and 2012 <ref type="bibr" target="#b0">[1]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we describe a framework for sparse estimation with CNNs and present a novel region-of-interest detector and classification model which reduces manual engineering and improves state-of-the-art detection performance with real-time and near real-time evaluation rates. Utilizing deconvolution and skip layers first described in the context of semantic segmentation, we demonstrated a highly computationally efficient model with tightly coupled RoI, class prediction and bounding box regression. We provide further evidence that skip connections consistently improved detection rates for small and medium sized objects. While the wide model variant highlighted the importance of corner map resolution for small and medium sized objects, and provides a natural pathway for future development. Analysis suggests our model performs particularly well when finer object localization is desirable. We propose that the improved localization is due to the much larger set of possible sampling bounding boxes that are feasible with our sparse sampling method i.e. 4.2 × 10 6 compared to less than 2.5 × 10 4 for SSD512 and RPN. This feature allows the model to potentially select a bounding box (before bounding box regression) which is significantly closer to the ground truth. Furthermore, since we no longer define a set of reference bounding boxes, this approach has reduced manual engineering requirements and can adapt well to problems which utilize bounding boxes with a very large range of aspect ratios and scales e.g. rotationally variant or non-rigid objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A high level flow diagram depicting the DeNet methodology. The CNN's are highlighted in blue, the novel components in purple and the outputs in yellow. The sampling bounding box dependency BS (highlighted in red) is held constant during back propagation to produce an end-to-end trained model. The corner distribution and final classification distribution are jointly optimized using cross entropy loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 . 2 . 3 . 4 . 5 . 6 .</head><label>123456</label><figDesc>Search the corner distribution for corners {k, y, x} ∈ C λ where Pr(t = 1|k, y, x) &gt; λ. For each corner type, select the M corners with the greatest likelihood C M ⊆ C λ Generate a set of unique bounding boxes by matching every corner within C M of type top − left with every one of type bottom − right. Calculate the probability of each bounding box being non-null via Equation 1. Repeat steps 2 and 3 with corners of type top − right and bottom − left. Sort bounding boxes by probability and keep the N 2 largest to produce the sampling bounding boxes B S .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The MAP on the Pascal VOC 2007 validation dataset with varying number of bounding box samples during training (see legend) and testing (displayed on x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>DeNet: A ResNet derived model for DSS Object Detec- tion with a 512×512 input image. Layers in the base models above the line are initialized with a pretrained ResNet-34 or ResNet-101 ImageNet 2012 classification model.• Corner: Estimates a corner distribution via the soft- max function and produces a sampling feature map. See Section 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Model overview detailing maximum input image sizes, batch size at test time (BS), number of activation layers (L) and approximate number of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>MAP (%) 49.6 63.3 64.9 71.8 72.8 72.1 70.6 Optimizing cost hyperparameters {λs, λt, λ b }, see Equation 2. MAP is provided for Pascal VOC 2007 val dataset.</figDesc><table><row><cell></cell><cell cols="2">λ s</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell></cell><cell cols="2">λ t</cell><cell>1</cell><cell>10</cell><cell>10</cell><cell cols="3">100 100 100 500</cell></row><row><cell></cell><cell cols="2">λ b</cell><cell>1</cell><cell>1</cell><cell>10</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell>1</cell></row><row><cell cols="3">Sample BBoxs</cell><cell cols="6">64 144 256 400 576 784 1024</cell></row><row><cell cols="3">Coverage (%)</cell><cell>81</cell><cell>87</cell><cell>91</cell><cell>93</cell><cell>95</cell><cell>96</cell><cell>97</cell></row><row><cell cols="4">Eval. Rate (Hz) 96</cell><cell>90</cell><cell>84</cell><cell>76</cell><cell>69</cell><cell>61</cell><cell>55</cell></row><row><cell cols="9">Table 5. Sample bounding boxes vs coverage over training dataset</cell></row><row><cell cols="4">and evaluation rate.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAP (%)</cell><cell>68 70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>66</cell><cell></cell><cell></cell><cell cols="3">BBox=64</cell><cell></cell></row><row><cell></cell><cell>64</cell><cell>0</cell><cell>256</cell><cell></cell><cell>512</cell><cell></cell><cell>768</cell><cell>1,024</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">#Sample BBoxs @ Test Time</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>(denoted 07+12 in table) to produce 16,551 training samples. For testing we used Pascal VOC 2007 test containing 4,991 samples. We note that this dataset is considerably smaller than MSCOCO and therefore more susceptible to overtraining and image augmentation methods. Training time was ∼13 hours for DeNet-34 using 2× Tesla P100s and ∼20 hours for DeNet-101 using 4× Tesla P100s. InTable 9, we provide the MAP and timing results. We observed the skip layer variant DeNet-34 improving upon SSD300's peak MAP by 1.6% and 20Hz. In the near real-time domain DeNet-101 matches SSD512 at a higher evaluation rate.Table 8. MSCOCO average precision (AP) and average recall (AR) results evaluated on test-dev2015 dataset. Pascal VOC 2007 mean average precision and timing. 4.6. Pascal VOC 2012 In this experiment we combine trainvaltest from Pascal VOC 2007 and trainval from Pascal VOC 2012 [1] (denoted 07++12 in table) to produce 21,503 training samples. Test scores are evaluated on 10,991 samples by the Pascal VOC 2012 testing server. For this dataset the DeNet-34 model matches SSD300, however, for reasons unknown, DeNet-101 demonstrates results below SSD512. For reference, we note that DeNet-101 obtains results near identical to the other ResNet-101 based model, RPN (ResNet) with an order of magnitude improvement in evaluation rate. Training time was ∼18 hours for DeNet-34 with 2× Tesla P100s and ∼28 hours for DeNet-101 with 4× Tesla P100s.</figDesc><table><row><cell></cell><cell>Eval.</cell><cell cols="3">AP@IoU (%)</cell><cell cols="3">AP@Area (%)</cell><cell cols="3">AR@Dets (%)</cell><cell cols="3">AR@Area (%)</cell></row><row><cell>Model</cell><cell>Rate</cell><cell cols="3">0.5:0.95 0.5 0.75</cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>1</cell><cell>10</cell><cell>100</cell><cell>S</cell><cell>M</cell><cell>L</cell></row><row><cell>RPN (VGG)</cell><cell>7 Hz</cell><cell>21.9</cell><cell>42.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RPN+ (ResNet)</cell><cell>&lt;1 Hz</cell><cell>34.9</cell><cell>55.7</cell><cell>-</cell><cell cols="3">15.6 38.7 50.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>R-FCN</cell><cell>9 Hz</cell><cell>29.9</cell><cell>51.9</cell><cell>-</cell><cell cols="3">10.8 32.8 45.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSD300</cell><cell>58 Hz</cell><cell>23.2</cell><cell cols="11">41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5</cell></row><row><cell>SSD512</cell><cell>23 Hz</cell><cell>26.8</cell><cell cols="11">46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0</cell></row><row><cell>DeNet-34</cell><cell>83 Hz</cell><cell>29.4</cell><cell cols="11">46.2 31.2 7.8 30.8 47.4 26.9 38.0 38.5 11.2 41.9 63.0</cell></row><row><cell>DeNet-34 (skip)</cell><cell>82 Hz</cell><cell>29.5</cell><cell cols="11">47.9 31.1 8.8 30.9 47.0 26.9 38.0 38.6 13.2 41.7 61.6</cell></row><row><cell>DeNet-34 (wide)</cell><cell>44 Hz</cell><cell>30.0</cell><cell cols="11">48.9 31.8 10.1 30.9 45.7 27.3 39.5 40.3 17.0 42.8 60.9</cell></row><row><cell>DeNet-101</cell><cell>34 Hz</cell><cell>31.9</cell><cell cols="11">50.5 34.2 9.7 34.9 50.6 28.4 39.8 40.3 13.1 44.8 64.1</cell></row><row><cell>DeNet-101 (skip)</cell><cell>33 Hz</cell><cell>32.3</cell><cell cols="11">51.4 34.6 10.5 35.1 50.9 28.5 40.2 40.8 14.7 44.9 63.8</cell></row><row><cell cols="2">DeNet-101 (wide) 17 Hz</cell><cell>33.8</cell><cell cols="11">53.4 36.1 12.3 36.1 50.8 29.6 42.6 43.5 19.3 46.9 64.3</cell></row><row><cell>Model</cell><cell cols="3">Dataset Eval. Rate</cell><cell>MAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RPN (VGG)</cell><cell>07+12</cell><cell>7 Hz</cell><cell></cell><cell>73.2%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RPN (ResNet)</cell><cell>07+12</cell><cell>2 Hz</cell><cell></cell><cell>76.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>R-FCN</cell><cell>07+12</cell><cell>9 Hz</cell><cell cols="2">80.5%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fast YOLO</cell><cell>07+12</cell><cell>155 Hz</cell><cell></cell><cell>52.7%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>YOLO</cell><cell>07+12</cell><cell>45 Hz</cell><cell></cell><cell>63.4%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD300</cell><cell>07+12</cell><cell>58 Hz</cell><cell></cell><cell>74.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD512</cell><cell>07+12</cell><cell>23 Hz</cell><cell></cell><cell>76.8%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeNet-34</cell><cell>07+12</cell><cell>83 Hz</cell><cell></cell><cell>75.3%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeNet-34 (skip)</cell><cell>07+12</cell><cell>82 Hz</cell><cell></cell><cell>75.9%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeNet-101</cell><cell>07+12</cell><cell>34 Hz</cell><cell></cell><cell>77.0%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DeNet-101 (skip) 07+12</cell><cell>33 Hz</cell><cell></cell><cell>77.1%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regionbased convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via regionbased fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>abs/1612.03144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1512.02325</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
