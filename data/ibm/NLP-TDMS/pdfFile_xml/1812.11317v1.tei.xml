<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Support Vector Guided Softmax Loss for Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-29">29 Dec 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
							<email>shifeng.zhang@nlpr.ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@jd.com</email>
							<affiliation key="aff0">
								<orgName type="department">JD AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Support Vector Guided Softmax Loss for Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-29">29 Dec 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face recognition has witnessed significant progresses due to the advances of deep convolutional neural networks (CNNs), the central challenge of which, is feature discrimination. To address it, one group tries to exploit miningbased strategies (e.g., hard example mining and focal loss) to focus on the informative examples. The other group devotes to designing margin-based loss functions (e.g., angular, additive and additive angular margins) to increase the feature margin from the perspective of ground truth class. Both of them have been well-verified to learn discriminative features. However, they suffer from either the ambiguity of hard examples or the lack of discriminative power of other classes. In this paper, we design a novel loss function, namely support vector guided softmax loss (SV-Softmax), which adaptively emphasizes the mis-classified points (support vectors) to guide the discriminative features learning. So the developed SV-Softmax loss is able to eliminate the ambiguity of hard examples as well as absorb the discriminative power of other classes, and thus results in more discrimiantive features. To the best of our knowledge, this is the first attempt to inherit the advantages of mining-based and margin-based losses into one framework. Experimental results on several benchmarks have demonstrated the effectiveness of our approach over state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition is a fundamental and of great practice values task in the community of computer vision and patter recognition. The task of face recognition contains two categories, face identification to classify a given face to a specific identity, and face verification to determine whether a pair of face images are of the same identity. Though it has been extensively studied for decades <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21]</ref>, there still exist a great many challenges for accurate face recognition, especially on large-scale test datasets, such as MegaFace Challenge <ref type="bibr" target="#b8">[9]</ref> or Trillion Pairs Challenge 1 .</p><p>In recent years, the advanced face recognition models are usually built upon deep convolutional neural networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref> and the learned discriminative features play a significant role. To train deep models, the CNNs are generally equipped with classification loss functions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36]</ref>, metric learning loss functions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> or both <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Metric learning loss functions such as contrastive loss <ref type="bibr" target="#b25">[26]</ref> or triplet loss <ref type="bibr" target="#b19">[20]</ref> usually suffer from high computational cost. To avoid this problem, they require carefully designed sample mining strategies and the performance is very sensitive to these strategies. So increasingly more researchers shift their attentions to construct deep face recognition models by redesigning the classification loss functions.</p><p>Intuitively, face features are discriminative if their intraclass compactness and inter-class separability are well maximized. However, as pointed out by many recent studies <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4]</ref>, the current prevailing classification loss function (i.e., Softmax loss) usually lacks the power of feature discrimination for deep face recognition. To address this issue, one group proposes to explore the miningbased loss functions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref>. Shrivastava et al. <ref type="bibr" target="#b21">[22]</ref> develop a hard mining softmax (HM-Softmax) to improve the feature discrimination by constructing mini-batches using high-loss examples. Among which, the percentage of hard examples is empirically decided and the easy examples are completely discarded. In contrast, Lin et al. <ref type="bibr" target="#b11">[12]</ref> design a relatively soft mining softmax, namely Focal loss (F-Softmax), to focus training on a sparse set of hard examples. It usually achieves more promising results than the simple hard mining softmax. Yuan et al. <ref type="bibr" target="#b38">[39]</ref> select the hard examples based on model complexity and train an ensemble to model examples of different hard levels. The other group prefers to design margin-based loss functions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>. This group does not focus on optimizing hard examples but directly increasing the feature margin between different classes. Wen et al. <ref type="bibr" target="#b36">[37]</ref> develop a center loss to learn centers for each identity to enhance the intra-class compactness. Wang et al. <ref type="bibr" target="#b31">[32]</ref> and Ranjan et al. <ref type="bibr" target="#b18">[19]</ref> propose to use a scale parameter to control the temperature of softmax loss, producing higher gradients to the well-separated samples to shrink the intra-class variance. Liu et al. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> introduce an angular margin (A-Softmax) between the ground truth class and other classes to encourage the larger interclass variance. However, it is usually unstable and the optimal parameters are hard to determinate. To enhance the stability of A-Softmax loss, several alternative approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4]</ref> have been proposed. Wang et al. <ref type="bibr" target="#b29">[30]</ref> design an additive margin (AM-Softmax) loss to stabilize the optimization and have achieved promising performance. Deng et al. <ref type="bibr" target="#b3">[4]</ref> develop an additive angular margin (Arc-Softmax) loss, which has a more clear geometric interpretation.</p><p>Although these two groups have been well-verified to learn discriminative features for face recognition. The motivation of mining-based losses is to focus on hard examples while margin-based losses are to enlarge the feature margin between different classes. Currently, they develop independently and both of them have their own intrinsic drawbacks. To the mining-based losses, the definition of hard examples is ambiguous and they are often empirically selected. How to semantically decide the hard examples is still an open problem. To the margin-based losses, most of them learn discriminative features by enlarging the feature margin, only from the perspective of ground truth class (self-motivation). They usually ignore the discriminative power from the perspective of other non-ground truth classes (other-motivation). Moreover, the relation between mining-based and margin-based losses remains unclear.</p><p>To overcome the above shortcomings, this paper tries to design a new loss function, which adaptively emphasizes on the informative support vectors to bridge the gap between mining-based and margin-based losses and semantically integrate them into one framework. To sum up, the main contributions of this paper can be summarized as follows:</p><p>• We propose a novel SV-Softmax loss, which eliminates the ambiguity of hard examples as well as absorbs the discriminative power of other classes by focusing on support vectors. To the best of our knowledge, this is the first attempt to semantically fuse the mining-based and margin-based losses into one framework.</p><p>• We deeply analyze the relations of our SV-Softmax loss to the current mining-based and margin-based losses, and further develop an improved version SV-X-Softmax loss to enhance the feature discrimiantion. Our code will be available at https://github. com/xiaoboCASIA/SV-X-Softmax.</p><p>• We conduct extensive experiments on the benchmarks of LFW <ref type="bibr" target="#b7">[8]</ref>, MegaFace Challenge <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> and Trillion Pairs Challenge, which have verified the superiority of our new approach over the baseline Softmax loss, the mining-based Softmax losses, the margin-based Softmax losses, and their naive fusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary Knowledge</head><p>Softmax. Softmax loss is defined as the pipeline combination of the last fully connected layer, the softmax function and the cross-entropy loss. In face recognition, the weights w k , (where k ∈ {1, 2, . . . , K} and K is the number of classes) and the feature x of the last fully connected layer are usually normalized and the magnitude is replaced as a scale parameter s <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>. In consequence, given an input feature vector x with its corresponding ground truth label y, the softmax loss can be formulated as follows:</p><formula xml:id="formula_0">L 1 = − log e s cos(θw y ,x) e s cos(θw y ,x) + K k =y e s cos(θw k ,x ) ,<label>(1)</label></formula><p>where cos(θ w k ,x ) = w T k x is the cosine similarity and θ w k ,x is the angle between w k and x. As pointed out by a great many studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>, the learned features with softmax loss are prone to be separable, rather than to be discriminative for face recognition.</p><p>Mining-based Softmax. Hard example mining is becoming a common practice to effectively train deep CNNs. Its idea is to focus training on the informative examples, thus it usually results in more discriminative features. There are recent works that select hard examples based on loss value <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref> or model complexity <ref type="bibr" target="#b38">[39]</ref> to learn discriminative features. Generally, they can be summarized as: ,</p><p>where p y = e s cos(θw y ,x ) e s cos(θw y ,x) + K k =y e s cos(θw k ,x ) is the predicted ground truth probability and g(p y ) is an indicator function. Basically, to the soft mining method Focal loss <ref type="bibr" target="#b11">[12]</ref> (F-Softmax), g(p y ) = (1 − p y ) γ , γ is a modulating factor. To the hard mining method HM-Softmax <ref type="bibr" target="#b21">[22]</ref>, g(p y ) = 0 when the sample is indicated as easy while g(p y ) = 1 when the sample is hard. However, the definition of hardness is ambiguous and they usually lead to sensitive performance. Margin-based Softmax. To directly enhance the feature discrimination, several margin-based softmax loss functions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref> have been proposed in recent years. In summary, they can be defined as follows:</p><formula xml:id="formula_2">L 3 = − log e sf (m,θw y ,x) e sf (m,θw y ,x) + K k =y e s cos(θw k ,x) ,<label>(3)</label></formula><p>where f (m, θ wy,x ) is a carefully designed margin function. Basically, f (m 1 , θ wy,x ) = cos(m 1 θ wy,x ) is the motivation of A-Softmax loss <ref type="bibr" target="#b13">[14]</ref>, where m 1 ≥ 1 and is an integer. f (m 2 , θ wy,x ) = cos(θ wy ,x ) − m 2 with m 2 &gt; 0 is the AM-Softmax loss <ref type="bibr" target="#b29">[30]</ref>. f (m 3 , θ wy,x ) = cos(θ wy,x + m 3 ) with m 3 &gt; 0 is the Arc-Softmax loss <ref type="bibr" target="#b3">[4]</ref>. More generally, the margin function can be summarized into a combined version: f (m, θ wy,x ) = cos(m 1 θ wy,x + m 3 ) − m 2 . However, all these methods achieve the feature margin only from the perspective of ground truth class y. They are not aware of the importance of other non-ground truth classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Naive Mining-Margin Softmax Loss</head><p>The mining-based loss functions aim to focus on the hard examples while the margin-based loss functions are to enlarge the feature margin between different classes. Therefore, these two branches can seamlessly incorporate into each other. The naive motivation to directly integrate them can be formulated as: .</p><formula xml:id="formula_3">L 4 = −g(p y ) log e sf (m</formula><p>(4) However, this formulation Eq. (4) only absorbs their own merits. It can not solve their respective shortcomings. Detailedly, it only encourages the feature margin from the perspective of the ground truth class by f (m, θ wy,x ) (selfmotivation), ignoring the feature discriminative power of other non-ground truth classes (other-motivation). Moreover, the hard examples are still empirically selected by the indicator function g(p y ), without semantic guidance. In other words, the definition of hard examples is ambiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Support Vector Guided Softmax Loss</head><p>Intuition says that considering the well-separated feature vectors has little effect on the learning problem. That means the mis-classified feature vectors are more crucial to enhance the feature discriminability. Motivated by this, the hard example mining <ref type="bibr" target="#b21">[22]</ref> and the recent Focal loss <ref type="bibr" target="#b11">[12]</ref>  without intuitive interpretation.</p><p>To address it, we alternatively introduce a more elegant way to focus training on the informative features (i.e., support vectors). Specifically, we define a binary mask to adaptively indicate whether a sample is selected as the support vector by a specific classifier in the current stage. To the end, the binary mask is defined as follows:</p><formula xml:id="formula_4">I k = 0, cos(θ wy,x ) − cos(θ w k ,x ) ≥ 0 1, cos(θ wy,x ) − cos(θ w k ,x ) &lt; 0 .<label>(5)</label></formula><p>From the definition, we can see that if a sample is misclassified, i.e., cos(θ wy,x ) − cos(θ w k ,x ) &lt; 0, it will be emphasized temporarily. In this way, the concept of hard examples is clearly defined and we mainly focus on such a sparse set of support vectors. Consequently, our Support Vector Guided Softmax (SV-Softmax) loss is formulated:</p><formula xml:id="formula_5">L 5 = − log e s cos(θw y ,x ) e s cos(θw y ,x) + K k =y h(t, θ w k ,x , I k )e s cos(θw k ,x) ,<label>(6)</label></formula><p>where t is a preset hyperparameter and the indicator function h(t, θ w k ,x , I k ) is defined as:</p><formula xml:id="formula_6">h(t, θ w k ,x , I k ) = e s(t−1)(cos(θw k ,x )+1)I k .<label>(7)</label></formula><p>Obviously, when t = 1, the designed SV-Softmax loss becomes identical to the original softmax loss. <ref type="figure" target="#fig_1">Figure 1</ref> gives the geometrical interpretation of our SV-Softmax loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Relation to Mining-based Softmax Losses</head><p>To illustrate the advantages of our SV-Softmax loss over the traditional mining-based loss functions (e.g., Focal loss <ref type="bibr" target="#b11">[12]</ref>), we use the binary classification case as an example. Assume that we have two samples x 1 and x 2 , both of them are from class 1. <ref type="figure" target="#fig_0">Figure 2</ref> gives a diagram, where x 1 is relatively hard while x 2 is relatively easy. The traditional mining-based Focal loss is to differentially re-weight the  losses of hard and easy examples, such that:</p><formula xml:id="formula_7">loss 1 * loss 2 * &gt; loss 1 loss 2 .<label>(8)</label></formula><p>In that way, the importance of hard examples is emphasized. This strategy is directly from the loss perspective and the definition of hard examples is ambiguous. While our SV-Softmax loss is from a different way. Firstly, we semantically define the hard examples (support vectors) according to the decision boundary. Then, to the support vector x 1 , we reduce its probability, such that:</p><formula xml:id="formula_8">loss 1 * loss 2 &gt; loss 1 loss 2 .<label>(9)</label></formula><p>In summary, the differences between SV-Softmax loss and mining-based Focal loss <ref type="bibr" target="#b11">[12]</ref> are displayed in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Relation to Margin-based Softmax Losses</head><p>Similarly, assume that we have a sample x from class 1, and it is a little far way from its ground truth class, (e.g., the red circle point in <ref type="figure" target="#fig_5">Figure 4</ref>). The original softmax loss aims to make w T 1 x &gt; w T 2 x ⇐⇒ cos(θ 1 ) &gt; cos(θ 2 ). To make the objective more rigorous, margin-based losses usually introduce a margin function f (m, θ 1 ) = cos(m 1 θ 1 + m 3 ) − m 2 ! " ! " <ref type="figure">Figure 5</ref>. From left to right: SV-Softmax loss vs. SV-X-Softmax loss. To increase the mining range, we adopt the margin-based decision boundaries to select support vectors. Thus the non-support vectors in SV-Softmax may be support vectors in SV-X-Softmax. from the perspective of ground truth class <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4]</ref>:</p><formula xml:id="formula_9">cos(θ 1 ) ≥ f (m, θ 1 ) &gt; cos(θ 2 ).<label>(10)</label></formula><p>In contrast, our SV-Softmax loss enlarge the feature margin from the perspective of other non-ground truth classes. Specifically, we have introduced a margin function h * (t, θ 2 ) to these mis-classified features:</p><formula xml:id="formula_10">cos(θ 1 ) &gt; h * (t, θ 2 ) ≥ cos(θ 2 ),<label>(11)</label></formula><p>where h * (t, θ 2 ) = log[h(t, θ 2 )e cos(θ2) ] = t cos(θ 2 ) + t − 1.</p><p>Our SV-Softmax loss semantically enlarges the feature margin from other non-ground truth classes while margin-based losses make theirs efforts from the ground truth class. For multi-class case, Our SV-Softmax loss is class-specific margins. <ref type="figure" target="#fig_5">Figure 4</ref> gives their geometrical comparison. To sum up, <ref type="figure" target="#fig_4">Figure 3</ref> shows the pipeline of our SV-Softmax loss and its relations to the mining-based and margin-based losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">SV-X-Softmax</head><p>According to the above discussions, our SV-Softmax loss semantically fuses the motivation of mining-based and margin-based losses into one framework, but from different viewpoints. Therefore, we can also absorb their strengths into our SV-Softmax loss. Specifically, to increase the mining range, we adopt the margin-based decision boundaries to indicate the support vectors. Consequently, the improved SV-X-Softmax loss can be formulated as:</p><formula xml:id="formula_11">L 6 = − log e sf (m,θw y,x ) e sf (m,θw y ,x) + K k =y h(t, θ w k ,x , I k )e s cos(θw k ,x) ,<label>(12)</label></formula><p>where X is the margin-based losses. It can be A-Softmax <ref type="bibr" target="#b13">[14]</ref>, AM-Softmax <ref type="bibr" target="#b29">[30]</ref> and Arc-Softmax <ref type="bibr" target="#b3">[4]</ref> etc. The indicator mask I k is re-computed according to margin-based decision boundaries 2 . Specifically, <ref type="figure">Figure 5</ref> gives the geometrical illustration of our SV-X-Softmax loss. It is best because from the motivation of margin-based losses, SV-X-Softmax loss enlarges the feature margin by integrating the self-motivation of ground truth class and the other-motivation of other classes into one framework. While from the motivation of mining-based losses, it semantically enlarges the mining range.</p><formula xml:id="formula_12">I k = 0, f (m, θ wy,x ) − cos(θ w k ,x ) ≥ 0 1, f (m, θ wy,x ) − cos(θ w k ,x ) &lt; 0 .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimazation</head><p>In this section, we show that the proposed SV-Softmax loss <ref type="formula" target="#formula_5">(6)</ref> is trainable and can be easily optimized by the typical stochastic gradient descent. The difference between the original softmax loss and the proposed SV-Softmax loss lies in the last fully connected layer</p><formula xml:id="formula_13">v = [v 1 , v 2 , . . . , v K ] T = [cos(θ w1,x ), cos(θ w2,x ), . . . , cos(θ wK ,x )] T .</formula><p>To the forward, when k = y, it is the same as the original softmax loss (i.e., v y = cos(θ wy,x )). When k = y, it has two cases, if the feature vector is easy for an specific class, it is the same as the original softmax (i.e., v k = cos(θ w k ,x )). Otherwise, it will be recomputed as log[h(t, θ w k ,x )e cos(θw k ,x) ] = t cos(θ w k ,x ) + t − 1. To the backward propagation, we use the chain rule to compute the partial derivative. The derivative of W and the CNN feature x of the last fully connected layer should be re-emphasized:</p><formula xml:id="formula_14">∂L 5 ∂W =                ∂L 5 ∂v ∂v ∂w y = ∂L 5 ∂v x, k = y ∂L 5 ∂v ∂v ∂w k = ∂L 5 ∂v x, k = y; v y ≥ v k ∂L 5 ∂v ∂v ∂w k = t ∂L 5 ∂v x, k = y; v y &lt; v k<label>(14)</label></formula><formula xml:id="formula_15">∂L 5 ∂x =              ∂L 5 ∂v ∂v ∂x = ∂L 5 ∂v w y , k = y ∂L 5 ∂v ∂v ∂x = ∂L 5 ∂v w k , k = y; v y ≥ v k ∂L 5 ∂v ∂v ∂x = t ∂L 5 ∂v w k , k = y; v y &lt; v k<label>(15)</label></formula><p>2 That why we uniformity call the hard examples as "support vectors", because it is similar to the definition in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: SV-Softmax</head><p>Input: A CNN feature x with its corresponding label y.</p><p>Initialized parameters Θ in convolution layers. Parameter W in the last fully connected layer. The learning rate λ and the indicator parameter t. </p><formula xml:id="formula_16">W (α+1) = W (α) − λ (α) ∂L 5 ∂W (α) ; Θ (α+1) = Θ (α) − λ (α) ∂L 5 ∂x (α) ∂x (α) ∂Θ (α) ; end Output: Parameters Θ and W .</formula><p>where the computation form of ∂L5 ∂v is the same as the original softmax loss. The whole scheme for a single image is summarized in Algorithm 1. It is trivial to perform derivation with mini-batch input. Moreover, it is also straightforward to the SV-X-Softmax loss case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Training Data. The MS-Celeb-1M dataset <ref type="bibr" target="#b5">[6]</ref> contains about 100k identities with 10 million images. However, it consists of a great many noisy face images. Fortunately, the trillionpairs consortium has made their efforts to get a highquality version MS-Celeb-1M-v1c, which is well-cleaned with 86,876 identities and 3,923,399 aligned images. Validation Data. We employ Labelled Faces in the Wild (LFW) <ref type="bibr" target="#b7">[8]</ref> as the validation data. LFW contains 13,233 web-collected images from 5,749 different identities, with large variations in pose, expression and illuminations. Test Data. We use two datasets, MegaFace <ref type="bibr" target="#b8">[9]</ref> and Trillion Pairs 3 , as the test data. MegaFace datasets aim at evaluating the performance of face recognition algorithms at the million scale of distractors, which include gallery set and probe set. The gallery set, a subset of Flickr photos from Yahoo, consists of more than one million images from 690,000 different individuals. The probe set has two existing databases: Facescrub <ref type="bibr" target="#b16">[17]</ref> and FGNET <ref type="bibr" target="#b0">[1]</ref>. In this study, we use the Facescrub as the probe set, which contains 100,000 photos of 530 unique individuals, wherein 55,742 images are males, and 52,076 images are females. Trillion Pairs datasets are recently released as a public available testing benchmark, which are consisted of the following two parts, ELFW and DELFW. ELFW is the face images of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Settings</head><p>Data Processing. We detect the faces by adopting the Face-Boxes detector <ref type="bibr" target="#b39">[40]</ref> and localize five landmarks (two eyes, nose tip and two mouth corners) through a simple 6-layer CNN <ref type="bibr" target="#b4">[5]</ref>. The detected faces are cropped and resized to 120×120, and each pixel (ranged between [0,255]) in RGB images is normalized by subtracting 127.5 and then being divided by 128. For all the training faces, they are horizontally flipped with probability 0.5 for data augmentation. CNN Architecture. In face recognition, there are many kinds network architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29]</ref>. To be fair, the CNN architecture should be the same to test different loss functions. As suggested by the work <ref type="bibr" target="#b28">[29]</ref>, we use Attention-56 <ref type="bibr" target="#b30">[31]</ref> as our baseline architecture to achieve a good balance between computation and accuracy. The output of Attention-56 has and finally gets a 512-dimension feature by the operation of averaging pooling. The scale parameter s has already been discussed sufficiently in previous works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. In this paper, we directly fixed it to 30. For details, the adopted Attention-56 architecture is provided in supplementary materials.</p><p>Training. All the CNN models are trained with stochastic gradient descent (SGD) algorithm and trained from scratch, with the batch size of 32 on 4 P40 GPUs parallelly, total batch size 128. The weight decay is set to 0.0005 and the momentum is 0.9. The learning rate is initially 0.1 and divided by 10 at the 100k, 160k, 220k iterations, and we finish the training process at 240k iterations. Test. At the testing stage, only the features of original image are employed (512-dimension) to compose the face rep-resentation. All the reported results in this paper are evaluated by a single model, without model ensemble or other fusion strategies.</p><p>To the evaluation metrics, the cosine distance of features is computed as the similarity score. Face identification and verification are conducted by ranking and thresholding the scores. Specifically, for face identification, the Cumulative Match Characteristics (CMC) curves are adopted to evaluate the Rank-1 face identification accuracy. For face verification, the Receiver Operating Characteristic (ROC) curves are adopted. The true positive rate (TPR) at low false acceptance rate (FAR) is emphasized since in real applications false acceptance gives higher risks than false rejection. We test our models on several popular public face datasets, including LFW <ref type="bibr" target="#b7">[8]</ref>, MegaFace Challenge <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> and the recent Trillion Pairs Challenge. Specifically, for LFW, the unrestricted with labeled outside data on 6000 pairs accuracy <ref type="bibr" target="#b7">[8]</ref> and the BLUFR <ref type="bibr" target="#b10">[11]</ref> protocols are reported. For Megaface Challenge, the identification Rank-1 accuracy and the verification rate TPR@FAR =1e-6 are reported. For Trillion Pairs Challenge, every pair between ELFW and DELFW is used. There are in total 0.4 trillion pairs. To the face identification task, they provide a 1.58 million-size gallery and a 270k-size query for top-1 identification and the metric TPR@FAR=1e-3 is reported. While to the face verification task, the verification rate TPR@FAR=1e-9 is reported. For more details about the protocols, please refer to the works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>To the compared methods, we compare our method with the baseline Softmax loss (Softmax) and the recently proposed state-of-the-arts, including 2 mining-based softmax losses (i.e., hard example mining (HM-Softmax <ref type="bibr" target="#b21">[22]</ref>) and Focal loss (F-Softmax <ref type="bibr" target="#b11">[12]</ref>)), 3 margin-based softmax losses (the angular Softmax loss (A-Softmax <ref type="bibr" target="#b33">[34]</ref>), the <ref type="figure" target="#fig_0">0 1.05 1.1 1.15 1.2 1.25 1</ref>  <ref type="figure" target="#fig_0">0 1.05 1.1 1.15 1.2 1.25 1</ref> additive margin Softmax loss (AM-Softmax <ref type="bibr" target="#b29">[30]</ref>), and the additive angular margin Softmax loss (Arc-Softmax[4])) and their 4 naive fusions (F-AM-Softmax, F-Arc-Softmax, HM-AM-Softmax and HM-Arc-Softmax). For all the compared methods, their source codes can be downloaded from the github or from authors' webpages. The corresponding parameters are determined according to their suggestions (e.g., the feature margin parameter m is 0.35 for AM-Softmax and is 0.5 for Arc-Softmax). For more details, please refer to the supplementary materials.</p><formula xml:id="formula_17">1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effects of indicator parameter t</head><p>Since the indicator parameter t plays an important role in the developed SV-Softmax loss, we first conduct experiments to search its possible best value. By varying t from 1.0 to 1.3 (If t is larger than 1.4, the model may fail to converge), we use the Attention-56 network and the SV-Softmax loss to train models on the MS-Celeb-1M-v1c dataset and evaluate its performance on the validation set LFW. As illustrated in the left sub-figure of <ref type="figure" target="#fig_6">Figure 6</ref>, with t being increased, the 6000 pairs accuracy and the BLUFR of LFW are improved consistently, and get saturated at t = 1.2. This demonstrates the effectiveness of our SV-Softmax loss (compared t = 1.0 with t = 1.0). To validate the sensitivity of our indicator parameter t, we directly use the trained models to test them on MegaFace, the effects are reported in the right sub-figure of <ref type="figure" target="#fig_6">Figure 6</ref>. From the curves, we can see that our SV-Softmax loss is insensitive to the indicator parameter t in a certain range. According to this study, t is set to fixed 1.2 in the subsequent experiments. <ref type="table" target="#tab_5">Table 4</ref> provides the quantitative results of all the competitors on LFW dataset. The bold number in each column represents the best performance. To the 6000 pairs accuracy protocol, it is well-known that this protocol is typical and easy for deep face recognition, and all the competitors can achieve over 99% accuracy rate. So the improvement of our SV-Softmax loss is not quite large. From the numbers, we observe that the naive fusions of mining-based and marginbased losses, e.g., HM-AM-Softmax and F-AM-Softmax,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Experiments on LFW</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Identification Verification Rank1@1e6 TPR@FAR=1e-6 Softmax 86.29 87.63 F-Softmax <ref type="bibr" target="#b11">[12]</ref> 88.29 89.83 HM-Softmax <ref type="bibr" target="#b21">[22]</ref> 86.58 88.39 A-Softmax <ref type="bibr" target="#b13">[14]</ref> 88.54 89.40 Arc-Softmax <ref type="bibr" target="#b3">[4]</ref> 93.67 94.47 AM-Softmax <ref type="bibr" target="#b29">[30]</ref> 94  outperform the simple mining-based or margin-based ones. Despite this, our imporved SV-AM-Softmax still achieves about 0.3% improvements. To the BLUFR protocol, the similar trends as the 6000 pairs accuracy, our improved SV-AM-Softmax loss achieves the best performance among all the competitors. Due to the evaluation protocols on LFW are nearly to be saturated, it would be better to test our models on MegaFace and Trillion Pair Challenges. <ref type="table">Table 5</ref> shows the identification and verification results on MegaFace dataset. In particular, compared with the baseline Softmax loss and the mining-based Softmax losses, our SV-Softmax loss achieves at least 3% improvements at both the Rank-1 identification rate and the verification TPR@FAR=1e-6 rate. The reason is that our SV-Softmax loss has clearly defined the hard examples (i.e., support vectors), thus it is better than existing mining-based losses. While compared with the margin-based Softmax losses, the performance of our SV-Softmax loss is slightly lower than them. This is reasonable because the support vectors decided by the Softmax decision boundary in SV-Softmax loss may not be enough for learning discriminative fea-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experiments on MegaFace Challenge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Identification</head><p>Verification TPR@FAR=1e-3 TPR@FAR=1e-9 Softmax 36.61 33.87 F-Softmax <ref type="bibr" target="#b11">[12]</ref> 39.80 37.14 HM-Softmax <ref type="bibr" target="#b21">[22]</ref> 36.75 34.46 A-Softmax <ref type="bibr" target="#b13">[14]</ref> 43.89 43.76 Arc-Softmax <ref type="bibr" target="#b3">[4]</ref> 57.48 57.45 AM-Softmax <ref type="bibr" target="#b29">[30]</ref> 61 tures. Our improved versions SV-Arc-Softmax and SV-AM-Softmax losses, wherein the support vectors are determined by the margin-based decision boundaries, can further boost the performance because they absorb the complementary merits of margin-based losses. Specifically, to our SV-AM-Softmax loss, it beats the best margin-based competitor AM-Softmax loss by a large margin (about 2.4% at Rank-1 identification rate and 1.9% verification rate). Compared with the naive fusions of mining-based and margin-based losses, our improved SV-AM-Softmax loss is also better than them. It is about 2.4% higher at Rank-1 identification rate and 1.8% higher at verification rate than the second best competitor HM-AM-Softmax loss. To sum up, our imporved SV-X-Softmax losses, which eliminate the ambiguity of hard examples as well as absorb the discriminative power of other classes by focusing on support vectors, are inherently the best in the current stage. In <ref type="figure" target="#fig_7">Figure 7</ref>, we draw both of the CMC curves to evaluate the performance of face identification and the ROC curves to evaluate the performance of face verification on MegaFace Set 1. From the curves, we can see the similar trends at other measures. In this experiment, our SV-Softmax loss with its improved version SV-AM-Softmax approach have shown their superiority for both the identification and verification tasks. <ref type="table">Table 3</ref> displays the performance comparison on the recent Trillion Pairs Challenge, from which, we can conclude that the results exhibit the same trends that emerged on LFW and MegaFace datasets. Besides, the trends are more obvious. Concretely, both of the current mining-based and margin-based losses are better than the simple softmax loss for face recognition. However, the margin-based losses usually achieve higher performance than the mining-based  losses, because the motivation of margin-based losses is to enhance the feature discrimination while the motivation of mining-based losses is to focus training on hard examples. Their naive fusions can slightly improve the performance further. However, the naive fusions are still suffering from the ambiguity of hard examples and the lack of discriminative power of other classes. Therefore, they are limited for face recognition. Our SV-X-Softmax (e.g., SV-AM-Softmax) losses absorb the strengths and discard the drawbacks of the current ming-based and margin-based loss functions, thus they achieve the highest performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Experiments on Trillion Pairs Challenge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Improvement by Designing Architectures</head><p>To further boost the performance, we try to make the adopted Attention-56 <ref type="bibr" target="#b30">[31]</ref> architecture deeper. Specifically, we change the stages of [1,1,1] used in Attention-56 into <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref>. Moreover, inspired by <ref type="bibr" target="#b3">[4]</ref>, we incorporate the IRSE module into the architecture. The results are displayed in Tables 4-6. Note that all current results are training based on the simple MS-Celeb-1Mv1c dataset and only the single model performance is reported. From the numbers, we can see that our SV-AM-Softmax loss has achieved the competitive absolute performance. In the future, it would be better to fuse the MS1M-ArcFace <ref type="bibr" target="#b3">[4]</ref> and Asian datasets <ref type="bibr" target="#b3">4</ref> and design model ensemble methods (e.g., feature concatenation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper has proposed a simple but very effective loss function, namely support vector guided softmax loss (i.e., SV-Softmax), for face recognition. In specific, SV-Softmax loss explicitly concentrates on optimizing the support vectors. Thus it semantically integrates the motivation of mining-based and margin-based loss functions into one framework. Consequently, it is intrinsically better than the current mining-based losses, margin-based losses and their naive fusions. Extensive experiments on several benchmark datasets have clearly demonstrated the advantages of our new approach over the state-of-the-art alternatives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>L 2 =</head><label>2</label><figDesc>−g(p y ) log e s cos(θw y ,x) e s cos(θw y ,x) + K k =y e s cos(θw k ,x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>&amp; = 0; cos(' ) * ,+ -. / cos(' ) 1 ,+ -. 2 0 # &amp; % ",$ = 3; cos(' ) * ,+ 4 . / cos ' ) 1 ,+ 4 &lt; 0 A geometrical interpretation of SV-Softmax loss from feature perspective. The support vectors (red circle points) are those who are mis-classified by the current classifiers. SV-Softmax loss semantically focuses on optimizing such support vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>,θw y,x ) e sf (m,θw y ,x) + K k =y e s cos(θw k ,x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>techniques are proposed to focus training on a sparse set of hard examples and ignore the vast number of easy ones during training. However, they either empirically sample hard examples according to loss values or empirically downweight the easy examples by a modulating factor. In other words, the definition of hard examples is ambiguous, and From left to right: SV-Softmax loss vs. Mining-based softmax loss (e.g., Focal loss [12]). SV-Softmax loss semantically defines the hard examples (support vectors) and emphasizes them from the probability view, while the hard examples of Focal loss are ambiguous and are concerned from the loss view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>.29 : = cos6&lt; &gt; 78.29  ? &lt; @ : -&lt; AMargin-basedMining-based(! " ) = #(1 $ ! " ) % &amp;Normalized feature and weights Logits Logits Probability Ground truth Cross-entropy lossSV-SoftmaxLogits Pipeline of our SV-Softmax loss and its relations to the existing mining-based and margin-based losses. Our SV-Softmax loss semantically integrates the motivation of mining-based and margin-based losses into one framework, but from different viewpoints. cos $ " + % &amp; 1 cos ' ! $ ! + '( &amp; ' "    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>From left to right: SV-Softmax loss vs. Margin-based softmax loss. SV-Softmax loss enlarges the feature margin from other classes (other-motivation) while current margin-based losses are directly from the ground truth class (self-motivation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>From left to right: Identification and Verification performance (%) of SV-Softmax loss with different indicator parameter t on LFW and MegaFace, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Left: CMC curves of different loss functions with 1M distractors on MegaFace [9] Set 1. Right: ROC curves of different loss functions with 1M distractors on MegaFace [9] Set 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.3 Parameter t 95 96 97 98 99 100 LFW Accuracy VR@FAR=1e-3 VR@FAR=1e-4 VR@FAR=1e-5 1.</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Performance (%) of SV-AM-Softmax loss on LFW.</figDesc><table><row><cell>LFW</cell><cell cols="3">LFW BLUFR LFW BLUFR LFW BLUFR</cell></row><row><cell>6000</cell><cell>TPR</cell><cell>TPR</cell><cell>TPR</cell></row><row><cell>Accuracy</cell><cell>@FAR=1e-3</cell><cell>@FAR=1e-4</cell><cell>@FAR=1e-5</cell></row><row><cell>99.85 (our)</cell><cell>99.92</cell><cell>99.89</cell><cell>99.13</cell></row><row><cell>99.87 (1st)</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell cols="4">MegaFace Identification MegaFace Verification</cell></row><row><cell></cell><cell>Rank-1@1e6</cell><cell cols="2">TPR@FAR=1e-6</cell></row><row><cell></cell><cell>98.82 (our)</cell><cell cols="2">99.03 (our)</cell></row><row><cell></cell><cell>99.93 (1st)</cell><cell cols="2">99.93 (1st)</cell></row><row><cell cols="4">Table 5. Performance (%) of SV-AM-Softmax loss on MegaFace.</cell></row><row><cell cols="4">Trillion Pairs Identification Trillion Pairs Verification</cell></row><row><cell cols="2">TPR@FAR=1e-3</cell><cell cols="2">TPR@FAR=1e-9</cell></row><row><cell cols="2">82.25 (our)</cell><cell cols="2">78.49 (our)</cell></row><row><cell></cell><cell>85.67 (1st)</cell><cell cols="2">82.29 (1st)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Performance (%) of SV-AM-Softmax loss on Trillion Pairs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://trillionpairs.deepglint.com/overview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://trillionpairs.deepglint.com/overview</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://trillionpairs.deepglint.com/data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.fgnet.rsunit.com/.2010" />
		<title level="m">Fg-net aging database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector guided dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained enviroments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Soft-margin softmax for deep classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A benchmark study of large-scale unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning deep features via congenerous cosine loss for person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crossmodality face recognition via heterogeneous joint bayesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06904</idno>
		<title level="m">Residual attention network for image classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Normface: l2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09414</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adaptively unified semisupervised dictionary learning with active points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ensemble soft-margin softmax loss for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03922</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Faceboxes: A cpu real-time face detector with high accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
