<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive NMS: Refining Pedestrian Detection in a Crowd</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
							<email>liusongtao@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
							<email>dhuang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
							<email>yhwang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive NMS: Refining Pedestrian Detection in a Crowd</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection in a crowd is a very challenging issue. This paper addresses this problem by a novel Non-Maximum Suppression (NMS) algorithm to better refine the bounding boxes given by detectors. The contributions are threefold: (1) we propose adaptive-NMS, which applies a dynamic suppression threshold to an instance, according to the target density; (2) we design an efficient subnetwork to learn density scores, which can be conveniently embedded into both the single-stage and two-stage detectors; and (3) we achieve state of the art results on the CityPersons and CrowdHuman benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last two decades, pedestrian detection, as a special branch of general object detection, has received considerable attention. In the literature, many solutions have been presented to handle such an issue, and similar as in general object detection, the past several years have witnessed its technical development from models relying on hand-crafted features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref> to deep learning networks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>. Due to the capability of learning discriminative features, Convolutional Neural Networks (CNN) based approaches dominate this area, and the results on public benchmarks are significantly promoted.</p><p>In recent years, pedestrian detection is urgently required in the real-world scenario where the density of people is high, i.e., airports, train stations, shopping malls etc. Despite great progress achieved, detecting pedestrians in those scenes still remains difficult, evidenced by significant performance drops of state of the art methods. For example, OR-CNN <ref type="bibr" target="#b48">[49]</ref>, a more recent work, reports a Miss Rate (MR) of 4.1% on the Caltech database <ref type="bibr" target="#b5">[6]</ref>, which does not consider this challenge. Its MR degrades to 11.0% on CityPersons <ref type="bibr" target="#b46">[47]</ref>, where 26.4% pedestrians are overlapped * corresponding author with an Intersection over Union (IoU) above 0.3 and the average of pairwise overlap between two human instances (larger than 0.5 IoU) is 0.32 per image. Therefore, it becomes a necessity to work on pedestrian detection in a crowd. While one may argue that this problem is the same as occlusion, they are indeed different, as in a crowd scene, pedestrians whose appearances are similar often occlude each other by a large part, making it even more challenging.</p><p>This work focuses on this issue, and we start with the analysis of deep learning based detectors. As we know, existing detectors either directly regress the default anchors into detection boxes on the feature maps (single-stage detectors, e.g., SSD <ref type="bibr" target="#b22">[23]</ref>, YOLO <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, RetinaNet <ref type="bibr" target="#b20">[21]</ref>), or first generate category independent region proposals and then refine them (two-stage detectors, e.g., Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, R-FCN <ref type="bibr" target="#b18">[19]</ref>). All the methods produce large numbers of false positives near the ground truth, and the greedy Non-Maximum Suppression (NMS) is necessary to screen out final detections by sharply reducing the false positives. In a crowded scenario, however, greedy-NMS encounters a problem. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, even with a powerful detector that can predict exactly the same bounding boxes as the ground truth, the highly overlapped ones are still suppressed by the post process of greedy-NMS with a normal threshold of 0.5. It makes the current CNN based detectors confront with a dilemma for the single threshold of greedy-NMS: a lower threshold leads to missing highly overlapped objects while a higher one brings in more false positives.</p><p>To address this problem, <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b48">[49]</ref> propose additional penalties to produce more compact bounding boxes and thus become less sensitive to the threshold of NMS. The ideal solution for crowds under their pipelines with greedy-NMS is to set a high threshold to preserve highly overlapped objects and predict very compact (higher than the threshold) detection boxes for all instances to reduce false positives. Unfortunately, this is not so easy, as the CNN based detectors often assign correlated scores to the neighboring regions around the object.</p><p>Recently, <ref type="bibr" target="#b0">[1]</ref> proposes a soft version of NMS, which decreases the associated detection scores according to an increasing function of overlap instead of discarding them. There also exist some works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> that build an extra module or network to learn the NMS function from data. They show a better performance than greedy-NMS in general object detection. In contrast, in a crowded scenario, the NMS function has to process a much larger set of highly-overlapped boxes and a considerable part of them are true positives. While similar softer heuristics or learning methods may also be applied, they are inefficient as soft-NMS still blindly penalizes highly overlapped boxes. Furthermore, the similarity of CNN based appearance features blurs the boundaries between highly overlapped true positives and duplicates. <ref type="bibr" target="#b33">[34]</ref> presents a quadratic unconstrained binary optimization solution to replace the greedy NMS in pedestrian detection, but it also sets a hard threshold to suppress all highly-overlapped detection boxes like greedy-NMS. <ref type="bibr" target="#b17">[18]</ref> extends the optimization model with individualness scores, which relies on discriminative CNN features.</p><p>In this paper, we propose a new NMS algorithm named adaptive-NMS that acts as a more effective alternative to deal with pedestrian detection in a crowd. Intuitively, a high NMS threshold keeps more crowded instances while a low NMS threshold wipes out more false positives. The adaptive-NMS thus applies a dynamic suppression strategy, where the threshold rises as instances gather and occlude each other and decays when instances appear separately. To this end, we design an auxiliary and learnable sub-network to predict the adaptive NMS threshold for each instance.</p><p>Experiments are conducted on the CityPersons <ref type="bibr" target="#b46">[47]</ref> and CrowdHuman <ref type="bibr" target="#b35">[36]</ref> databases, and our adaptive-NMS delivers promising improvements for both the two-stage and single-stage detectors on crowded pedestrian detection, indicating its effectiveness. Additionally, we reach state of the art performance, i.e. 10.8% MR −2 on CityPersons and 49.73% MR −2 on CrowdHuman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generic object detection. The traditional approaches to object detection are based on sliding window or region proposal classification using hand-crafted features. In the era of deep learning, R-CNN <ref type="bibr" target="#b9">[10]</ref>, builds the two-stage framework by combining the straightforward strategy of box proposal generation like SS <ref type="bibr" target="#b41">[42]</ref> and a CNN based classifier on these region candidates and displays a breathtaking improvement. Its descendants (e.g., Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>) update the two-stage framework and achieve dominant performance. In contrast to the two-stage approaches, another alternative is single-stage framework based (e.g., SSD <ref type="bibr" target="#b22">[23]</ref>, YOLO <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>), which skips the proposal generation step and directly predicts bounding boxes and class probabilities on deep CNN features, aiming to accelerate detection.</p><p>Pedestrian detection. Traditional pedestrian detectors, such as ACF <ref type="bibr" target="#b3">[4]</ref>, LDCF <ref type="bibr" target="#b10">[11]</ref> and Checkerboards <ref type="bibr" target="#b47">[48]</ref>, extend the Viola and Jones paradigm <ref type="bibr" target="#b42">[43]</ref> to exploit various filters on Integral Channel Features (ICF) <ref type="bibr" target="#b4">[5]</ref> with the sliding window strategy.</p><p>Afterward, coupled with the prevalence of deep learning techniques, CNN-based models rapidly dominate this field. In <ref type="bibr" target="#b44">[45]</ref>, hand-crafted features are replaced with deep neural network features before being fed into a boosted decision forest. <ref type="bibr" target="#b1">[2]</ref> performs detection at multiple layers to match objects of different scales, and adopts an upsampling operation to handle small instances. <ref type="bibr" target="#b25">[26]</ref> presents a jointly learning framework with extra features to further improve performance. <ref type="bibr" target="#b23">[24]</ref> explores the potential of single-stage detectors on pedestrian detection by stacking multi-step prediction for asymptotic localization.</p><p>For the occlusion issue, many efforts have been made in the past years. A common framework <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52]</ref> for occlusion handling is to learn a series of part detectors and integrate the results to localize occluded pedestrians. More recently, several works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref> focus on a more challenging issue of detecting pedestrian in a crowd. <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b35">[36]</ref> propose two pedestrian datasets (i.e., CityPersons and CrowdHuman) to better evaluate detectors in crowd scenarios. <ref type="bibr" target="#b49">[50]</ref> employs an attention mechanism across channels to represent various occlusion patterns. <ref type="bibr" target="#b37">[38]</ref> operates somatic topological line localization to reduce ambiguity. <ref type="bibr" target="#b43">[44]</ref> introduces a bounding box regression loss to not only push each proposal to reach its designated target, but also keep it away from other surrounding objects. Similarly, <ref type="bibr" target="#b48">[49]</ref> designs an aggregation penalty to enforce the proposals locate closely and compactly to the ground-truth objects. These two works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b48">49]</ref> ameliorate detectors to produce more compact proposals and thus become less sensitive to the threshold of NMS in crowded scenes. Another interesting attempt <ref type="bibr" target="#b38">[39]</ref> uses a recurrent LSTM to sequentially generate detections without NMS, but this detection pipeline suffers from scale variations.</p><p>Non-Maximum Suppression. NMS is a widely used post process algorithm in computer vision. It is an essential component of many detection methods, such as edge detection <ref type="bibr" target="#b32">[33]</ref>, feature point detection <ref type="bibr" target="#b24">[25]</ref> and object detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Moreover, despite significant progress in general object detection by deep learning, the hand-crafted and greedy NMS is still the most effective method for this task.</p><p>Recently, soft-NMS <ref type="bibr" target="#b0">[1]</ref> and learning NMS <ref type="bibr" target="#b13">[14]</ref> are proposed to improve NMS results. Instead of discarding all the surrounding proposals with the scores below the threshold, soft-NMS lowers the detection scores of neighbors by an increasing function of their overlap with the higher scored bounding box. It is conceptually satisfying, but still treats all highly-overlapped boxes as false positives. <ref type="bibr" target="#b13">[14]</ref> attempts to learn a deep neural network to perform the NMS function using only boxes and their scores as input, but the network is specifically designed and very complex. <ref type="bibr" target="#b14">[15]</ref> proposes an object relation module to learn the NMS function as an end-to-end general object detector. <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b16">[17]</ref> replace the classification scores of proposals used in the NMS process with learned localization confidences to guide NMS to preserve more accurately localized bounding boxes. These methods prove effective in general object detection, but as we state, pedestrian detection in a crowd has its own challenge. Therefore, different from them, we propose to learn the density around each ground truth object as its own suppression threshold, sharing some similarity with the crowd density map estimation in the people counting task <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. It reduces the requirement for instance-discriminative CNN features, which is the major issue in the crowd scene.</p><p>To address pedestrian detection in a crowd, <ref type="bibr" target="#b33">[34]</ref> proposes a quadratic unconstrained binary optimization solution to suppress detection boxes, which uses detection scores as a unary potential and overlaps between detections as a pairwise potential to produce final results. But it still applies a hard threshold to blindly suppress detection boxes as greedy-NMS does. <ref type="bibr" target="#b17">[18]</ref> adopts the determinantal point process based optimal model with additional individualness scores to discriminate different pedestrians. However, as detectors pay less attention to intra-class differences, the CNN features for crowded individuals tend to be less discriminative, and its optimization procedure also consumes more time. As a result, how to robustly process detection proposals in crowded scenarios is still one of the most criti-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy-NMS</head><p>Adaptive-NMS <ref type="figure">Figure 2</ref>. The pseudo code in red is replaced by that in green in adaptive-NMS, which adaptively suppresses the detections by scaling their NMS threshold according to their densities. cal issues for pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Greedy-NMS Revisit</head><p>In pedestrian detection, the commonly used detection evaluation metric is log-average Miss Rate on False Positive Per Image (FPPI) in [10 −2 , 10 0 ] (denoted as MR or MR −2 following [6]), where the overlap criterion for a true positive is usually 0.5. MR is a good indicator for the detectors applied in the real-world applications since it shows the ability of the detector for balancing recall and precision. As shown in <ref type="figure">Fig. 2</ref>, starting with a set of detection boxes B with corresponding scores S, greedy-NMS firstly selects the one M with the maximum score and moves it from set B to the set of final detections F. It then removes any box in B and its score in S that has an overlap with M higher than a manually set threshold N t . This process is repeated for the remaining B set.</p><p>Applying greedy NMS with a low threshold N t like 0.5 may increase the miss-rate, especially in crowd scenes. The reason lies in there may be many pairs of crowded objects which have higher overlaps than this suppressing threshold <ref type="bibr">Figure 3</ref>. Density prediction framework for both the two-stage and one-stage detectors. We add the density prediction subnet on the top of RPN for two-stage detectors, taking the objectness predictions, bounding box predictions and conv features as input. For one-stage detectors, the subnet is deployed behind the final detection network in a similar way. N t . Within these pairs, when the proposal with the maximum score M is selected, all the surrounding detection boxes that have overlaps greater than N t are suppressed, including the nearby detections that actually locate the other ground truth instances. In this case, true positives may be removed after the NMS processing with a low N t , increasing the miss rate.</p><p>Also, a high N t like 0.7 may increase false positives as many neighboring proposals that are overlapped often have correlated scores. Although more highly overlapped true positives can be kept, the increase in false positives may be more serious because the number of objects is typically smaller than the number of proposals generated by a detector. Therefore, using a high NMS threshold is not a good choice either.</p><p>To address this issue, the soft version of the greedy-NMS algorithm, i.e. soft-NMS <ref type="bibr" target="#b0">[1]</ref>, writes the suppressing step as a re-scoring function:</p><formula xml:id="formula_0">s i = s i , iou(M, b i ) &lt; N t s i f (iou(M, b i )), iou(M, b i ) ≥ N t ,</formula><p>where f (iou(M, b i )) is an overlap based weighting function to change the classification score s i of a box b i which has a high overlap with M. According to this formulation, in greedy-NMS, f (iou(M, b i )) ≡ 0, which means that b i should be directly removed. In soft-NMS, either</p><formula xml:id="formula_1">f (iou(M, b i )) = (1 − iou(M, b i )) or f (iou(M, b i )) = e − iou(M,b i ) 2 σ</formula><p>decays the scores of detections as an increasing function of overlap with M. With the soft penalty, if b i contains another object not covered by M, it does not lead to a miss at a lower detection threshold. However, as an increasing function, it still assigns a greater penalty to the highly overlapped boxes, which approximately equals to that in greedy-NMS.</p><p>Actually, both the design of greedy-NMS and soft-NMS follows the same hypothesis: the detection boxes with higher overlaps with M should have a higher likelihood of being false positives. This hypothesis has no problem when it is used in general object detection, as occlusions in a crowd rarely happen. However, this assumption does not hold in the crowded scenario, where human instances are highly overlapped with each other and should not be treated as false positives. Therefore, to adapt to pedestrian detectors in crowd scenes, NMS should take the following conditions into account,</p><p>• For detection boxes which are far away from M, they have a smaller likelihood of being false positives and they should thus be retained.</p><p>• For highly overlapped neighboring detections, the suppression strategy depends on not only the overlaps with M but also whether M locates in the crowded region.</p><p>If M locates at the crowded region, its highly overlapped neighboring proposals are very likely to be true positives and should be assigned a lighter penalty or preserved. But for the instance in the sparse region, the penalty should be higher to prune false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adaptive-NMS</head><p>According to the above analysis, increasing the NMS threshold to preserve neighboring detections with high overlaps when the object is in a crowded region seems to be a promising solution to NMS in crowd scenes. It is also clear that the highly-overlapped proposals in the sparse region should be removed, as they are more likely to be false positives.</p><p>To quantitatively design the pruning strategy, we first define the object density as follows,</p><formula xml:id="formula_2">d i := max bj ∈G,i =j iou(b i , b j ),</formula><p>where the density of the object i is defined as the max bounding box IoU with other objects in the ground truth set G. The density of objects indicates the level of crowd occlusion.</p><p>With this definition, we propose to update the pruning step with the following strategy,</p><formula xml:id="formula_3">N M := max(N t , d M ), s i = s i , iou(M, b i ) &lt; N M s i f (iou(M, b i )), iou(M, b i ) ≥ N M ,</formula><p>where N M denotes the adaptive NMS threshold for M, and d M is the density of the object M covers. We note three properties of this suppression strategy. The adaptive-NMS algorithm is formally described in <ref type="figure">Fig. 2</ref>. As we only replace the fixed threshold N t with the adaptive ones, the computational complexity for adaptive-NMS is the same as traditional greedy-NMS and soft-NMS. The only extra cost for adaptive-NMS is an N -element list that stores the predicted density for each proposal, which is negligible for today's hardware configuration. Hence the adaptive-NMS does not affect the running time of current detectors much, keeping the efficiency as that of greedy-NMS and soft-NMS.</p><p>Note that adaptive-NMS works well with both greedy-NMS and soft-NMS. For fair comparison with soft-NMS, we adopt the original re-scoring function in greedy-NMS by default if not specified.</p><p>Once we know the density of the object, the adaptive-NMS flexibly preserves its neighbors and prunes the false positives. But we actually skip a major issue that is how to predict the density of each object, which is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Density Prediction</head><p>We treat density prediction as a regression task, where the target density value is calculated following its definition and the training loss is the Smooth-L1 loss.</p><p>A natural way for this regression is to add a parallel head layer at the top of the network just like classification and localization. However, the features used for detection only contain the information of the object itself, e.g., appearance, semantic feature and position. For density prediction, it is very difficult to estimate the density using the individual object information since it needs more clues about the surrounding objects.</p><p>To counter this, we design an extra subnet of three convolutional layers, as shown in <ref type="figure">Fig. 3</ref>, to predict the density of each proposal. We note that this subnet is compatible with both the two-stage and one-stage detectors. For two-stage detectors, we construct the density subnet behind RPN. We first apply a 1 × 1 conv layer to reduce the dimension of the convolutional feature maps, and we then concatenate the reduced feature maps as well as the objectness and bounding boxes predicted by RPN as the input of the density subnet. Moreover, we apply a large kernel (5 × 5) at the final conv layer of the density subnet to take the surrounding information into account. For one-stage detectors, the density subnet is deployed behind the final detection network in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the proposed adaptive-NMS method, we conduct several experiments on two crowd pedestrian datasets: CityPersons <ref type="bibr" target="#b46">[47]</ref> and CrowdHuman <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CityPersons</head><p>Dataset and Evaluation Metrics. The CityPersons <ref type="bibr" target="#b46">[47]</ref> dataset is a new pedestrian detection dataset which is built on top of the semantic segmentation dataset CityScapes <ref type="bibr" target="#b2">[3]</ref>. It records street views across 18 different cities in Germany with various weather conditions. The dataset includes 5, 000 images (2, 975 for training, 500 for validation and 1, 525 for testing) with ∼ 35, 000 labeled persons plus ∼ 13, 000 ignored region annotations. Both bounding box annotations of full bodies and visible parts are provided. Moreover, there are approximately 7 pedestrians in average per image, with 0.32 pairwise crowd instances (density higher than 0.5).</p><p>Following the evaluation protocol in CityPersons, all of our models on this dataset are trained on the reasonable training set and evaluated on the reasonable validation set. The log MR averaged over FPPI range of [10 −2 , 10 0 ] (MR −2 ) is used to evaluate the detection performance (lower is better).</p><p>Detector. To demonstrate the effectiveness of adaptive-NMS, we conduct two types of baseline detectors.</p><p>For two-stage detectors, we generally follow the adapted Faster R-CNN framework <ref type="bibr" target="#b46">[47]</ref> and use the pre-trained VGG-16 <ref type="bibr" target="#b36">[37]</ref> as the backbone. We also keep the same anchor sizes and ratios as in <ref type="bibr" target="#b46">[47]</ref>. To improve the detection performance of small pedestrians, we adopt a common trick to use dilated convolution and the final feature map is 1/8 of the input size.</p><p>For one-stage detectors, we modify RFB Net <ref type="bibr" target="#b21">[22]</ref> and also use the VGG-16 <ref type="bibr" target="#b36">[37]</ref> pre-trained on ILSVRC CLSLOC <ref type="bibr" target="#b34">[35]</ref> as the backbone network. Besides, we follow the extension strategy in <ref type="bibr" target="#b21">[22]</ref> to up-sample the conv7 fc feature maps and concat it with the conv4 3 to improve the detection accuracy of pedestrians of small scales.</p><p>For fair comparison, we train the two base detectors with the density sub-network together. All the parameters in the new convolutional layers are randomly initialized with the MSRA method <ref type="bibr" target="#b11">[12]</ref>. We optimize both two detectors using Stochastic Gradient Descent (SGD) with 0.9 momentum and 0.0005 weight decay. For adapted Faster-RCNN, we train it on 4 Titan X GPUs with the mini-batch of 1 image per GPU. The learning rate starts at 10 −3 for the first 20k iterations, and decays to 10 −4 for another 10k iterations. For RFB Net, we set the batch size at 8 on 4 Titian X GPUs. We also follow its "warm-up" strategy <ref type="bibr" target="#b21">[22]</ref> that gradually ramps up the learning rate from 10 −6 to 2 × 10 −3 , and then divide the learning rate by 10 at 120 and 180 epochs with totally 200 epochs in training.</p><p>Ablation Study on Adaptive-NMS. We first ignore the predicted densities and apply greedy-NMS and soft-NMS on detection results with various parameters. We search the NMS threshold N t in greedy-NMS and soft-NMS with the "linear" method to report the best results at N t = 0.5. We also try several normalizing parameters σ in soft-NMS using the "Gaussian" method, but they all increase the miss rate by about 1%. We thus only report the "linear" results for clear presentation in the rest of the paper. We also report the total recall and Average Precision (AP) on the Reasonable set for more reference.</p><p>As shown in <ref type="table">Table 1</ref>, using the traditional greedy-NMS, the adapted Faster R-CNN detector achieves 14.5% MR −2 on the validation set, which is slightly better than the reported result (15.4% MR −2 ) in <ref type="bibr" target="#b46">[47]</ref>. The RFB Net detector achieves 13.9% MR −2 , which is slightly better than the current single-shot detectors <ref type="bibr" target="#b37">[38]</ref> in CityPersons.</p><p>The soft-NMS with the "linear" method slightly reduces the MR −2 by 0.3% (i.e., 14.2% MR −2 vs. 14.5% MR −2 ) for Faster R-CNN detector. For RFB Net, soft-NMS does not work well. Combining adaptive-NMS with soft-NMS also has minor or even negative improvements on metric  MR −2 . The reason is that the low-score detections soft-NMS keeps could be out of the right-hand boundary of FPPI range [10 −2 , 10 0 ]. So MR −2 does not benefit from it. With the proposed adaptive-NMS method, the MR −2 score of the Faster R-CNN detector significantly drops to 12.9% with a 1.6% reduction, and that of the RFB Net detector also reduces by 1.2% (i.e., 13.9% MR −2 vs. 12.7% MR −2 ). These results indicate that adaptive-NMS keeps more true positives, and it is a more effective postprocessing algorithm for detecting pedestrians in crowded scenarios.</p><p>Analysis. The average log MR and recall on the reasonable validation set do not explain us clearly where adaptive-NMS obtains significant gains in performance. We further divide the pedestrians with at least 50 pixel height in the validation set into 5 subsets according to their density (density ≤ 0.4, 0.4 &lt; density ≤ 0.5, 0.5 &lt; density ≤ 0.6, 0.6 &lt; density ≤ 0.7, density &gt; 0.7). For better demonstration, we compare the results of Faster R-CNN with greedy-NMS, soft-NMS ("linear") as well as adaptive-NMS on these subsets. From <ref type="figure" target="#fig_2">Fig. 4</ref>, we can infer that for sparse pedestrians whose density is less than 0.4, all the three NMS algorithms show similar performance. When the density increases, the proposed adaptive-NMS significantly reduces the miss rate compared with the two counterparts. This demonstrates that  <ref type="table">Table 2</ref>. Comparison of detection performance on the CityPersons validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Greedy-NMS</head><p>Adaptive-NMS SoftNMS adaptive-NMS performs better-post processing in the crowd scene, keeping more highly-overlapped true positives.</p><p>In addition, we also show some visual results of the Faster R-CNN detector with greedy-NMS, soft-NMS and adaptive-NMS for comparison. As <ref type="figure" target="#fig_3">Fig. 5</ref> shows, adaptive-NMS keeps more crowded true positives and still removes false positives in the sparse region at the same time.</p><p>Comparison to the State-of-the-art. As adaptive-NMS only focuses on the post process of detectors, it conveniently works with typical advanced pedestrian detectors. Moreover, as illustrated in <ref type="figure" target="#fig_4">Fig. 6</ref>, the minor punishment in the crowd instances increases false positives if the proposals of the ground-truth objects are not compact. Hence, to better validate the effectiveness of adaptive-NMS, we follow <ref type="bibr" target="#b48">[49]</ref> to add the AggLoss term on the regression loss to enforce the proposals locate closely and compactly to the ℳ = 0.73 ℳ = 0.81 ground-truth, which is defined as</p><formula xml:id="formula_4">L com ({t i }, {t * i }) = 1 Ncom Ncom i=1 ∆(t * i − 1 |Φi| j∈Φi t j ),</formula><p>where N com is the total number of ground truths associated with more than one anchor, |Φ i | is the number of anchors associated with the i-th ground truth object,t * i and t i are the associated coordinates of the ground truth and proposals.</p><p>In <ref type="table">Table 2</ref>, we follow the strategy in <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b48">[49]</ref> to divide the Reasonable subset (occlusion &lt; 35%) in the validation set into the Partial (10% &lt; occlusion &lt; 35 %) and Bare (occlusion ≤ 10%) subsets. Meanwhile, we denote the pedestrians with the occlusion ratio of more than 35% as the Heavy set. With the ×1 scale of input images, adaptive-NMS improves the baseline detectors to reach comparable results with those of other counterpart pedestrian detectors without any additional module. For Faster R-CNN, when we add AggLoss <ref type="bibr" target="#b48">[49]</ref> with adaptive-NMS, it achieves the state-of-the-art results on the validation set of CityPersons by reducing 0.9% MR −2 (i.e., 11.9% vs. 12.8% of <ref type="bibr" target="#b48">[49]</ref>). For RFB Net, adaptive-NMS with AggLoss also pushes the performance to 12.0% MR −2 .</p><p>We then enlarge the size of the input image as in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. Due to the GPU memory issue, we do not train the RFB Net detector with ×1.3 scale of input size. For Faster R-CNN, it achieves the best performance of 10.8% MR −2 .</p><p>In addition, we also evaluate the proposed Adaptive-NMS method on the testing set of CityPersons and report the results in <ref type="table">Table 3</ref>. With ×1.3 scale and AggLoss, the Faster R-CNN detector achieves 11.79% MR −2 , while Adaptive-NMS further improves the result to 11.40% MR −2 . It is worth noting that other counterparts either employ a part occlusion-aware pooling module <ref type="bibr" target="#b48">[49]</ref> or a stronger backbone network <ref type="bibr" target="#b43">[44]</ref> (i.e,, ResNet-50). As adaptive-NMS has few constraints for the architecture of detectors, we believe the performance of adaptive-NMS can be further improved with these techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Scale Reasonable Adapted FasterRCNN <ref type="bibr" target="#b46">[47]</ref> VGG-16 ×1.3 12.97 Repulsion Loss <ref type="bibr" target="#b43">[44]</ref> ResNet-50 ×1.  Dataset and Evaluation Metrics. Recently, Crowd-Human <ref type="bibr" target="#b35">[36]</ref> has been released to specifically target to the crowd issue in the human detection task. It collects 15, 000, 4, 370 and 5, 000 images from the Internet for training, validation and testing respectively. There are ∼ 340k persons and ∼ 99k ignore region annotations in the training set. Moreover, the CrowdHuman dataset is of much higher crowdedness compared with all the previous ones (e.g., CityPersons <ref type="bibr" target="#b46">[47]</ref>, KITTI <ref type="bibr" target="#b7">[8]</ref> and Caltech <ref type="bibr" target="#b5">[6]</ref>). As shown in <ref type="table">Table 4</ref>, it contains approximately 22.6 pedestrians in average per image as well as 2.4 pairwise crowd instances (density higher than 0.5).</p><p>We follow the evaluation metric used in CrowdHuman <ref type="bibr" target="#b35">[36]</ref>, denoted as MR −2 as introduced in Section 4.1. All the experiments are trained in the CrowdHuman training set and evaluated in the validation set, and only the full body region annotations are used for training and evaluation.</p><p>Detector. We also conduct two baseline detectors to evaluate the performance of adaptive-NMS.</p><p>For two-stage detectors, as Faster-RCNN <ref type="bibr" target="#b46">[47]</ref> with the VGG-16 backbone fails to reach a good baseline result in our early experiments, we follow <ref type="bibr" target="#b35">[36]</ref> to employ the Feature Pyramid Network (FPN) <ref type="bibr" target="#b19">[20]</ref> with a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> as the new backbone network. We also use the same settings of design parameters, such as [1.0,1.5,2.0,2.5,3.0] anchor ratios and no clipping proposals. For one-stage detectors, we use RFB Net with the same architecture as in Section 4.1.</p><p>As the images of CrowdHuman are collected from websites with various sizes, we resize them so that the shorter image side is 800 pixels for FPN. The input size of RFB Net is set as 800 × 1200. The base learning rate is set to 0.02 and 0.002 for FPN and RFB Net respectively, and divided by 10 at 150k and 450k for FPN, and 400k and 600k for RFB Net. The SGD solver with 0.9 momentum is adopted to optimize the networks on 4 Titian X GPUs with the minibatch of 2 images per GPU, while the weight decay is set at 0.0001 and 0.0005 for FPN and RFB Net respectively. For fair comparison with <ref type="bibr" target="#b35">[36]</ref>, we do not use additional losses such as AggLoss <ref type="bibr" target="#b48">[49]</ref> or Repulsion Loss <ref type="bibr" target="#b43">[44]</ref>.</p><p>Evaluation Results. In <ref type="table">Table 5</ref>, our baseline detectors achieve comparable results as <ref type="bibr" target="#b35">[36]</ref> does. When we replace greedy-NMS with adaptive-NMS, the miss rate drops by 2.62% MR −2 and 2.19% MR −2 for FPN and RFB Net respectively. It proves that the proposed adaptive-NMS algorithm is effective and has a good potential for processing detectors in crowd scenes.  <ref type="table">Table 5</ref>. Evaluation of full body detections on the CrowdHuman validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we present a new adaptive-NMS method to better refine the bounding boxes in crowded scenarios. Adaptive-NMS applies a dynamic suppression strategy, where an additionally learned sub-network is designed to predict the threshold according to the density for each instance. Experiments are conducted on the CityPersons <ref type="bibr" target="#b46">[47]</ref> and CrowdHuman <ref type="bibr" target="#b35">[36]</ref> databases, and state of the art results are reached, showing its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>7 Figure 1 .</head><label>71</label><figDesc>(a) original image (b) prediction before NMS (c) NMS threshold =0.5 (d) NMS threshold =0.Illustration of greedy-NMS results of different thresholds. The blue box shows the missing object, while the red ones highlight false positives. The bounding boxes in (b) are generated using Faster R-CNN. In a crowd scene, a lower NMS threshold may remove true positives (c) while a higher NMS threshold may increase false positives (d). The threshold for visualization is above 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>When the neighboring boxes which are far away from M (i.e., iou(M, b i ) &lt; N t ), they are retained the same as the original NMS does. (2) If M locates in the crowded region (i.e., d M &gt; N t ), the density of M is used as the adaptive NMS threshold. Hence, the neighboring proposals are preserved, as they probably locate other objects around M. (3) For the objects in a sparse region (i.e., d M ≤ N t ), the NMS threshold N M equals to N t . Then, the pruning step is equivalent to the original NMS, where very close boxes are suppressed as false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>The MR −2 results in 5 groups with different levels of crowd occlusions. Adaptive-NMS works much better on the higher density groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparisons of the Faster R-CNN pedestrian prediction results (green boxes) with greedy-NMS, soft-NMS and adaptive-NMS. Blue boxes are missing objects, while red boxes are false positives. The scores thresholded for visualization are above 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Failure cases of adaptive-NMS with the 0.3 visual score threshold. Red boxes are false positives. As the NMS threshold (NM) increases for crowd instances, more false positives are also preserved if the proposals are not compact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodScale Backbone Reasonable Heavy Partial Bare</figDesc><table><row><cell>Adapted Faster RCNN [47]</cell><cell>×1 ×1.3</cell><cell>VGG-16 VGG-16</cell><cell>15.4 12.8</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>Repulsion Loss [44]</cell><cell cols="2">×1 ×1.3 ResNet-50 ResNet-50</cell><cell>13.2 11.6</cell><cell>56.9 55.3</cell><cell>16.8 14.8</cell><cell>7.6 7.0</cell></row><row><cell>OR-CNN [49]</cell><cell>×1 ×1.3</cell><cell>VGG-16 VGG-16</cell><cell>12.8 11.0</cell><cell>55.7 51.3</cell><cell>15.3 13.7</cell><cell>6.7 5.9</cell></row><row><cell>AggLoss [49] Adaptive-NMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster RCNN</cell><cell>×1</cell><cell>VGG-16</cell><cell>12.9</cell><cell>56.4</cell><cell>14.4</cell><cell>7.0</cell></row><row><cell></cell><cell>×1</cell><cell>VGG-16</cell><cell>13.2</cell><cell>56.0</cell><cell>14.0</cell><cell>7.7</cell></row><row><cell></cell><cell>×1</cell><cell>VGG-16</cell><cell>11.9</cell><cell>55.2</cell><cell>12.6</cell><cell>6.2</cell></row><row><cell></cell><cell>×1.3</cell><cell>VGG-16</cell><cell>11.4</cell><cell>55.6</cell><cell>11.9</cell><cell>6.2</cell></row><row><cell></cell><cell>×1.3</cell><cell>VGG-16</cell><cell>10.8</cell><cell>54.0</cell><cell>11.4</cell><cell>6.2</cell></row><row><cell>RFB Net</cell><cell>×1</cell><cell>VGG-16</cell><cell>12.7</cell><cell>51.9</cell><cell>11.7</cell><cell>7.6</cell></row><row><cell></cell><cell>×1</cell><cell>VGG-16</cell><cell>13.1</cell><cell>51.7</cell><cell>12.0</cell><cell>7.4</cell></row><row><cell></cell><cell>×1</cell><cell>VGG-16</cell><cell>12.0</cell><cell>51.2</cell><cell>11.9</cell><cell>6.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison of detection performance on CityPersons test. Comparison in terms of the average number of persons and pair-wise overlap between two instances on the three datasets.</figDesc><table><row><cell>4.2. CrowdHuman</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Caltech [6] City [47] Crowd [36]</cell></row><row><cell># person/img</cell><cell>0.32</cell><cell>6.47</cell><cell>22.64</cell></row><row><cell># pair/img</cell><cell></cell><cell></cell><cell></cell></row><row><cell>iou&gt;0.3</cell><cell>0.06</cell><cell>0.96</cell><cell>9.02</cell></row><row><cell>iou&gt;0.5</cell><cell>0.02</cell><cell>0.32</cell><cell>2.40</cell></row><row><cell>iou&gt;0.7</cell><cell>0.00</cell><cell>0.08</cell><cell>0.33</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms: improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rogerio S Feris, and Nuno Vasconcelos. A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast feature pyramids for object detection. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian classification with partial occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Eigenstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariu M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Individualness and determinantal point processes for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonho</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning efficient single-stage pedestrian detectors by asymptotic localization fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Distinctive image features from scaleinvariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Handling occlusions with franken-classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Edge and curve detection for visual scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azriel</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thurston</surname></persName>
		</author>
		<idno>1971. 3</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimized pedestrian detection for multiple and occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitapa</forename><surname>Rujikietgumjorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Small-scale pedestrian detection based on topological line localization and temporal feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leiyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving object localization with fitness nms and bounded iou loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lachlan</forename><surname>Tychsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Repulsion loss: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Citypersons: A diverse dataset for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Occlusion-aware r-cnn: Detecting pedestrians in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-label learning of part detectors for heavily occluded pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bi-box regression for pedestrian detection and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
