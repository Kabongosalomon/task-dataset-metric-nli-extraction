<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Variational Representation for Heterogeneous Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
							<email>huaibo.huang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vpatel36@jhu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 N. Charles St</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
							<email>znsun@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Variational Representation for Heterogeneous Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for crossmodal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, methods based on deep convolution neural network (CNN) have shown impressive performance improvements for face detection and recognition problems <ref type="bibr" target="#b13">(Parkhi, Vedaldi, and Zisserman 2015;</ref><ref type="bibr">Wu et al. 2018a)</ref>. Despite the success of CNN-based methods in addressing various challenges in face recognition such as variations in pose, expression, aging, occlusion, disguise, and illumination, they are specifically designed to recognize face images that are collected at or near the visible (VIS) domain. However, in many real-world applications such as surveillance at night-time and in low-light conditions, one has to be able to <ref type="figure">Figure 1</ref>: An overview of the proposed Disentangled Variational Representation (DVR) approach for VIS-NIR matching. The NIR and VIS representations x N and x V are disentangled into (µ N , σ N ) and (µ V , σ V ), respectively. We assume that there is a linear relationship, P , between lighting variations, i.e., σ V = P σ N . The mean discrepancy is used to measure the difference between the NIR and VIS distributions in the latent space. The reconstructionsx N andx V are obtained from the likelihood p(x N |z N ) and p(x V |z V ), respectively and are constrained by the cross-entropy loss. recognize faces collected in thermal or near infrared (NIR) domains. The performance of many CNN-based face recognition methods often degrades significantly when confronted by the NIR face images. This is mainly due to the significant distributional change between the NIR and VIS domains.</p><p>Another issue that one has to overcome when designing CNN-based models for heterogeneous face recognition (HFR) is over-fitting, which happens due to the lack of sufficient training samples. One of the reasons why CNN-based face recognition methods provide impressive performance improvements on various face recognition benchmarks is that they are trained on thousands and millions of annotated face images often downloaded from the internet. In contrast, there is no publicly available large-scale annotated NIR face dataset for training deep networks. As a result, CNNs trained on small-scale NIR data often tend to overfit. Hence, it is necessary to explore other methods that can deal with this issue in HFR.</p><p>Various methods have been developed in the literature for VIS to NIR cross-modal face recognition <ref type="bibr">Reale et al. 2016)</ref>. In particular, methods such as <ref type="bibr" target="#b13">(Liu et al. 2016;</ref><ref type="bibr">Wu et al. 2018b;</ref><ref type="bibr" target="#b17">Song et al. 2018;</ref><ref type="bibr" target="#b2">Di, Zhang, and Patel 2018)</ref> attempt to reduce the domain gap between the NIR and VIS domains and learn domain invariant representations for HFR.</p><p>In contrast, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) to deal with the two aforementioned challenges. First, inspired by the observation that the facial appearance is composed of the identity information and the variation information, as shown in <ref type="figure">Fig. 1</ref>, we assume that there exists an independent latent variable, which can be composed of an intrinsic variable for identity and an intra-personal variable for within-person variation. Second, benefiting from the variational lower bound (Kingma and Welling 2014) to tackle the marginal likelihood estimation, we model the approximate posterior and obtain disentangled latent variable. Next, when imposing the minimization of the identity information for the same subject and the assumption of correlation alignment <ref type="bibr" target="#b18">(Sun and Saenko 2016)</ref> between different modality variations, we obtain more compact and discriminative disentangled latent space for DVR. Although there are large light spectrum variations, spectrum variations are often assumed to be on linear subspaces. Hence, we employ a relaxed correlation alignment item to constrain the variations of different modalities. Furthermore, generating samples from the approximate posterior significantly alleviates the need for having large number of samples during training the fully connected layers of deep HFR models. Since the effectiveness of the generated samples from the likelihood depends on the estimated approximate posterior, we propose an alternative optimization approach for the DVR framework during training in which HFR network can contribute to the disentangled representation training and vice versa.</p><p>To summarize, the following are our main contributions: • An end-to-end DVR framework is developed for crossmodal NIR-VIS face matching. We introduce a variational lower bound to estimate the posterior and optimize the latent variable space, aiming at disentangling the NIR and VIS face representations. • We propose to minimize the identity information for the same subject and the relaxed correlation alignment constraint on modality variations that facilitate modeling the compact and discriminative disentangled latent variable spaces for heterogeneous modalities. • An alternative optimization is proposed to provide mutual promotion between HFR network and disentangled variational representation part. Thus, DVR can both reduce the domain discrepancy and alleviate over-fitting. • Extensive experimental results are conducted on three HFR databases, including the CASIA NIR-VIS 2.0 database , the Oulu-CASIA NIR-VIS database <ref type="bibr" target="#b0">(Chen et al. 2009</ref>) and the BUAA-VisNir database <ref type="bibr" target="#b7">(Huang, Sun, and Wang 2012)</ref>, and comparisons are performed against several recent state-of-the-art approaches. Furthermore, an ablation study is conducted to demonstrate the improvements obtained by various components of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We follow the notations in <ref type="bibr" target="#b24">(Zhu et al. 2014;</ref><ref type="bibr" target="#b17">Song et al. 2018)</ref> while providing a brief survey of HFR and disentangled representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous Face Recognition (HFR)</head><p>The problem of HFR has gained a lot of traction in recent years <ref type="bibr" target="#b20">(Xiao et al. 2013;</ref><ref type="bibr" target="#b13">Ouyang et al. 2016</ref>). According to <ref type="bibr" target="#b24">(Zhu et al. 2014)</ref>, the existing methods are divided into the following three main categories: Latent subspace learning aims to project the heterogenous data onto a common latent space in which the relevance of heterogeneous data can be measured. Lin <ref type="bibr" target="#b12">(Lin and Tang 2006)</ref> proposed a Common Discriminant Feature Extraction (CDFE) method to incorporate both discriminative and locality information. By introducing feature selection via nuclear norm, a common subspace learning was employed in <ref type="bibr" target="#b19">(Wang et al. 2016</ref>). Shao et al <ref type="bibr" target="#b15">(Shao, Kit, and Fu 2014)</ref> project NIR and VIS data into a generalized subspace where each NIR sample can be represented by a combination of VIS samples. Restricted Boltzmann Machines (RBMs) are employed in <ref type="bibr" target="#b21">(Yi et al. 2015)</ref> to learn a shared representation between different domains and then Principal Component Analysis (PCA) is applied to remove the redundancy and heterogeneity. Wang Data synthesis attempts to address the domain discrepancy at image level by transforming face images from one modality into another via image synthesis. Data synthesis is first proposed to synthesize and recognize a sketch image from a face photo in <ref type="bibr" target="#b19">(Tang and Wang 2003)</ref>. Wang (Wang and Tang 2009) applies Markov Random Field (MRF) to transform pseudo-sketch to face photo in a multi-scale way. In (Juefei-Xu, Pal, and Savvides 2015), joint dictionary learning is used to reconstruct face images and then perform face matching. Lezama et al (Lezama, Qiu, and Sapiro 2017) propose a cross-spectral hallucination and low-rank embedding to synthesize a heterogeneous image in a patch way. With developments of a photo-realistic synthesis image by Generative Adversarial Network (GAN) (Goodfellow et al. 2014), "recognition via generation" <ref type="bibr" target="#b23">(Zhao et al. 2017;</ref><ref type="bibr" target="#b6">Huang et al. 2017;</ref><ref type="bibr" target="#b5">Hu et al. 2018</ref>) is drawn attention by lots of researchers. Song et al <ref type="bibr" target="#b17">(Song et al. 2018</ref>) utilize a Cycle-GAN ) to realize a cross-spectral face hallucination, facilitating heterogeneous face recognition via generation. However, due to the small number of images in the training set, there are still challenges to synthesize photorealistic VIS face images from NIR images.  <ref type="bibr" target="#b1">(Chen et al. 2012)</ref> propose joint Bayesian formulation to decompose a face representation into three parts, including intrinsic difference, transformation difference and noise. An expectation maximization-like learning procedure is employed to optimize the joint formulation and they achieve promising performance on the face recognition tasks. Shi et al <ref type="bibr" target="#b16">(Shi et al. 2017</ref>) extend the original joint Bayesian approach by modeling the gallery and probe images using two different Gaussian distributions to propose a heterogeneous joint Bayesian approach for HFR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Disentangled Representations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>We begin this section by reviewing the Wasserstein CNN method ) that introduces a probabilistic framework for HFR and shows promising results. Based on the Wasserstein CNN, we give the details of our disentangled variational representation method and the corresponding optimization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revisiting Wasserstein CNN</head><p>Let x N ∈ R d and x V ∈ R d denote the NIR and VIS domain data representations, respectively. In Wasserstein CNN , it is assumed that the data distributions of the representations for the same identity follow a Gaussian distribution. Hence, x N ∼ N (m N , C N ) and</p><formula xml:id="formula_0">x V ∼ N (m V , C V ),</formula><p>where m N , m V are the mean vectors and C N , C V are the covariance matrices. The 2-Wasserstein distance between x N and x V corresponding to the same identity is defined as</p><formula xml:id="formula_1">W (x N , x V ) = m N − m V 2 2 + trace(C N + C V − 2(C 1 2 V C N C 1 2 V )). (1)</formula><p>Due to the ability of measuring the consistency between two distributions, Eq. (1) is used to reduce the domain gap between the NIR and VIS images. However, the Wasserstein distance is directly imposed on the NIR and VIS representations, which are obtained from a CNN. It is well-known that CNN-based NIR and VIS representations contain various high-level information including identity, spectrum, pose, noise, etc., which are not disentangled. Therefore, directly matching representation distributions may not lead to better performance especially when the training set is not large enough for HFR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangled Variational Representation</head><p>Let</p><formula xml:id="formula_2">{x (i) ∈ R d } N i=1 and {z (i) ∈ R h } N i=1</formula><p>denote N observations and the independent latent variables corresponding to one identity, respectively. For each sample x (i) , we can obtain</p><formula xml:id="formula_3">z (i) = µ (i) + σ (i) , (2) where µ (i) represents the identity information, σ (i) contains variations, ∼ N (0, I), µ (i) , σ (i) , ∈ R d ,</formula><p>and denotes the Hadamard product. Note that the marginal likelihood p(x) = p(x|z)p(z) is intractable. Hence, different from the common simplifying assumptions about the marginal or posterior probabilities, we introduce the variational lower bound (or evidence lower bound, ELBO)</p><formula xml:id="formula_4">log p(x (i) ) ≥ −KL(q(z|x (i) )||p(z)) + E q(z|x (i) ) log p(x (i) |z) ,<label>(3)</label></formula><p>where q φ (z|x (i) ) can be implemented by a probabilistic encoder, q φ (z|x (i) ) ∼ N (z; µ (i) , σ 2(i) I), and φ denotes the parameters. Note that the posterior p(x (i) |z) can be treated as the reconstruction part. Let the prior over the latent variables z be a centered isotropic multivariate Gaussian p(z) ∼ N (0, I). As a result, the disentangled formulation in Eq.</p><p>(3) can be treated as a variational autoencoder (Kingma and Welling 2014). Let z N ∈ R h , z V ∈ R h represent the latent variables corresponding to the NIR and VIS representations x N ∈ R d , x V ∈ R d , respectively. Then, one can approximate the posterior as follows:</p><formula xml:id="formula_5">q N (z N |x (i) N ) ∼ N (z N ; µ (i) N , σ 2(i) N I) q V (z V |x (i) V ) ∼ N (z V ; µ (i) V , σ 2(i) V I),<label>(4)</label></formula><p>where z N = µ N + σ N , z V = µ V + σ V and ∼ N (0, I). Here, φ N and φ V denote the parameters of the NIR and VIS approximate posterior estimator, respectively. In NIR-to-VIS face recognition, the main discrepancy comes from the variation in the light spectrum of NIR and VIS domains. We assume that the light spectrum variations are related as follows</p><formula xml:id="formula_6">σ V = P σ N ,<label>(5)</label></formula><p>where P ∈ R h×h is a correlation alignment matrix. Different from <ref type="bibr" target="#b18">(Sun and Saenko 2016)</ref>, we assume that there is a linear relationship between covariance matrices rather than requiring them to be similar. Since the latent variables z N and z V are independent, we impose an orthogonality constraint on P . Therefore, Eq. (4) can be reformulated as</p><formula xml:id="formula_7">q N (z N |x (i) N ) ∼ N (z N ; µ (i) N , σ 2(i) N I) q V (z V |x (i) V ) ∼ N (z V ; µ (i) V , σ 2(i) V I) s.t. σ V = P σ N , P P = I.<label>(6)</label></formula><p>The correlation alignment matrix P plays the role of constraining the variations of σ N and σ V . It makes the representations of NIR and VIS images vary in a subspace. Experimental results also verify the effectiveness of this correlation alignment constraint. Furthermore, since µ N and µ V represent the identity information, benefiting from the Wasserstein CNN, we minimize µ N −µ V 2 2 for the same identities to reduce the domain discrepancy.</p><p>With the above definitions, the proposed DVR formulation is as follows</p><formula xml:id="formula_8">J DVR = − KL(q(z N |x (i) N )||p(z N )) + KL(q(z V |x (i) V )||p(z V )) approximate posterior estimator parts + E log p(x (i) N |z N ) + E log p(x (i) V |z V ) reconstruction parts + λ 1 µ (i) N − µ (i) V 2 2</formula><p>mean discrepancy part s.t. σ V = P σ N , P P = I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>Using the Lagrange multipliers and the reparameterization trick (Kingma and Welling 2014), Eq. <ref type="formula">(7)</ref> can be reformulated as</p><formula xml:id="formula_9">J DVR = − 1 2 j 1 + log σ 2(i) N j − µ 2(i) N j − σ 2(i) N j NIR approximate posterior estimator − 1 2 j 1 + log σ 2(i) V j − µ 2(i) V j − σ 2(i) V j VIS approximate posterior estimator + E log p(x (i) N |z N ) + E log p(x (i) V |z V ) reconstruction parts + λ 1 µ (i) N − µ (i) V 2 2 mean discrepancy part + λ 2 σ V − P σ N 2 2 + λ 3 P P − I 2 F correlation alignment constraint ,<label>(8)</label></formula><p>where j denotes the j-th element of vectors µ</p><formula xml:id="formula_10">(i) N , µ (i) V , σ (i) N and σ (i)</formula><p>V , · 2 F denotes the Frobenius norm, and λ 1 , λ 2 are the trade-off parameters.</p><p>As for the reconstruction parts, we let p(x|z) be a multivariate Gaussian which is computed from z with a multilayer perceptron (MLP). Therefore, given x </p><formula xml:id="formula_11">(i) V −x (i) V 2 2</formula><p>are used for the reconstruction parts. In DVR, except for the L2 reconstruction loss, we further impose the cross-entropy loss between the reconstructionsx </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heterogeneous Recognition Network</head><p>Given NIR and VIS face images, I N and I V respectively, we denote the CNN features as</p><formula xml:id="formula_12">x i = f (I i ; Θ), i ∈ {N, V }.</formula><p>The output of a CNN feature is normally fed into a softmax layer for supervised training,</p><formula xml:id="formula_13">J cls = softmax(x i ; W, Θ), i ∈ {N, V }.<label>(9)</label></formula><p>Given a training sample (x i , y), i ∈ {N, V }, we can generate (x i , y), i ∈ {N, V } using the DVR framework, which can also be fed into a softmax layer as follows</p><formula xml:id="formula_14">J cls = softmax(x i , y; W, Θ) + softmax(x i , y; W, Θ), i ∈ {N, V } .</formula><p>(10) On the one hand, benefiting from the generated samplesx N andx V , the CNN feature extraction part f (·; Θ) can be better optimized and more robust, especially when the training sets for HFR are not large enough. On the other hand, the more robust the CNN feature extraction f (·; Θ) is, the more precisely the approximate posteriors q(z i |x i ), i ∈ {N, V } in DVR can be estimated. Inspired by this assumption, we propose an alternative optimization method to obtain domaininvariant representations for HFR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization</head><p>In this section, we present an alternative optimization method for the DVR framework. The CNN feature extraction part f (·; Θ) is initialized by a pre-trained model. First, we directly optimize the approximate posteriors q(z i |x i ), i ∈ {N, V } until convergence by Eq. <ref type="formula" target="#formula_9">(8)</ref>, but without mean discrepancy and correlation alignment parts, from the random initialization. Second, we generatex N andx V according to Eq. (2) and Eq. (4). We then fix the parameters φ N and φ V in approximate posterior estimator parts and compute Eq. (10) as the loss function to optimize the parameters of the recognition network Θ and W . Finally, the parameters Θ and W in the recognition network are fixed. We utilize the output x i = f (I i ; Θ), i ∈ {N, V } as the input, which contributes to the optimization of the approximate posterior estimator parts. The optimization details are summarized in Algorithm 1.</p><p>Regarding testing for HFR, we directly employ the outputs of heterogeneous recognition network f (·; Θ) to obtain Algorithm 1 Disentangled Variational Representation (DVR) Training. Require: Training set: NIR images I N , VIS images I V , the learning rate α and the trade-off parameters λ 1 , λ 2 , λ 3 . Ensure: The CNN parameters Θ, W , the approximate posterior estimators φ N , φ V and correlation alignment matrix P . 1: Initialize Θ, W by pre-trained model; 2: Obtain x N = f (I N ; Θ), x V = f (I V ; Θ); 3: Initialize φ N , φ V , P randomly; 4: for t = 1, . . . , T do 5:</p><p>Optimize φ N , φ V without mean discrepancy and correlation alignment parts; 6: end for; 7: for t = 1, . . . , T do Update P by gradient descent; 17: end for; 18: Return Θ, W, φ N , φ V , P ;</p><p>x i (i ∈ {N, V }) as feature representations. The cosine distance is used to compute the similarity score between different heterogenous representations for evaluations. Note that the parameters φ N , φ V , P of disentangled variational part and correlation alignment part are not utilized for testing. These two parts aim to disentangle representations and play a role of regularization; therefore, they are only utilized for training to reduce the domain discrepancy and alleviate overfitting. Experimental results demonstrate that these two parts can facilitate convolutional layers to learn a better feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, the proposed variational representation learning framework is systemically evaluated against several state-of-the-art HFR methods. We follow the experimental settings proposed in <ref type="bibr">)(Wu et al. 2018b</ref>) <ref type="bibr" target="#b17">(Song et al. 2018</ref>) and mainly employ NIR and VIS images to perform experiments. Both quantitative results and qualitative results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Protocols</head><p>Three publicly available VIS-to-NIR face recognition datasets are used to evaluate the performance of different HFR methods.</p><p>The CASIA NIR-VIS 2.0 Face Database ) is the largest and most challenging NIR-VIS heterogeneous face recognition database due to the large variations in lighting, expression and pose. It consists of 725 identities, each with 1 to 22 VIS and 5 to 50 NIR images. It consists of 10-fold experiments. For training, there are about 2,500 VIS and 6,100 NIR images from 360 identities. For testing, the gallery set in each fold is constructed from 358 identities and each identity only has one VIS image. The probe set contains over 6,000 NIR images from the same 358 identities. All the NIR images in the probe set are to be matched against the VIS images in the gallery set, resulting in a 6000 × 358 similarity matrix. The Rank-1 accuracy and verification rate (VR)@ false accept rate (FAR)=0.1% are reported.</p><p>The Oulu-CASIA NIR-VIS Database <ref type="bibr" target="#b0">(Chen et al. 2009</ref>) contains 80 identities with 6 expression variations. Following the protocols in , we select 20 identities as the training set and 20 identities as the testing set. Eight face images from each expression are randomly selected from both NIR and VIS sets. Hence, there are totally 96 images per each subject. All the VIS images of the 20 subjects are used as the gallery set and all the NIR images are treated as the probe set. The similarity matrix between the probe set and the gallery set is of size 960 × 960. The rank-1 accuracy, VR@FAR=1% and VR@FAR=0.1% are reported for comparisons.</p><p>The BUAA-VisNir Face Database <ref type="bibr" target="#b7">(Huang, Sun, and Wang 2012)</ref> consists of data from 150 subjects with 9 VIS and 9 NIR face images per subject. The training set and testing set are composed of 900 images from 50 identities and 1800 images from the remaining 100 identities, respectively. Only one VIS image is selected in the gallery set and the probe set contains 900 NIR images during testing. The similarity matrix between the probe set and the gallery set is of size 900 × 100. The rank-1 accuracy, VR@FAR=1% and VR@FAR=0.1% are reported for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We employ the Light CNN (Wu et al. 2018a) as a basic network architecture for HFR. Both LightCNN-9 and LightCNN-29 models 1 are used as the backbone networks, which are pre-trained on the MS-Celeb-1M dataset <ref type="bibr" target="#b3">(Guo et al. 2016)</ref>. All the images in the training set are aligned to 144 × 144 and randomly cropped to 128 × 128 as the input. Stochastic gradient descent (SGD) is used, where the momentum is set to 0.9 and weight decay is set to 5e-4. The learning rate is set to 1e-4 initially and reduced to 5e-5 gradually. The batch size is set to 128 and the dropout ratio is 0.5.</p><p>A multilayer perceptron (MLP) is used to model the DVR parts. It contains four hidden layers with h dimensions to represent µ N , µ V , σ N and σ V . Moreover, the correlation alignment matrix P is an h × h matrix. Specifically, in the experiments, the dimension h is set equal to 64. The input and the output layers are both 256-d, which are similar to the dimensions of features from the face recognition network. During training, the parameters of MLP are initialized by a Gaussian, while P is initialized by an identity matrix I. Adam (Kingma and Ba 2015) is used for back-propagation and the initial learning rate is set 1e-3 and gradually reduced to 1e-5. The batch size is set to 128. The trade-off parame- ters λ 1 , λ 2 and λ 3 are set equal to 1.0, 0.1 and 0.001, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the Proposed Method</head><p>We first compare the performance between LightCNN-9 and LightCNN-29 models since they are used as the backbones for HFR. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the LightCNN-29 achieves better performance than LightCNN-9 on all the three HFR databases. This clearly shows that LightCNN-29 is more suitable and robust as a backbone network for HFR. The aim of the proposed DVR is to model the disentangled latent variables z N and z V via q(z N |x N ) and q(z V |x V ) for NIR and VIS representations x N and x V , respectively. And then, we can easily samplex N andx V according to the likelihood p(x N |z N ) and p(x V |z V ). <ref type="table" target="#tab_0">Table 1</ref> presents that on the Oulu-CAISA NIR-VIS database, with the disentangled variational part, the performance on VR@FAR=0.1% is improved from 43.8% to 50.7% for LightCNN-9 and from 68.3% to 79.8% for LigthCNN-29, respectively. The results indicate that DVR can alleviate the lack of training data for HFR.</p><p>Similar with the Wasserstein CNN, minimizing mean discrepancy on µ N and µ V can significantly reduce the domain gap, which achieves 0.8%, 10.6% and 2.7% improvements on VR@FAR=0.1% with LightCNN-9 for CASIA NIR-VIS 2.0, Oulu-CASIA NIR-VIS and BUAA-VisNir, respectively. Furthermore, imposing the correlation alignment constraint in Eq. (5) can also boost the performance, which indicates that the assumptions of modeling the light spectrum variations via correlation alignment is reasonable and effective.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, the improvements benefiting from three parts, including disentangled variational part, mean discrepancy part and correlation alignment constraint, verifies that our DVR method can significantly reduce the domain discrepancy and alleviate overfitting even if the number of training samples is not large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons</head><p>The performance of the proposed DVR method based on both LightCNN-9 and LightCNN-29 is compared with some recent state-of-the-art HFR methods in <ref type="table" target="#tab_2">Table 2</ref> on the three datasets. The compared state-of-the-art HFR methods include both traditional handcrafted feature-based methods as well as deep learning-based methods. In particular, the performance of handcrafted feature-based methods, such as KDSR <ref type="bibr" target="#b5">(Huang et al. 2013</ref>), H2(LBP3) <ref type="bibr" target="#b14">(Shao and Fu 2017)</ref>, Gabor+RBM <ref type="bibr" target="#b21">(Yi et al. 2015)</ref>, <ref type="bibr">Recon.+UDP (Juefei-Xu, Pal, and Savvides 2015)</ref>, Gabor+JB <ref type="bibr" target="#b1">(Chen et al. 2012)</ref> and Gabor+HJB <ref type="bibr" target="#b16">(Shi et al. 2017</ref> For the most challenging CASIA NIR-VIS 2.0 database, it can be observed from the <ref type="table" target="#tab_2">Table 2</ref> that DVR performs better than the other compared methods. For fair comparisons, DVR on LightCNN-9 obtains 99.1% on Rank-1 accuracy and 98.6% on VR@FAR=0.1%, which outperforms the other state-of-the-art methods on LightCNN-9, including TRIVET <ref type="bibr" target="#b13">(Liu et al. 2016)</ref>, IDR , ADFL <ref type="bibr" target="#b17">(Song et al. 2018</ref><ref type="bibr">), CDL (Wu et al. 2018b</ref>) and W-CNN . When the backbone is changed to LightCNN-29, DVR further gains 0.8% on Rank-1 accuracy and 1.0% on VR@FAR=0.1%. The experimental results suggest that the domain discrepancy between NIR and VIS can be reduced by DVR.</p><p>For the Oulu-CASIA NIR-VIS and BUAA-VisNir databases, since the number of samples in the training set are not large enough, <ref type="table" target="#tab_2">Table 2</ref>    <ref type="bibr" target="#b21">(Yi et al. 2015)</ref> 86.2 ± 1.0 81.3 ± 1.8 ------Recon.+UDP (Juefei-Xu, Pal, and Savvides 2015) 78.5 ± 1.7 85.8 ------Gabor+JB <ref type="bibr" target="#b1">(Chen et al. 2012)</ref> 89.5 ± 0.8 83.2 ± 1.0 ------Gabor+HJB <ref type="bibr" target="#b16">(Shi et al. 2017)</ref> 91.6 ± 0.8 89.9 ± 0.9 ------IDNet <ref type="bibr">(Reale et al. 2016)</ref> 87.1 ± 0.9 74.5 ------HFR-CNN <ref type="bibr" target="#b13">(Saxena and Verbeek 2016)</ref> 85.9 ± 0.9 78.0 ------Hallucination <ref type="bibr" target="#b10">(Lezama, Qiu, and Sapiro 2017)</ref> 89.6 ± 0.9 -------TRIVET <ref type="bibr" target="#b13">(Liu et al. 2016)</ref> 95.7 ± 0.5 91.0 ± 1.3 92.2 67.9 33.6 93.9 93.0 80.9 IDR  97.   <ref type="figure">Figure 2</ref>: The ROC curves on the CASIA NIR-VIS 2.0, the Oulu-CASIA NIR-VIS and the BUAA-VisNir databases, respectively resentation (DVR), was proposed in this paper. It provides a novel way to disentangle the NIR and VIS representations with the identity information and their within-person variations. A variational lower bound is used to estimate the posterior and optimize the disentangled latent variable space. The minimization of the identity information for the same subject and the correlation alignment constraint on the modality variations further improve the representative ability of the disentangled latent variable. An alternative optimization is employed to provide mutual promotion for both disentangled variational representation and HFR network. In this way, we can easily generate NIR and VIS samples from the likelihood according to the disentangled representations, which can effectively alleviate overfitting for HFR on the limited number of training data. Experimental results demonstrate that the proposed DVR framework leads to excellent matching accuracy on three challenging HFR databases. In addition, an ablation study is developed to demonstrate the improvements obtained by the different modules of the proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al (Wang et al. 2015) propose several deep neural network-based methods with Canonical Correlation Analysis (CCA) in unsupervised subspace feature learning for HFR. He et al (He et al. 2017; He et al. 2018) divide the high-level representation into two orthogonal subspaces to obtain domain-invariant identity information and domain-related spectrum information. Modality-invariant feature learning explores domaininvariant features that are only related to the face identity. Traditional methods are based on the handcrafted local features (Liao et al. 2009; Klare, Li, and Jain 2011; Goswami et al. 2011), including Local Binary Patterns (LBP), Gabor features (Lei et al. 2007), Histograms of Oriented Gradients (HOG) and Difference of Gaussian (DoG). Liao et al (Liao et al. 2009) combine DoG filtering and multi-block LBP to encode NIR and VIS images. Klare et al (Klare, Li, and Jain 2011) utilize HOG features with sparse representation to improve the performance of HFR. Goswami et al (Goswami et al. 2011) combine the LBP histogram representation with Linear Discriminant Analysis (LDA) to extract domain invariant features. As for deep learning, Kan et al (Kan, Shan, and Chen 2016) address the discriminant domain invariant feature learning by analyzing the within-class and between-class scatter. Coupled Deep Learning (CDL) (Wu et al. 2018b) utilizes nuclear norm constraint on fully connected layer to alleviate overfitting, and proposes a cross-modal ranking to reduce domain discrepancy. He et al (He et al. 2018) decrease the domain gap by Wasserstein distance to obtain domain invariant features for HFR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Early work (Schmidhuber 1992) attempts to disentangle representations in an autoencoder via penalizing predictability of latent variables. A variant of Boltzmann Machine (Desjardins, Courville, and Bengio 2012) is used to disentangle factors of variations in the training data. Kingma (Kingma and Welling 2014) propose the Variational Auto-Encoder (VAE) framework to achieve limited disentangling performance on simple datasets. Matthey et al (Matthey et al. 2017) augment the original VAE framework with a single hyper-parameter β, called β-VAE, that controls the degree of disentanglement in the latent representations. Besides, Fac-torVAE (Kim and Mnih 2018) is proposed to disentangle by encouraging the distribution of representation to be factorial and independent across the dimensions. Chen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the posteriors and the identity label y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(0, I), generatex N andx V via Eq. φ N , φ V , P ; 11: Update Θ, W via back-propagation; 12: Obtain x N = f (I N ; Θ), x V = f (I V ; Θ); 13: Fix Θ, W, P 14: Update φ N , φ V by Eq. (8); 15: Fix Θ, W, φ N , φ V 16:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>), as well as deep learningbased methods including IDNet (Reale et al. 2016), HFR-CNN (Saxena and Verbeek 2016), Hallucination (Lezama, Qiu, and Sapiro 2017), TRIVET (Liu et al. 2016), IDR (He et al. 2017), ADFL (Song et al. 2018), CDL (Wu et al. 2018b) and W-CNN (He et al. 2018) are compared in Table 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The ablation study for DVR. Both LightCNN-9 and LightCNN-29 are used as the backbones.</figDesc><table><row><cell>Backbone</cell><cell cols="11">Disentangled Variational Part Discrepancy Alignment Rank-1 FAR=0.1% Rank-1 FAR=1% FAR=0.1% Rank-1 FAR=1% FAR=0.1% Mean Correlation CASIA NIR-VIS 2.0 Oulu-CASIA NIR-VIS BUAA-VisNir</cell></row><row><cell>LightCNN-9</cell><cell>-√ √ √</cell><cell>--√ √</cell><cell>---√</cell><cell>97.1 98.0 98.2 99.1</cell><cell>93.7 97.3 98.1 98.6</cell><cell>93.8 96.3 98.0 99.3</cell><cell>80.4 85.9 88.6 89.7</cell><cell>43.8 50.7 61.3 65.8</cell><cell>94.8 96.5 97.3 97.9</cell><cell>94.3 95.8 96.6 97.0</cell><cell>83.5 88.3 91.0 92.8</cell></row><row><cell>LightCNN-29</cell><cell>-√ √ √</cell><cell>--√ √</cell><cell>---√</cell><cell>98.1 99.0 99.5 99.7</cell><cell>97.4 99.1 99.3 99.6</cell><cell>99.0 100.0 100.0 100.0</cell><cell>93.1 95.2 96.5 97.2</cell><cell>68.3 79.8 83.0 84.9</cell><cell>96.8 98.0 98.9 99.2</cell><cell>97.0 97.9 98.4 98.5</cell><cell>89.4 93.0 95.6 96.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>demonstrates that benefiting from disentangled latent variables modeling, DVR outperforms previous state-of-the-art method such as W-CNN (He et al. 2018) by a large margin (89.7% vs 81.5% on VR@FAR=1% on Oulu-CASIA NIR-VIS as well as 97.0% vs 96.0% on VR@FAR=1% on the BUAA-VisNir database). LightCNN-29, further improves the VR@FAR=0.1% performance by 19.1% and 4.1% on the Oulu-CASIA NIR-VIS and BUAA-VisNir databases, respectively. Fig. 2 shows the ROC curves corresponding to TRIVET (Liu et al. 2016), IDR (He et al. 2017), ADFL (Song et al. 2018), CDL (Wu et al. 2018b), W-CNN (He et al. 2018), DVR(LightCNN-9) and DVR(LightCNN-29). It can be observed that the ROC curves corresponding to the DVR method based on both LightCNN-9 and LightCNN-29 are significantly better than all the other methods. Again, this clearly shows the significance of the proposed framework for HFR. When the False Positive Rate is larger than 0.01, the True Positive Rates of all the methods are close. When the False Positive Rate tends to be small, there are large gaps between the curves of DVR and others. Conclusion A framework to disentangle the NIR and VIS heterogeneous face representations, called Disentangled Variational Rep-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with other state-of-the-art HFR methods on the CASIA NIR-VIS 2.0 database, the Oulu-CASIA NIR-VIS database and the BUAA-VisNir database.</figDesc><table><row><cell>Method</cell><cell cols="8">CASIA NIR-VIS 2.0 Rank-1 FAR=0.1% Rank-1 FAR=1% FAR=0.1% Rank-1 FAR=1% FAR=0.1% Oulu-CASIA NIR-VIS BUAA-VisNir</cell></row><row><cell>KDSR (Huang et al. 2013)</cell><cell>37.5</cell><cell>9.3</cell><cell>66.9</cell><cell>56.1</cell><cell>31.9</cell><cell>83.0</cell><cell>86.8</cell><cell>69.5</cell></row><row><cell>H2(LBP3) (Shao and Fu 2017)</cell><cell>43.8</cell><cell>10.1</cell><cell>70.8</cell><cell>62.0</cell><cell>33.6</cell><cell>88.8</cell><cell>88.8</cell><cell>73.4</cell></row><row><cell>Gabor+RBM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/AlfredXiangWu/LightCNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is funded by National Natural Science Foundation of China (Grants No. 61622310) and Youth Innovation Promotion Association CAS (2015190).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning mappings for face synthesis from near infrared to visual light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation via generative entangling</title>
		<idno>abs/1210.5474</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV. [Desjardins, Courville, and Bengio</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Bayesian face revisited: A joint formulation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Polarimetric thermal to visible face verification via attribute preserved synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Windridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>BTAS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning invariant deep representation for NIR-VIS face recognition</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regularized discriminative spectral regression method for heterogeneous face matching</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE TIP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Wasserstein CNN: learning invariant features for NIR-VIS face recognition. TPAMI</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nir-vis heterogeneous face recognition via cross-spectral joint dictionary learning and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>IRIP-TR-12-FR-001</idno>
	</analytic>
	<monogr>
		<title level="m">Pal, and Savvides</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR Workshops</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>The BUAA-VisNir face database instructions</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-view deep network for cross-view classification. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen ; Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Ba; Welling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jain ; Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Not afraid of the dark: Nir-vis face recognition via cross-spectral hallucination and low-rank embedding</title>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Face recognition by discriminant analysis with gabor tensor representation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition from local structures of normalized appearance</title>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>ICB</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intermodality face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on heterogeneous face recognition: Sketch, infra-red, 3d and low-resolution. IVC. [Parkhi, Vedaldi, and Zisserman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ECCV Workshop. Schmidhuber 1992] Schmidhuber, J. 1992. Learning factorial codes by predictability minimization. Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Crossmodality feature learning through generic hierarchical hyperlingual-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generalized transfer subspace learning through low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cross-modality face recognition via heterogeneous joint bayesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE SPL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial discriminative heterogeneous face recognition</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint feature selection and subspace learning for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE TPAMI</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coupled feature selection for cross-sensor iris recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shared representation learning for heterogeneous face recognition</title>
	</analytic>
	<monogr>
		<title level="m">FG Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial network-based synthesis of visible faces from polarimetrie thermal faces</title>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual-agent gans for photorealistic and identity preserving profile face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Matching nir face to vis face using transduction</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE TIFS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
