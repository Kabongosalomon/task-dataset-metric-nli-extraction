<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving sentence compression by learning to predict gaze</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigrid</forename><surname>Klerke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@hum.ku.dk</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving sentence compression by learning to predict gaze</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gaze during reading</head><p>Readers fixate longer at rare words, words that are semantically ambiguous, and words that are mor-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b9">Klerke et al., 2015)</ref>, as well as helping poor readers in need of assistive technologies <ref type="bibr" target="#b1">(Canning et al., 2000)</ref>. This work suggests using eye-tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with perceived text difficulty <ref type="bibr" target="#b12">(Rayner et al., 2012)</ref>.</p><p>These two observations recently lead <ref type="bibr" target="#b9">Klerke et al. (2015)</ref> to suggest using eye-tracking measures as metrics in text simplification. We go beyond this by suggesting that eye-tracking recordings can be used to induce better models for sentence compression for text simplification. Specifically, we show how to use existing eye-tracking recordings to improve the induction of Long Short-Term Memory models (LSTMs) for sentence compression.</p><p>Our proposed model does not require that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior.</p><p>Several approaches to sentence compression have been proposed, from noisy channel models <ref type="bibr">(Knight and Marcu, 2002)</ref> over conditional random fields <ref type="bibr">(Elming et al., 2013)</ref> to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, <ref type="bibr">Filippova et al. (2015)</ref> successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to <ref type="bibr">Filippova et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions</head><p>• We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye-tracking corpus.</p><p>• Our method is fully competitive with state-ofthe-art across three corpora.</p><p>• Our code is made publicly available at https://bitbucket.org/soegaard/ gaze-mtl16.</p><p>phologically complex <ref type="bibr" target="#b12">(Rayner et al., 2012)</ref>. These are also words that are likely to be replaced with simpler ones in sentence simplification, but it is not clear that they are words that would necessarily be removed in the context of sentence compression. <ref type="bibr" target="#b6">Demberg and Keller (2008)</ref> show that syntactic complexity (measured as dependency locality) is also an important predictor of reading time. Phrases that are often removed in sentence compressionlike fronted phrases, parentheticals, floating quantifiers, etc.-are often associated with non-local dependencies. Also, there is evidence that people are more likely to fixate on the first word in a constituent than on its second word <ref type="bibr" target="#b8">(Hyönä and Pollatsek, 2000)</ref>. Being able to identify constituent borders is important for sentence compression, and reading fixation data may help our model learn a representation of our data that makes it easy to identify constituent boundaries.</p><p>In the experiments below, we learn models to predict the first pass duration of word fixations and the total duration of regressions to a word. These two measures constitute a perfect separation of the total reading time of each word split between the first pass and subsequent passes. Both measures are described below. They are both discretized into six bins as follows with only non-zero values contributing to the calculation of the standard deviation (SD): 0: measure = 0 or 1: measure &lt; 1 SD below reader's average or 2: measure &lt; .5 SD below reader's average or 3: measure &lt; .5 above reader's average or 4: measure &gt; .5 SD above reader's average or 5: measure &gt; 1 SD above reader's average First pass duration measures the total time spent reading a word first time it is fixated, including any immediately following re-fixations of the same word. This measure correlates with word length, frequency and ambiguity because long words are likely to attract several fixations in a row unless they are particularly easily predicted or recognized. This effect arises because long words are less likely to fit inside the fovea of the eye. Note that for this measure the value 0 indicates that the word was not fixated by this reader. <ref type="table" target="#tab_1">Are  4  4  tourists  2  0  enticed  3  0  by  4  0  these  2  0  attractions  3  0  threatening  3  3  their  5  0  very  3  3  existence  3  5  ?</ref> 3 5 Regression duration measures the total time spent fixating a word after the gaze has already left it once. This measure belongs to the group of late measures, i.e., measures that are sensitive to the later cognitive processing stages including interpretation and integration of already decoded words. Since the reader by definition has already had a chance to recognize the word, regressions are associated with semantic confusion and contradiction, incongruence and syntactic complexity, as famously experienced in garden path sentences. For this measure the value 0 indicates that the word was read at most once by this reader. See <ref type="table" target="#tab_0">Table 1</ref> for an example of first pass duration and regression duration annotations for one reader and sentence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words FIRST PASS REGRESSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence compression using multi-task deep bi-LSTMs</head><p>Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees <ref type="bibr" target="#b12">(Riezler et al., 2003;</ref><ref type="bibr" target="#b11">Nomoto, 2007;</ref><ref type="bibr">Filippova and Strube, 2008;</ref><ref type="bibr" target="#b3">Cohn and Lapata, 2008;</ref><ref type="bibr" target="#b4">Cohn and Lapata, 2009)</ref> or by incorporating syntactic information in their model <ref type="bibr" target="#b10">(McDonald, 2006;</ref><ref type="bibr">Clarke and Lapata, 2008)</ref>. <ref type="bibr">Recently, however, Filippova et al. (2015)</ref> presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (bi-RNNs) read in sequences in both regular and reversed order, enabling conditioning predictions on both left and right context. In the forward pass, we run the input data through an embedding layer and compute the predictions of the forward and backward states at layers 0, 1, . . ., until we compute the softmax predictions for word i based on a linear transformation of the concatenation of the of standard and reverse RNN outputs for location i. We then calculate the objective function derivative for the sequence using cross-entropy (logistic loss) and use backpropagation to calculate gradients and update the weights accordingly. A deep bi-RNN or klayered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal.</p><p>Bi-LSTMs have already been used for finegrained sentiment analysis <ref type="bibr">(Liu et al., 2015)</ref>, syntactic chunking <ref type="bibr" target="#b7">(Huang et al., 2015)</ref>, and semantic role labeling <ref type="bibr" target="#b13">(Zhou and Xu, 2015)</ref>. These and other recent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We instead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regularize our sentence compression model.</p><p>Specifically, we use bi-LSTMs with three layers. Our baseline model is simply this three-layered model trained to predict compressions (encoded as label sequences), and we consider two extensions thereof as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Our first extension, MULTI-TASK-LSTM, includes the gaze prediction task during training, with a separate logistic regression classifier for this purpose; and the other, CASCADED-LSTM, predicts gaze measures from the inner layer. Our second extension, which is superior to our first, is basically a one-layer bi-LSTM for predicting reading fixations with a two-layer bi-LSTM on top for predicting sentence compressions.</p><p>At each step in the training process of MULTI-TASK-LSTMand CASCADED-LSTM, we choose a random task, followed by a random training instance of this task. We use the deep LSTM to predict a label sequence, suffer a loss with respect to the true labels, and update the model parameters. In CASCADED-LSTM, the update for an instance of CCG super tagging or gaze prediction only affects the parameters of the inner LSTM layer.</p><p>Both MULTI-TASK-LSTM and CASCADED-LSTM do multi-task learning <ref type="bibr" target="#b2">(Caruana, 1993</ref>). In multi-task learning, the induction of a model for one task is used as a regularizer on the induction of a model for another task. Caruana (1993) did multitask learning by doing parameter sharing across several deep networks, letting them share hidden layers; a technique also used by <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> for various NLP tasks. These models train task-specific classifiers on the output of deep networks (informed by the task-specific losses). We extend their models by moving to sequence prediction and allowing the task-specific sequence models to also be deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Gaze data</head><p>We use the Dundee Corpus <ref type="bibr" target="#b9">(Kennedy et al., 2003)</ref> as our eye-tracking corpus with tokenization and measures similar to the Dundee Treebank . The corpus contains eye-tracking recordings of ten native English-speaking subjects reading 20 newspaper articles from The Independent. We use data from nine subjects for training and one subject for development. We do not evaluate the gaze prediction because the task is only included as a way of regularizing the compression model. S: Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amid mounting loan defaults. T: Regulators shut down a small Florida bank S: Intel would be building car batteries, expanding its business beyond its core strength, the company said in a statement. T: Intel would be building car batteries  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compression data</head><p>We use three different sentence compression datasets, <ref type="bibr">ZIFF-DAVIS (Knight and Marcu, 2002)</ref>, <ref type="bibr">BROADCAST (Clarke and Lapata, 2006)</ref>, and the publically available subset of <ref type="bibr">GOOGLE (Filippova et al., 2015)</ref>. The first two consist of manually compressed newswire text in English, while the third is built heuristically from pairs of headlines and first sentences from newswire, resulting in the most aggressive compressions, as exemplified in <ref type="table" target="#tab_0">Table 1</ref>. We present the dataset characteristics in <ref type="table" target="#tab_1">Table 2</ref>. We use the datasets as released by the authors and do not apply any additional pre-processing. The CCG supertagging data comes from CCGbank, 1 and we use sections 0-18 for training and section 19 for development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines and system</head><p>Both the baseline and our systems are three-layer bi-LSTM models trained for 30 iterations with pretrained (SENNA) embeddings. The input and hidden layers are 50 dimensions, and at the output layer we predict sequences of two labels, indicating whether to delete the labeled word or not. Our baseline (BASELINE-LSTM) is a multi-task learning 1 http://groups.inf.ed.ac.uk/ccg/ bi-LSTM predicting both CCG supertags and sentence compression (word deletion) at the outer layer. Our first extension is MULTITASK-LSTM predicting CCG supertags, sentence compression, and reading measures from the outer layer. CASCADED-LSTM, on the other hand, predicts CCG supertags and reading measures from the initial layer, and sentence compression at the outer layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and discussion</head><p>Our results are presented in <ref type="table">Table 3</ref>. We observe that across all three datasets, including all three annotations of BROADCAST, gaze features lead to improvements over our baseline 3-layer bi-LSTM. Also, CASCADED-LSTM is consistently better than MULTITASK-LSTM. Our models are fully competitive with state-of-the-art models.  <ref type="table">Table 3</ref>: Results (F1). For all three datasets, the inclusion of gaze measures (first pass duration (FP) and regression duration (Regr.)) leads to improvements over the baseline. All models include CCG-supertagging as an auxiliary task. Note that BROADCASTwas annotated by three annotators. The three columns are, from left to right, results on annotators 1-3.</p><p>the first sentence. With the harder datasets, the impact of the gaze information becomes stronger, consistently favouring the cascaded architecture, and with improvements using both first pass duration and regression duration, the late measure associated with interpretation of content. Our results indicate that multi-task learning can help us take advantage of inherently noisy human processing data across tasks and thereby maybe reduce the need for taskspecific data collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example sentence from the Dundee Corpus</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example compressions from the GOOGLE dataset. S is the source sentence, and T is the target compression.</figDesc><table><row><cell></cell><cell cols="4">Sents Sent.len Type/token Del.rate</cell></row><row><cell></cell><cell></cell><cell>TRAINING</cell><cell></cell><cell></cell></row><row><cell>ZIFF-DAVIS</cell><cell>1000</cell><cell>20</cell><cell>0.22</cell><cell>0.59</cell></row><row><cell>BROADCAST</cell><cell>880</cell><cell>20</cell><cell>0.21</cell><cell>0.27</cell></row><row><cell>GOOGLE</cell><cell>8000</cell><cell>24</cell><cell>0.17</cell><cell>0.87</cell></row><row><cell></cell><cell></cell><cell>TEST</cell><cell></cell><cell></cell></row><row><cell>ZIFF-DAVIS</cell><cell>32</cell><cell>21</cell><cell>0.55</cell><cell>0.47</cell></row><row><cell>BROADCAST</cell><cell>412</cell><cell>19</cell><cell>0.27</cell><cell>0.29</cell></row><row><cell>GOOGLE</cell><cell>1000</cell><cell>25</cell><cell>0.42</cell><cell>0.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dataset characteristics. Sentence length is for source sentences.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">On a "randomly selected" annotator; unfortunately, they do not say which. James Clarke (p.c) does not remember which annotator they used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yoav Goldberg was supported by the Israeli Science Foundation Grant No. 1555/15. Anders Søgaard was supported by ERC Starting Grant No. 313695. Thanks to Joachim Bingel and Maria Barrett for preparing data and for helpful discussions, and to the anonymous reviewers for their suggestions for improving the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maria Barrett,Željko Agić, and Anders Søgaard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 14th International Workshop on Treebanks and Linguistic Theories</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>The dundee treebank. TLT 14</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cohesive generation of syntactically simplified newspaper text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Jointly learning to extract and compress</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global inference for sentence compression: An integer linear programming approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<editor>COLING. [Clarke and Lapata2008] James Clarke and Mirella Lapata</editor>
		<imprint>
			<biblScope unit="page" from="399" to="429" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Constraint-based sentence compression an integer programming approach</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata2008] Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentence compression as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapata2009] Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="637" to="674" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Katja Filippova, Enrique Alfonseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals. 2015. Sentence compression by deletion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller ; Anders Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigrid</forename><surname>Klerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Lapponi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<editor>NAACL. [Filippova and Strube2008] Katja Filippova and Michael Strube</editor>
		<meeting>the Fifth International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Jakob Elming</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Processing of finnish compound words in reading. Reading as a perceptual process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jukka</forename><surname>Hyönä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pollatsek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="65" to="87" />
		</imprint>
	</monogr>
	<note>Hyönä and Pollatsek2000</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Summarization beyond sentence extraction: a probabilistic approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pengfei Liu, Shafiq Joty, and Helen Meng. 2015. Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<editor>Knight and Marcu2002] Kevin Knight and Daniel Marcu</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with soft syntactic evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative sentence compression with conditional random fields. Information Processing and Management: an International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1571" to="1587" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>NAACL. [Woodsend and Lapata2011] Kristian Woodsend and Mirella Lapata</editor>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Learning to simplify sentences with quasi-synchronous grammar and integer programming</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">End-toend learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu2015] Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
