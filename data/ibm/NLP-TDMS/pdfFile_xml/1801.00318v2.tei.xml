<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">M</forename><surname>Abien</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarap</surname></persName>
						</author>
						<title level="a" type="main">Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Security and privacy → Malware and its mitigation</term>
					<term>• Com- puting methodologies → Supervised learning by classifica- tion</term>
					<term>Support vector machines</term>
					<term>Neural networks</term>
					<term>KEYWORDS artificial intelligence</term>
					<term>artificial neural networks</term>
					<term>classification</term>
					<term>con- volutional neural networks</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>mal- ware classification</term>
					<term>multilayer perceptron</term>
					<term>recurrent neural net- work</term>
					<term>supervised learning</term>
					<term>support vector machine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective and efficient mitigation of malware is a long-time endeavor in the information security community. The development of an anti-malware system that can counteract an unknown malware is a prolific activity that may benefit several sectors. We envision an intelligent anti-malware system that utilizes the power of deep learning (DL) models. Using such models would enable the detection of newly-released malware through mathematical generalization. That is, finding the relationship between a given malware x and its corresponding malware family y, f : x → y. To accomplish this feat, we used the Malimg dataset[12] which consists of malware images that were processed from malware binaries, and then we trained the following DL models 1 to classify each malware family: CNN-SVM[16], GRU-SVM[3], and MLP-SVM. Empirical evidence has shown that the GRU-SVM stands out among the DL models with a predictive accuracy of ≈84.92%. This stands to reason for the mentioned model had the relatively most sophisticated architecture design among the presented models. The exploration of an even more optimal DL-SVM model is the next stage towards the engineering of an intelligent anti-malware system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Effective and efficient mitigation of malware is a long-time endeavor in the information security community. The development of an anti-malware system that can counteract an unknown malware is a prolific activity that may benefit several sectors.</p><p>To intercept an unknown malware or even just an unknown variant is a laborious task to undertake, and may only be accomplished by constantly updating the anti-malware signature database. The mentioned database contains the information on all known malware by the particular system <ref type="bibr" target="#b14">[15]</ref>, which is then used for malware detection. Consequently, newly-released malware which are not yet included in the database will go undetected.</p><p>We envision an intelligent anti-malware system that employs a deep learning (DL) approach which would enable the detection <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/AFAgarap/malware-classification of newly-released malware through its capability to generalize on data. Furthermore, we amend the conventional DL models to use the support vector machine (SVM) as their classification function.</p><p>We take advantage of the Malimg dataset <ref type="bibr" target="#b11">[12]</ref> which consists of visualized malware binaries, and use it to train the DL-SVM models to classify each malware family.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY 2.1 Machine Intelligence Library</head><p>Google TensorFlow <ref type="bibr" target="#b1">[2]</ref> was used to implement the deep learning algorithms in this study, with the aid of other scientific computing libraries: matplotlib <ref type="bibr" target="#b7">[8]</ref>, numpy <ref type="bibr" target="#b16">[17]</ref>, and scikit-learn <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Dataset</head><p>The deep learning (DL) models in this study were evaluated on the Malimg dataset <ref type="bibr" target="#b11">[12]</ref>, which consists of 9,339 malware samples from 25 different malware families. <ref type="table" target="#tab_0">Table 1</ref> shows the frequency distribution of malware families and their variants in the Malimg dataset <ref type="bibr" target="#b11">[12]</ref>.  <ref type="bibr" target="#b11">[12]</ref> created the Malimg dataset by reading malware binaries into an 8-bit unsigned integer composing a matrix M ∈ R m×n . The said matrix may be visualized as a grayscale image having values in the range of [0, 255], with 0 representing black and 1 representing white.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset Preprocessing</head><p>Similar to what <ref type="bibr" target="#b5">[6]</ref> did, the malware images were resized to a 2dimensional matrix of 32 × 32, and were flattened into a n × n-size array, resulting to a 1 × 1024-size array. Each feature array was then labelled with its corresponding indexed malware family name (i.e. 0 − 24). Then, the features were standardized using Eq. 1.</p><formula xml:id="formula_0">z = X − µ σ<label>(1)</label></formula><p>where X is the feature to be standardized, µ is its mean value, and σ is its standard deviation. The standardization was implemented using StandardScaler().fit_transform() of scikit-learn <ref type="bibr" target="#b12">[13]</ref>.</p><p>Granted that the dataset consists of images, and standardization arXiv:1801.00318v2 [cs.NE] 7 Feb 2019 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Computational Models</head><p>This section presents the deep learning (DL) models, and the support vector machine (SVM) classifier used in the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Support Vector Machine (SVM).</head><p>The support vector machine (SVM) was developed by Vapnik <ref type="bibr" target="#b4">[5]</ref> for binary classification. Its objective is to find the optimal hyperplane f (w, x) = w · x + b to separate two classes in a given dataset, with features x ∈ R m . SVM learns the parameters w and b by solving the following constrained optimization problem:</p><formula xml:id="formula_1">min 1 p w T w + C p i=1 ξ i (2) s.t y ′ i (w · x + b) ≥ 1 − ξ i (3) ξ i ≥ 0, i = 1, . . . , p<label>(4)</label></formula><p>where w T w is the Manhattan norm (also known as L1 norm), C is the penalty parameter (may be an arbitrary value or a selected value using hyper-parameter tuning), and ξ is a cost function.</p><p>The corresponding unconstrained optimization problem of Eq. 2 is given by Eq. 5.</p><formula xml:id="formula_2">min 1 p w T w + C p i=1 max 0, 1 − y ′ i (w T x i + b) (5)</formula><p>where y ′ is the actual label, and w T x+b is the predictor function. This equation is known as L1-SVM, with the standard hinge loss. Its differentiable counterpart, L2-SVM (given by Eq. 6), provides more stable results <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_3">min 1 p ∥w∥ 2 2 + C p i=1 max 0, 1 − y ′ i (w T x i + b) 2 (6)</formula><p>where ∥w∥ 2 is the Euclidean norm (also known as L2 norm), with the squared hinge loss.</p><p>Despite intended for binary classification, SVM may be used for multinomial classification as well. One approach to achieve this is the use of kernel tricks, which converts a linear model to a non-linear model by applying kernel functions such as radial basis function (RBF). However, for this study, we utilized the linear L2-SVM for the multinomial classification problem. We then employed the one-versus-all (OvA) approach, which treats a given class c i as the positive class, and others as negative class.</p><p>Take for example the following classes: airplane, boat, car. If a given image belongs to the airplane class, it is taken as the positive class, which leaves the other two classes the negative class.</p><p>With the OvA approach, the L2-SVM serves as the classifier of each deep learning model in this study (CNN, GRU, and MLP). That is, the learning parameters weight and bias of each model is learned by the SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Convolutional Neural</head><p>Network. Convolutional Neural Networks (CNNs) are similar to feedforward neural networks for they also consist of hidden layers of neurons with "learnable" parameters. These neurons receive inputs, performs a dot product, and then follows it with a non-linearity such as siдmoid or tanh. The whole network expresses the mapping between raw image pixels x ∈ R m and class scores y, f : x → y. For this study, the CNN architecture used resembles the one laid down in <ref type="bibr" target="#b0">[1]</ref>:</p><p>(1) INPUT: 32 × 32 × 1 (2) CONV5: 5 × 5 size, 36 filters, 1 stride (3) LeakyReLU: max(0.01h θ (x)), h θ (x)) (4) POOL: 2 × 2 size, 1 stride (5) CONV5: 5 × 5 size, 72 filters, 1 stride (6) LeakyReLU: max(0.01h θ (x)), h θ (x)) (7) POOL: 2 × 2 size, 1 stride (8) FC: 1024 Hidden Neurons (9) LeakyReLU: max(0.01h θ (x)), h θ (x)) (10) DROPOUT: p = 0.85 (11) FC: 25 Output Classes The modification introduced in the architecture design was the size of layer inputs and outputs (e.g. input of 32 × 32 × 1 instead of 28 × 28 × 1, and output of 25 classes), the use of LeakyReLU instead of ReLU, and of course, the introduction of L2-SVM as the network classifier instead of the conventional Softmax function. This paradigm of combining CNN and SVM was actually proposed by <ref type="bibr" target="#b15">Tang (2013)</ref> <ref type="bibr" target="#b15">[16]</ref>.  <ref type="bibr" target="#b2">[3]</ref> proposed a neural network architecture combining the gated recurrent unit (GRU) <ref type="bibr" target="#b3">[4]</ref> variant of a recurrent neural network (RNN) and the support vector machine (SVM) <ref type="bibr" target="#b4">[5]</ref> for the purpose of binary classification.</p><formula xml:id="formula_4">z = σ (W z · [h t −1 , x t ])<label>(7)</label></formula><formula xml:id="formula_5">r = σ (W r · [h t −1 , x t ])<label>(8)</label></formula><formula xml:id="formula_6">h t = tanh(W · [r t * h t −1 , x t ])<label>(9)</label></formula><formula xml:id="formula_7">h t = (1 − z t ) * h t −1 + z t * h t<label>(10)</label></formula><p>where z and r are the update gate and reset gate of a GRU-RNN respectively,h t is the candidate value, and h t is the new RNN cell state value <ref type="bibr" target="#b3">[4]</ref>. In turn, the h t is used as the predictor variable x in the L2-SVM predictor function (given by wx + b) of the network instead of the conventional Softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Multilayer</head><p>Perceptron. The perceptron model was developed by <ref type="bibr" target="#b13">Rosenblatt (1958)</ref> <ref type="bibr" target="#b13">[14]</ref> based on the neuron model by Mc-Culloch &amp; Pitts (1943) <ref type="bibr" target="#b10">[11]</ref>. A perceptron may be represented by a linear function (given by Eq. 11), which is then passed to an activation function such as sigmoid σ , sign, or tanh. These activation functions introduce non-linearity (except for the sign function) to represent complex functions.</p><p>As the term itself implies, a multilayer perceptron (MLP) is a neural network that consists of hidden layers of perceptrons. In this study, the activation function used was the LeakyReLU[10] function (given by Eq. 12).</p><formula xml:id="formula_8">h θ (x) = n i=0 θ i x i + b (11) f h θ (x) = max 0.01h θ (x), h θ (x)<label>(12)</label></formula><p>The learning parameters weight and bias for each DL model were learned by the L2-SVM using the loss function given by Eq. 6. The computed loss is then minimized through Adam <ref type="bibr" target="#b8">[9]</ref> optimization. Then, the decision function f (x) = siдn(wx + b) produces a vector of scores for each malware family. In order to get the predicted labels y for a given data x, the arдmax function is used (see Eq. 13).</p><formula xml:id="formula_9">y ′ = arдmax siдn(wx + b)<label>(13)</label></formula><p>The arдmax function shall return the indices of the highest scores across the vector of predicted classes wx + b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data Analysis</head><p>There were two phases of experiment for this study: (1) training phase, and (2) test phase. All the deep learning algorithms described in Section 2.4 were trained and tested on the Malimg dataset <ref type="bibr" target="#b11">[12]</ref>. The dataset was partitioned in the following fashion: 70% for training phase, and 30% for testing phase.</p><p>The variables considered in the experiments were the following:</p><p>(1) Test Accuracy (the predictive accuracy on unseen data) (2) Epochs (number of passes through the entire dataset) </p><formula xml:id="formula_11">T PR = True Positive True Positive + False N eдative<label>(16)</label></formula><p>The classification measures F1 score, precision, and recall were all computed using the classification_report() function of sklearn.metrics <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>All experiments in this study were conducted on a laptop computer with Intel Core(TM) i5-6300HQ CPU @ 2.30GHz x 4, 16GB of DDR3 RAM, and NVIDIA GeForce GTX 960M 4GB DDR5 GPU. <ref type="table" target="#tab_3">Table 2</ref> shows the hyper-parameters used by the DL-SVM models in the conducted experiments. <ref type="table" target="#tab_4">Table 3</ref> summarizes the experiment results for the presented DL-SVM models. As opposed to what <ref type="bibr" target="#b5">[6]</ref> did on dataset partitioning, the relative populations of each malware family were not considered in the splitting process. All the DL-SVM models were trained on ≈70% of the preprocessed Malimg dataset <ref type="bibr" target="#b11">[12]</ref>, i.e. 6400 malware family variants (6400 mod 256 = 0), for 100 epochs. On the other hand, the models were tested on ≈30% of the preprocessed Malimg dataset <ref type="bibr" target="#b11">[12]</ref>, i.e. 2560 malware family variants (2560 mod 256 = 0), for 100 epochs. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the training accuracy of the DL-SVM models for 100 epochs (equivalent to 2500 steps, since 6400 × 100 ÷ 256 = 2500). First, the CNN-SVM model accomplished its training in 3 minutes and 41 seconds with an average training accuracy of   . Confusion Matrix for CNN-SVM testing results, showing its predictive accuracy for each malware family described in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows the testing performance of CNN-SVM model in multinomial classification on malware families. The mentioned model had a precision of 0.84, a recall of 0.77, and a F1 score of 0.79. <ref type="figure">Figure 4</ref> shows the testing performance of GRU-SVM model in multinomial classification on malware families. The mentioned model had a precision of 0.85, a recall of 0.85, and a F1 score of 0.85. <ref type="figure">Figure 5</ref> shows the testing performance of MLP-SVM model in multinomial classification on malware families. The mentioned model had a precision of 0.83, a recall of 0.80, and a F1 score of 0.81.</p><p>As shown in the confusion matrices, the DL-SVM models had better scores for the malware families with the high number of variants, most notably, Allaple.A and Allaple.L. This may be pointed to the omission of relative populations of each malware family during the partitioning of the dataset into training data and <ref type="figure">Figure 4</ref>: Plotted using matplotlib <ref type="bibr" target="#b7">[8]</ref>. Confusion Matrix for GRU-SVM testing results, showing its predictive accuracy for each malware family described in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure">Figure 5</ref>: Plotted using matplotlib <ref type="bibr" target="#b7">[8]</ref>. Confusion Matrix for MLP-SVM testing results, showing its predictive accuracy for each malware family described in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>testing data. However, unlike the results of <ref type="bibr" target="#b5">[6]</ref>, only Allaple.A and Allaple.L had some misclassifications between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSION</head><p>It is palpable that the GRU-SVM model stands out among the DL-SVM models presented in this study. This finding comes as no surprise as the GRU-SVM model did have the relatively most sophisticated architecture design among the presented models, most notably, its 5-layer design. As explained in <ref type="bibr" target="#b6">[7]</ref>, the number of layers of a neural network is directly proportional to the complexity of a function it can represent. In other words, the performance or Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification , , accuracy of a neural network is directly proportional to the number of its hidden layers. By this logic, it stands to reason that the less number of hidden layers that a neural network has, the less its performance or accuracy is. Hence, the findings in this study corroborates the literature explanation as the MLP-SVM came second (having ≈80.47% test accuracy) to GRU-SVM with a 3-layer design, and the CNN-SVM came last (having ≈77.23% test accuracy) with a 2-layer design.</p><p>The reported test accuracy of ≈84.92% clearly states that the GRU-SVM model has the strongest predictive performance among the DL-SVM models in this study. This is attributed to the fact that the GRU-SVM model has the relatively most complex design among the presented models. First, its 5-layer design allows it to represent increasingly complex mappings between features and labels, i.e. function mappings f : x → y. Second, its capability to learn from data of sequential nature, in which an image data belongs. This nature of the GRU-RNN comes from its gating mechanisms, given by equations in Section 2.4.3. Through the mentioned mechanisms, the GRU-RNN solves the problem of vanishing gradients and exploding gradients <ref type="bibr" target="#b3">[4]</ref>. Thus, being able to connect information with a considerable gap. However, as indicated by the training summary given by <ref type="figure" target="#fig_1">Figure 2</ref>, the GRU-SVM has the caveat of relatively longer computing time. Having finished its training in 11 minutes and 32 seconds, it was the slowest among the DL-SVM models. From a high-level inspection of the presented equations of each DL-SVM model (CNN-SVM in Section 2.4.2, GRU-SVM in Section 2.4.3, and MLP-SVM in Section 2.4.4), it was a theoretical implication that the GRU-SVM would have the longest computing time as it had more non-linearities introduced in its computation. On the other hand, with the least non-linearities (having only used LeakyReLU), it was also theoretically implied that the MLP-SVM model would have the shortest computing time.</p><p>From the literature explanation <ref type="bibr" target="#b6">[7]</ref> and empirical evidence, it can be inferred that increasing the complexity of the architectural design (e.g. more hidden layers, better non-linearities) of the CNN-SVM and MLP-SVM models may catapult their predictive performance, and would be more on par with the GRU-SVM model. In turn, this implication warrants a further study and exploration that may be prolific to the information security community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND RECOMMENDATION</head><p>We used the Malimg dataset prepared by <ref type="bibr" target="#b11">[12]</ref>, which consists of malware images for the purpose of malware family classification. We employed deep learning models with the L2-SVM as their final output layer in a multinomial classification task. The empirical data shows that the GRU-SVM model by <ref type="bibr" target="#b2">[3]</ref> had the highest predictive accuracy among the presented DL-SVM models, having a test accuracy of ≈84.92%.</p><p>Improving the architecture design of the CNN-SVM model and MLP-SVM model by adding more hidden layers, adding better nonlinearities, and/or using an optimized dropout, may provide better insights on their application on malware classification. Such insights may reveal an information as to which architecture may serve best in the engineering of an intelligent anti-malware system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Image from<ref type="bibr" target="#b11">[12]</ref>. Visualizing malware as a grayscale image.Nataraj et al. (2011)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Plotted using matplotlib<ref type="bibr" target="#b7">[8]</ref>. Training accuracy of the DL-SVM models on malware classification using the Malimg dataset<ref type="bibr" target="#b11">[12]</ref>.80.96875%. Meanwhile, the GRU-SVM model accomplished its training in 11 minutes and 32 seconds with an average training accuracy of 90.9375%. Lastly, the MLP-SVM model accomplished its training in 12 seconds with an average training accuracy of 99.5768229%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Plotted using matplotlib<ref type="bibr" target="#b7">[8]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Malware families found in the Malimg Dataset<ref type="bibr" target="#b11">[12]</ref>.</figDesc><table><row><cell>No.</cell><cell>Family</cell><cell cols="2">Family Name No. of Variants</cell></row><row><cell>01</cell><cell>Dialer</cell><cell>Adialer.C</cell><cell>122</cell></row><row><cell>02</cell><cell>Backdoor</cell><cell>Agent.FYI</cell><cell>116</cell></row><row><cell>03</cell><cell>Worm</cell><cell>Allaple.A</cell><cell>2949</cell></row><row><cell>04</cell><cell>Worm</cell><cell>Allaple.L</cell><cell>1591</cell></row><row><cell>05</cell><cell>Trojan</cell><cell>Alueron.gen!J</cell><cell>198</cell></row><row><cell>06</cell><cell>Worm:AutoIT</cell><cell>Autorun.K</cell><cell>106</cell></row><row><cell>07</cell><cell>Trojan</cell><cell>C2Lop.P</cell><cell>146</cell></row><row><cell>08</cell><cell>Trojan</cell><cell>C2Lop.gen!G</cell><cell>200</cell></row><row><cell>09</cell><cell>Dialer</cell><cell>Dialplatform.B</cell><cell>177</cell></row><row><cell cols="2">10 Trojan Downloader</cell><cell>Dontovo.A</cell><cell>162</cell></row><row><cell>11</cell><cell>Rogue</cell><cell>Fakerean</cell><cell>381</cell></row><row><cell>12</cell><cell>Dialer</cell><cell>Instantaccess</cell><cell>431</cell></row><row><cell>13</cell><cell>PWS</cell><cell>Lolyda.AA 1</cell><cell>213</cell></row><row><cell>14</cell><cell>PWS</cell><cell>Lolyda.AA 2</cell><cell>184</cell></row><row><cell>15</cell><cell>PWS</cell><cell>Lolyda.AA 3</cell><cell>123</cell></row><row><cell>16</cell><cell>PWS</cell><cell>Lolyda.AT</cell><cell>159</cell></row><row><cell>17</cell><cell>Trojan</cell><cell>Malex.gen!J</cell><cell>136</cell></row><row><cell cols="3">18 Trojan Downloader Obfuscator.AD</cell><cell>142</cell></row><row><cell>19</cell><cell>Backdoor</cell><cell>Rbot!gen</cell><cell>158</cell></row><row><cell>20</cell><cell>Trojan</cell><cell>Skintrim.N</cell><cell>80</cell></row><row><cell cols="3">21 Trojan Downloader Swizzor.gen!E</cell><cell>128</cell></row><row><cell cols="3">22 Trojan Downloader Swizzor.gen!I</cell><cell>132</cell></row><row><cell>23</cell><cell>Worm</cell><cell>VB.AT</cell><cell>408</cell></row><row><cell cols="2">24 Trojan Downloader</cell><cell>Wintrim.BX</cell><cell>97</cell></row><row><cell>25</cell><cell>Worm</cell><cell>Yuner.A</cell><cell>800</cell></row><row><cell cols="4">may not be suitable for such data, but take note that the images</cell></row><row><cell cols="4">originate from malware binary files. Hence, the features are not</cell></row><row><cell cols="3">technically images to begin with.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Towards Building an Intelligent Anti-Malware System: A Deep Learning Approach using Support Vector Machine (SVM) for Malware Classification , , 2.4.3 Gated Recurrent Unit. Agarap (2017)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters used in the DL-SVM models.</figDesc><table><row><cell>Hyper-parameters</cell><cell cols="2">CNN-SVM GRU-SVM</cell><cell>MLP-SVM</cell></row><row><cell>Batch Size</cell><cell>256</cell><cell>256</cell><cell>256</cell></row><row><cell>Cell Size</cell><cell>N/A</cell><cell cols="2">[256 × 5] [512, 256, 128]</cell></row><row><cell>No. of Hidden Layers</cell><cell>2</cell><cell>5</cell><cell>3</cell></row><row><cell>Dropout Rate</cell><cell>0.85</cell><cell>0.85</cell><cell>None</cell></row><row><cell>Epochs</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Learning Rate</cell><cell>1e-3</cell><cell>1e-3</cell><cell>1e-3</cell></row><row><cell>SVM C</cell><cell>10</cell><cell>10</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Summary of experiment results on the DL-SVM models.</figDesc><table><row><cell>Variables</cell><cell>CNN-SVM</cell><cell cols="2">GRU-SVM MLP-SVM</cell></row><row><cell>Accuracy</cell><cell cols="3">77.2265625% 84.921875% 80.46875%</cell></row><row><cell>Data points</cell><cell>256000</cell><cell>256000</cell><cell>256000</cell></row><row><cell>Epochs</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>F1</cell><cell>0.79</cell><cell>0.85</cell><cell>0.81</cell></row><row><cell>Precision</cell><cell>0.84</cell><cell>0.85</cell><cell>0.83</cell></row><row><cell>Recall</cell><cell>0.77</cell><cell>0.85</cell><cell>0.80</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENT</head><p>We extend our statement of gratitude to the open-source community, especially to TensorFlow. An appreciation as well to Lakshmanan Nataraj, S. Karthikeyan, Gregoire Jacob, and B.S. Manjunath for the Malimg dataset <ref type="bibr" target="#b11">[12]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.tensorflow.org/get_started/mnist/pros" />
	</analytic>
	<monogr>
		<title level="j">Deep MNIST for Experts</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/Softwareavailablefromtensorflow.org" />
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03082</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00994018</idno>
		<ptr target="https://doi.org/10.1007/BF00994018" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Random Forest for Malware Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felan</forename><surname>Carlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">P</forename><surname>Muga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Matplotlib: A 2D graphics environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hunter</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2007.55</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2007.55" />
	</analytic>
	<monogr>
		<title level="j">Computing In Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="90" to="95" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The bulletin of mathematical biophysics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Malware images: visualization and automatic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmanan</forename><surname>Nataraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international symposium on visualization for cyber security</title>
		<meeting>the 8th international symposium on visualization for cyber security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The perceptron: A probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering Computers, Complete: Your Interactive Guide to the Digital World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misty</forename><forename type="middle">E</forename><surname>Shelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vermaat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cengage Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The NumPy array: a structure for efficient numerical computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Stéfan Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
