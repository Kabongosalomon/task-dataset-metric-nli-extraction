<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
							<email>2yalesong@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Yahoo Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
							<email>youngjin.kim@vision.snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
							<email>gunhee@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form <ref type="bibr" target="#b1">[2]</ref>. Part of the reasons for the success of VQA is that there exists a number of large-scale datasets with welldefined tasks and evaluation protocols <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, which provided a common ground to researchers to compare their methods in a controlled setting.</p><p>While we have seen a rapid progress in video analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>, most existing work in VQA have focused primarily on images. We believe that the limited progress in video VQA, compared to its image counter-Q) What is the color of the bird? part, is due in part to the lack of large-scale datasets with well-defined tasks. Some early attempts have been made to fill this gap by introducing datasets that leverage movie data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, focusing on storyline comprehension either from short video clips <ref type="bibr" target="#b29">[30]</ref> or from movies and scripts <ref type="bibr" target="#b33">[34]</ref>. However, existing question-answer pairs are either an extension to the conventional image VQA tasks, e.g., by adding action verbs as the new answer type <ref type="bibr" target="#b29">[30]</ref> to the existing categories of object, number, color, and location <ref type="bibr" target="#b28">[29]</ref>, or require comprehensive understanding of long textual data, e.g., movie scripts <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this paper, we contribute to the literature in VQA in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatiotemporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA that extends existing work in image VQA with our new tasks. Finally, we propose a dual-LSTM based approach with an attention mechanism to solve our problem, and show its effectiveness over conventional VQA tech-  niques through empirical evaluations. Our intention is not to compete with existing literature in VQA, but rather to complement them by providing new perspectives on the importance of spatio-temporal reasoning in VQA. Our design of video VQA tasks is inspired by existing works in video understanding, e.g., repetition counting <ref type="bibr" target="#b21">[22]</ref> and state transitions <ref type="bibr" target="#b14">[15]</ref>, intending to serve as a bridge between video understanding and video VQA. We define three tasks: (1) count the number of repetitions of a given action;</p><p>(2) detect a repeating action given its count; and (3) identify state transitions, i.e., what has happened before or after a certain action state. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, solving our tasks requires comprehensive spatio-temporal reasoning from videos, an ideal scenario for evaluating video analysis techniques. In addition to our new tasks, we also include the standard image VQA type tasks by automatically generating question-answer pairs from video captions <ref type="bibr" target="#b28">[29]</ref>. Following the existing work in VQA, we formulate our questions as either open-ended or multiple choice. This allows us to take advantage of well-defined evaluation protocols.</p><p>To create a benchmark for our tasks, we collected a new dataset for video VQA based on the Tumblr GIF (TGIF) dataset <ref type="bibr" target="#b22">[23]</ref>, which was originally proposed for video captioning. The TGIF dataset utilizes animated GIFs as their visual data, which have recently emerged as an attractive source of data in computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> due to their concise format and cohesive storytelling nature <ref type="bibr" target="#b4">[5]</ref>; this makes it especially ideal for vision and language understanding. We therefore extend the TGIF dataset to the VQA domain, adding 165K QA pairs from 72K animated GIFs from the TGIF dataset; we name our dataset TGIF-QA.</p><p>The current state-of-the-art in VQA have focused on finding visual-textual associations from images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>, employing a spatial attention mechanism to learn "where to look" in an image given the question <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. While existing techniques demonstrated impressive performance on image VQA, they are inadequate for the video domain because a video contains visual information both in spatial and temporal dimensions, requiring an appropriate spatio-temporal reasoning mechanism. In this work, we leverage spatio-temporal information from video by employing LSTMs not only for the QA pairs, as in the previous works, but also for the video input. We also evaluate spatial and temporal attention mechanisms to selectively attend to specific parts of a video. We discuss various design considerations and report empirical results in Section 5.</p><p>In this updated version of the paper, we extend our dataset by collecting more question and answer pairs (the total count has increased from 104K to 165K) and update all relevant statistics, including <ref type="table">Table 1</ref>. Also, we retake all the evaluations with the extended dataset and include languageonly baseline results in <ref type="table">Table 5</ref>.</p><p>To summarize, our major contributions include:</p><p>1. We propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly.</p><p>2. We introduce a new dataset, TGIF-QA, that consists of 165K QA pairs from 72K animated GIFs.</p><p>3. We propose a dual-LSTM based approach with an attention mechanism to solve our video QA tasks.</p><p>4. Code and the dataset are available on our project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>VQA is a relatively new problem domain first introduced by Malinowski et al. <ref type="bibr" target="#b24">[25]</ref> and became popularized by Antol et al. <ref type="bibr" target="#b1">[2]</ref>. Despite its short history, there has been a flourishing amount of research produced within the past few years <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b5">6]</ref>. Here, we position our research and highlight key differences compared to previous work in VQA.</p><p>Datasets. Most existing VQA datasets are imagecentric, e.g., DAQUAR <ref type="bibr" target="#b24">[25]</ref>, abstract scenes <ref type="bibr" target="#b23">[24]</ref>, VQA <ref type="bibr" target="#b1">[2]</ref>, Visual Madlibs <ref type="bibr" target="#b40">[41]</ref>, DAQUAR-Consensus <ref type="bibr" target="#b25">[26]</ref>, FM-IQA <ref type="bibr" target="#b10">[11]</ref>, COCO-QA <ref type="bibr" target="#b28">[29]</ref>, and Visual7W <ref type="bibr" target="#b42">[43]</ref>. Also, appearing in the same proceedings are CLEVR <ref type="bibr" target="#b15">[16]</ref>, VQA2.0 <ref type="bibr" target="#b11">[12]</ref>, and Visual Dialog <ref type="bibr" target="#b7">[8]</ref>, which all address image-based VQA. Our work extends existing works to the video domain, creating QA pairs from short video clips rather than static images.</p><p>There have been some recent efforts to create video VQA datasets based on movies. Rohrbach et al. <ref type="bibr" target="#b29">[30]</ref> extended the LSMDC movie description dataset <ref type="bibr" target="#b29">[30]</ref> to the VQA domain. Similarly, Tapaswi et al. <ref type="bibr" target="#b33">[34]</ref> introduced the MovieQA dataset by leveraging movies and movie scripts. Our work contributes to this line of research, but instead of restricting the source of video to the movie clips, here we leverage animated GIFs from the Internet, which have concise format and deliver cohesive visual stories <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Tasks. Existing QA pairs in the VQA literature have one of the following forms: open-ended and multiple choice; we consider fill-in-the-blank as a special case of the openended form. Open-ended questions provide either a complete or incomplete sentence and the system must guess the correct answer word. Multiple choice questions, on the other hand, provide a number of answer candidates, either as texts <ref type="bibr" target="#b1">[2]</ref> or bounding boxes <ref type="bibr" target="#b42">[43]</ref>, and the system must choose the correct one. Our dataset contains questions in the open-ended and multiple choice forms.</p><p>Most existing VQA tasks are image-centric and thus ask questions about visual concepts that appear only in images, e.g., objects, colors, and locations <ref type="bibr" target="#b1">[2]</ref>. In the video domain, the LSMDC-QA dataset <ref type="bibr" target="#b29">[30]</ref> introduced the movie fill-inthe-blank task by adding action verbs to the answer set, requiring spatio-temporal reasoning from videos at the sequence level (similar to action recognition). Our tasks also require spatio-temporal reasoning from videos, but at the frame level -counting the number of repetitions and memorizing state transitions from a video requires more comprehensive spatio-temporal reasoning.</p><p>The MovieQA dataset <ref type="bibr" target="#b33">[34]</ref> introduced an automatic story comprehension task from video and movie script. The questions are designed to require comprehensive visualtextual understanding of a movie synopsis, to the level of details of proper nouns (e.g., names of characters and places in a movie). Compared to the MovieQA dataset, our task is on spatio-temporal reasoning rather than story comprehension, and we put more focus on understanding visual signals (animated GIFs) rather than textual signals (movie scripts).</p><p>Techniques. Most existing techniques in VQA are designed to solve image VQA tasks. Various techniques have demonstrated promising results, such as the compositional model <ref type="bibr" target="#b0">[1]</ref> and the knowledge-based model <ref type="bibr" target="#b36">[37]</ref>. The current state-of-the-art techniques employ a spatial attention mechanism with visual-textual joint embedding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>. Our work extends this line of work to the video domain, by employing spatial and temporal attention mechanisms to solve video VQA tasks.</p><p>There are very few approaches designed specifically to solve video VQA. Yu et al. <ref type="bibr" target="#b41">[42]</ref> used LSTMs to represent both videos and QA pairs and adopted a semantic attention mechanism <ref type="bibr" target="#b39">[40]</ref> on both input word representation and output word prediction. We also use LSTMs to represent both videos and QA pairs, with a different attention mechanism to capture complex spatio-temporal patterns in videos. To the best of our knowledge, our model is the first to leverage temporal attention for video VQA tasks, which turns out to improve the QA performance in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TGIF-QA Dataset</head><p>Our dataset consists of 165,165 QA pairs collected from 71,741 animated GIFs. We explain our new tasks designed for video VQA and present the data collection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task Definition</head><p>We introduce four task types used in our dataset. Three of them are new and unique to the video domain, including: Repetition count. One task that is truly unique to videos would be counting the number of repetitions of an action. We define this task as an open-ended question about counting the number of repetitions of an action, e.g., <ref type="figure" target="#fig_1">Figure 2</ref> (a). There are 11 possible answers (from 0 to 10+).</p><p>Repeating action. A companion to the above, this task is defined as a multiple choice question about identifying an action that has been repeated in a video, e.g., <ref type="figure" target="#fig_1">Figure 2</ref> (b). We provide 5 options to choose from. State transition. Another task unique to videos is asking about transitions of certain states, including facial expressions (e.g., from happy to sad), actions (e.g., from running to standing), places (e.g., from the table to the floor), and object properties (e.g., from empty to full). We define this task as a multiple choice question about identifying the state before (or after) another state, e.g., <ref type="figure" target="#fig_1">Figure 2</ref> (c). We provide 5 options to choose from.</p><p>The three tasks above require analyzing multiple frames of a video; we refer to them collectively by video QA.</p><p>Besides our three video QA tasks, we also include another one, which we call frame QA to highlight the fact that questions in this task can be answered from one of the frames in a video. Depending on the video content, it can be any frame or one particular from of a video. For this task, we leverage the video captions provided in the TGIF dataset <ref type="bibr" target="#b22">[23]</ref> and use the NLP-based technique proposed in Ren et al. <ref type="bibr" target="#b28">[29]</ref> to generate QA pairs automatically from the captions. This task is defined as an open-ended question about identifying the best answer (from a dictionary of words of type object, number, color, and location) given a question in a complete sentence, e.g., <ref type="figure" target="#fig_1">Figure 2</ref> (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">QA Collection</head><p>For the frame QA, we use the same setup of Ren et al. <ref type="bibr" target="#b28">[29]</ref> and apply their method on the captions provided in the TGIF dataset <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="table">Table 1</ref>, this produced a total of 53,083 QA pairs from 39,479 GIFs. For the video QA, we generate QA pairs by using a combination of crowdsourcing and template-based approach. This produced a total of 112,082 QA pairs from 53,247 GIFs.</p><p>Crowdsourcing. We conducted two crowdsourcing studies, collecting the following information:</p><p>• Repetition: subject, verb, object, and the number of repetitions (from 2 to 10+ times) for a repeating action.</p><p>• State transition: subject, transition type (one of facial expression, action, place, or object property), previous state, next state for the changed states, if any.</p><p>We used drop-down menus to collect answers for the number of repetitions and the transition type, and used text boxes for all the others. A total of 595 workers have participated and were compensated by 5 cents per video clip. Quality control. Our task includes many free-form input; proper quality control is crucial. Inspired by Li et al. <ref type="bibr" target="#b22">[23]</ref>, we filter out suspiciously negligent workers by automatic validation. Specifically, we collect a small set of video clips (159 for repetition and 172 for state transition) as the validation set, and manually annotate each example with a set of appropriate answers; we consider those the gold standard. We then include one of the validation samples to each main task and check if a worker answers it correctly by matching their answers to our gold standard set. We reject the answers from workers who fail to pass our validation, and add those workers to our blacklist so that they cannot participate in other tasks. We regularly reviewed rejected answers to correct the mistakes made by our auto- matic validation, removing the worker from our blacklist and adding their answers to our gold standard set.</p><p>Post processing. We lemmatize all verbs with the Word-Net lemmatizer and find the main verb in each state using the VerbNet <ref type="bibr" target="#b19">[20]</ref>. We detect proper nouns in the collected answers using the DBpedia Spotlight <ref type="bibr" target="#b6">[7]</ref> and replace them with the corresponding common noun, e.g., person names, body parts, animal names, etc. We also remove any possessive determiners for the phrases used in answers.</p><p>QA generation. We generate QA pairs using the templates shown in <ref type="table" target="#tab_2">Table 2</ref>. It is possible that the generated questions have grammatical errors; we fix those using the LanguageTool. We then generate multiple choice options for each QA pair, selecting four phrases from our dataset.</p><p>Specifically, we represent all verbs in our dictionary as a 300D vector using the GloVe word embedding <ref type="bibr" target="#b26">[27]</ref>, pretrained on the Common Crawl dataset. We then select four verbs, one by one in a greedy manner, whose cosine similarity with the verb from the answer phrase is smaller than the 50th percentile, while at the same time the average cosine similarity from the current set of candidate verbs is minimal -this encourages diversity in negative answers. We then choose four phrases by maximizing cosine similarity of skip-thought vectors <ref type="bibr" target="#b20">[21]</ref> pretrained on the BookCorpus dataset <ref type="bibr" target="#b43">[44]</ref>.</p><p>For the repetition counting task, we automatically added samples that had zero count of an action, by randomly pairing a question from our question list with a GIF that was identified as having no repeating action. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Other Video VQA Datasets</head><formula xml:id="formula_0">• • • • • $ Question • • • • • &amp; • • • • • GloVe &lt;BOA&gt; play . . . hand • • • • • $ Answer • • • • • &amp; • • • • • C3D "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-152</head><p>Video " Temporal Att. Spatial Att.  <ref type="figure" target="#fig_3">Figure 4</ref> for the structure of spatial and temporal attention modules.</p><p>QA, we include tasks unique to video VQA. Therefore, our dataset can complement existing datasets with unique tasks. <ref type="table" target="#tab_3">Table 3</ref> shows the distribution of verbs used in answers. We show top five most common verb categories obtained from the WordNet hierarchy. Most notably, TGIF-QA contains more dynamic verbs, such as the ones from the motion and the contact categories. This is an important characteristic of our dataset because it suggests the need for spatiotemporal reasoning to understand the content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>We present spatio-temporal VQA (ST-VQA) model for our task (see <ref type="figure" target="#fig_2">Figure 3</ref>). The input to our model is a tuple (v, q, a) of a video v, a question sentence q, and an answer phrase a; the answer phrase a is optional and provided only from multiple choice questions (indicated as red dashed box in <ref type="figure" target="#fig_2">Figure 3</ref>). The output is either a single word (for open-ended questions) or a vector of compatibility scores (for multiple choice questions). Our ST-VQA model captures visual-textual association between a video and QA sentences using two dual-layer LSTMs, one for each input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Representation</head><p>Video representation. We represent a video both at the frame-level and at the sequence-level. For the frame features, we use the ResNet-152 <ref type="bibr" target="#b13">[14]</ref> pretrained on the Ima-geNet 2012 classification dataset <ref type="bibr" target="#b30">[31]</ref>. For the sequence features, we use the C3D <ref type="bibr" target="#b34">[35]</ref> pretrained on the Sport1M dataset <ref type="bibr" target="#b16">[17]</ref>. We sample one every four frames to reduce the frame redundancy. For the C3D features, we take 16 subsequent frames centered at each time step, and pad the first or the last frame if too short. We denote the two video descriptors, ResNet-152 and C3D, by {f t } T t=1 and {s t } T t=1 , respectively; T is the sequence length.</p><p>Depending on whether we use our spatio-temporal attention mechanism (explained in Section 4.4), we use different feature representations. For the ResNet-152 feature, we take the feature map of the res5c layer (∈ R 7×7×2,048 ) for the spatial attention mechanism and the pool5 features (∈ R 2,048 ) for the others. Similarly, for the C3D features, we take the conv5b layer (∈ R 7×7×1,024 ) for the spatial attention mechanism and the fc6 feature for the others.</p><p>Text representation. There are two types of text inputs: question and answer. A question is a complete sentence, while an answer is a phrase. We simply consider both as a sequence of words and represent them in the same way. For a given input, we represent each word as a 300D vector using the GloVe word embedding <ref type="bibr" target="#b26">[27]</ref> pretrained on the Common Crawl dataset. We denote the text descriptor for questions and answers by {q n } N n=1 and {a m } M m=1 , respectively; N and M are the sequence lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video and Text Encoders</head><p>Video encoder. We encode video features {s t } T t=1 and {f t } T t=1 using the video encoding LSTM, shown in the purple dashed box in <ref type="figure" target="#fig_2">Figure 3</ref>. We first concatenate the features m t = [s t ; f t ], and feed them into the dual-layer LSTM one at a time, producing a hidden state h v t ∈ R D at each step:</p><p>h v Text encoder. We encode text features of question {q n } N n=1 and answer choices {a m } M m=1 using the text encoding LSTM, shown in the navy dashed box in <ref type="figure" target="#fig_2">Figure 3</ref>. While open-ended questions involve only a question, multiple choice questions come with a question and a set of answer candidates. We encode a question {q n } N n=1 and each of the answer choices {a m } M m=1 using a dual-layer LSTM:</p><formula xml:id="formula_1">h q n = LSTM(q n , h q n−1 ), h q 0 = h v T . (2) h a m = LSTM(a m , h a m−1 ), h a 0 = h q N<label>(3)</label></formula><p>We set the initial hidden state h q 0 to the last hidden state of the video encoder h v T , so that visual information is "carried over" to the text encoder -an approach similar to other sequence-to-sequence models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>. To indicate the starting point of the answer candidate, we put a special character, &lt;BOA&gt; (begin of answer). We also use the last hidden state of the question encoder as the initial hidden state of the answer encoder. Similar to the video encoder, we set the dimension of all the hidden states to D = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Answer Decoders</head><p>We design three decoders that provide answers: one for the multiple choice, the other two for the open-ended.</p><p>Multiple choice. We define a linear regression function that takes as input the final hidden states from the answer encoder, h a M , and outputs a real-valued score for each answer candidate,</p><formula xml:id="formula_2">s = W s h a M (4)</formula><p>where W s ∈ R 1,024 is the model parameter. We train the decoder by minimizing the hinge loss of pairwise comparisons, max(0, 1 + s n − s p ), where s n and s p are scores computed from an incorrect and correct answers, respectively. We use this decoder to solve repeating action and state transition tasks.</p><p>Open-ended, number. Similar to the above, we define a linear regression function that takes as input the final hidden states from the answer encoder, and outputs an integervalued answer by adding a bias term b s to Equation (4). We train the decoder by minimizing the 2 loss between the answer and the predicted value. We use this encoder to solve the repetition count task.</p><p>Open-ended, word. We define a linear classifier that takes as input the final hidden states from the question encoder, h q N ∈ R 1,024 , and selects an answer from a vocabulary of words V by computing a confidence vector o ∈ R |V|</p><formula xml:id="formula_3">o = softmax W o h q N + b o<label>(5)</label></formula><p>where W o ∈ R |V|×1,024 and b o ∈ R |V| are model parameters. We train the decoder by minimizing the softmax loss function. The solution is obtained by y = argmax y∈V (o). We use this encoder to solve the frame QA task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attention Mechanism</head><p>While our tasks require spatio-temporal reasoning from videos, the model explained so far is inadequate for such tasks because, in theory, the video encoder "squashes" necessary details of the spatio-temporal visual information into a flat representation. We now explain our spatial and temporal attention mechanisms, illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. The former allows us to learn which regions in each frame of a video to attend to, while the latter allows us to learn which frames in a video to attend to solve our tasks. As such, we employ different mechanisms to model each attention type, based on Xu et al. <ref type="bibr" target="#b37">[38]</ref> for spatial attention and Bahdanau et al. <ref type="bibr" target="#b3">[4]</ref> for temporal attention.</p><p>Spatial attention. To learn which regions in a frame to attend for each word, we use visual representation that preserves spatial information and associate it with a QA pair. Also, we need textual signals when encoding each frame in the video decoder. However, as the model takes a QA pair only after encoding a video, this information is not available a priori. We solve this issue by simply defining another dual-layer LSTM that shares its model parameters with the text encoder. <ref type="figure" target="#fig_3">Figure 4</ref> (a) illustrates our spatial attention mechanism. For each time step t in a video sequence, we compute a 7 × 7 spatial attention mask α t = f att (h q N , m t ), where h q N ∈ R 1,024 is the output of the text encoder and m t ∈ R 7×7×3,072 is the visual feature map. We then pass the attended visual feature α t m t ∈ R 3,072 to the video encoder. The function f att (·, ·) is a multi-layer perceptron (MLP) that operates over each of 7 × 7 spatial locations, followed by the softmax function. Our MLP is a single layer of 512 hidden nodes with the tanh activation function.</p><p>Temporal attention. To learn which frames in a video to attend to, we use a visual representation that preserves temporal information and associate it with a QA pair. <ref type="figure" target="#fig_3">Figure 4</ref> (b) shows our temporal attention mechanism. After we encode video and question sequences, we compute a 1 × T temporal attention mask α = f att (h q N , H v ), where h q N ∈ R 1,024 is the last state of the text encoder and H v ∈ R T ×1,024 is a state sequence from the video encoder. We then compute the attended textual signal tanh(αH v W α ) ⊕ h q N , where W α ∈ R 1,024×1,024 and ⊕ is an element-wise sum, and pass it to the answer decoder. We use the same f att (·, ·) as with our spatial attention, with its MLP operating over the temporal dimension T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Implementation Details</head><p>We use the original implementations of ResNet <ref type="bibr" target="#b13">[14]</ref>, C3D <ref type="bibr" target="#b34">[35]</ref>, and GloVe <ref type="bibr" target="#b26">[27]</ref> to obtain features from videos and QA text. All the other parts of our model are implemented using the TensorFlow library. Except for extracting the input features, we train our model end-to-end. For the dual-layer LSTMs, we apply layer normalization <ref type="bibr" target="#b2">[3]</ref> to all cells, with the dropout <ref type="bibr" target="#b27">[28]</ref> with a rate of 0.2. For training, we use the ADAM optimizer <ref type="bibr" target="#b18">[19]</ref> with an initial learning rate of 0.001. All weights in LSTMs are initialized from a uniform distribution, and all the other weights are initialized from a normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We tackle open-ended word and multiple choice tasks as multi-class classification, and use the accuracy as our evaluation metric, reporting the percentage of correctly answered questions. For the open-ended number task, we use the mean 2 loss as our evaluation metric to account for the ordinal nature of the numerical labels. We split the data into training and test sets as shown in <ref type="table">Table 1</ref>, following the setting in the original TGIF dataset <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head><p>We compare our approach against two recent imagebased VQA methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref>, as well as one video-based method <ref type="bibr" target="#b41">[42]</ref>. For fair comparisons, we re-implemented the baselines in TensorFlow and trained them from scratch using the same set of input features.</p><p>Image-based. We select two state-of-the-art methods in image-based VQA: VIS+LSTM <ref type="bibr" target="#b28">[29]</ref> and VQA-MCB <ref type="bibr" target="#b9">[10]</ref>. VIS+LSTM combines image representation with textual features encoded by an LSTM, after which it solves openended questions using a softmax layer <ref type="bibr" target="#b28">[29]</ref>. VQA-MCB, on the other hand, uses multimodal compact bilinear pooling to handle visual-textual fusion and spatial attention <ref type="bibr" target="#b9">[10]</ref>. This model is the winner of the VQA 2016 challenge.</p><p>Since both methods take a single image as input, we adjust them to be applicable to video VQA. We evaluate two simple approaches: aggr and avg. The aggr method aggregates input features of all frames in a video by averaging them, and uses it as input to the model. The avg method, on the other hand, solves the question using each frame of a video, one at a time, and report the average accuracy across all frames of all videos, i.e.,</p><formula xml:id="formula_4">1/N N i=1 (1/M i Mi j=1 I[y i,j = y * i ]),</formula><p>where N is the number of videos, M i is the number of frames for the i-th video,  <ref type="table">Table 5</ref>. Experimental results of VQA according to different problem types on our TGIF-QA dataset. (Sp.) indicates the spatial attention and (Tp.) means temporal one. We report the mean 2 loss for the repetition count task, and the accuracy for the other three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I[·]</head><p>is an indicator function, y i,j is a predicted answer for the j-th frame of the i-th video, and y * i is an answer for the i-th video.</p><p>Video-based. We select the state-of-the-art method in video VQA, Yu et al. <ref type="bibr" target="#b41">[42]</ref>, which has won the retrieval track in the LSMDC 2016 benchmark. We use their retrieval model that employs the same decoder as explained in section 4.3. Although the original method used an ensemble approach, we here use a single model for a fair comparison.</p><p>Variants of our method. To conduct an ablation study of our method, we compare seven variants of our model, as shown in <ref type="table">Table 5</ref>. The four (Text, ResNet, C3D, Concat) compare different representations for the video input; Text uses neither ResNet nor C3D features, whereas Concat uses both ResNet and C3D features. They also do not employ our spatial and temporal attention mechanisms. The next two variants (Spatial and Temporal) include either one of the attention mechanisms. Finally, we evaluate a combination of the two attention mechanisms, by training the temporal part first and finetuning the spatial part later. <ref type="table">Table 5</ref> summarizes our results. We observe that videobased methods outperform image-based methods, suggesting the need for spatio-temporal reasoning in solving our video QA tasks. We note, however, that the differences may not be seen significant; we believe this is because the C3D features already capture spatio-temporal information to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Analysis</head><p>A comparison between different input features of our method (Text, ResNet, C3D, Concat) suggests the importance of having both visual representations in our model.  Among the four baselines, the Concat approach that uses both features achieves the best performance across all tasks.</p><p>A comparison between different attention mechanisms (Spatial and Temporal) shows the effectiveness of our temporal attention mechanism, achieving the best performance in three tasks. Similar results are reported in the literature; for example, in the video captioning task, Yao et al. <ref type="bibr" target="#b38">[39]</ref> obtained the best result by considering both local and global temporal structures.</p><p>Finally, <ref type="figure" target="#fig_5">Figure 5</ref> shows some qualitative examples from different approaches on the four task types of TGIF-QA. We observe that answering the questions indeed requires spatio-temporal reasoning. For example, the cat in Figure 5 (b) puts head down multiple times, which cannot be answered without spatio-temporal reasoning. Our method successfully combines spatial and temporal visual representation from the input-level via ResNet and C3D features, and learns to selectively attend to them via our two attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Our work complements and extends existing work on VQA with three main contributions: (i) proposing three new tasks that require spatio-temporal reasoning from videos, (ii) introducing a new large-scale dataset of video VQA with 165K QA pairs from 72K animated GIFs, and (iii) designing a dual-LSTM based approach with both spatial and temporal attention mechanisms.</p><p>Moving forward, we plan to improve our ST-VQA model in several directions. Although our model is based on a sequence-to-sequence model <ref type="bibr" target="#b35">[36]</ref> to achieve simplicity, it can be improved in different ways, such as adopting the concept of 3D convolution <ref type="bibr" target="#b34">[35]</ref>. Another direction is to find better ways to combine visual-textual information. Our model without the attention module (e.g. Concat in <ref type="table">Table 5</ref>) combines visual-textual information only at the text encoding step. Although our attention mechanisms explored ways to combine the two modalities to some extent, we believe there can be more principled approaches to do it efficiently, such as the recently proposed multimodal compact bilinear pooling <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Document Changelog</head><p>To help readers understand how it had changed over time, here's a brief changelog describing the revisions. v1 (Initial) CVPR 2017 camera-ready version. v2 Added statistics and results, including text-only baseline, for extended dataset. v3 Updated the results in <ref type="table">Table 5</ref> and uploaded relevant files to our repository.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>How many times does the cat touch the dog? Much of conventional VQA tasks focus on reasoning from images (top). This work proposes a new dataset with tasks designed specifically for video VQA that requires spatio-temporal reasoning from videos to answer questions correctly (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our TGIF-QA dataset introduces three new tasks for video QA, which require spatio-temporal reasoning from videos (e.g. (a) repetition count, (b) repeating action, and (c) state transition). It also includes frame QA tasks that can be answered from one of frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed ST-VQA model for spatio-temporal VQA. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Our spatial and temporal attention mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>What does the model do after lower coat? (Ours) Pivot around (VQA-MCB) Hold up a decoration (VIS+LSTM) Bend over (LSMDC-ret.) Bend over (b) Repeating Action Q) What does the cat do 3 times? (Ours) Put head down (VQA-MCB) Dance on floor (VIS+LSTM) Move legs (LSMDC-ret.) Move legs (a) Repetition Count Q) How many times does the cat lick?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison of VQA results from different approaches, on the four task types of our TGIF-QA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1704.04497v3 [cs.CV] 3 Dec 2017</figDesc><table><row><cell></cell><cell>Video QA</cell><cell></cell><cell>Frame QA</cell></row><row><cell>(a) Repetition Count</cell><cell>(b) Repeating Action</cell><cell>(c) State Transition</cell><cell>(d) Object / Number / Color / Location</cell></row><row><cell>Q) How many times does the</cell><cell>Q) What does the duck do</cell><cell>Q) What does the bear on right</cell><cell>Q) What is dancing in the cup?</cell></row><row><cell>man wrap string? A) 5 times</cell><cell>3 times? A) Shake head</cell><cell>do after sitting? A) Stand</cell><cell>A) Tree</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Count 26,843 3,554 30,397 26,843 3,554 30,397 Rep. Action 20,475 2,274 22,749 20,475 2,274 22,749 Transition 52,704 6,232 58,936 26,352 3,116 29,468 Object 16,755 5,586 22,341 15,584 3,209 18,793</figDesc><table><row><cell></cell><cell>Task</cell><cell>Train</cell><cell># QA pairs Test</cell><cell>Total</cell><cell># GIFs Train Test Total</cell></row><row><cell cols="2">Video QA Rep. Frame Number</cell><cell cols="4">8,096 3,148 11,244 8,033 1,903 9,936</cell></row><row><cell>QA</cell><cell>Color</cell><cell cols="4">11,939 3,904 15,843 10,872 3,190 14,062</cell></row><row><cell></cell><cell>Location</cell><cell cols="4">2,602 1,053 3,655 2,600 917 3,517</cell></row><row><cell></cell><cell>Total</cell><cell cols="4">139,414 25,751 165,165 62,846 9,575 71,741</cell></row></table><note>Table 1. Statistics of our dataset, organized into different tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Templates used for creating video QA pairs.</figDesc><table><row><cell>Task</cell><cell>Question</cell><cell>Answer</cell></row><row><cell cols="2">Repetition How many times does the count [SUB] [VERB] [OBJ] ?</cell><cell>[#Repeat]</cell></row><row><cell cols="2">Repeating What does the [SUB] do action [#Repeat] times ?</cell><cell>[VERB] [OBJ]</cell></row><row><cell></cell><cell>What does the [SUB] do</cell><cell>[Previous</cell></row><row><cell>State</cell><cell>before [Next state] ?</cell><cell>state]</cell></row><row><cell>transition</cell><cell>What does the [SUB] do</cell><cell>[Next</cell></row><row><cell></cell><cell>after [Previous state] ?</cell><cell>state]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>QA<ref type="bibr" target="#b29">[30]</ref> 27.98% 19.09% 14.78% 4.43% 5.19% MovieQA<ref type="bibr" target="#b33">[34]</ref> 13.90% 11.76% 4.95% 2.18% 12.17% TGIF-QA(ours) 38.04% 24.78% 9.45% 7.13% 6.78 % Distributions of verbs in the answers from different datasets. We show top five most common categories with example verbs. Percp.: perception, comm.: communication.</figDesc><table><row><cell>Category</cell><cell cols="5">Motion Contact Percp. Body Comm.</cell></row><row><cell></cell><cell>jump</cell><cell>stand</cell><cell>look</cell><cell>smile</cell><cell>nod</cell></row><row><cell></cell><cell>turn</cell><cell>touch</cell><cell>stare</cell><cell cols="2">blink point</cell></row><row><cell>Examples</cell><cell>shake</cell><cell>put</cell><cell cols="2">show blow</cell><cell>talk</cell></row><row><cell></cell><cell>run</cell><cell>open</cell><cell>hide</cell><cell cols="2">laugh wave</cell></row><row><cell></cell><cell>move</cell><cell>sit</cell><cell cols="2">watch wink</cell><cell>face</cell></row><row><cell>LSMDC-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 compares</head><label>4</label><figDesc>our dataset with two existing video VQA datasets. LSMDC-QA refers to the subset of the data used for the VQA task in the LSMDC 2016 Challenge.It shows that TGIF-QA is unique in terms of the objective and the sources of video and text. i.e., it includes short video clips (GIFs) collected over social media, whereas the other two includes movie clips. Ours also includes both types of questions, open-ended and multiple choice, unlike other datasets. While our dataset is smaller than LSMDC-QA(ours) Spatio-temporal reasoning from video OE &amp; MC Social media Caption &amp; crowdsourced 165,165 71,741Table 4. Comparison of three video VQA datasets (Q.: question, OE: open-ended, and MC: multiple choice).</figDesc><table><row><cell>Dataset</cell><cell>Objective</cell><cell>Q. Type</cell><cell cols="2">Video Source Text Source</cell><cell cols="2"># QA pairs # Clips</cell></row><row><cell cols="3">LSMDC-QA [30] Fill-in-the-blank for caption completion OE</cell><cell>Movie</cell><cell>Movie caption</cell><cell cols="2">348,998 111,744</cell></row><row><cell>MovieQA [34]</cell><cell>Visual-textual story comprehension</cell><cell>MC</cell><cell>Movie</cell><cell>Movie synopsis</cell><cell>14,944</cell><cell>6,771</cell></row><row><cell cols="6">TGIF-FC Video Encoder LSTM Text Encoder LSTM</cell><cell>Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Output</cell></row><row><cell></cell><cell>"</cell><cell>GloVe</cell><cell></cell><cell></cell><cell>Regression</cell><cell></cell></row><row><cell></cell><cell>Concat</cell><cell cols="2">what does . . . times</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multiple Choice</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t = LSTM(m t , h v t−1 ).(1)Since we employ a dual-layer LSTM, we obtain pairs of hidden states h v t = (h v,1 t , h v,2 t ). For brevity, we use the combined form h v t for the rest of the paper. We set the dimension D = 512.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Miran Oh for the discussions related to natural language processing, as well as Jongwook Choi for helpful comments about the model. We also appreciate Cloud &amp; Mobile Systems lab and Movement Research lab at Seoul National University for renting a few GPU servers for this research. This work is partially supported by Big Data Institute (BDI) in Seoul National University and Academic Research Program in Yahoo Research. Gunhee Kim is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Module Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer Normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast, Cheap, and Good -Why Animated GIFs Engage Us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakhshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muscat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Efficiency and Accuracy in Multilingual Entity Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">I-Semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Survey of Current Datasets for Vision and Language Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video2GIF: Automatic Generation of Animated GIFs from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering States and Transformations in Image Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-Scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multimodal Residual Learning for Visual QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-O</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ADAM: A Method For Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kipper-Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>UPenn CIS</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skip-Thought Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Live Repetition Counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<title level="m">TGIF: A New Dataset and Benchmark on Animated GIF Description. In CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Don&apos;t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-visual Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove -Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout Improves Recurrent Neural Networks for Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring Models and Data for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">Movie Description. IJCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Video Representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to Sequence -Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What Value do Explicit High Level Concepts Have in Vision to Language Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Describing Videos by Exploiting Temporal Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image Captioning with Semantic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual Madlibs: Fill in the Blank Description Generation and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
