<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-29">November 29, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
							<email>benjamingraham@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
							<email>martin@robots.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
						</author>
						<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-29">November 29, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard "dense" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional networks (ConvNets) constitute the stateof-the art method for a wide range of tasks that involve the analysis of data with spatial and/or temporal structure, such as photos, videos, or 3D surface models. While such data frequently comprises a densely populated (2D or 3D) grid, other datasets are naturally sparse. For instance, handwriting is made up of one-dimensional lines * Facebook AI Research † University of Oxford, intern at Facebook AI Research in two-dimensional space, pictures made by RGB-D cameras are three-dimensional point clouds, and polygonal mesh models form two-dimensional surfaces in 3D space. The curse of dimensionality applies, in particular, to data that lives on grids that have three or more dimensions: the number of points on the grid grows exponentially with its dimensionality. In such scenarios, it becomes increasingly important to exploit data sparsity whenever possible in order to reduce the computational resources needed for data processing. Indeed, exploiting sparsity is paramount when analyzing, e.g., RGB-D videos which are sparsely populated 4D structures.</p><p>Traditional convolutional network implementations are optimized for data that lives on densely populated grids, and cannot process sparse data efficiently. More recently, a number of convolutional network implementations have been presented that are tailored to work efficiently on sparse data <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b3">3]</ref>. Mathematically, some of these implementations are identical to regular convolutional networks, but they require fewer computational resources in terms of FLOPs and/or memory <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b3">3]</ref>. Prior work uses a sparse version of the im2col operation that restricts computation and storage to "active" sites <ref type="bibr" target="#b4">[4]</ref>, or uses the voting algorithm from <ref type="bibr" target="#b22">[22]</ref> to prune unnecessary multiplications by zeros <ref type="bibr" target="#b3">[3]</ref>. OctNets <ref type="bibr" target="#b18">[18]</ref> modify the convolution operator to produce "averaged" hidden states in parts of the grid that are outside the region of interest.</p><p>One of the downsides of prior sparse implementations of convolutional networks is that they "dilate" the sparse data in every layer by applying "full" convolutions. In this work, we show that it is possible to create convolutional <ref type="figure">Figure 1</ref>: Examples of 3D point clouds of objects from the ShapeNet part-segmentation challenge <ref type="bibr" target="#b23">[23]</ref>. The colors of the points represent the part labels. networks that keep the same level of sparsity throughout the network. To this end, we develop a new implementation for performing sparse convolutions (SCs) and introduce a novel convolution operator termed submanifold sparse convolution (SSC). <ref type="bibr" target="#b0">1</ref> We use these operators as the basis for submanifold sparse convolutional networks (SS-CNs) that are optimized for efficient semantic segmentation of 3D point clouds, e.g., on the examples shown in <ref type="figure">Figure 1</ref>.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we present the performance of SSCNs on the test set of a recent part-based segmentation competition <ref type="bibr" target="#b23">[23]</ref> and compare it to some of the top-performing entries in the competition: SSCNs outperform all of these entries. Source code for our library is publicly available online 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work primarily builds upon previous literature on sparse convolutional networks <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>, and image segmen-1 These operators appeared earlier in an unpublished technical report <ref type="bibr" target="#b5">[5]</ref>  tation using dense convolutional networks <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b24">24]</ref>. Examples of applications of dense 3D convolutions on volumetric data include classification <ref type="bibr" target="#b15">[15]</ref> and segmentation <ref type="bibr" target="#b1">[2]</ref>; these methods suffer from high memory usage and slow inference, limiting the size of models that can be used. Methods for processing 3D point clouds without voxelization have also been developed <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b17">17]</ref>. This may seem surprising given the dominance of ConvNets for processing 2D inputs; it is likely due to the computational obstacles involved in using dense 3D convolutional networks.</p><p>Prior work on sparse convolutions implements a convolutional operator that increases the number of active sites with each layer <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4]</ref>. In <ref type="bibr" target="#b4">[4]</ref>, all sites that have at least one "active" input site are considered as active. In <ref type="bibr" target="#b3">[3]</ref>, a greater degree of sparsity is attained after the convolution has been calculated by using ReLUs and a special loss function. In contrast, we introduce submanifold sparse convolutions that fix the location of active sites so that the sparsity remains unchanged for many layers. We show that this makes it practical to train deep and efficient networks similar to VGG networks <ref type="bibr" target="#b20">[20]</ref> or ResNets <ref type="bibr" target="#b7">[7]</ref>, and that it is well suited for the task of point-wise semantic segmentation.</p><p>OctNets <ref type="bibr" target="#b18">[18]</ref> are an alternative form of sparse convolution. Sparse voxels are stored in oct-trees: a data structure in which the grid cube is progressively subdivided into 2 3 smaller sub-cubes until the sub-cubes are either empty or contain a single active site. OctNet operates on the surfaces of empty regions, so a size-3 OctNet convolution on an empty cube of size 8 3 requires 23% of the calculation of a dense 3D convolution. Conversely, submanifold convolutions require no calculations in empty regions.</p><p>Another approach to segmenting point clouds is to avoid voxelizing the input, which may lead to a loss of information due to the finite resolution. This can be done by either using carefully selected data structures such as Kd-trees <ref type="bibr" target="#b10">[10]</ref>, or by directly operating on the unordered set of points <ref type="bibr" target="#b17">[17]</ref>. Kd-Networks <ref type="bibr" target="#b10">[10]</ref> build a Kd-tree by recursively partitioning the space along the axis of largest variation until each leaf of the tree represents one input point. This takes O(N log N ) time for N input points. PointNet <ref type="bibr" target="#b17">[17]</ref> uses a pooling operation to produce a global feature vector.</p><p>Fully convolutional networks (FCNs) were proposed in <ref type="bibr" target="#b14">[14]</ref> as a method of 2D image segmentation; FCNs make use of information at multiple scales to preserve low-level information to accurately delineate object boundaries. U-Nets <ref type="bibr" target="#b19">[19]</ref> extend FCNs by using convolutions to more accurately merge together the information from the different scales before the final classification stage; see <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spatial sparsity for ConvNets</head><p>We define a d-dimensional convolutional network as a network that takes as input a (d + 1)-dimensional tensor: the input tensor contains d spatio-temporal dimensions (such as length, width, height, time, etc.) and one additional feature-space dimension (e.g., RGB color channels or surface normal vectors). The input corresponds to a ddimensional grid of sites, each of which is associated with a feature vector. We define a site in the input to be active if any element in the feature vector is not in its ground state, e.g., if it is non-zero <ref type="bibr" target="#b3">3</ref> . In many problems, thresholding may be used to eliminate input sites at which the feature vector is within a small distance from the ground state. Note that even though the input tensor is (d + 1)dimensional, activity is a d-dimensional phenomenon: entire lines along the feature dimension are either active or inactive.</p><p>Similarly, the hidden layers of a d-dimensional convolutional network are represented by d-dimensional grids of feature-space vectors. When propagating the input data through the network, a site in a hidden layer is active if any of the sites in the layer that it takes as input is active. (Note that when using size-3 convolutions, each site is connected to 3 d sites in the hidden layer below.) Activity in a hidden layer thus follows an inductive definition in which each layer determines the set of active states in the next layer. In each hidden layer, inactive sites all have the same feature vector: the one corresponding to the ground state. The value of the ground state only needs to be calculated once per forward pass at training time, and only once for all forward passes at test time. This allows for substantial savings in computational and memory requirements; the exact savings depend on data sparsity and network depth.</p><p>However, we argue that the framework described above is unduly restrictive, in particular, because the convolution operation has not been modified to accommodate the sparsity of the input data. If the input data contains a single active site, then after applying a 3 d convolution, there will be 3 d active sites. Applying a second convolution of the same size will yield 5 d active sites, and so on. This rapid growth of the number of active sites is a poor prospect when implementing modern convolutional network architectures that comprise tens or even hundreds of convolutional layers, such as VGG networks, ResNets, or DenseNets <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>Of course, convolutional networks are not often applied to inputs that only have a single active site, but the aforementioned dilation problems are equally problematic when the input data comprises one-dimensional curves in spaces with two or more dimensions, or two-dimensional surfaces in three or more dimensions. We refer to this problem as the "submanifold dilation problem", which is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. The figure illustrates that even when we apply small 3 × 3 convolutions on this grid, the sparsity of the grid rapidly disappears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submanifold Convolutional Networks</head><p>We explore a simple solution to the submanifold dilation problem that restricts the output of the convolution only to the set of active input points. A potential problem of this approach is that hidden layers in the network may not receive all the information they require to classify the input data: in particular, two neighboring connected components are treated completely independently. We resolve this problem by using convolutional networks that incorporate some form of pooling, or use strided convolutions. Such operations are essential in the sparse convolutional networks <ref type="bibr" target="#b4">4</ref> we investigate, as they allow information to flow between disconnected components in the input. The closer the components are spatially, the fewer strided operations are necessary for the components to "communicate" in the intermediate representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sparse Convolutional Operations</head><p>We define a sparse convolution SC(m, n, f, s) with m input feature planes, n output feature planes, a filter size of f , and stride s. An SC convolution computes the set of active sites in the same way as a regular convolution: it looks for the presence of any active sites in its receptive field of size f d . If the input has size then the output will have size ( − f + s)/s. Unlike a regular convolution (and the sparse convolution from <ref type="bibr" target="#b4">[4]</ref>), an SC convolution discards the ground state for non-active sites by assuming that the input from those sites is zero. This seemingly small change to the convolution brings computational benefits in practice.</p><p>Submanifold sparse convolution. Next, we define a second type of sparse convolution, which forms the main contribution of this paper. Let f denote an odd number. We define a submanifold sparse convolution SSC(m, n, f ) as a modified SC(m, n, f, s = 1) convolution. First, we pad the input with (f − 1)/2 zeros on each side, so that the output will have the same size as the input. Next, we restrict an output site to be active if and only if the site at the corresponding site in the input is active (i.e., if the central site in the receptive field is active). Whenever an output site is determined to be active, its output feature vector is computed by the SSC convolution; see <ref type="figure" target="#fig_1">Figure 3</ref> for an illustration. <ref type="table" target="#tab_2">Table 2</ref> shows the computational and memory requirements of a regular convolution (C) and our SC and SSC convolutions. Submanifold sparse convolutions are similar to Oct-Nets <ref type="bibr" target="#b18">[18]</ref> in that they preserve the sparsity structure. However, unlike OctNets, empty space imposes no computational or memory overhead in the implementation of submanifold sparse convolutions.</p><p>Other operators. To construct convolutional networks using SC and SSC, we also need activation functions, batch normalization, and pooling. Activation functions are defined as usual, but are restricted to the set of active sites. Similarly, we define batch normalization in terms of regular batch normalization applied over the set of active sites. Max-pooling MP(f, s) and averagepooling AP(f, s) operations are defined as a variant of SC(·, ·, f, s). MP takes the maximum of the zero vector and the input feature vectors in the receptive field. AP calculates f −d times the sum of the active input vectors. We also define a deconvolution <ref type="bibr" target="#b25">[25]</ref> operation DC(·, ·, f, s) as an inverse of the SC(·, ·, f, s) convolution. The set of active output sites from a DC convolution is exactly the same as the set of input active sites to the corresponding  Notation: a is the number of active inputs to the spatial location, m the number of input feature planes, and n the number of output feature planes.</p><p>SC convolution: the connections between input and output sites are simply inverted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>To implement (S)SC convolutions efficiently, we store the state of a input/hidden layer in two parts: a hash table <ref type="bibr" target="#b5">5</ref> and a matrix. The matrix has size a × m and contains one row for each of the a active sites. The hash table contains (location, row) pairs for all active sites: the location is a tuple of integer coordinates, and the row number indicates the corresponding row in the feature matrix. Given a convolution with filter size f , let F = {0, 1, . . . , f − 1} d denote the spatial size of the convolutional filter. Define a rule book to be a collection R = (R i : i ∈ F ) of f d integer matrices each with two columns. To implement an SC(m, n, f, s) convolution, we: 2. Initialize the output matrix to all zeros. For each i ∈ F , there is a parameter matrix W i with size m × n.</p><p>For each row (j, k) in R i , multiply the j-th row of the input feature matrix by W i and add it to the kth row of the the output feature matrix. This can be implemented very efficiently on GPUs because it is a matrix-matrix multiply-add operation.</p><p>To implement an SSC convolution, we re-use the input hash table for the output, and construct an appropriate rule book. Note that because the sparsity pattern does not change, the same rule book can be re-used in VGG/ResNet/DenseNet networks until a pooling or subsampling layer is encountered in the architecture.</p><p>If there are a active points in the input layer, the cost of building the input hash-table is O(a). For FCN and U-Net networks, assuming the number of active sites reduces by a multiplicative factor with each downsampling operation, the cost of building all the hash-tables and rule-books is also O(a) regardless of the depth of the network.</p><p>The above implementation differs from <ref type="bibr" target="#b4">[4]</ref> in that the cost of calculating an output site is proportional to the number of active inputs, rather than the size of the receptive field. For SC convolutions this is similar to the voting algorithm <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b3">3]</ref> -the filter weights are never multiplied with inactive input locations -but for SSC convolutions, this implementation is less computationally intensive than voting as there is no interaction between active input locations and inactive neighboring output locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submanifold FCNs and U-Nets for Semantic Segmentation</head><p>Three-dimensional semantic segmentation involves the segmentation of 3D objects or scenes represented as point clouds into their constituent parts; each point in the input cloud must be assigned a part label. As substantial progress has been made in the segmentation of 2D images using convolutional neural networks <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b24">24]</ref>, interest in the problem of 3D semantic segmentation has grown recently. This interest was fueled, in particular, by the release of a new dataset for the part-based segmentation of 3D objects, and an associated competition <ref type="bibr" target="#b23">[23]</ref>. We use a sparse voxelized input representation similar to <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b3">3]</ref>, and a combination of SSC convolutions and strided SC convolutions to construct versions of the popular FCN <ref type="bibr" target="#b14">[14]</ref> and U-Net <ref type="bibr" target="#b1">[2]</ref> architectures. The resulting convolutional network architectures are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>; see the associated caption for details. We refer to these networks as submanifold sparse convolutional networks (SSCNs), because they process low-dimensional data living in a space of higher dimensionality. <ref type="bibr" target="#b6">6</ref> The basic building block for our models are "preactivated" SSC(·, ·, 3) convolutions. Each convolution is preceded by batch normalization and a ReLU nonlinearity. In addition to FCN and U-Nets with standard convolutional layers, we also experiment with variants of these networks that use pre-activated residual blocks <ref type="bibr" target="#b8">[8]</ref> that contain two SSC(·, ·, 3) convolutions. Herein, the residual connections are identity functions: the number of input and output features are equal. Whenever the networks reduce the spatial scale by a factor of two, we use SC(·, ·, 2, 2) convolutions. Our implementation of FCNs upsamples feature maps to their original resolution rather than performing deconvolutions using residual blocks. This substantially reduces the number of parameters and the number of multiplication-addition operations of the FCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we perform experiments with SSCNs on the ShapeNet competition dataset from <ref type="bibr" target="#b23">[23]</ref>. We compare SSCNs against three strong baseline models in terms of performance and computational cost. Specifically, we consider shape contexts <ref type="bibr" target="#b0">[1]</ref>, dense 3D convolutional networks, and 2D multi-view convolutional networks <ref type="bibr" target="#b21">[21]</ref> as baselines. Throughout our experimental evaluation, we focus on the trade-off between segmentation accuracy and computational efficiency measured in FLOPs 7 . In a second set of experiments, we also study SSCN performance on the NYU Depth (v2) dataset <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>View type IoU accuracy</head><p>Aligned 63.5% Random pose 47.8% <ref type="table">Table 3</ref>: Accuracy of segmentation classifiers based on shape-context features on (1) the original ShapeNet dataset and (2) a variant of the dataset in which objects are randomly rotated. The results show that removing the alignment of the ShapeNet objects via random 3D rotations makes the segmentation problem more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>The ShapeNet segmentation dataset <ref type="bibr" target="#b23">[23]</ref> comprises 16 different object categories (plane, chair, hat, etc.), each of which is composed of up to 6 different parts. For instance, a "plane" is segmented into "wings", "engine", "body", and "tail". Across all object categories, the dataset contains a total of 50 different object part classes. Each object is represented as a 3D point cloud that was obtained by sampling points uniformly from the surface of the underlying CAD model. Each point cloud contains between 2, 000 and 3, 000 points. To increase the size of the validation set, we re-split the training and validation sets us- In the original dataset, the objects are axis-aligned: for instance, rockets always point along the z-axis. To make the problem more challenging, we perform a random 3D translation and rotation on each point cloud before classifying it. The results in <ref type="table">Table 3</ref> show that removing the alignment, indeed, makes the segmentation task more challenging.</p><p>To evaluate the accuracy of our models, we adopt the intersection-over-union (IoU) metric of <ref type="bibr" target="#b23">[23]</ref>. The IoU is computed for each part per object category and averaged over parts and examples for the category to produce a "per-category IoU". This way of averaging the IoU scores rewards models that make accurate predictions even for object-parts that are very small: small parts have the same weight in the final accuracy measure as larger parts. The final accuracy measure is obtained by taking a weighted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Details of Experimental Setup</head><p>In all experiments, the same data pre-processing procedure is used. Specifically, each point cloud is centered and re-scaled to fit into a sphere with diameter S; scale S determines the size of the voxelized representation. We use S ∈ {16, 32, 48} in our experiments. At scale S = 48, the voxels are approximately 99% sparse. In experiments with dense convolutional networks, we place the sphere randomly in a grid of size S. For submanifold sparse convolutional networks, we place the sphere randomly in a grid of size 4S. To voxelize the point cloud, we measure the number of points per voxel and normalize them so that non-empty voxels have a mean density of one.</p><p>Networks are trained using the same optimization hyperparameters, unless otherwise noted. We use stochastic gradient descent (SGD) with a momentum of 0.9, Nesterov updates, and L 2 weight decay of 10 −4 . The initial learning rate is set to 0.1, and the learning rate is decayed by a factor of e −0.04 after every epoch. We train all networks for 100 epochs using a batch size of 16. We train a single network on all 16 object categories jointly using a multi-class negative log-likelihood loss function over all 50 part labels.</p><p>For our SSCNs, we experiment with two types of network architectures. The first architecture (C3) operates on a single spatial resolution by stacking SSC(·, ·, 3) convolutions; we use with 8, 16, 32, or 64 filters per layer, and 2, 4, or 6 layers. The second architecture type comprises FCNs and U-Nets with three layers of downsampling. These networks have 8, 16, 32, or 64 filters in the first layer, and double the number of filters each time the data is downsampled. For the convolutional blocks in these networks, we use stacks of 1, 2, or 3 SSC convolutions, or stacks of 1, 2, or 3 residual blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on testing.</head><p>At test time, we only compute softmax probabilities for part labels that actually appear in the object that is being segmented, i.e., we assume the models know the category of the object they are segmenting. Softmax probabilities for irrelevant part classes are set to zero (and the distribution over part labels is re-normalized).</p><p>For each of the three network types (C3, FCN, and U-Net), we train a range of models with varying sizes, as described above, and monitor their accuracy on the validation set. For each network type, we select the networks that correspond to local maxima on the accuracy vs. FLOPs curve, and report test set accuracies for those networks. Akin to multi-crop testing that is commonly used with 2D convolutional networks, we measure test accura-cies using "multi-view" testing. In particular, we generate k different views of the object with k ∈ {1, . . . , K} by randomly rotating them, and average the model predictions for each point over the k different views of the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines</head><p>In addition to SSCNs, we consider three baseline models in our experiments: (1) shape contexts <ref type="bibr" target="#b0">[1]</ref>; (2) dense 3D convolutional networks; and (3) 2D multi-view convolutional networks <ref type="bibr" target="#b21">[21]</ref>. Details of the four baseline models are described separately below.</p><p>Shape contexts. Inspired by <ref type="bibr" target="#b0">[1]</ref>, we define a voxelized shape context vector. Specifically, we define a ShapeContext layer as a special case of the SSC(1, 27, 3, 1) submanifold convolution operator: we set the weight matrix of the operator to be a 27 × 27 identity matrix so that it accumulates the voxel intensities in its 3 3 neighborhood.</p><p>We scale the data using average pooling with sizes 2, 4, 8 and 16 to create four additional views. Combined this gives each voxel a 135-dimensional feature vector. The feature vector is then fed into a non-convolutional multilayer perceptron (MLP) with two hidden layers, followed by a 50-class softmax classifier. The MLPs have 32, 64, 128, 256, or 512 units per layer. At test time, we use multi-view testing with K = 3.</p><p>Dense 3D convolutional networks. For dense 3D convolutional networks, we simply considered dense versions of the SSCN networks. Due to computational constraints, we restricted the FCN and U-Net convolutional blocks to a single C3-layer. We trained some of the models with a reduced learning rate due to numerical instabilities we observed during training. Again, we use K = 3 multi-view testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional networks on multi-view 2D projections.</head><p>This baseline model discards the inherent 3D structure of the data by projecting the point cloud into a twodimensional view by assuming infinite focal length, applying a 2D convolutional network on this projection, and averaging the predictions over multiple views. An immediate advantage of this approach is that well-studied models from 2D vision can be used out-of-the-box without further adaptations. Moreover, the computational cost scales with the surface area, rather than the volume of the point cloud.</p><p>In our implementation of this approach, we first convert the point clouds into a 3D grid of size S 3 as we did for the previous baseline. We then project to a plane of size S 2 , i.e. a face of the cube, with two feature channels. One feature channel is the first visible, non-zero voxel along the corresponding column. The second channel is the distance to the visible voxel, normalized to the range [0, 2]; this is analogous to the depth channel of an RGB-D image. Our network architectures are two-dimensional versions of the dense 3D convolutional networks described above.</p><p>During training, a random projection of the point cloud is passed into the model. Points in the point cloud that fall into the same voxel are given the same prediction. Some voxels are occluded by others-the network receives no information on the occluded voxels. We modify the multiview testing procedure to take into account the occlusion of voxels. Similar to before, predictions are performed using a weighted sum over k random projections. We found that 2D networks require more views to obtain high accuracy and therefore use K = 10. Voxels that are observed in the 2D projection are given a weight of 1. The weight of occluded voxels decays exponentially with the distance to the voxel that occludes them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>In <ref type="figure" target="#fig_6">Figure 5</ref>, we report the average IoU of a range of differently sized variants of (1) the three baseline models and (2) submanifold C3, FCN and U-Nets on the ShapeNet test set. The average IoU is shown as a function of the number of multiplication-addition operations (FLOPs) required by the models for computing the predictions. Note that these results are not directly comparable with those in <ref type="bibr" target="#b23">[23]</ref> because we are testing the models in the more challenging "random-pose" setting.  SSCNs vs. baselines. <ref type="figure" target="#fig_6">Figure 5(a)</ref> compares SSCNs with the three baselines. <ref type="bibr" target="#b8">8</ref> The results show that shape context features, multi-view 2D ConvNets, and dense 3D ConvNets perform roughly on par in terms of accuracy per FLOP. SSCN networks outperform all baseline models by a substantial margin. For instance, at 10 8 FLOPs, the average IoU of SSCNs is 6-8% higher than that of the baselines. Importantly, our results show that restricting information to travel along submanifolds in the data does not hamper the performance of SSCNs, whilst it does lead to considerable computational and memory savings that can be exploited to train larger models with better accuracies.</p><p>Ablation. In <ref type="figure" target="#fig_6">Figure 5</ref>(b), we compare the three SSCN architectures presented in Section 6.2. We observe that SSCNs involving downsampling and upsampling operations (FCNs and U-Nets) outperform SSCNs operating on a single spatial resolution and we conjecture that this is due to the increased receptive field obtained by downsampling. <ref type="figure" target="#fig_6">Figure 5</ref>(c) shows the performance of SSCNs at three different scales S (using all three architectures; C3, FCN, and U-Net). We observe that the performance of SSCNs is similar for different values of S, particularly for a low number of FLOPs. At a higher number of FLOPs, the models operating at a larger scale perform slightly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results on Competition Data</head><p>To compare SSCNs with the entries to the competition in <ref type="bibr" target="#b23">[23]</ref>, we also trained a FCN SSCN on the aligned point clouds. In this experiment, we performed data augmentation using random affine transforms. We set S = 24 and use 64 filters in the input layer, three levels of downsampling, and two residual blocks per spatial resolution. The results of 10-view testing are compared with the competition entries in <ref type="table" target="#tab_1">Table 1</ref>. With a test error of 85.98%, our network outperforms other methods by ≥ 0.49% IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Semantic Segmentation of Scenes</head><p>We also performed experiments on the NYU Depth dataset (v2) <ref type="bibr" target="#b16">[16]</ref> for semantic segmentation of scenes rather than objects. The dataset contains 1, 449 RGB-D images, which are semantically segmented into 894 different classes. <ref type="figure">Figure 6</ref> shows two pairs of an RGB image and the associated depth map from the dataset. Following <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">14]</ref>, we crop the images and reduce the number of classes to 40. To assess the performance of our models, <ref type="figure">Figure 6</ref>: Two examples of RGB-D images from the NYU Depth dataset (v2) <ref type="bibr" target="#b16">[16]</ref>.</p><p>we measure their pixel-wise classification accuracy. We compare our models to a 2D FCN <ref type="bibr" target="#b14">[14]</ref>.</p><p>We perform experiments with two differently sized SSCN-FCN networks. Network A has 16 filters in the input layer, and one SSC(·, ·, 3) convolution per level. Network B has 24 filters in the input layer, and two SSC(·, ·, 3) convolutions per level. Both networks use eight levels of downsampling; we increase the number of filters in the networks when downsampling, adding 16 (A) or 24 (B) features with each reduction of scale.</p><p>We use the depth information to convert the RGB-D images into a 3D point cloud. Each point in the cloud has the three (RGB) features that were normalized to the range [−1, 1], and a fourth indicator features that is set to 1 for each point in the point cloud. During training, we perform data augmentation by applying random affine transformations to the point cloud. Before voxelizing the point cloud, we downscale by a factor of two, and place the points into the model's receptive field. We form voxels by averaging the feature vectors of the points corresponding to the voxel. At test time, we perform multi-view testing with k = 1, 4.</p><p>The results of our experiments on the NYU Depth dataset (v2) are presented in <ref type="table">Table 4</ref>. The results in the table show that SSCNs outperform 2D FCN in terms of pixel accuracy by up to 7%, whilst also substantially reducing the computational costs of the model.  <ref type="table">Table 4</ref>: Semantic segmentation performance of five different convolutional networks on the NYU Depth test set (v2) on 40 classes. We report, the pixel-wise classification accuracy, the computational costs (in FLOPs), and the memory requirements (c.f. <ref type="table" target="#tab_2">Table 2</ref>).</p><p>To verify that SSCN-FCN A actually uses depth information, we repeat the experiment whilst setting all the depth values to zero; this prevents the SSCN from exploiting depth information. We observe: (1) a reduction of FLOPs by 60%, as there are fewer active voxels; and (2) a drop in accuracy from 64.1% to 50.8%, which demonstrates that SSCNs do use 3D structure when performing segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we presented submanifold sparse convolutional networks (SSCNs) for the efficient processing of high-dimensional, sparse input data. We demonstrated the efficacy of SSCNs in a series of experiments on semantic segmentation of three-dimensional point clouds. Specifically, our SSCN networks outperform a range of state-ofthe-art approaches for this problem, both when identifying parts within an object and when recognizing objects in a larger scene. Moreover, SSCNs are computationally efficient compared to alternative approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Example of "submanifold" dilation. Left: Original curve. Middle: Result of applying a regular 3 × 3 convolution with weights 1/9. Right: Result of applying the same convolution again. The example shows that regular convolutions substantially reduce the sparsity of the features with each convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>SSC(·, ·, 3) receptive field centered at different active spatial locations. Active locations in the field are shown in green. Red locations are ignored by SSC so the pattern of active locations remains unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ing the first bit of the MD5 hash of the point cloud files to obtain a training set with 6,955 examples and a validation set with 7,052 examples. The test set contains 2,874 examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustrations of our submanifold sparse FCN (a) and U-Net (b) architectures. Dark blue boxes represents one or more "pre-activated" SSC(·, ·, 3) convolutions, which may have residual connections. Red boxes represent size-2, stride-2 downsampling convolutions; green deconvolutions "invert" these convolutions. Purple upsampling boxes perform "nearest-neighbor" upsampling. The final linear and softmax layers are applied separately on each active input voxel. average of the per-category IoUs, using the fraction of training examples per category as weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Comparison with baseline methods.(b) Comparison between architectures (see 6.2).(c) SSCN with different scales, S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Average interaction-over-union (IoU) on the test set of SSCNs trained for 3D semantic segmentation on the ShapeNet competition data set (higher is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Average intersection-over-union (IoU) of six ap- proaches on the test set of a recent part-based segmenta- tion competition on ShapeNet [23]. Higher is better. Our SSCNs outperform all alternative approaches.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Computational and memory requirements of</cell></row><row><cell>three convolutional operations: regular convolution (C),</cell></row><row><cell>sparse convolution (SC), and submanifold sparse convo-</cell></row></table><note>lution (SSC). We consider convolutions with size f = 3 and padding s = 1 at a single location in d dimensions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1. Iterate once through the input hash-table. We build the output hash table and rule book on-the-fly by iterating over points in the input layers, and all the points in the output layer that can see them. When an output site is visited for the first time, a new entry is created in the output hash table. For each active input point x located at point i in the receptive field of an output</figDesc><table /><note>5 https://github.com/sparsehash/sparsehash point y, add a row (input-hash(x), output-hash(y)) to element R i of the rule book.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the ground state does not necessarily have to be zero, in particular, when convolutions with a bias term are used.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">By "sparse convolutional networks", we mean networks designed to operate on spatially-sparse input data. We do not mean networks that have sparse parameter matrices<ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We note that this is a slight abuse of the term "submanifold". We emphasize that the data on which these networks are applied may contain multiple connected components, and even a mixture of 1D and 2D objects embedded in 3D space.<ref type="bibr" target="#b7">7</ref> We ignore the FLOPs from the final classification layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The number of FLOPs reported for shape contexts may be slightly misleading: the computational costs of calculating shape context features is not reflected in the number of FLOPs, as it involves integer arithmetic.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Shape Matching and Object Recognition using Shape Contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<title level="m">3D U-Net: Learning Dense Volumet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">ric Segmentation from Sparse Annotation. International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vote3Deep: Fast Object Detection in 3D Point Clouds using Efficient Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3D Convolutional Neural Networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.01307.2" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual Organization and Recognition of Indoor Scenes from RGB-D Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Identity Mappings in Deep Residual Networks. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01222</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">G L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00606</idno>
		<title level="m">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal Brain Damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Penksy</surname></persName>
		</author>
		<title level="m">Sparse Convolutional Neural Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">door Segmentation and Support Inference from RGBD Images. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<title level="m">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05009</idno>
		<title level="m">Octnet: Learning Deep 3D Representations at High Resolutions</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<title level="m">Multi-View Convolutional Neural Networks for 3D Shape Recognition. International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Voting for voting in online point cloud object detection. Robotics: Science and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<title level="m">Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Deconvolutional Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
