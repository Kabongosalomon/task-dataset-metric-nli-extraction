<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Neural Networks with Local Error Signals</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arild</forename><surname>Nøkland</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">H</forename><surname>Eidnes</surname></persName>
						</author>
						<title level="a" type="main">Training Neural Networks with Local Error Signals</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is backpropagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss func-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility. Code is available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks for classification are typically trained with a global cross-entropy loss, with the prediction error being back-propagated layer-by-layer from the output layer to hidden layers (D. E. <ref type="bibr" target="#b9">Rumelhart, 1986)</ref>. The hidden layer weights cannot be updated before the forward and backward pass has completed. This backward locking prevents parallelization of the weight updates. It also prevents reuse of the memory used to store hidden layer activations. Several methods have been proposed to avoid the backward locking * Equal contribution 1 Kongsberg Seatex, Trondheim, Norway 2 Trondheim, Norway. Correspondence to: Arild Nøkland &lt;arild.nokland@gmail.com&gt;, Lars H. Eidnes &lt;larsei-dnes@gmail.com&gt;.</p><p>Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s). <ref type="bibr">1</ref> The code for the experiments is available at https:// github.com/anokland/local-loss and memory reuse problems <ref type="bibr">(Jaderberg et al., 2017;</ref><ref type="bibr" target="#b17">Gomez et al., 2017)</ref>.</p><p>Back-propagation of global errors is not biologically plausible for a number of reasons <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref>. Several more realistic alternatives have been proposed <ref type="bibr" target="#b2">(Bengio, 2014;</ref><ref type="bibr" target="#b31">Lee et al., 2015b;</ref><ref type="bibr" target="#b33">Lillicrap et al., 2014;</ref><ref type="bibr">Nøkland, 2016;</ref><ref type="bibr" target="#b50">Scellier &amp; Bengio, 2017)</ref>. These methods do not seem to scale to larger and more complicated problems like CIFAR10 and ImageNet <ref type="bibr" target="#b0">(Bartunov et al., 2018)</ref>.</p><p>In this paper, we demonstrate that the backward-locking problem can be avoided by layer-wise training of the hidden layers with locally generated errors. The local loss functions do not depend on a globally generated error, the gradient is not backpropagated to previous layers, and the hidden layer weights can be updated during the forward pass. At the inference stage, the network behaves as a standard network trained with global back-prop. When the weights for a hidden layer have been updated, the gradient and the activations do not have to be kept in memory any more. This alleviates the memory requirements when training deep networks. Although we train all layers at the same time, locally generated errors also enables greedily training layers one-at-a-time, which can reduce the memory footprint even more, and also reduce the training time.</p><p>Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be propagated back to hidden layers. The global target can be projected back to hidden layers instead.</p><p>Despite the promise that local loss functions can make training faster, more memory efficient, more parallel and more biologically plausible, layer-wise supervised training has been poorly explored in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Local Loss Functions</head><p>Local loss functions have been used to pre-train hidden layers independently of the global loss, and this has in certain cases been shown to improve the performance after finetuning using global back-propagation <ref type="bibr" target="#b21">(Hinton et al., 2006;</ref><ref type="bibr" target="#b3">Bengio et al., 2007;</ref><ref type="bibr" target="#b49">Salakhutdinov &amp; Hinton, 2009;</ref><ref type="bibr" target="#b13">Erhan et al., 2010;</ref><ref type="bibr" target="#b55">Vincent et al., 2008;</ref><ref type="bibr" target="#b42">Paine et al., 2014;</ref><ref type="bibr">Dong arXiv:1901</ref><ref type="bibr">.06656v2 [stat.ML] 7 May 2019</ref>. Local loss functions have been used as an auxillary objective to improve performance <ref type="bibr" target="#b30">(Lee et al., 2015a;</ref><ref type="bibr" target="#b64">Zhang et al., 2016;</ref><ref type="bibr" target="#b53">Szegedy et al., 2015;</ref><ref type="bibr" target="#b57">Wang et al., 2015;</ref><ref type="bibr" target="#b58">Weston et al., 2012)</ref>. Using supervised layer-wise loss functions, without fine-tuning, has also been explored previously <ref type="bibr">(Mostafa et al., 2017;</ref><ref type="bibr" target="#b35">Malach &amp; Shalev-Shwartz, 2018;</ref><ref type="bibr" target="#b36">Marquez et al., 2018)</ref>. The best reported result on CIFAR-10 is 7.2% using local classifiers and ensembling, approaching the results of global backprop <ref type="bibr" target="#b1">(Belilovsky et al., 2018)</ref>. Our contribution is to show that local classifiers combined with a local similarity matching loss can match global backprop in terms of test error.</p><p>Training hidden layers with synthetic gradients is another way to avoid the backward locking problem <ref type="bibr">(Jaderberg et al., 2017)</ref>. This method uses local loss functions to train subnetworks to approximate the true gradient. The synthetic gradient modules are trained with an L2 loss to predict the true gradient from the layer above. The input to the module is the hidden layer activation and in some cases, the target vector. The method relates to layer-wise supervised training because the target information is used to train hidden layers. The method differs from our approach because we don't try to approximate a back-propagated gradient, instead we utilize the target vector to create an error signal independently of the layers above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Similarity Measures in Neuroscience</head><p>Similarity measures have been used in the neuroscience field to characterize neural activity patterns. Representational similarity analysis (RSA) measures similarity of representations under different experimental conditions <ref type="bibr" target="#b26">(Kriegeskorte et al., 2008a)</ref>. By comparing the activity associated with each pair of experimental conditions one can obtain a representational dissimilarity matrix (RDM), much like the similarity matrix we use in this work. For instance, RSA performed on recordings from the inferior temporal (IT) cortex in monkeys show that neural responses to images are clustered according to object categories <ref type="bibr" target="#b24">(Kiani et al., 2007)</ref>. The category clusters surprisingly match between humans and monkeys when exposed to the same real-world object images <ref type="bibr" target="#b27">(Kriegeskorte et al., 2008b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Similarity Measures in Machine Learning</head><p>The similarity matching loss function in this paper can be related to previous work in unsupervised clustering and feature learning. Suppose we have n datapoints as the columns of matrix H = (h 1 , . . . , h n ). A decomposition of this matrix can be expressed as follows:</p><formula xml:id="formula_0">min C,G H − CG 2 F<label>(1)</label></formula><p>If we enforce orthogonality on C, requiring C T C = I, the minimization in (1) implements the subspace version of principal component analysis (PCA). If an L1 penalty is placed on G, the minimization performs sparse coding. Under the constraint that the n columns of G are one-hot cluster selectors, solving this minimization finds a k-means clustering of the data, where the k columns of C are the cluster centroids. Under the constraint that H, C, G ≥ 0, the minimization finds a non-negative matrix factorization (NMF) of the data. Each of these methods have been used for unsupervised feature learning in computer vision <ref type="bibr" target="#b45">Raina et al., 2007;</ref><ref type="bibr" target="#b32">Lee &amp; Seung, 1999)</ref>.</p><p>Given some self-similarity measure S(.), consider the objective: min</p><formula xml:id="formula_1">G S(H) − S(G) 2 F<label>(2)</label></formula><p>If S(.) measures the euclidian distance between data points, this minimization implements multidimensional scaling (MDS). If we define S(X) = X T X and constrain G to be ≥ 0, the minimization in <ref type="formula" target="#formula_1">(2)</ref> finds what is called the symmetric NMF <ref type="bibr" target="#b29">(Kuang et al., 2012)</ref>. If instead of non-negativity we enforce orthogonality, requiring GG T = I, the minimization in (2) implements <ref type="bibr" target="#b11">(Ding et al., 2005;</ref><ref type="bibr" target="#b29">Kuang et al., 2012)</ref> the family of methods called spectral clustering <ref type="bibr" target="#b41">(Ng et al., 2002)</ref>. By choosing different similarity measures S(.) in the first term of (2), this minimization can perform different graph clustering objectives, namely ratio association, kernel clustering and normalized cuts <ref type="bibr" target="#b29">(Kuang et al., 2012)</ref>.</p><p>The above are unsupervised clustering and feature learning methods. For our purposes in this paper, we use what can be seen as a supervised clustering loss, where two data points belong to the same cluster if they have the same label. Given a label matrix Y = (y 1 , . . . , y n ), whose columns are the one-hot encoded labels of the data, we minimize:</p><formula xml:id="formula_2">min θ S(N euralN et(H; θ)) − S(Y ) 2 F<label>(3)</label></formula><p>Here the matrix Y is fixed, and instead the parameters θ of N euralN et are adjusted to minimize the loss. A straightforward interpretation of this loss is that it encourages the neural network to learn representations of the data such that distinct classes have distinct representations.</p><p>This supervised clustering loss is related to methods like linear disciminative analysis (LDA) <ref type="bibr" target="#b14">(Fisher, 1936)</ref>, and neighbourhood component analysis (NCA) <ref type="bibr" target="#b16">(Goldberger et al., 2005)</ref> because they both utilize label information to perform clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We use standard convolutional and fully connected network architectures, but instead of globally back-propagating errors, each weight layer is trained by a local learning signal, that is not back-propagated down the network. The learning signal is provided by two separate single-layer sub-networks, each with their own distinct loss function. One sub-network is trained with a standard cross-entropy loss, and the other with a similarity matching loss (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Similarity Matching Loss</head><p>The similarity matching loss measures the L2 distance between matrices, where the elements contain the pair-wise similarities between examples in a mini-batch. We denote this loss as L sim or sim loss. Given a mini-batch of hidden layer activations H = (h 1 , . . . , h n ), and a one-hot encoded label matrix Y = (y 1 , . . . , y n ), we have:</p><formula xml:id="formula_3">L sim = S(N euralN et(H)) − S(Y ) 2 F<label>(4)</label></formula><p>When H is the output of a linear layer, N euralN et(.) is a linear layer. When H is the output of a convolutional layer, N euralN et(.) is a convolutional layer with kernel size 3x3, stride 1 and padding 1, followed by a standard deviation operation over each feature map. This operation will reduce the dimension of the output to 2. S(X) is the adjusted cosine similarity matrix, or correlation matrix, of a mini-batch X. S(X) contains elements s ij where</p><formula xml:id="formula_4">s ij = s ji = x T i x j x i 2 x j 2<label>(5)</label></formula><p>Subscripts i and j denote indices in the mini-batch. x i denotes the mean-centered vector x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prediction Loss</head><p>The prediction loss measures the cross-entropy between a prediction from a local classifier and the target. We denote this loss as L pred or pred loss.</p><p>The pred loss for a matrix H of hidden layer-activations:</p><formula xml:id="formula_5">L pred = CrossEntropy(Y, W T H)<label>(6)</label></formula><p>where W is a weight matrix with width equal to the number of classes and height equal to hidden dimension, and Y is matrix of one-hot encoded targets. If H is the output of a convolutional layer, average-pooling is performed first to reduce the size, then the feature maps are flattened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backprop Free Version</head><p>A more biologically plausible version of the similarity matching loss is to replace N euralN et(.) in (4) with a standard deviation operation over each feature map, and apply the similarity matching objective directly on these features. Then no back-propagation is required to calculate the gradient for the hidden layer. To eliminate the requirement for the global target to be available at each hidden layer, the one-hot encoded target vector is replaced with a random transformation of the same target vector. We denote this loss as L sim−bio or sim-bpf loss.</p><p>The L pred loss can be made more biologically plausible by using feedback alignment <ref type="bibr" target="#b33">(Lillicrap et al., 2014)</ref> to transport the prediction error back to the hidden layer. To eliminate the requirement for the global target to be available at each hidden layer, the classifier is trained to predict a binarized random transformation of the target vector using a binary cross-entropy loss. We denote this loss as L pred−bpf or pred-bpf loss.</p><p>The experiment section includes one experiment on CIFAR-10 using these two versions, and their combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Combined Loss</head><p>We denote the weighted combination of the above loss functions as L predsim or simply as predsim loss.</p><formula xml:id="formula_6">L predsim = (1 − β)L pred + βL sim<label>(7)</label></formula><p>And equally for the more biologically plausible loss functions. We denote this loss as L predsim−bpf or simply as predsim-bpf loss.</p><formula xml:id="formula_7">L predsim−bpf = (1 − β)L pred−bpf + βL sim−bpf (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We performed experiments on MNIST, Fashion-MNIST, Kuzushiji-MNIST, CIFAR-10, CIFAR-100, STL-10 and SVHN to evaluate the performance of the training method. We used fully-connected architetures and two VGG-like architectures that we found to perform well <ref type="bibr" target="#b51">(Simonyan &amp; Zisserman, 2014)</ref>. For each model we compared the performance when trained with the glob loss (i.e. standard global back-prop), the pred loss, the sim loss and the predsim loss. We also trained the best performing model with cutout regularization <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>, keeping all hyper-parameters except the dropout rate identical, to see if it could improve the result further.</p><p>The architectures and hyper-parameters were chosen to give good performance for the predsim loss. The hyperparameters were kept identical for all loss variations for a given dataset and architecture combination. Experiments were kept as simple as possible, and only dropout rate, learning rate, length of training, hidden layer dimension and average-pooling kernel size (used in the pred loss) varied across experiments.</p><p>We used two different simple VGG-like convolutional networks. Both consist of 2x2 max-pooling layers, 3x3 convolutional layers with stride 1 and padding 1, and fully connected layers. The first architecture is denoted VGG8B. The layers are conv128-conv256-pool-conv256-conv512-pool-conv512-pool-conv512-pool-fc1024-fc. The dimension of the output layer depends on the number of classes.</p><p>The second network is deeper and is denoted VGG11B.</p><formula xml:id="formula_8">The layers are conv128-conv128-conv128-conv256-pool- conv256-conv512-pool-conv512-conv512-pool-conv512- pool-fc1024-fc.</formula><p>The dimension of the output layer depends on the number of classes.</p><p>The experiments were executed using the PyTorch framework. For local loss functions, the computational graph was detached after each hidden layer to prevent backward gradient flow from the output loss. The output layer was trained normally with a cross-entropy loss function.</p><p>A batch size of 128 was used in all experiments. ADAM was used for optimization <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>. The weighting factor β was manually tuned and set to 0.99 for all experiments with the predsim loss.</p><p>For networks trained with global or pred loss we used the ReLU non-linearity. For networks trained with sim or predsim loss we used leaky-ReLU with a negative slope of 0.01 <ref type="bibr" target="#b34">(Maas et al., 2013)</ref> because it delivered more stable training. Before each non-linearity we applied batch normalization <ref type="bibr" target="#b22">(Ioffe &amp; Szegedy, 2015)</ref>. After each non-linearity we applied dropout <ref type="bibr" target="#b52">(Srivastava et al., 2014)</ref>, with equal dropout rate for all layers.</p><p>The training time was 100 epochs for MNIST and Kuzushiji-MNIST, 200 epochs for Fashion-MNIST, and SVHN, and 400 epochs for the other datasets. The learning rate was multiplied by a factor of 0.25 at 50%, 75% and 89% and 94% of the total training time. Because of the high number of experiments, we performed only single-time runs. We report the test error for the last training epoch.</p><p>In some of the experiments, the number of convolutional filters were multiplied by a factor of 2 or 3. This is denoted in the tables as (2x) or (3x) trailing the network name.</p><p>Despite the large number pf parameters, we were able to train the networks on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MNIST</head><p>MNIST consists of hand-written digits and is the most commonly used dataset within the deep learning community. The dataset is trivial to learn and good performance here does not say much about the performance on harder tasks. We have included these experiments here for completeness.</p><p>The initial learning rate was 5e-4. The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 1024. The dropout rate was 0.1 for MLP and 0.2 for VGG8B. For the cutout experiment, the cutout hole size was 14.</p><p>We used 2 pixel jittering for data augmentation as done in the CapsNet paper <ref type="bibr" target="#b47">(Sabour et al., 2017)</ref>. We provide the CapsNet result here as a baseline for convolutional networks, even though better results have been achieved with ensembling and extensive data augmentation <ref type="bibr" target="#b56">(Wan et al., 2013)</ref>. We provide the performance of the Ladder network as baseline for fully-connected networks <ref type="bibr" target="#b46">(Rasmus et al., 2015)</ref>.</p><p>It is clear that jittering is helping substantially for all fullyconnected networks, but the best result is with the predsim loss. VGG8B with predsim loss and cutout is on par with CapsNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fashion-MNIST</head><p>Fashion-MNIST is a rather new dataset with different classes of clothing and is a drop-in replacement for MNIST <ref type="bibr" target="#b60">(Xiao et al., 2017)</ref>. It is harder, but has the same size, input dimension and number of classes as MNIST.</p><p>The initial learning rate was 5e-4 for MLP and VGG8B, and 3e-4 for VGG8B(2x). The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 1024 for VGG8B and 2048 for VGG8B(2x). The dropout rate was 0.025 for MLP, 0.1 for VGG8B and 0.2 for VGG8b(2x). For the cutout experiment, the cutout hole size was 14.</p><p>As a baseline we show the test error for a WideResNet-28-10 <ref type="bibr" target="#b63">(Zagoruyko &amp; Komodakis, 2016)</ref> with and without random erasing data augmentation <ref type="bibr" target="#b65">(Zhong et al., 2017)</ref>. 2 This is to our knowledge the best published results on this dataset. Note that the baseline network has about 5 times more parameters than VGG8B. To make the comparison more fair, we also trained a version of VGG8B where the number of convolutional filters were doubled. This version performs better than the baseline, even though the number of parameters was smaller. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Kuzushiji-MNIST</head><p>Kuzushiji-MNIST is another drop-in replacement for MNIST containing hand-drawn japanese characters <ref type="bibr" target="#b5">(Clanuwat et al., 2018)</ref>.</p><p>The initial learning rate was 5e-4. The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 1024. The dropout rate was 0.2 for MLP and 0.3 for VGG8B. For the cutout experiment, the dropout rate was 0.15 and the cutout hole size was 14.</p><p>As a baseline we have included the first published results on this dataset, a PreActResNet-18 <ref type="bibr" target="#b20">(He et al., 2016)</ref> with and without manifold mixup regularization <ref type="bibr" target="#b5">(Clanuwat et al., 2018)</ref>. VGG8B with predsim loss and cutout achieved a test error that surpasses the baseline, even though the number of parameters was smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">CIFAR-10</head><p>CIFAR-10 consist of 50000 training images of dimension 32x32 pixels <ref type="bibr" target="#b28">(Krizhevsky, 2009)</ref>. The dataset has 10 classes.</p><p>The initial learning rate was 5e-4 for MLP, VGG8B and VGG11B, and 3e-4 for VGG11B(2x) and VGG11B(3x). The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 2048 for VGG8B and VGG11B, and 4096 for VGG11(2x) and VGG11B(3x). The dropout rate was 0.1 for MLP, 0.2 for VGG8B and VGG11B, 0.25 for VGG11B(2x) and 0.3 for VGG11B(3x). For the cutout experiment, the cutout hole size was 16.</p><p>As a baseline we have included test error for WideResNet-40-10 with and without cutout <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>. Note that better results have been reported using regularization and data augmentation techniques <ref type="bibr" target="#b54">(Verma et al., 2018;</ref><ref type="bibr" target="#b8">Cubuk et al., 2018)</ref>. The predsim loss worked better than the other loss functions for the tested architectures. By multiplying the number of convolutional filters by 3, the VGG11B model trained with predsim loss approaches the test error of WideResNet. We also tested the backprop free training methods on this dataset, with a random target projection of size 128. The weighting factor β was set to 0.01 for the predsim-bpf loss.</p><p>The initial learning rate was 5e-4 for VGG8B, and 3e-4 for VGG8B(2x). The average-pooling kernel size for the pred and sim loss was chosen so that the flattened output dimension was 4096. The dropout rate was 0.05 for VGG8B and 0.1 for VGG8B(2x).</p><p>The best published results on this task with no backpropagation is to our knowledge 16.9% using dense feedback alignment <ref type="bibr" target="#b37">(Moskovitz et al., 2018)</ref>. The second best result is 18.0% using K-means and SVM . If we consider sign-concordant feedback as backprop free training, the best result is 12.6% <ref type="bibr" target="#b37">(Moskovitz et al., 2018)</ref>. Our result with the predsim-bpf loss surpasses these results by a large margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">CIFAR-100</head><p>CIFAR-100 consist of 50000 training images of dimension 32x32 pixels <ref type="bibr" target="#b28">(Krizhevsky, 2009)</ref>. The dataset has 100 classes.</p><p>The initial learning rate was 5e-4 for MLP, VGG8B and VGG11B, and 3e-4 for VGG11B(2x) and VGG11B(3x). The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 4096. The dropout rate was 0.025 for MLP, 0.05 for VGG8B and VGG11B, 0.1 for VGG11B(2x) and 0.15 for VGG11B(3x). We were not able to improve the test error with cutout on this dataset, so this result is excluded from the table.</p><p>Because of the high number of classes, we limited the number of classes in each mini-batch to 20 until first drop in learning rate. This was to make the target similarity matrix S(.) less sparse, and we found that this improved the final result.</p><p>As a baseline we have included test error for WideResNet-40-10 with and without cutout <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>. Note that better results have been reported using regularization and data augmentation techniques <ref type="bibr" target="#b62">(Yamada et al., 2018;</ref><ref type="bibr" target="#b8">Cubuk et al., 2018)</ref>. The predsim loss worked better than the other loss functions for the tested architectures. By multiplying the number of convolutional filters by 3, the VGG11B model trained with predsim loss approaches the test error of WideResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">SVHN</head><p>SVHN is a dataset with house number images of dimension 32x32 pixels <ref type="bibr" target="#b40">(Netzer et al., 2011)</ref>. The training set has 73257 images and the extra training set has 531131 images. We used both training sets in our experiments. No data The initial learning rate was 3e-4. The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 2048. The dropout rate was 0.3. For the cutout experiment, the dropout rate was 0.15 and the cutout hole size was 16.</p><p>As a baseline, we show the test error for WideResNet-16-8 with and wihout cutout <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>. Note that better results are reported with extensive data augmentation <ref type="bibr" target="#b8">(Cubuk et al., 2018)</ref>. The predsim loss clearly worked better than the other loss functions for the tested architecture, but lags behind the test error for WideResNet. STL-10 is a dataset of images belonging to 10 classes . The image dimension is 96x96 pixels. The training dataset consists of 5000 labeled images and a lot of unlabeled images. We used only labeled images, and we did not use the prescribed testing protocol, we just trained one model on all training examples. No data augmentation was used, making this a difficult task because of the small amount of training data.</p><p>The network architecture here was identical to the earlier description, except that the first convolutional layer was replaced with a 7x7 kernel layer with stride 2. This was to reduce the feature map size early in the network.</p><p>The initial learning rate was 5e-4. The average-pooling kernel size for the pred loss was chosen so that the input dimension to the local classifier was 2048. The dropout rate was 0.1. For the cutout experiment, the cutout hole size was 48.</p><p>As a baseline we show the results for WideResNet-16-8 with and without cutout regularization <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>. The authors use the same training and testing protocol as used here. Our result with predsim loss is better than the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results with Local Loss Functions</head><p>Our first observation is that the local prediction loss (pred) achieved test errors close to those of global backpropagation. This is in line with previous work <ref type="bibr">(Mostafa et al., 2017;</ref><ref type="bibr" target="#b1">Belilovsky et al., 2018)</ref>, but still interesting because hidden layers are decoupled from the layers above during training.</p><p>A surprising observation is that the local similarity matching loss (sim) is able to provide a remarkable good training signal for hidden layers. In some cases, the test error is lower than for a back-propagated global cross-entropy loss. The loss encourages examples from distinct classes to have distinct representations, measured by the cosine similarity. This can be seen as a kind of supervised clustering. This objective is sufficient to create a hidden representation that is suitable for classification, independently of the layers above.</p><p>The results indicate that training with a local cross-entropy (pred) or similarity matching (sim) loss alone does not match a global loss in terms of test error. However, if both loss functions are combined (predsim), the results improve significantly. The performance of the training method varies across architectures and datasets. Overall, we observe no loss in accuracy when comparing to global back-propagation in VGG-like architectures. This conclusion is based on the assumption that the global loss results for VGG architectures are representative. If we compare them with results for residual-free architectures reported in the literature and in open-source implementations, they seem to be equally good or better.</p><p>For supervised layer-wise training on CIFAR-10, we improve the state-of-the-art from 7.2% <ref type="bibr" target="#b1">(Belilovsky et al., 2018)</ref> to 3.6% test error. Layer-wise training of VGG-like models is competitive with residual architectures trained with global back-propagation on several datasets. For STL-10 with no data augmentation, our result is the best reported.</p><p>We use dropout <ref type="bibr" target="#b52">(Srivastava et al., 2014)</ref>, and batchnormalization <ref type="bibr" target="#b22">(Ioffe &amp; Szegedy, 2015)</ref>, for regularization. Without these, the results are much worse. We have demonstrated that more advanced regularization methods like cutout, <ref type="bibr" target="#b10">(Devries &amp; Taylor, 2017)</ref>, can improve the results further.</p><p>We found that VGG-like architectures work best with the proposed training method. For residual architectures like ResNet and WideResNet, we got better results when the residual connections were removed. We also tried to replace max-pooling layers with 2-strided convolutional layers, but this did not work equally well.</p><p>Avoiding a global loss function has several benefits for practical neural network training. The backward-locking problem is no longer a problem, and weights can be updated during the forward pass. This alleviates the memory requirements since activations do not have to be kept in memory for the backward pass. We trained all layers simultaneously, but using local loss functions can also enable greedy training of hidden layers one-by-one. It also allows for model and data parallelism, where different parts of the model can be trained on different GPU's, with each GPU processing different batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Decoupling Optimization from Generalization</head><p>We have compared test errors of various losses on a wide range of datasets. These results do not on their own help us disentangle effects on optimization from effects on generalization. Looking at the training error can shed light on this.</p><p>In general, full backprop achieved a faster drop in training error, and as low or lower final training error, compared to the local losses in our experiments. This is not too surprising, considering it has access to the true gradient of the global loss at each layer.</p><p>On STL-10, the predsim loss achieved the best reported test error without using unlabeled data. This dataset is characterized by relatively large images (96x96), and few training examples (5000). Large models are prone to overfit on this data. Both backprop and each of the local losses were able to reach a training error &lt; 0.2%. At the same time, each of the local losses found solutions with lower test error than that of backprop. This immediately suggests that local learning may provide an inductive bias towards solutions that generalize well. Observing that the combined local loss generally achieves lower test error than each loss on their own, one could argue that the sim loss may be adding a regularizing effect. Looking at the training curves ( <ref type="figure" target="#fig_1">Figure 2)</ref> shows that this is not the full story. Here, the sim loss achieved a significantly lower training error than pred, and their combination in predsim achieved lower training error than either method on its own. In every experiment where the local losses could be compared, it held true that the predsim loss achieved a lower training error than either local loss on its own. This shows that both losses help optimization in the context of local learning in deep networks.</p><p>To summarize, our evidence points to full backprop generally achieving a faster drop in training error, and lower final training error, but that local learning in many experiments appeares to add an inductive bias that reduces overfitting. In the context of local learning, the sim and pred losses both help with optimization in a complementary way.</p><p>We have shown that the predsim loss can achieve strong test error results on many datasets. If our results come from better generalization, one could suspect local learning to perform worse on large datasets like ImageNet, where models are less prone to overfit. Due to time and compute constraints, we do not have ImageNet results in this paper. However, recent results from <ref type="bibr" target="#b1">(Belilovsky et al., 2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Biological Plausibility</head><p>We have proposed a combination of local loss functions that offers an alternative to end-to-end training with global backpropagation. Neither of the two losses provide backprop free training. However, the error does not have to be transported back through the whole network, a single step of backpropagation is sufficient.</p><p>If we remove the back-propagation requirement and add direct projections from the global target to hidden layers (predsim-bpf), the performance deteriorated, but 7.8% error on CIFAR-10 is still the best reported result for backprop free methods. This method is biologically plausible in many ways, but some issues still exist. We use unrealistic weightsharing in convolutional layers, and we allow the weights to switch sign. We also use batch-normalization, which doesn't have a biologically realistic counterpart. The method is an offline algorithm since we use mini-batch training. In addition we have ignored that real neurons communicate with spikes.</p><p>Local loss functions could be a step towards a solution to the credit assignment problem <ref type="bibr" target="#b4">(Bengio et al., 2015)</ref>. Using local classifiers to generate training signals has been investigated previously. Our contribution is to show that such classifiers can be trained with feedback alignment, and that the target can be replaced with a random projection of the target. We also show increased accuracy when the prediction loss is combined with a similarity matching loss.</p><p>With the backprop free pred-bpf loss, the weight transport problem, <ref type="bibr" target="#b18">(Grossberg, 1987)</ref>, is avoided because feedback alignment does not require symmetric weights. An online version of feedback alignment learning can be implemented in a biologically plausible way using multi-compartment neurons <ref type="bibr" target="#b19">(Guerguiev et al., 2017)</ref>, and in cortical microcircuits for continuous learning without separate forward and backward passes <ref type="bibr" target="#b48">(Sacramento et al., 2018</ref>). An online version of the sim-bpf loss can be implemented using local Hebbian and anti-Hebbian learning rules, at least for unsupervised learning <ref type="bibr" target="#b43">(Pehlevan &amp; Chklovskii, 2014;</ref><ref type="bibr" target="#b44">Pehlevan et al., 2018;</ref><ref type="bibr" target="#b15">Giovannucci et al., 2018)</ref>.</p><p>Biologically plausible algorithms for error-driven learning have so far focused on how to transport the error back to hidden layers <ref type="bibr" target="#b33">(Lillicrap et al., 2014;</ref><ref type="bibr">Nøkland, 2016)</ref>, or how to transport a target back to hidden layers <ref type="bibr" target="#b61">(Xie &amp; Seung, 2003;</ref><ref type="bibr" target="#b2">Bengio, 2014;</ref><ref type="bibr" target="#b31">Lee et al., 2015b;</ref><ref type="bibr" target="#b50">Scellier &amp; Bengio, 2017;</ref><ref type="bibr" target="#b59">Whittington &amp; Bogacz, 2017)</ref>. In the context of experiments performed in this paper, global error transportation is not a requirement for error-driven learning. Neither is it a requirement to propagate the global target backwards through the network. The hidden layers can be trained independently of the layers above, without loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A. Training and Test Errors <ref type="figure">Figure 3</ref>. Training and test classification errors on all datasets with different loss functions. Note that the CIFAR100 runs are less comparable to each other, because the sim and predsim runs had batches sampled to have only 20 classes per batch during training, which we found to cause a higher training error, but lower test error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Matching as a Complementary Objective</head><p>Although somewhat out of scope of this work, we performed some experiments with the sim loss as a local complementary loss. We trained the networks with global back-propagation combined with a local sim loss. In this way, hidden layers were trained based on a global cross-entropy loss, and back-propagated similarity matching losses from the layers above.</p><p>Hyper-parameters and training details were identical to the experiments with the local sim loss, except that we did not detach the computational graph.</p><p>The results are summarized in <ref type="table" target="#tab_9">Table 9</ref>. We can see that similarity matching is a powerful auxillary objective for classification, also in a global loss context. For all datasets we can see an improvement in test error compared to global back-propagation alone. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>With the local loss functions, each weight layer is trained by two single-layer sub-networks, each with their own distinct loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Training and test classification errors on CIFAR10 with full backprop and different local loss functions, with a VGG11B(1x) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MNIST with 2 pixel jittering. Test error in percent.</figDesc><table><row><cell>Local loss functions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Fashion-MNIST with 2 pixel jittering and horizontal flipping. Test error in percent.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Local loss functions</cell></row><row><cell>Model</cell><cell>#par</cell><cell cols="3">glob pred sim</cell><cell>predsim</cell></row><row><cell cols="6">3x1024 MLP 2.9M 8.37 8.60 9.70 8.54</cell></row><row><cell>VGG8B</cell><cell cols="5">7.3M 4.53 5.66 5.12 4.65</cell></row><row><cell>VGG8B(2x)</cell><cell cols="5">28M 4.55 5.11 4.92 4.33</cell></row><row><cell>8B(2x)+CO</cell><cell cols="2">28M -</cell><cell>-</cell><cell>-</cell><cell>4.14</cell></row><row><cell>WRN</cell><cell cols="3">37M 4.63 -</cell><cell>-</cell><cell>-</cell></row><row><cell>WRN+RE</cell><cell cols="3">37M 4.16 -</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Kuzushiji-MNIST with no data augmentation. Test error in percent.</figDesc><table><row><cell>Local loss functions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>CIFAR10 with standard data augmentation. Test error in percent.</figDesc><table><row><cell>Local loss functions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>CIFAR10 with standard data augmentation. No backpropagation. Test error in percent.</figDesc><table><row><cell>Model</cell><cell>#par</cell><cell cols="3">pred-bpf sim-bpf predsim-bpf</cell></row><row><cell>VGG8B</cell><cell cols="2">8.9M 9.80</cell><cell>13.39</cell><cell>9.02</cell></row><row><cell cols="3">VGG8B(2x) 31M -</cell><cell>-</cell><cell>7.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>CIFAR100 with standard data augmentation. Test error in percent.</figDesc><table><row><cell>Local loss functions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>SVHN with extra training data, but no data augmentation. Test error in percent.</figDesc><table><row><cell>Local loss functions</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>STL-10 with no data augmentation. Test error in percent.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Local loss functions</cell></row><row><cell>Model</cell><cell>#par glob</cell><cell>pred</cell><cell>sim</cell><cell>predsim</cell></row><row><cell>VGG8B</cell><cell cols="4">12M 33.08 26.83 23.15 20.51</cell></row><row><cell cols="2">VGG8B+CO 12M -</cell><cell>-</cell><cell>-</cell><cell>19.25</cell></row><row><cell>WRN</cell><cell cols="2">11M 23.48 -</cell><cell>-</cell><cell>-</cell></row><row><cell>WRN+CO</cell><cell cols="2">11M 20.77 -</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>2% on ImageNet with a loss similar to what we call pred loss. This leaves open the possibility that adding a sim loss could improve this result further.</figDesc><table><row><cell>estab-</cell></row><row><cell>lished that local layer-wise training can scale to ImageNet,</cell></row><row><cell>by achieving a surprisingly low top-5 single-crop test error</cell></row><row><cell>of 10.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Similarity matching as a complementary objective. Test error in percent.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>#par</cell><cell cols="2">glob predsim glob+sim</cell></row><row><cell>MNIST</cell><cell>VGG8B</cell><cell cols="2">7.3M 0.26 0.31</cell><cell>0.24</cell></row><row><cell>Fashion-MNIST</cell><cell>VGG8B</cell><cell cols="2">7.3M 4.53 4.65</cell><cell>4.16</cell></row><row><cell cols="2">Kuzushiji-MNIST VGG8B</cell><cell cols="2">7.3M 1.53 1.36</cell><cell>1.13</cell></row><row><cell>CIFAR-10</cell><cell cols="3">VGG11B 12M 5.56 5.30</cell><cell>4.33</cell></row><row><cell>CIFAR-100</cell><cell cols="3">VGG11B 12M 25.2 24.1</cell><cell>22.2</cell></row><row><cell>SVHN</cell><cell>VGG8B</cell><cell cols="2">8.9M 2.29 1.74</cell><cell>1.95</cell></row><row><cell>STL-10</cell><cell>VGG8B</cell><cell cols="2">12M 33.1 20.5</cell><cell>25.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Due to an issue in the early version of the dataset, the baseline numbers reported in<ref type="bibr" target="#b65">(Zhong et al., 2017)</ref> are too low, see https: //github.com/zhunzhong07/Random-Erasing. We rerun the released code with the current dataset version and report the test error from the last epoch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The test error was 5.60% in epoch 399.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Assessing the scalability of biologically-motivated deep learning algorithms and architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1807.04587</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1807.html#abs-1807-04587" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Greedy layerwise learning can scale to imagenet. CoRR, abs/1812.11446</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eickenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1812.11446" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How auto-encoders could provide credit assignment in deep networks via target propagation. CoRR, abs/1407</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1407.7906" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7906</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards biologically plausible deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<idno>abs/1502.04156</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep learning for classical japanese literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kitamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1812.01718</idno>
		<ptr target="http://arxiv.org/abs/1812.01718" />
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinberger</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4293-selecting-receptive-fields-in-deep-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>K. Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v15/coates11a/coates11a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<ptr target="http://arxiv.org/abs/1805.09501" />
		<title level="m">Learning augmentation policies from</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the equivalence of nonnegative matrix factorization and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 SIAM International Conference on Data Mining</title>
		<meeting>the 2005 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deep representations using convolutional autoencoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8462085</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2018.8462085" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Calgary, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-15" />
			<biblScope unit="page" from="3006" to="3010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://dl.acm.org/citation.cfm?id=1756025" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient principal subspace projection of streaming data through fast similarity matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giovannucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Minden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pehlevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
		<idno>abs/1808.02083</idno>
		<ptr target="http://arxiv.org/abs/1808.02083" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2566-neighbourhood-components-analysis.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Saul, L. K., Weiss, Y., and Bottou, L.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6816-the-reversible-residual-network-backpropagat" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="2211" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Competitive learning: From interactive activation to adaptive resonance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6708.1987.tb00862.x</idno>
		<ptr target="https://doi.org/10.1111/j.1551-6708.1987.tb00862.x" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="63" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards deep learning with segregated dendrites. eLife, 6:e22901</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerguiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.22901</idno>
		<idno>2050-084X. doi: 10.7554/ eLife.22901</idno>
		<ptr target="https://doi.org/10.7554/eLife.22901" />
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<idno>doi: 10.1007/ 978-3-319-46493-0\_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_38" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>Part IV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
		<ptr target="http://dx.doi.org/10.1162/neco.2006.18.7.1527" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled neural interfaces using synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/icml/icml2017.html#JaderbergCOVGSK17" />
	</analytic>
	<monogr>
		<title level="j">JMLR Workshop and Conference Proceedings</title>
		<editor>Precup, D. and Teh, Y. W.</editor>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1627" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object category structure in response patterns of neuronal population in monkey inferior temporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esteky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mirpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="page" from="4296" to="4309" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Representational similarity analysis -connecting the branches of systems neuroscience. Frontiers in systems neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bandettini</surname></persName>
		</author>
		<idno type="DOI">10.3389/neuro.06.004</idno>
		<idno type="PMCID">PMC2605405</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matching categorical object representations in inferior temporal cortex of man and monkey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bodurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esteky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bandettini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2008.10.043.URLhttp:/www.sciencedirect.com/science/article/pii/S0896627308009434</idno>
		<idno>0896- 6273. doi</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2008.10.043.URLhttp://www.sciencedirect.com/science/article/pii/S0896627308009434" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1126" to="1141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Symmetric nonnegative matrix factorization for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM international conference on data mining</title>
		<meeting>the 2012 SIAM international conference on data mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/aistats/aistats2015.html#LeeXGZT15" />
	</analytic>
	<monogr>
		<title level="m">of JMLR Proceedings. JMLR.org</title>
		<editor>Lebanon, G. and Vishwanathan, S. V. N.</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Machine Learning and Knowledge Discovery in Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD (1)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="498" to="515" />
		</imprint>
	</monogr>
	<note>Difference target propagation</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page">788</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Random feedback weights support learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cownden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Tweed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Akerman</surname></persName>
		</author>
		<idno>abs/1411.0247</idno>
		<ptr target="http://arxiv.org/abs/1411.0247" />
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A provably correct algorithm for deep learning that actually works. CoRR, abs/1803.09522</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1803.09522" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep cascade learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Marquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2805098</idno>
		<idno>doi: 10.1109/ TNNLS.2018.2805098</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2018.2805098" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5475" to="5485" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feedback alignment in deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Moskovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Litwin-Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep supervised learning using local errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1711.06756</idno>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1711.html#abs-1711-06756" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nøkland, A. Direct feedback alignment provides learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/nips/nips2016.html#Nokland16" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R.</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An analysis of unsupervised pre-training in light of recent advances. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6597" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hebbian/anti-hebbian network derived from online non-negative matrix factorization can cluster and discover sparse features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pehlevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signals</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="769" to="775" />
		</imprint>
	</monogr>
	<note>48th Asilomar Conference on</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Why do similarity matching objectives lead to hebbian/anti-hebbian networks? Neural Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pehlevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/neco/neco30.html#PehlevanSC18" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selftaught learning: Transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1273496.1273592</idno>
		<ptr target="http://doi.acm.org/10.1145/1273496.1273592" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1390156.1390294</idno>
		<ptr target="http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Dendritic cortical microcircuits approximate the backpropagation algorithm. CoRR, abs/1810.11393</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/corr/corr1810.html#abs-1810-11393" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v5/salakhutdinov09a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelfth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Equilibrium propagation: Bridging the gap between energy-based models and backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scellier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2017.00024</idno>
		<ptr target="https://doi.org/10.3389/fncom.2017.00024" />
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Neurosci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2670313" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>978-1-4673-6964-0</idno>
		<ptr target="http://dblp.uni-trier.de/db/conf/cvpr/cvpr2015.html#SzegedyLJSRAEVR15" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manifold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236v3</idno>
		<title level="m">Encouraging meaningful on-manifold interpolation as a regularizer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1390156.1390294</idno>
		<ptr target="http://doi.acm.org/10.1145/1390156.1390294" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v28/wan13.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Training deeper convolutional networks with deep supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno>abs/1505.02496</idno>
		<ptr target="http://arxiv.org/abs/1505.02496" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_34</idno>
		<idno>doi: 10.1007/ 978-3-642-35289-8\_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-35289-8_34" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade -Second Edition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C R</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bogacz</surname></persName>
		</author>
		<idno type="DOI">10.1162/NECO_a_00949</idno>
		<idno>doi: 10. 1162/NECO\_a\_00949</idno>
		<ptr target="https://doi.org/10.1162/NECO_a_00949" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<ptr target="https://github.com/zalandoresearch/fashion-mnist" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Equivalence of backpropagation and contrastive hebbian learning in a layered network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shakedrop Regularization</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1802.02375</idno>
		<ptr target="http://arxiv.org/abs/1802.02375" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<ptr target="http://www.bmva.org/bmvc/2016/papers/paper087/index.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Augmenting supervised neural networks with unsupervised objectives for largescale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/icml/icml2016.html#ZhangLL16" />
	</analytic>
	<monogr>
		<title level="m">ICML, volume 48 of JMLR Workshop and Conference Proceedings</title>
		<editor>Balcan, M.-F. and Weinberger, K. Q.</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="612" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation. CoRR, abs/1708.04896</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.04896" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
