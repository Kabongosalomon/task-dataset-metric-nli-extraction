<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 valeo.ai</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-theart performance in semantic segmentation on two challenging "synthetic-2-real" set-ups 1 and show that the approach can also be used for detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of assigning class labels to all pixels in an image. In practice, segmentation models often serve as the backbone in complex computer vision systems like autonomous vehicles, which demand high accuracy in a large variety of urban environments. For example, under adverse weathers, the system must be able to recognize roads, lanes, sideways or pedestrians despite their appearances being largely different from ones in the training set. A more extreme and important example is so-called "synthetic-2-real" set-up <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref> -training samples are synthesized by game engines and test samples are real scenes. Current fully-supervised approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b1">2]</ref> have not yet guaranteed a good generalization to arbitrary test cases. Thus a model trained on one domain, named as source, usually undergoes a drastic drop in performance when applied on another domain, named as target. <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/valeoai/ADVENT. Unsupervised domain adaptation (UDA) is the field of research that aims at learning only from source supervision a well performing model on target samples. Among the recent methods for UDA, many address the problem by reducing cross-domain discrepancy, along with the supervised training on the source domain. They approach UDA by minimizing the difference between the distributions of the intermediate features or of the final outputs for source and target data respectively. It is done at single <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref> or multiple levels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> using maximum mean discrepancies (MMD) or adversarial training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>. Other approaches include self-training <ref type="bibr" target="#b50">[51]</ref> to provide pseudo labels or generative networks to produce target data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Semi-supervised learning addresses a closely related problem of learning from the data of which only a subset is annotated. Thus, it inspires several approaches for UDA, for example, self-training, generative model or class balancing <ref type="bibr" target="#b48">[49]</ref>. Entropy minimization is also one of the successful approaches used for semi-supervised learning <ref type="bibr" target="#b37">[38]</ref>.</p><p>In this work, we adapt the principle of entropy minimization to the UDA task in semantic segmentation. We start from a simple observation: models trained only on source domain tend to produce over-confident, i.e., low-entropy, predictions on source-like images and under-confident, i.e., high-entropy, predictions on target-like ones. Such a phenomenon is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Prediction entropy maps of scenes from the source domain look like edge detection results with high entropy activations only along object borders. On the other hand, predictions on target images are less certain, resulting in very noisy, high entropy outputs. We argue that one possible way to bridge the domain gap between source and target is by enforcing high prediction certainty (low-entropy) on target predictions as well. To this end, we propose two approaches: direct entropy minimization using an entropy loss and indirect entropy minimization using an adversarial loss. While the first approach imposes the low-entropy constraint on independent pixel-wise predictions, the latter aims at globally matching source and target distributions in terms of weighted self-information. <ref type="bibr" target="#b1">2</ref> We summarize our contributions as follows:</p><p>• For semantic segmentation UDA, we propose to leverage an entropy loss to directly penalize low-confident predictions on target domain. The use of this entropy loss adds no significant overhead to existing semantic segmentation frameworks. • We introduce a novel entropy-based adversarial training approach targeting not only the entropy minimization objective but also the structure adaptation from source domain to target domain. • To improve further the performance in specific settings, we suggest two additional practices: (i) training on specific entropy ranges and (ii) incorporating class-ratio priors. We discuss practical insights in the experiments and ablation studies.</p><p>The entropy minimization objectives push the model's decision boundaries toward low-density regions of the target domain distribution in prediction space. This results in "cleaner" semantic segmentation outputs, with more refined object edges as well as large ambiguous image regions being correctly recovered, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The proposed models outperform state-of-the-art approaches on several UDA benchmarks for semantic segmentation, in particular the two main synthetic-2-real benchmarks, GTA5→Cityscapes and SYNTHIA→Cityscapes. <ref type="bibr" target="#b1">2</ref> Connection to the entropy is discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Unsupervised Domain Adaptation is a well researched topic for the task of classification and detection, with recent advances in semantic segmentation also. A very appealing application of domain adaptation is on using synthetic data for real world tasks. This has encouraged the development of several synthetic scene projects with associated datasets, such as Carla <ref type="bibr" target="#b7">[8]</ref>, SYNTHIA <ref type="bibr" target="#b30">[31]</ref>, and others <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>The main approaches for UDA include discrepancy minimization between source and target feature distributions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>, self-training with pseudo-labels <ref type="bibr" target="#b50">[51]</ref> and generative approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>. In this work, we are particularly interested in UDA for the task of semantic segmentation. Therefore, we only review the UDA approaches for semantic segmentation here (see <ref type="bibr" target="#b6">[7]</ref> for a more general literature review).</p><p>Adversarial training for UDA is the most explored approach for semantic segmentation. It involves two networks. One network predicts the segmentation maps for the input image, which could be from source or target domain, while another network acts as a discriminator which takes the feature maps from the segmentation network and tries to predict domain of the input. The segmentation network tries to fool the discriminator, thus making the features from the two domains have a similar distribution. Hoffman et al. <ref type="bibr" target="#b14">[15]</ref> are the first to apply the adversarial approach for UDA on semantic segmentation. They also have a category specific adaptation by transferring the label statistics from the source domain. A similar approach of global and class-wise alignment is used in <ref type="bibr" target="#b4">[5]</ref> with the class-wise alignment being done using adversarial training on grid-wise soft pseudolabels. In <ref type="bibr" target="#b3">[4]</ref>, adversarial training is used for spatial-aware adaptation along with a distillation loss to specifically address synthetic-2-real domain shift. <ref type="bibr" target="#b15">[16]</ref> uses a residual net to make the source feature maps similar to target's ones using adversarial training, the feature maps being then used for the segmentation task. In <ref type="bibr" target="#b40">[41]</ref>, the adversarial approach is used on the output space to benefit from the structural consistency across domain. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> propose another interesting way of using adversarial training: They get two predictions on the target domain image, this is done either by two classifiers <ref type="bibr" target="#b32">[33]</ref> or using dropout in the classifier <ref type="bibr" target="#b31">[32]</ref>. Given the two predictions the classifier is trained to maximize the discrepancy between the distributions while the feature extractor part of the network is trained to minimize it.</p><p>Some methods build on generative networks to generate target images conditioned on the source. Hoffman et al. <ref type="bibr" target="#b13">[14]</ref> propose Cycle-Consistent Adversarial Domain Adaptation (CyCADA), in which they adapt at both pixel-level and feature-level representation. For pixel-level adaptation they use Cycle-GAN <ref type="bibr" target="#b47">[48]</ref> to generate target images conditioned on the source images. In <ref type="bibr" target="#b33">[34]</ref>, a generative model is learned to reconstruct images from the feature space. Then, for domain adaptation, the feature module is trained to produce target images on source features and vice-versa using the generator module. In DCAN <ref type="bibr" target="#b42">[43]</ref>, channel-wise feature alignment is used in the generator and segmentation network. The segmentation network is learned on generated images with the content of the source and style of the target for which source segmentation map serves as the groundtruth. The authors in <ref type="bibr" target="#b49">[50]</ref> use generative adversarial networks (GAN) <ref type="bibr" target="#b10">[11]</ref> to align the source and target embeddings. Also, they replace the cross-entropy loss by a conservative loss (CL) that penalizes the easy and hard cases of source examples. The CL approach is orthogonal to most of the UDA methods, including ours: it could benefit any method that uses cross-entropy for source.</p><p>Another approach for UDA is self-training. The idea is to use the prediction from an ensembled model or a previous state of model as pseudo-labels for the unlabeled data to train the current model. Many semi-supervised methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> use self-training. In <ref type="bibr" target="#b50">[51]</ref>, self-training is employed for UDA on semantic segmentation which is further extended with class balancing and spatial prior. Self-training has an interesting connection to the proposed entropy minimization approach as we discuss in Section 3.1.</p><p>Among some other approaches, <ref type="bibr" target="#b25">[26]</ref> uses a combination of adversarial and generative techniques through multiple losses, <ref type="bibr" target="#b45">[46]</ref> combines the generative approach for appearance adaptation and adversarial training for representation adaptation, and <ref type="bibr" target="#b44">[45]</ref> proposes a curriculum-style learning for UDA by enforcing the consistency on local (superpixellevel) and global label distributions.</p><p>Entropy minimization has been shown to be useful for semi-supervised learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref> and clustering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. It has also been recently applied on domain adaptation for classification task <ref type="bibr" target="#b24">[25]</ref>. To our knowledge, we are first to successfully apply entropy based UDA training to obtain competitive performance on semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approaches</head><p>In this section, we present our two proposed approaches for entropy minimization using (i) an unsupervised entropy loss and (ii) adversarial training. To build our models, we start from existing semantic segmentation frameworks and add an additional network branch used for domain adaptation. <ref type="figure">Figure 2</ref> illustrates our architectures.</p><p>Our models are trained with a supervised loss on source domain. Formally, we consider a set X s ⊂ R H×W ×3 of sources examples along with associated ground-truth C-class segmentation maps, Y s ⊂ (1, C) H×W . Sample x s is a H × W color image and entry y </p><formula xml:id="formula_0">min θ F 1 |X s | xs∈Xs L seg (x s , y s ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Direct entropy minimization</head><p>For the target domain, as we do not have the annotations y t for image samples x t ∈ X t , we cannot use (1) to learn F . Some methods use the model's predictionŷ t as a proxy for y t . Also, this proxy is used only for pixels where prediction is sufficiently confident. Instead of using the highconfident proxy, we propose to constrain the model such that it produces high-confident prediction. We realize this by minimizing the entropy of the prediction.</p><p>We introduce the entropy loss L ent to directly maximize prediction certainty in the target domain. In this work, we use the Shannon Entropy <ref type="bibr" target="#b35">[36]</ref>. Given a target input image x t , the entropy map E xt ∈ [0, 1] H×W is composed of the independent pixel-wise entropies normalized to [0, 1] range:</p><formula xml:id="formula_1">E (h,w) xt = −1 log(C) C c=1 P (h,w,c) xt log P (h,w,c) xt ,<label>(2)</label></formula><p>at pixel (h, w). An example of entropy map is shown in <ref type="figure">Figure 2</ref>. The entropy loss L ent is defined as the sum of all pixel-wise normalized entropies:</p><formula xml:id="formula_2">L ent (x t ) = h,w E (h,w) xt .<label>(3)</label></formula><p>During training, we jointly optimize the supervised segmentation loss L seg on source samples and the unsupervised entropy loss L ent on target samples. The final optimization problem is formulated as follows:</p><formula xml:id="formula_3">min θ F 1 |X s | xs L seg (x s , y s ) + λ ent |X t | xt L ent (x t ),<label>(4)</label></formula><p>with λ ent as the weighting factor of the entropy term L ent .</p><p>Connection to self-training. Pseudo-labeling is a simple yet efficient approach for semi-supervised learning <ref type="bibr" target="#b20">[21]</ref>. Recently, the approach has been applied to UDA in semantic segmentation task with an iterative self-training (ST) procedure <ref type="bibr" target="#b50">[51]</ref>. The ST method assumes that the set K ⊂ <ref type="figure">Figure 2</ref>: Approach overview. The figure shows our two approaches for UDA. First, direct entropy minimization minimizes the entropy of the target Px t , which is equivalent to minimizing the sum of weighted self-information maps Ix t . In the second, complementary approach, we use adversarial training to enforce the consistency in Ix across domains. Red arrows are used for target domain and blue arrows for source. An example of entropy map is shown for illustration.</p><p>(1, H) × (1, W ) of high-scoring pixel-wise predictions on target samples are correct with high probability. Such an assumption allows the use of cross-entropy loss with pseudolabels on target predictions. In practice, K is constructed by selecting high-scoring pixels with a fixed or scheduled threshold. To draw a link with entropy minimization, we write the training problem of the ST approach as:</p><formula xml:id="formula_4">min θ F 1 |X s | xs L seg (x s , y s ) + λ pl |X t | xt L seg (x t ,ŷ t ),<label>(5)</label></formula><p>whereŷ t is the one-hot class prediction for x t and with:</p><formula xml:id="formula_5">L seg (x t ,ŷ t ) = − (h,w)∈K C c=1ŷ (h,w,c) t log P (h,w,c) xt .<label>(6)</label></formula><p>Comparing equations (2-3) and <ref type="formula" target="#formula_5">(6)</ref>, we note that our entropy loss L ent (x t ) can be seen as a soft-assignment version of the pseudo-label cross-entropy loss L seg (x t ,ŷ t ). Different to ST <ref type="bibr" target="#b50">[51]</ref>, our entropy-based approach does not require a complex scheduling procedure for choosing threshold. Even, contrary to ST assumption, we show in Section 4.3 that, in some cases, training on the "hard" or "mostconfused" pixels produces better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Minimizing entropy with adversarial learning</head><p>The entropy loss for an input image is defined in equation (3) as the sum of independent pixel-wise prediction entropies. Therefore, a mere minimization of this loss neglects the structural dependencies between local semantics. As shown in <ref type="bibr" target="#b40">[41]</ref>, for UDA in semantic segmentation, adaptation on structured output space is beneficial. It is based on the fact that source and target domain share strong similarities in semantic layout.</p><p>In this part, we introduce a unified adversarial training framework which indirectly minimizes the entropy by having target's entropy distribution similar to the source. This allows the exploitation of the structural consistency between the domains. To this end, we formulate the UDA task as minimizing distribution distance between source and target on the weighted self-information space. <ref type="figure">Figure 2</ref> illustrates our adversarial learning procedure. Our adversarial approach is motivated by the fact that the trained model naturally produces low-entropy predictions on source-like images. By aligning weighted self-information distributions of target and source domains, we indirectly minimize the entropy of target predictions. Moreover, as the adaptation is done on the weighted self-information space, our model leverages structural information from source to target.</p><p>In detail, given a pixel-wise predicted class score P (h,w,c) x , the self-information or "surprisal" <ref type="bibr" target="#b39">[40]</ref> is defined as − log P ]. We here perform adversarial adaptation on weighted self-information maps I x composed of pixellevel vectors I <ref type="bibr" target="#b2">3</ref> Such vectors can be seen as the disentanglement of the Shannon Entropy. We then construct a fully-convolutional discriminator network D with parameters θ D taking I x as input and that produces domain classification outputs, i.e., class label 1 (resp. 0) for the source (resp. target) domain. Similar to <ref type="bibr" target="#b10">[11]</ref>, we train the discriminator to discriminate outputs coming from source and target images, and at the same time, train the segmentation network to fool the discriminator. In detail, let L D the cross-entropy domain classification loss. The training objective of the discriminator is: <ref type="bibr" target="#b6">(7)</ref> and the adversarial objective to train the segmentation network is: min</p><formula xml:id="formula_6">(h,w) x = −P (h,w) x · log P (h,w) x .</formula><formula xml:id="formula_7">min θ D 1 |X s | xs L D (I xs , 1) + 1 |X t | xt L D (I xt , 0),</formula><formula xml:id="formula_8">θ F 1 |X t | xt L D (I xt , 1).<label>(8)</label></formula><p>Combining <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_8">(8)</ref>, we derive the optimization problem</p><formula xml:id="formula_9">min θ F 1 |X s | xs L seg (x s , y s ) + λ adv |X t | xt L D (I xt , 1),<label>(9)</label></formula><p>with the weighting factor λ adv for the adversarial term L D .</p><p>During training, we alternatively optimize networks D and F using objective functions in <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_9">(9)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Incorporating class-ratio priors</head><p>Entropy minimization can get biased towards some easy classes. Therefore, sometimes it is beneficial to guide the learning with some prior. To this end, we use a simple class-prior based on the distribution of the classes over the source labels. We compute the class-prior vector p s as a 1 -normalized histogram of number of pixels per class over the source labels. Now based on the predicted P xt , too large discrepancy between the expected probability for any class and class-prior p s is penalized, using</p><formula xml:id="formula_10">L cp (x t ) = C c=1 max 0, µp (c) s − E c (P (c) xt ) ,<label>(10)</label></formula><p>where µ ∈ [0, 1] is used to relax the class prior constraint. This addresses the fact that class distribution on a single target image is not necessarily close to p s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present our experimental results. Section 4.1 introduces the used datasets as well as our training parameters. In Section 4.2 and Section 4.3, we report and discuss our main results. In Section 4.3, we discuss a preliminary result on entropy-based UDA for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental details</head><p>Datasets. To evaluate our approaches, we use the challenging synthetic-2-real unsupervised domain adaptation set-ups. Models are trained on fully-annotated synthetic data and are validated on real-world data. In such set-ups, the models have access to some unlabeled real images during training. To train our models, we use either GTA5 <ref type="bibr" target="#b29">[30]</ref> or SYNTHIA <ref type="bibr" target="#b30">[31]</ref> as source domain synthetic data, along with the training split of Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref> as target domain data. Similar set-ups have been previously used in other works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51]</ref>. In detail:</p><p>• GTA5→Cityscapes: The GTA5 dataset consists of 24, 966 synthesized frames captured from a video game. Images are provided with pixel-level semantic annotations of 33 classes. Similar to <ref type="bibr" target="#b14">[15]</ref>, we use the 19 classes in common with the Cityscapes dataset. • SYNTHIA→Cityscapes: We use the SYNTHIA-RAND-CITYSCAPES set 4 with 9, 400 synthesized images for training. We train our models with 16 common classes in SYNTHIA and Cityscapes. While evaluating we compare the performance on 16-and 13class subsets following the protocol used in <ref type="bibr" target="#b50">[51]</ref>.</p><p>In both set-ups, 2, 975 unlabeled Cityscapes images are used for training. We measure segmentation performance with the standard mean-Intersection-over-Union (mIoU) metric <ref type="bibr" target="#b8">[9]</ref>. Evaluation is done on the 500 validation images. Network architectures. We use Deeplab-V2 <ref type="bibr" target="#b1">[2]</ref> as the base semantic segmentation architecture F . To better capture the scene context, Atrous Spatial Pyramid Pooling (ASPP) is applied on the last layer's feature output. Sampling rates are fixed as {6, 12, 18, 24}, similar to the ASPP-L model in <ref type="bibr" target="#b1">[2]</ref>. We experiment on the two different base deep CNN architectures: VGG-16 <ref type="bibr" target="#b36">[37]</ref> and ResNet-101 <ref type="bibr" target="#b12">[13]</ref>. Following <ref type="bibr" target="#b1">[2]</ref>, we modify the stride and dilation rate of the last layers to produce denser feature maps with larger field-of-views. To further improve performance on ResNet-101, we perform adaptation on multi-level outputs coming from both conv4 and conv5 features <ref type="bibr" target="#b40">[41]</ref>.</p><p>The adversarial network D introduced in Section 3.2 has the same architecture as the one used in DCGAN <ref type="bibr" target="#b27">[28]</ref>. Weighted self-information maps I x are forwarded through 4 convolutional layers, each coupled with a leaky-ReLU layer with a fixed negative slope of 0.2. At the end, a classifier layer produces classification outputs, indicating if the inputs correspond to source or target domain. Implementation details. We employ PyTorch deep learning framework <ref type="bibr" target="#b26">[27]</ref> in the implementations. All experiments are done on a single NVIDIA 1080TI GPU with 11 GB memory. Our model, except the adversarial discriminator mentioned in Section 3.2, is trained using Stochastic Gradient Descent optimizer <ref type="bibr" target="#b0">[1]</ref> with learning rate 2.5 × 10 −4 , momentum 0.9 and weight decay 10 −4 . We use Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with learning rate 10 −4 to train the discriminator. To schedule the learning rate, we follow the polynomial annealing procedure mentioned in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Weighting factors of entropy and adversarial losses: To set the weight for L ent , the training set performance provides important indications. If λ ent is large then the entropy drops too quickly and the model is strongly biased towards a few classes. When λ ent is chosen in a suitable range however, the performance is better and not sensitive to the pre-  <ref type="table">Table 1</ref>: Semantic segmentation performance mIoU (%) on Cityscapes validation set of models trained on GTA5 (a) and SYNTHIA (b). We show results of our approaches using the direct entropy loss (MinEnt) and using adversarial training (AdvEnt). In each subtable, top and bottom parts correspond to VGG-16-based and ResNet-101-based models respectively. The "Adapt-SegMap*" denotes our retrained model of <ref type="bibr" target="#b40">[41]</ref>. The abbreviations "Adv", "ST" and "Ent" stand for adversarial training, self-training and entropy minimization approaches. cise value. Thus, we use the same λ ent = 0.001 for all our experiments regardless of the network or the dataset. Similar arguments hold for the weight λ adv in <ref type="bibr" target="#b8">(9)</ref>. We fix λ adv = 0.001 in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We present experimental results of our approaches compared to different baselines. Our models achieve state-ofthe-art performance in the two UDA benchmarks. In what follows, we show different behaviors of our approaches in different settings, i.e., training sets and base CNNs.</p><p>GTA5→Cityscapes: We report in <ref type="table">Table 1</ref>-a semantic segmentation performance in terms of mIoU (%) on Cityscapes validation set. Our first approach of direct entropy minimization, termed as MinEnt in <ref type="table">Table 1</ref>-a, achieves comparable performance to state-of-the-art base-lines on both VGG-16 and ResNet-101-based CNNs. The MinEnt outperforms Self-Training (ST) approach without and with class-balancing <ref type="bibr" target="#b50">[51]</ref>. Compared to <ref type="bibr" target="#b40">[41]</ref>, the ResNet-101-based MinEnt shows similar results but without resorting to the training of a discriminator network. The light overhead of the entropy loss makes training time much less for the MinEnt model. Another advantage of our entropy approach is the ease of training. Indeed, training adversarial networks is generally known as a difficult task due to its instability. We observed a more stable behavior training models with the entropy loss.</p><p>Interestingly, we find that in some cases, only applying entropy loss on certain ranges works best. Such a phenomenon is observed with the ResNet-101-based models. Indeed, we get a better model by training on pixels having entropy values within the top 30% of each target sample.</p><p>The model is termed as MinEnt+ER in <ref type="table">Table 1</ref>-a. We obtain 43.1% mIoU using this strategy on the GTA5→Cityscapes set-up. More details are given in Section 4.3.</p><p>Our second approach using adversarial training on the weighted self-information space, noted as AdvEnt, shows consistent improvement to the baselines on the two base networks. In general, AdvEnt works better than MinEnt. On the GTA5→Cityscapes UDA set-up, AdvEnt achieves state-of-the-art mIoU of 43.8. Such results confirm our intuition on the importance of structural adaptation. With the VGG-16-based network, adaptation on the weighted selfinformation space brings +3.3% mIoU improvement compared to the direct entropy minimization. With the ResNet-101-based network, the improvement is less, i.e., +1.5% mIoU. We conjecture that, as GTA5 semantic layouts are very similar to ones in Cityscapes, the segmentation network F with high capacity base CNN like ResNet-101 is capable of learning some spatial priors from the supervision on source samples. As for lower-capacity base model like VGG-16, an additional regularization on the structured space with adversarial training is more beneficial.</p><p>By combining results of the two models MinEnt and Ad-vEnt, we observe a decent boost in performance, compared to results of single models. The ensemble achieves 45.5% mIoU on the Cityscape validation set. Such a result indicates that complementary information are learned by the two models. Indeed, while the entropy loss penalizes independent pixel-level predictions, the adversarial loss operates more on the image-level, i.e., scene topology. Similar to <ref type="bibr" target="#b40">[41]</ref>, for a more meaningful comparison to other UDA approaches, in <ref type="table" target="#tab_2">Table 2</ref>-a we show the performance gap between UDA models and the oracle, i.e., the model trained with full supervision on the Cityscapes training set. Compared to models trained by other methods, our single and ensemble models have smaller mIoU gaps to the oracle.</p><p>In <ref type="figure">Figure 3</ref>, we illustrate some qualitative results of our models. Without domain adaptation, the model trained only on source supervision produces noisy segmentation predictions as well as high entropy activations, with a few exceptions on some classes like "building" and "car". Still, there exist many confident predictions (low entropy) which are completely wrong. Our models, on the other hand, manage to produce correct predictions at high level of confidence. We observe that overall, the AdvEnt model achieves lower prediction entropy compared to the MinEnt model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYNTHIA→Cityscapes</head><p>: <ref type="table">Table 1</ref>-b shows results on the 16and 13-class subsets of the Cityscapes validation set. We notice that scene images in SYNTHIA cover more diverse viewpoints than the ones in GTA5 and Cityscape. This results in different behaviors of our approaches.</p><p>On the VGG-16-based network, the MinEnt model shows comparable results to state-of-the-art methods. Compared to Self-Training <ref type="bibr" target="#b50">[51]</ref>, our model achieves +3.6%   and +4.7% on 16and 13class subsets respectively. However, compared to stronger baselines like the class-balanced self-training, we observe a significant drop in class "road". We argue that it is due to the large layout gaps between SYNTHIA and Cityscapes. To target this issue, we incorporate the class-ratio priors from source domain, as introduced in Section 3.3. By constraining target output distribution using class-ratio priors, noted as CP in <ref type="table">Table 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>The experimental results shown in Section 4.2 have validated the advantages of our approaches. To further push the performance, we proposed two different ways to regularize the training in two particular settings. This section discusses our experimental choices and explain the intuitions behind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5→Cityscapes:</head><p>Training on specific entropy ranges. In this setup, we observe that the performance of model MinEnt using ResNet-101-based network can be improved by training on target pixels having entropy values in a specific range. Interestingly, the best MinEnt model was  trained on the top 30% highest-entropy pixels on each target sample -gaining 0.8% mIoU over the vanilla model. We note that high-entropy pixels are the "most confusing" ones, i.e., where the segmentation model is indecisive between multiple classes. One reason is that the ResNet-101based model generalizes well in this particular setting. Accordingly, among the "most confusing" predictions, there is a decent amount of correct but "low confident" ones. Minimizing the entropy loss on such a set still drives the model toward the desirable direction. This assumption, however, does not hold for the VGG-16-based model.</p><p>SYNTHIA→Cityscapes: Using class-ratio prior. As discussed before, SYNTHIA has significantly different layout and viewpoints than Cityscapes. This disparity can cause very bad prediction for some classes, which is then further encouraged with minimization of entropy or their use as label proxy in self-training. Thus, it can lead to strong class biases or, in extreme cases, to missing some classes completely in the target domain. Adding class-ratio prior encourages the presence of all the classes and thus helps avoid such degenerate solutions. As mentioned in Sec. 3.3, we use µ to relax the source class-ratio prior, for example µ = 0 means no prior while µ = 1 implies enforcing exact class-ratio prior. Having µ = 1 is not ideal as it means that each target image should follow the class-ratio from the source domain. We choose µ = 0.5 to let the target classratio to be somewhat different from the source.</p><p>Application on UDA for object detection. The proposed entropy-based approaches are not limited to semantic segmentation and can be applied to UDA for other recognition tasks like object detection. We conducted experiments in the UDA object detection set-up Cityscapes→Cityscapes-Foggy, similar to the one in <ref type="bibr" target="#b2">[3]</ref>. A straight-forward application of the entropy loss and of the adversarial loss to the existing detection architecture SSD-300 <ref type="bibr" target="#b21">[22]</ref> significantly improves detection performance over the baseline model trained only on source. In terms of mean-average-precision (mAP), compared to the baseline performance of 14.7%, the MinEnt and AdvEnt models attain mAPs of 16.9% and 26.2%. In <ref type="bibr" target="#b2">[3]</ref>, the authors report a slightly better performance of 27.6% mAP, using Faster-RCNN <ref type="bibr" target="#b28">[29]</ref>, a more complex detection architecture than ours. We note that our detection system was trained and tested on images at lower resolution, i.e., 300 × 300. Despite these unfavorable factors, our improvement to the baseline (+11.5% mAP us-ing AdvEnt) is larger than the one reported in <ref type="bibr" target="#b2">[3]</ref> (+8.8%). Such a preliminary result suggests the possibility of applying entropy-based approaches on UDA for detection. <ref type="table" target="#tab_5">Table 3</ref> reports the per-class IoU and <ref type="figure">Figure 4</ref> shows qualitative results of the approaches on Cityscapes Foggy. More technical details are given in the Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we address the task of unsupervised domain adaptation for semantic segmentation and propose two complementary entropy-based approaches. Our models achieve state-of-the-art on the two challenging "synthetic-2-real" benchmarks. The ensemble of the two models improves the performance further. On UDA for object detection, we show a promising result and believe that the performance can get better using more robust detection architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Entropy-based UDA for object detection</head><p>Object detection framework. We use the Single Shot MultiBox Detector (SSD-300) <ref type="bibr" target="#b21">[22]</ref> with the VGG-16 base CNN <ref type="bibr" target="#b36">[37]</ref> as the detection backbone in our experiments. Given an input image, SSD-300 produces dense predictions from M feature maps at different resolutions. In detail, every location on each feature map m corresponds to a set of K m anchor boxes with predefined aspect ratios and scales. The detection pipeline ends with a non-maximum suppression (NMS) step to post-process the predictions. Readers are referred to <ref type="bibr" target="#b21">[22]</ref> for more details about the architecture and the training procedure. We denote the "soft-detection map" of the SSD model at feature map m of dimension </p><p>Similar to the semantic segmentation task, we jointly optimize the supervised object detection losses on source and the unsupervised entropy loss L ent on target samples.</p><p>Entropy minimization with adversarial learning. We apply the adversarial framework proposed in Section 3.2.</p><p>To this end, we first transform the soft-detection maps P m</p><p>x to the weighted self-information maps I m x . We then zeropad the I m x maps at lower resolutions to match the size of the largest one. Finally, we stack all zero-padded I m x to produce I x , which serves as the input to the discriminator. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed entropy-based unsupervised domain adaptation for semantic segmentation. The top two rows show results on source and target domain scenes of the model trained without adaptation. The bottom row shows the result on the same target domain scene of the model trained with entropy-based adaptation. The left and right columns visualize respectively the semantic segmentation outputs and the corresponding prediction entropy maps (see text for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(h,w) s = y (h,w,c) s c of associated map y s provides the label of pixel (h, w) as one-hot vector. Let F be a semantic segmen-tation network which takes an image x and predicts a C-dimensional "soft-segmentation map" F (x) = P x = P (h,w,c) x h,w,c . By virtue of final softmax layer, each C-dimensional pixel-wise vector P (h,w,c) x c behaves as a discrete distribution over classes. If one class stands out, the distribution is picky (low entropy), if scores are evenly spread, sign of uncertainty from the network standpoint, the entropy is large. The parameters θ F of F are learned to minimize the segmentation loss L seg (x s , y s ) = − w,c) xs on source samples. In the case of training only on source domain without domain adaptation, the optimization problem simply reads:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(h,w,c) x . Effectively, the entropy E (h,w) x in (2) is the expected value of the self-informationE c [− log P (h,w,c) x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>H m × W m as P m x ∈ [0, 1] Hm×Wm×Km×C . Direct entropy minimization. Considering a target input image x t , the entropy map produced at a feature map m, E m xt ∈ [0, 1] Hm×Wm×Km , is composed of the independent box-level entropies normalized to [0, 1]: E m(h,w,k) h,w,k,c) xt log P m(h,w,k,c) xt . (11) The entropy loss L ent is defined as the sum of normalized box entropies over all anchor boxes and all feature resolutions: L ent (x t ) = m h,w,k E m(h,w,k) xt .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Qualitative results in the GTA5→Cityscapes set-up. Column (a) shows a input image and the corresponding semantic segmentation ground-truth. Column (b), (c) and (d) show segmentation results (bottom) along with prediction entropy maps produced by different approaches (top). Best viewed in colors. Qualitative results for object detection. Column (a) shows the input images. Columns (b), (c) and (d) illustrate detection results of the baseline, our MinEnt and AdvEnt models. Detections of different classes are plotted in different colors. We visualize all the detections with scores greater than 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance gap between UDA models and the or-</figDesc><table><row><cell>acle in GTA5→Cityscapes and SYNTHIA→Cityscapes setups.</cell></row><row><cell>Top and bottom parts of each table correspond to VGG-16-based</cell></row><row><cell>and ResNet-101-based models respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>b, we improve MinEnt by +2.9% mIoU on both 16and 13class subsets. With adversarial training, we have an additional ∼ +1% mIoU. On the ResNet-101-based network, the AdvEnt model achieves state-of-the-art performance. Compared to the retrained model of<ref type="bibr" target="#b40">[41]</ref>, i.e., Adapt-SegMap*, the AdvEnt improves the mIoUs on 16and 13class subsets by +1.2% and +1.8%.Consistent with the GTA5 results above, the ensemble of the two models MinEnt and AdvEnt trained on SYNTHIA achieves the best mIoU of 41.2% and 48.0% on 16and 13class subsets respectively. According toTable 2-b, our models have the smallest mIoU gaps to the oracle.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>17.4 27.2 5.7 15.1 9.1 11.0 16.7 14.7 Ours (MinEnt) 15.8 22.0 28.3 5.0 15.2 15.0 13.0 20.6 16.9</figDesc><table><row><cell></cell><cell cols="6">Cityscapes → Cityscapes Foggy</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mcycle</cell><cell>bicycle</cell><cell>mAP</cell></row><row><cell cols="10">SSD-300 15.0 Ours (AdvEnt) 17.6 25.0 39.6 20.0 37.1 25.9 21.3 23.1 26.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Detection performance on Cityscapes Foggy.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Abusing notations, '·' and 'log' stand for Hadamard product and point-wise logarithm respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A split of the SYNTHIA dataset<ref type="bibr" target="#b30">[31]</ref> using compatible labels with the Cityscapes dataset.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>COMPSTAT. 2010. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Subic: A supervised, structured binary code for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a complete image indexing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Play and learn: Using video games to train computer vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Thermostatics and thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tribus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno>1530</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Computer Sciences, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Penalizing top performers: Conservative loss for semantic segmentation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
