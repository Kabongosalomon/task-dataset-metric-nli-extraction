<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Al-Theiabat</surname></persName>
							<email>haaltheiabat13@cit.just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aisha</forename><surname>Al-Sadi</surname></persName>
							<email>asalsadi16@cit.just.edu.jo</email>
							<affiliation key="aff0">
								<orgName type="institution">Jordan University of Science and Technology</orgName>
								<address>
									<settlement>Irbid</settlement>
									<country key="JO">Jordan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Inception Team at NSURL-2019 Task 8: Semantic Question Similarity in Arabic</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our method for the task of Semantic Question Similarity in Arabic in the workshop on NLP Solutions for Under Resourced Languages (NSURL). The aim is to build a model that is able to detect similar semantic questions in Arabic language for the provided dataset. Different methods of determining questions similarity are explored in this work. The proposed models achieved high F1-scores, which range from (88% to 96%). Our official best result is produced from the ensemble model of using pre-trained multilingual BERT model with different random seeds with 95.924% F1-Score, which ranks the first among nine participants teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic matching or semantic similarity is a significant part of natural language processing (NLP) field for its variety of tasks. It used to measure the similarity and the relationship between different textual elements, such as words, sentences, or documents. Semantic matching has been involved in many NLP applications; including question answering, where it is used to assess question answering and retrieval tasks by employing it to estimate the similarity of query answer among all candidate answers <ref type="bibr" target="#b21">(Wang et al., 2016)</ref>. In addition, it has played a significant role in top-k re-ranking in machine translation <ref type="bibr" target="#b0">(Brown et al., 1993)</ref>, information extraction <ref type="bibr" target="#b3">(Grishman, 1997)</ref> and automatic text summarization <ref type="bibr" target="#b14">(Ponzanelli et al., 2015)</ref>.</p><p>Natural language has complicated structures either from sequential or hierarchical perspectives, capturing the relationship between two questions is becoming a challenging task. For example, questions that have the same meaning while their words have a different order. An effective semantic match- * * These authors contributed equally to the work ing algorithm, therefore, needs to consider an appropriate semantic representation to capture the similarity without being affected with words order. This paper focuses on detecting semantic question similarity, which is a common challenge in Question-and-answer (Q&amp;A) websites, such as Quora and Stack Overflow. This work targets Arabic questions dataset published by Mawdoo3 AI 1 . Most of these questions are related to information provided by Mawdoo3.com which is the largest comprehensive Arabic content website. For these websites, the benefit of detecting duplicate questions is to improve the efficiency of search engines by being aware of the different paraphrases of the same question.</p><p>The rest of paper is organized as follows. Section 2 presents related works. While Section 3 presents some details about the dataset. Section 4, presents the proposed models for solving the semantic similarity in Arabic language task. Results for all proposed models and the final results are presented in Section 5. Finally, the paper conclusion is presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic matching has been a long-established problem in NLP. Many approaches were proposed to solve this problem. The conventional approaches were mainly based on representing text as a vector of word features. The bag-of-words (BoW) method <ref type="bibr" target="#b22">(Wu et al., 2008)</ref> employed the word occurrence and Term Frequency-Inverse Document Frequency (TF-IDF) <ref type="bibr" target="#b10">(Paltoglou and Thelwall, 2010)</ref> as the word feature. However, these types of models disregard word meaning, orders, and even grammar. In contrast, word embedding models such as word2vec <ref type="bibr" target="#b7">(Mikolov et al., 2013)</ref> and Glove <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> have been widely used instead of BoW as they can learn distributional semantic representation for words. So based on word embeddings, the Word Movers Distance (WMD) <ref type="bibr" target="#b5">(Kusner et al., 2015)</ref> was proposed to measure the dissimilarity between two texts assuming that similar words should have similar vectors. Although WMD can estimate semantic similarity between texts, the order, and interactions between words are excluded.</p><p>Recently many deep learning models have been proposed for text matching. A common framework has been adopted is the Siamese architecture <ref type="bibr" target="#b9">(Mueller and Thyagarajan, 2016;</ref><ref type="bibr" target="#b11">Pang et al., 2016;</ref><ref type="bibr" target="#b17">Severyn and Moschitti, 2015;</ref> where the encoder, which can be either Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN), is applied individually on the two input texts, so both texts are encoded into intermediate contextual representations. Then, the matching result is generated by performing a scoring mechanism over contextual representations. Although this framework supports parameter sharing in its network, it purely learns complicated relationships among texts.</p><p>Another framework is based on matching aggregation <ref type="bibr" target="#b19">(Wang and Jiang, 2017)</ref> which first matches the small units (such as words) of two texts to produce comparison vectors, then these vectors are aggregated and fed into a CNN or RNN for the final classification. This framework improves capturing the interactive features between two texts, but still it limits exploring the matching in only word-word manner.</p><p>As the main focus of this paper is to detect semantically equivalent questions, the following is the review of related approaches that were adopted to detect duplicate questions on Quora dataset. As Quora recently published a dataset of 400K labeled questions, massive researches have been proposed on this dataset for question paraphrase identification challenge (qou). One Relevant approach that was proposed for this challenge is the Bilateral Multi-Perspective Matching model (BIMPM) model  which encodes two questions with a Bidirectional Long Short-Term Memory Network (BiLSTM). Then, a multi-perspective matching in the two directions is applied to both questions, and for each time step, questions are matched using different types of extensive matching. On Quora dataset, the result of this model reached 88.17%. In <ref type="bibr" target="#b8">(Mirakyan et al., 2018)</ref>, a novel architecture can obtain a high-level understanding of the question pairs through extracting the semantic features using dense interaction tensors (attention) network which called Densely Interactive Inference Network (DIIN). DIIN outperforms BiLSTM on Quora to achieve accuracy of 89.06%. Moreover, Multi-Task Deep Neural Network (MT-DNN) <ref type="bibr" target="#b6">(Liu et al., 2019)</ref> achieved competitive performance on several tasks including question paraphrase on Quora with an accuracy of 89.6%. Specifically, MT-DNN Combined multi-task learning and pre-trained bidirectional transformer model for language representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Description</head><p>The dataset used in this task is provided by Maw-doo3 <ref type="bibr" target="#b16">(Seelawi et al., 2019)</ref>. It is a dataset for questions in Arabic language, it consists of 11,997 labeled question pairs as training data, and 3,715 question pairs as testing data. Label '1' means the question pairs are similar in semantic where label '0' means the opposite. 55% of the training question pairs are with label '0', and 45% are with label '1'. The max length of question 1 is 14 words with an average of 5.7 words per question, while the max length of question 2 is 28 words with an average of 5.3 words per question. <ref type="table" target="#tab_0">Table 1</ref> shows samples from the training dataset. The only processing step that was applied to the dataset is to unify countries names, some examples are shown in <ref type="table" target="#tab_1">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this work, four different deep learning approaches are presented to solve the semantic similarity task, which are RNN based model, CNN based model, multi-head attention based model, and finally BERT model. In this section, each model is discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convolutional Neural Network Model</head><p>In NLP field, CNN has shown the ability to extract most informative n-gram features from the input sequence, and then apply the activation on these features <ref type="bibr" target="#b4">(Kim, 2014)</ref>. Although CNN is known for the applications in the image processing field, it is used here for text classification application.</p><p>The proposed model architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Firstly, the words are mapped in the dictionary to get a representation for each word. Then each question is fed to three consecutive layers. In each layer, the convolutional layer is applied, followed by activation and then max pooling. Hence, each question's output is a feature representation which is used to get the similarity label by computing the cosine similarity between the two questions features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Neural Network Model</head><p>The significant advantage of RNNs is the computation of the same task over each element of the sequence, so the output for each block depends on the previous computations. Hence, RNN has been increasingly prevalent in NLP field specifically for RNN types that have a memory to remember the information through the sequence.</p><p>In this model, the input is a sequence of questionpairs that are concatenated to represent a single sequence. Then, the sequence is encoded by the dictionary to be fed into a bi-directional Gated Recurrent Units (GRUs) network with 128 hidden units to generate the similarity label as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-head Attention Network Model</head><p>Multi-head attention model <ref type="bibr" target="#b18">(Vaswani et al., 2017)</ref> allows to learn on various locations of the encoded words. Our network consists of a stacked encoderdecoder structure with eight heads.</p><p>For each question-pairs of sequence length n, at each layer l, the encoder maps a sequence of words Q l = w l 1 , .., w l n into hidden representation h l = h l 1 , .., h l n . After computing the attention on all positions jointly, the transformer stacks all hidden representation h l at the current layer l together into matrix H l . Given h, the decoder then generates output sequence y l = y l 1 , .., y l n , and after that apply softmax to estimate the output label. The transformer also contains two sub-layers, a multihead attention layer, and a position-encoding layer.</p><p>The position-encoding layer benefits the network to keep track of relative positions for each word in the sequence since the context and the meaning of a sequence depend on the order of its words.</p><p>In the multi-head attention layer, instead of computing single attention on the overall sequence, it jointly gets attention from different representations at different positions. As a result, each head looks differently on encoder output, and the decoder easily learns to retrieve valuable information from the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">BERT Model</head><p>Recently, pre-training language models have shown a significant role to improve many NLP tasks including question-pairs paraphrasing <ref type="bibr" target="#b2">(Dolan and Brockett, 2005)</ref>. There are two approaches to apply these pre-trained language representations on NLP tasks; either feature-based or fine-tuning. For the feature-based approach <ref type="bibr" target="#b13">(Peters et al., 2018)</ref>, researchers use the output of pre-trained model as additional features in their models, based on the task they target. On the other hand, the finetuning approach <ref type="bibr" target="#b15">(Radford et al., 2018)</ref> permits the model to be trained on another task by learning task-specific parameters. The two strategies were mentioned previously have limitations to learning general language representations since they adopt the left-to-right unidirectional architectures. On the other hand, Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b1">(Devlin et al., 2018)</ref>, has strongly outperformed previous cutting-edge unidirectional models.</p><p>BERT model relies on the multi-head selfattention mechanism, which enables it to achieve the state-of-the-art accuracy on a wide range of tasks such as, natural language inference, question answering, and sentence classification. The architecture of BERT model is built upon the transformer layer, which is called the self-attention layer. For each layer, the representations of words are exchanged from previous layers regardless of their positions, in contrast to traditional unidirectional models. For each input word, the model learns bidirectional encoder representations by using the masked language model, which randomly masks some of the words from the input to predict the masked word contextually.</p><p>As BERT offers pre-trained models for English language and multilingual model for 104 languages (ber) including the Arabic language, we applied the sentence pairs classification task on Arabic questions through fine-tuning the multilingual model as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>For each of the four models explained in the methodology section, different hyper-parameters are used, such as learning rate, number of hidden nodes, and number of epochs. <ref type="table" target="#tab_2">Table 3</ref> shows the main parameters values that give the best results for each model.</p><p>The evaluation metric that was used for this task is F1-Score that measures the precision p and recall r together as illustrated in the equations [1]-[3]:</p><formula xml:id="formula_0">F 1 = 2p.r/(p + r) (1) p = tp/(tp + f p) (2) r = tp/(tp + f n)<label>(3)</label></formula><p>where:</p><p>•  During the competition, the public score for each submitted file was shown directly. Then after the competition ended, the submitted file with the highest public score was chosen to calculate its private score and compete other teams based on it. <ref type="table" target="#tab_4">Table 4</ref> shows the highest F1-Score for each of the four models for the public score and the private score of the test data. As illustrated, BERT model with pre-trained multilingual outperforms the remaining models with F1-score of 96.050% on the public score, and 95.617% on the private score.</p><p>Note that the previous results are based on the best public score for every single model of the four models. Since BERT model gives the best results, we conducted other experiments with different random seeds in order to ensemble BERT model. Hard voting is used as ensemble method in which the predictions for each BERT experiments are involved in voting to get the final prediction.    <ref type="table" target="#tab_5">Table 5</ref> shows the results of the ensemble models of BERT with different number of experiments each with different random seed.</p><p>In the ensemble of four and six seeds when the number of votes is equal, high priority was given to the experiments with the best public score.</p><p>The result of the ensemble of four seeds has the best public score, so it was chosen for the final evaluation and got the first place. Although other seeds results had lower public scores, they had higher private scores than the official private score. So actually, our best result is 96.232% while the official best result is 95.924%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper describes our participation in NSURL Task 8; Semantic Question Similarity in Arabic. Different models were proposed for the task; RNN model, CNN model, Multi-head model, BERT model, and ensemble model of BERT. The ensemble model clearly outperforms all other models in this task by achieving 95.924% F1-Score. This performance ranks first place among nine participating teams.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>CNN model architecture used for detecting semantic questions similarity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>BERT model used for question pair similarity classification task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>tp: true positive examples • fp: false positive examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Question samples from Mawdoo3 dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Unify countries names example</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Main parameters for each proposed model</cell></row><row><cell>• fn: false negative examples</cell></row><row><cell>Test data evaluation is automatically done online</cell></row><row><cell>on Kaggle website by submitting the test predic-</cell></row><row><cell>tions file. The evaluation system is as the follow-</cell></row><row><cell>ing:</cell></row><row><cell>• Public score: calculated with approximately</cell></row><row><cell>30% of the data</cell></row><row><cell>• Private score: calculated with approximately</cell></row><row><cell>70% of the data</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of 30% of the test data</figDesc><table><row><cell>Model</cell><cell cols="2">Public Score (%) Private Score (%)</cell></row><row><cell cols="2">Ensemble of best 3 seeds 95.960</cell><cell>96.155</cell></row><row><cell cols="2">Ensemble of best 4 seeds 96.499</cell><cell>95.924</cell></row><row><cell cols="2">Ensemble of best 5 seeds 95.691</cell><cell>96.232</cell></row><row><cell cols="2">Ensemble of best 6 seeds 95.332</cell><cell>96.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>BERT Ensemble Results</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://ai.mawdoo3.com/nsurl-2019-task8 arXiv:2004.11964v1 [cs.CL] 24 Apr 2020</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information extraction: Techniques and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-63438-X_2</idno>
	</analytic>
	<monogr>
		<title level="m">Information Extraction: A Multidisciplinary Approach to an Emerging Information Technology, International Summer School, SCIE-97</title>
		<meeting><address><addrLine>Frascati, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="10" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving multi-task deep neural networks via knowledge distillation for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1904.09482</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Mirakyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<idno>abs/1802.03198</idno>
		<title level="m">Natural language inference over interaction space: ICLR 2018 reproducibility report. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A study of information retrieval weighting schemes for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="1386" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17<address><addrLine>Phoenix, Arizona, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>NAACL-. Long Papers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Summarizing complex development artifacts by mining heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ponzanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Mocci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Lanza</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSR.2015.49</idno>
	</analytic>
	<monogr>
		<title level="m">12th IEEE/ACM Working Conference on Mining Software Repositories, MSR 2015</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-16" />
			<biblScope unit="page" from="401" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NSURL-2019 task 8: Semantic question similarity in arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><surname>Seelawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Bataineh</forename><surname>Hesham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Farhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first International Workshop on NLP Solutions for Under Resourced Languages, NSURL &apos;19</title>
		<meeting>the first International Workshop on NLP Solutions for Under Resourced Languages, NSURL &apos;19<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767738</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08-09" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno>abs/1602.07019</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interpreting TF-IDF term weights as making relevance decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wing Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui-Lam</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwok</surname></persName>
		</author>
		<idno type="DOI">10.1145/1361684.1361686</idno>
		<idno>13:1-13:37</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
