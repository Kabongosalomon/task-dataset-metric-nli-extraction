<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Bhagat</surname></persName>
							<email>sarthak16189@iiitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">IIIT Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shagun</forename><surname>Uppal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IIIT Delhi</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Yin</surname></persName>
							<email>yinzhuyun@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Bioinformatics Institute</orgName>
								<address>
									<region>A*STAR</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nengli</forename><surname>Lim</surname></persName>
							<email>nenglilim@sutd.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce MGP-VAE (Multi-disentangled-features Gaussian Processes Variational AutoEncoder), a variational autoencoder which uses Gaussian processes (GP) to model the latent space for the unsupervised learning of disentangled representations in video sequences. We improve upon previous work by establishing a framework by which multiple features, static or dynamic, can be disentangled. Specifically we use fractional Brownian motions (fBM) and Brownian bridges (BB) to enforce an inter-frame correlation structure in each independent channel, and show that varying this structure enables one to capture different factors of variation in the data. We demonstrate the quality of our representations with experiments on three publicly available datasets, and also quantify the improvement using a video prediction task. Moreover, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold to improve learning. Our experiments show that the combination of the improved representations with the novel loss function enable MGP-VAE to outperform the baselines in video prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding good representations for data is one of the main goals of unsupervised machine learning <ref type="bibr" target="#b2">[3]</ref>. Ideally, these representations reduce the dimensionality of the data, and are structured such that the different factors of variation in the data get distilled into different channels. This process of disentanglement in generative models is useful as in addition to making the data interpretable, the disentangled representations can also be used to improve downstream tasks such as prediction.</p><p>In prior work on the unsupervised learning of video sequences, a fair amount of effort has been devoted to separating motion, or dynamic information from static content <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. To achieve this goal, typically the model is structured to consist of dual pathways, e.g. using two separate networks to separately capture motion and semantic content <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Such frameworks may be restrictive as it is not immediately clear how to extend them to extract multiple static and dynamic features. Furthermore, in complex videos, there usually is not a clear dichotomy between motion and content, e.g. in videos containing dynamic information ranging over different time-scales.</p><p>In this paper, we address this challenge by proposing a new variational autoencoder, MGP-VAE (Multi-disentangled-features Gaussian Processes Variational AutoEncoder), for the unsupervised learning of video sequences. It utilizes a latent prior distribution that consists of multiple channels of fractional Brownian motions and Brownian bridges. By varying the correlation structure along the time dimension in each channel to pick up different static or dynamic features, while maintaining independence between channels, MGP-VAE is able to learn multiple disentangled factors.</p><p>We then demonstrate quantitatively the quality of our disentanglement representations using a frame prediction task. To improve prediction quality, we also employ a novel geodesic loss function which incorporates the manifold structure of the data to enhance the learning process. <ref type="figure">Fig. 1</ref>. Network illustration of MGP-VAE: The network takes in a video sequence, an array of images, and encodes a Gaussian process latent space representation. The output of the encoder is the mean and covariance matrix of the Gaussian process, after which a sequence of points in R d is sampled where each point represents one frame.</p><p>Our main contributions can be summarized as follows:</p><p>-We use Gaussian processes as the latent prior distribution in our model MGP-VAE to obtain disentangled representations for video sequences. Specifically, we structure the latent space by varying the correlation between video frame distributions so as to extract multiple factors of variation from the data. -We introduce a novel loss function which utilizes the structure of the data manifold to improve prediction. In particular, the actual geodesic distance between the predicted point and its target on the manifold is used instead of squared-Euclidean distance in the latent space. -We test MGP-VAE against various other state-of-the-art models in video sequence disentanglement. We conduct our experiments on three datasets and use a video prediction task to demonstrate quantitatively that our model outperforms the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Disentangled Representation Learning for Video Sequences</head><p>There are several methods for improving the disentanglement of latent representations in generative models. InfoGAN <ref type="bibr" target="#b5">[6]</ref> augments generative adversarial networks <ref type="bibr" target="#b9">[10]</ref> by additionally maximizing the mutual information between a subset of the latent variables and the recognition network output. beta-VAE <ref type="bibr" target="#b12">[13]</ref> adds a simple coefficient (β) to the KL divergence term in the evidence lower bound of a VAE. It has been demonstrated that increasing β beyond unity improves disentanglement, but also comes with the price of increased reconstruction loss <ref type="bibr" target="#b17">[18]</ref>. To counteract this trade-off, both FactorVAE <ref type="bibr" target="#b17">[18]</ref> and β-TCVAE <ref type="bibr" target="#b4">[5]</ref> further decompose the KL divergence term, and identify a total correlation term which when penalized directly encourages factorization in the latent distribution. With regard to the unsupervised learning of sequences, there have been several attempts to separate dynamic information from static content <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, one latent variable is set aside to represent content, separate from another set of variables used to encode dynamic information, and they employ this graphical model for the generation of new video and audio sequences. <ref type="bibr" target="#b30">[31]</ref> proposes MCnet, which uses a convolutional LSTM for encoding motion and a separate CNN to encode static content. The network is trained using standard l 2 loss plus a GAN term to generate sharper frames. DRNet [7] adopts a similar architecture, but uses a novel adversarial loss which penalizes semantic content in the dynamic pathway to learn pose features. <ref type="bibr" target="#b13">[14]</ref> proposes DDPAE, a model with a VAE structure that performs decomposition on video sequences with multiple objects in addition to disentanglement. In their experiments, they show quantitatively that DDPAE outperforms MCnet and DRNet in video prediction on the Moving MNIST dataset.</p><p>Finally, it has been shown that disentangled representation learning can be placed in the framework of nonlinear ICA <ref type="bibr" target="#b16">[17]</ref>, particularly in the context of time-varying data <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">VAEs and Gaussian Process Priors</head><p>In <ref type="bibr" target="#b10">[11]</ref>, a variational auto-encoder which structures its latent space distribution into two components is used for video sequence learning. The "slow" channel extracts static features from the video, and the "fast" channel captures dynamic motion. Our approach is inspired by this method, and we go further by giving a principled way to shape the latent space prior so as to disentangle multiple features.</p><p>Outside of video analysis, VAEs with a Gaussian process prior have also been explored. In <ref type="bibr" target="#b3">[4]</ref>, they propose GPPVAE and train it on image datasets of different objects in various views. The latent representation is a function of an object vector and a view vector, and has a Gaussian prior imposed on it. They also introduce an efficient method to speed up computation of the covariance matrices.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, a deep VAE architecture is used in conjunction with a Gaussian process to model correlations in multivariate time series such that inference can be performed on missing data-points.</p><p>Bayes-Factor VAE <ref type="bibr" target="#b18">[19]</ref> uses a hierarchical Bayesian model to extend the VAE. As with our work, they recognize the limitations of restricting the latent prior distribution to standard normal, but they adopt heavy-tailed distributions as an alternative rather than Gaussian processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Manifold Learning</head><p>Recent work has shown that distances in latent space are not representative of the true distance between data-points <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. Rather, deep generative models learn a mapping from the latent space to the data manifold, a smoothly varying lower-dimensional subset of the original data space.</p><p>In <ref type="bibr" target="#b22">[23]</ref>, closed curves are abstractly represented as points on a shape manifold which incorporates the constraints of scale, rotational and translational invariance. The geodesic distance between points on this manifold is then used to give an improved measure of dissimilarity. In <ref type="bibr" target="#b27">[28]</ref>, several metrics are proposed to quantify the curvature of data manifolds arising from VAEs and GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we review the preliminaries on VAEs and Gaussian processes, and describe our model MGP-VAE in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VAEs</head><p>Variational autoencoders <ref type="bibr" target="#b19">[20]</ref> are powerful generative models which reformulate autoencoders in the framework of variational inference. Given latent variables z ∈ R M , the decoder, typically a neural network, models the generative distribution p θ (x | z), where x ∈ R N denotes the data. Due to the intractability of computing the posterior distribution p(z | x), an approximation q φ (z | x), again parameterized by another neural network called the encoder, is used. Maximizing the log-likelihood of the data can be achieved by maximizing the evidence lower bound</p><formula xml:id="formula_0">E q φ (z|x) log p θ (x, z) q φ (z x) ,<label>(1)</label></formula><p>which is equal to</p><formula xml:id="formula_1">E q φ (z|x) [log p θ (x|z)] − D KL q φ (z|x) p(z) ,<label>(2)</label></formula><p>with p(z) denoting the prior distribution of the latent variables. The negative of the first term in <ref type="formula" target="#formula_1">(2)</ref> is the reconstruction loss, and can be approximated by</p><formula xml:id="formula_2">1 L L l=1 − log p θ x z (l) ,<label>(3)</label></formula><p>where z (l) is drawn (L times) from the latent distribution, although typically only one sample is required in each pass as long as the batch size is sufficiently large <ref type="bibr" target="#b19">[20]</ref>. If p θ x z is modeled to be Gaussian, then this is simply mean-squared error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gaussian Processes</head><p>Given an index set T , {X t ; t ∈ T } is a Gaussian process <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> if for any finite set of indices {t 1 , ..., t n } of T , (X t1 , ..., X tn ) is a multivariate normal random variable. In this paper, we are concerned primarily in the case where T indexes time, i.e. T = R + or Z + , in which case {X t ; t ∈ T } can be uniquely characterized by its mean and covariance functions</p><formula xml:id="formula_3">µ(t) := E [X t ] ,<label>(4)</label></formula><formula xml:id="formula_4">R(s, t) := E [X t X s ] , ∀ s, t ∈ T.<label>(5)</label></formula><p>The following Gaussian processes are frequently encountered in stochastic models, e.g. in financial modeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, and the prior distributions employed in MGP-VAE will be the appropriately discretized versions of these processes. Fractional Brownian Motion (fBM). Fractional Brownian motion <ref type="bibr" target="#b23">[24]</ref> B H t ; t ∈ T is a Gaussian process parameterized by a Hurst parameter H ∈ (0, 1), with mean and covariance functions given by</p><formula xml:id="formula_5">µ(t) = 0,<label>(6)</label></formula><formula xml:id="formula_6">R(s, t) = 1 2 s 2H + t 2H − |t − s| 2H , ∀ s, t ∈ T.<label>(7)</label></formula><p>When</p><formula xml:id="formula_7">H = 1 2 , W t := B 1 2</formula><p>t is standard Brownian motion <ref type="bibr" target="#b11">[12]</ref> with independent increments, i.e. the discrete sequence (W 0 , W 1 , W 2 , . . .) is a simple symmetric random walk where W n+1 ∼ N (W n , 1).</p><p>Most notably, when H = 1 2 , the process is not Markovian. When H &gt; 1 2 , the disjoint increments of the process are positively correlated, whereas when H &lt; 1 2 , they are negatively correlated. We will demonstrate in our experiments how tuning H effects the clustering of the latent code. Brownian Bridge (BB). The Brownian bridge <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> </p><formula xml:id="formula_8">from a ∈ R to b ∈ R on the domain [0, T ] is the Gaussian process defined as X t = a 1 − t T + b t T + W t + t T W T .<label>(8)</label></formula><p>Its mean function is identically zero and its covariance function is given by</p><formula xml:id="formula_9">R(s, t) = min(s, t) − st T , ∀ s, t ∈ T.<label>(9)</label></formula><p>It can be also represented as the solution to the stochastic differential equation <ref type="bibr" target="#b15">[16]</ref> dX</p><formula xml:id="formula_10">t = b − X t T − t dt + dW t , X 0 = a,<label>(10)</label></formula><p>with solution</p><formula xml:id="formula_11">X t = a 1 − t T + b t T + (T − t) t 0 1 T − s dW s .<label>(11)</label></formula><p>From <ref type="formula" target="#formula_8">(8)</ref>, its defining characteristic is that it is pinned at the start and the end such that X 0 = a and X T = b almost surely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MGP-VAE</head><p>For VAEs in the unsupervised learning of static images, the latent distribution p(z) is typically a simple Gaussian distribution, i.e. z ∼ N (0, σ 2 I d ). For a video sequence input (x 1 , . . . x n ) with n frames, we model the corresponding latent code as</p><formula xml:id="formula_12">z = (z 1 , z 2 , . . . , z n ) ∼ N (µ 0 , Σ 0 ), z i ∈ R d ,<label>(12)</label></formula><formula xml:id="formula_13">µ 0 = µ (1) 0 , . . . , µ (d) 0 ∈ R n×d ,<label>(13)</label></formula><formula xml:id="formula_14">Σ 0 = Σ (1) 0 , . . . , Σ (d) 0 ∈ R n×n×d .<label>(14)</label></formula><p>Here d denotes the number of channels, where one channel corresponds to one sampled Gaussian path, and for each channel, µ</p><formula xml:id="formula_15">(i) 0 , Σ (i) 0</formula><p>are the mean and covariance of</p><formula xml:id="formula_16">V + σB H t , t = {1, . . . , n},<label>(15)</label></formula><p>in the case of fBM or</p><formula xml:id="formula_17">A 1 − t n + B t n + σ W t + t n W n<label>(16)</label></formula><p>in the case of Brownian bridge. V , A are initial distributions, and B is the terminal distribution for Brownian bridge. They are set to be standard normal, and we experiment with different values for σ. The covariances can be computed using <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_9">(9)</ref> and are not necessarily diagonal, which enables us to model more complex inter-frame correlations.</p><p>Rewriting z as z <ref type="bibr" target="#b0">(1)</ref> , . . . , z (d) , for each channel i = 1, . . . , d, we sample z</p><formula xml:id="formula_18">(i) ∈ R n ∼ N µ (i) 0 , Σ (i) 0</formula><p>by sampling from a standard normal ξ and computing</p><formula xml:id="formula_19">z (i) = µ (i) 0 + L (i) ξ,<label>(17)</label></formula><p>where L (i) is the lower-triangular Cholesky factor of Σ (i) 0 . The output of the encoder is a mean vector µ 1 and a symmetric positivedefinite matrix Σ 1 , i.e.</p><formula xml:id="formula_20">q(z | x) ∼ N (µ 1 , Σ 1 ),<label>(18)</label></formula><p>and to compute the KL divergence term in (2), we use the formula</p><formula xml:id="formula_21">DKL [q | p] = 1 2 tr Σ −1 0 Σ1 + µ1 − µ0, Σ −1 0 (µ1 − µ0) − k + log det Σ1 det Σ0 .<label>(19)</label></formula><p>Following <ref type="bibr" target="#b12">[13]</ref>, we add a β factor to the KL divergence term to improve disentanglement. We will describe the details of the network architecture of MGP-VAE in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Video Prediction Network and Geodesic Loss Function</head><p>For video prediction, we predict the last k frames of a sequence given the first n − k frames as input. To do so, we employ a simple three-layer MLP (16 units per layer) with ReLU activation which operates in latent space rather than on the actual frame data so as to best utilize the disentangled representations. The first n − k frames are first encoded by a pre-trained MGP-VAE into a sequence of points in latent space. These points are then used as input to the threelayer MLP to predict the next point, which is then passed through MGP-VAE's decoder to generate the frame. This process is then repeated k − 1 more times.</p><p>Given an output z 0 and a target z T , we use the geodesic distance between g(z 0 ) and g(z T ) as the loss function instead of the usual squared-distance z 0 − z T 2 . Here, g : R n×d → M ⊂ R N is the differentiable map from the latent space to the data manifold M which represents the action of the decoder. We use the following algorithm from <ref type="bibr" target="#b26">[27]</ref> to compute the geodesic distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Geodesic Interpolation</head><p>Input: Two points, z 0 , z T ∈ Z; α, the learning rate Output: Discrete geodesic path, z 0 , z 1 , ..., z T ∈ Z Initialize z i as the linear interpolation between z 0 and z T while ∆E zt &gt; do for i ∈ {1, 2, ..., T − 1} do Compute gradient using (21)</p><formula xml:id="formula_22">z i ← z i − α∇ zt E zt end for end while</formula><p>This algorithm finds the minimum of the energy of the path (and thus the geodesic)</p><formula xml:id="formula_23">E zt = 1 2 T i=0 1 δt g(z i+1 ) − g(z i ) 2<label>(20)</label></formula><p>by computing its gradient</p><formula xml:id="formula_24">∇ zt E zt = − (∇g(z i )) T [g(z i+1 ) − 2g(z i ) + g(z i−1 )] .<label>(21)</label></formula><p>Algorithm 1 initializes {z i } to be uniformly-spaced points along the line between z 0 and z T and gradually modifies them until the change in energy falls below a predetermined threshold. At this point, we use z 1 as the target instead of z T as z 1 − z 0 is more representative of the vector in which to update the prediction z 0 such that the geodesic distance is minimized; see <ref type="figure">Figure 3</ref> for an illustration. <ref type="figure">Fig. 3</ref>. Using the geodesic loss function as compared to squared-distance loss for prediction. By setting the target as z1 instead of z4, the model learns more efficiently to predict the next point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present experiments which demonstrate MGP-VAE's ability to disentangle multiple factors of variation in video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Moving MNIST 4 <ref type="bibr" target="#b28">[29]</ref> comprises of moving gray-scale hand-written digits. We generate 60,000 sequences for training, each with a single digit moving in a random direction across frames and bouncing off edges. Coloured dSprites is a modification of the dSprites 5 <ref type="bibr" target="#b12">[13]</ref> dataset. It consists of 2D shapes (square, ellipse, heart) with 6 values for scale and 40 values for orientation. We modify the dataset by adding 3 variations for colour (red, green, blue) and constrain the motion of each video sequence to be simple horizontal or vertical motion.</p><p>For each sequence, the scale is set to gradually increase or decrease a notch in each frame. Similarly, after an initial random selection for orientation, the subsequent frames rotate the shape clockwise or anti-clockwise one notch per frame. The final dataset consists of a total of approximately 100,000 datapoints. Sprites <ref type="bibr" target="#b25">[26]</ref> comprises of around 17,000 animations of synthetically rendered animated caricatures. There are 7 attributes: body type, sex, hair type, arm type, armor type, greaves type, and weapon type, with a total of 672 unique characters. In each animation, the physical traits of the sprite remain constant while the pose (hand movement, leg movement, orientation) is varied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture and Implementation Details</head><p>For the encoder, we use 8 convolutional layers with batch normalization between each layer. The number of filters begins with 16 in the first layer and increases to a maximum of 128 in the last layer. An MLP layer follows the last layer, and this is followed by another batch normalization layer. Two separate MLP layers are then applied, one which outputs a lower-triangular matrix which represents the Cholesky factor of the covariance matrix of q(z | x) and the other outputs the mean vector.</p><p>For the decoder, we have 7 deconvolutional layers, with batch normalization between each layer. The first layer begins with 64 filters and this decreases to 16 filters by the last layer. We use ELU for the activation functions between all layers to ensure differentiability, with the exception of the last layer, where we use a hyperbolic tangent function. <ref type="table" target="#tab_0">Table 1</ref> lists the settings for the manually tuned hyperparameters in the experiments. All channels utilizing Brownian bridge (BB) are conditioned to start at −2 and end at 2.  <ref type="figure">Figure 4</ref> shows the results from swapping latent channels in the Moving MNIST dataset, where we see that channel 1 (fBM(H = 0.1)) captures the digit identity, whereas channel 2 (fBM(H = 0.9)) captures the motion. <ref type="figure">Fig. 4</ref>. Results from swapping latent channels in Moving MNIST; channel 1 (fBM(H = 0.1)) captures digit identity; channel 2 (fBM(H = 0.9)) captures motion. <ref type="figure">Figure 5</ref> gives a visualization of the latent space (here we use two channels with H = 0.1 and two channels with H = 0.9). In our experiments, we observe that fBM channels with H = 0.9 are able to better capture motion in comparison to setting H = 0.5 (simple-symmetric random walk, cf. <ref type="bibr" target="#b10">[11]</ref>). We hypothesize that shifting the value of H away from that of the static channel sets the distributions apart and allows for better disentanglement.    Discussion. The disentanglement results were the best for Moving MNIST, where we achieved full disentanglement in more than 95% of the cases. We were also able to consistently disentangle three or more features in Coloured dSprites and Sprites, but disentanglement of four or more features occurred less frequently due to their complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Analysis</head><p>We found that including more channels than the number of factors of variation in the dataset improved disentanglement, even as the extra channels did not necessarily encode anything new. For the Coloured dSprites and Sprites dataset, we originally experimented with different combinations of fBMs (with varying H) and Brownian bridges, but found that simply using 4-5 channels of Brownian bridges gave comparable results. We observed that with complex videos not easily separated into static or dynamic content, incorporating multiple Brownian bridge channels each with different start and end points led to good disentanglement. We hypothesize that anchoring the start and end points of the sequence at various places in latent space "spreads out" and improves the representation.</p><p>Finally, we also tested other Gaussian processes such as the Ornstein-Ulenbeck process <ref type="bibr" target="#b24">[25]</ref> but as the results were not satisfactory, we shall defer a more detailed investigation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluating Disentanglement Quality</head><p>We first evaluate the disentangled representations by computing the mean average precision of a k-nearest neighbor classification over labeled attributes in the Coloured dSprites and Sprites datasets.   <ref type="bibr" target="#b13">[14]</ref>.</p><p>Next, we use a non-synthetic benchmark in the form of a video prediction task to illustrate the improvement in the quality of MGP-VAE's disentangled representations. We train a prediction network with the geodesic loss function as outlined in Section 3.4, where we set the number of interpolated points to be four. In addition, to speed up the algorithm for faster training, we ran the loop in Algorithm 1 for a fixed number of iterations (10-15) instead of until convergence.</p><p>We compute the pixel-wise mean-squared-error and binary cross-entropy between the predicted k frames and the actual last k frames, given the first n − k frames as input (n is set to 8 for Moving MNIST and Coloured dSprites, and set to 7 for Sprites). <ref type="table" target="#tab_3">Tables 3 and 4</ref> below summarize the results. The results show that MGP-VAE 9 , even without using the geodesic loss function, outperforms the other models. Using the geodesic loss functions further lowers MSE and BCE. DDPAE, a state-of-the-art model in video disentanglement, achieves comparable results, although we note that we had to train the model considerably longer on the Coloured dSprites and Sprites datasets as compared to Moving MNIST to get the same performance.</p><p>Using the geodesic loss function during the training of the prediction network also leads to qualitatively better results. <ref type="figure" target="#fig_5">Figure 8</ref> below shows that in a sequence with large MSE and BCE losses, the predicted point can generate an image frame which differs considerably from the actual image frame when the normal loss function is used. This is rectified with the geodesic loss function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce MGP-VAE, a variational autoencoder for obtaining disentangled representations from video sequences in an unsupervised manner. MGP-VAE uses Gaussian processes, such as fractional Brownian motion and Brownian bridge, as a prior distribution for the latent space. We demonstrate that different parameterizations of these Gaussian processes allow one to extract different static and time-varying features from the data. After training the encoder which outputs a disentangled representation of the input, we demonstrate the efficiency of the latent code by using it as input to a MLP for video prediction. We run experiments on three different datasets and demonstrate that MGP-VAE outperforms the baseline models in video frame prediction. To further improve the results, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold. This contribution is independent of MGP-VAE, and we believe it can be used to improve video prediction in other models as well.</p><p>For future work, we will continue to experiment with various combinations of Gaussian processes. In addition, enhancing our approach with more recent methods such as FactorVAE, β-TCVAE, or independent subspace analysis <ref type="bibr" target="#b29">[30]</ref> may lead to further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Sample paths for various Gaussian processes. Top-left: Brownian bridge from -2 to 2; top-right: fBM with H = 0.1; bottom-left: standard Brownian motion; bottomright: fBM with H = 0.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9 Fig. 5 .</head><label>95</label><figDesc>Latent space visualization of fBM channels for 6 videos. Each point represents one frame of a video. The more tightly clustered points in (a) capture digit identity whereas the scattered points in (b) capture motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Results from swapping latent channels in Sprites; channel 1 captures hair type, channel 2 captures armor type, channel 3 captures weapon type, and channel 4 captures body orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figures 6 and 7</head><label>7</label><figDesc>show the results from swapping latent channels in the Sprites dataset and Coloured dSprites dataset respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Results from swapping latent channels in Coloured dSprites; channel 2 captures shape, channel 3 captures scale, channel 4 captures orientation and position, and channel 5 captures color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative improvements from using the geodesic loss function: Left: without geodesic loss function; Right: with geodesic loss function; Top row: original video; Bottom row: video with the predicted last frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Hyperparameter settings for all datasets</figDesc><table><row><cell></cell><cell>Moving MNIST</cell><cell>Coloured dSprites</cell><cell>Sprites</cell></row><row><cell>Gaussian processes</cell><cell>Channel 1: fBM (H = 0.1) Channel 2: fBM (H = 0.9)</cell><cell cols="2">5 Channels of BBs 5 Channels of BBs</cell></row><row><cell>σ</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>β</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell><cell>0.008</cell><cell>0.010</cell></row><row><cell>No. of epochs</cell><cell>200</cell><cell>120</cell><cell>150</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>mAP values (%) for Coloured dSprites and Sprites VAE 96.2 94.0 77.9 76.4 72.8 83.4 80.3 71.8 76.8 82.3 79.9 79.8 78.5</figDesc><table><row><cell>Model</cell><cell>Coloured dSprites Shape Color Scale x-Pos y-Pos Avg. Gender Skin Vest Hair Arm Leg Avg. Sprites</cell></row><row><cell>MCnet</cell><cell>95.6 94.0 69.2 69.7 70.2 79.7 78.8 70.8 76.6 80.2 78.2 70.7 75.9</cell></row><row><cell>DRNet</cell><cell>95.7 94.8 69.6 72.4 70.6 80.6 80.5 70.8 77.0 78.6 79.7 71.4 76.3</cell></row><row><cell>DDPAE</cell><cell>95.6 94.2 70.3 71.6 72.4 80.8 79.8 72.0 77.4 79.3 78.3 74.6 76.9</cell></row><row><cell>MGP-</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 shows</head><label>2</label><figDesc>that our model is able to capture multiple features more effectively than the baselines MCnet 6 [31], DRNet 7 [7] and DDPAE 8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Prediction results on Moving MNIST</figDesc><table><row><cell></cell><cell>k = 1</cell><cell>k = 2</cell></row><row><cell>Model</cell><cell cols="2">MSE BCE MSE BCE</cell></row><row><cell>MCnet [31]</cell><cell cols="2">50.1 248.2 91.1 595.5</cell></row><row><cell>DRNet [7]</cell><cell cols="2">45.2 236.7 86.3 586.7</cell></row><row><cell>DDPAE [14]</cell><cell cols="2">35.2 201.6 75.6 556.2</cell></row><row><cell>Grathwohl, Wilson [11]</cell><cell cols="2">59.3 291.2 112.3 657.2</cell></row><row><cell>MGP-VAE</cell><cell cols="2">25.4 198.4 72.2 554.2</cell></row><row><cell cols="3">MGP-VAE (with geodesic loss) 18.5 185.1 69.2 531.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Last-frame (k = 1) prediction results for Coloured dSprites and Sprites</figDesc><table><row><cell>Dataset</cell><cell cols="2">Coloured dSprites</cell><cell>Sprites</cell></row><row><cell>Model</cell><cell>MSE</cell><cell>BCE</cell><cell>MSE BCE</cell></row><row><cell>MCnet [31]</cell><cell>20.2</cell><cell>229.5</cell><cell>100.3 2822.6</cell></row><row><cell>DRNet [7]</cell><cell>15.2</cell><cell>185.2</cell><cell>94.4 2632.1</cell></row><row><cell>DDPAE [14]</cell><cell>12.6</cell><cell>163.1</cell><cell>75.4 2204.1</cell></row><row><cell>MGP-VAE</cell><cell>6.1</cell><cell>85.2</cell><cell>68.8 1522.5</cell></row><row><cell cols="2">MGP-VAE (with geodesic loss) 4.5</cell><cell>70.3</cell><cell>61.6 1444.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.cs.toronto.edu/ nitish/unsupervised video 5 https://github.com/deepmind/dsprites-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/rubenvillegas/iclr2017mcnet 7 https://github.com/ap229997/DRNET 8 https://github.com/jthsieh/DDPAE-video-prediction 9 https://github.com/SUTDBrainLab/MGP-VAE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Space Oddity: on the Curvature of Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pricing under rough volatility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Friz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gatheral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Finance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="904" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<title level="m">Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussian Process Prior Variational Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">P</forename><surname>Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isolating Sources of Disentanglement in VAEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<title level="m">-foGAN: Interpretable Representation Learning by Information Maximizing Generative Adversial Nets</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference on Neural Information Processing Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Disentangled Representations from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04155</idno>
		<title level="m">Multivariate Time Series Imputation with Variational Autoencoders</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Monte-Carlo Methods in Financial Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Glasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Disentangling Space and Time in Video with Hierarchical Variational Auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hitsuda</surname></persName>
		</author>
		<title level="m">Gaussian Processes</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to Decompose and Disentangle Representations for Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3765" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Brownian Motion and Stochastic Calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Shreve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational Autoencoders and Nonlinear ICA: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>Chiappa, S., Calandra, R.</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics. Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR, Online</publisher>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling by Factorising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02820</idno>
		<title level="m">Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kühnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07632</idno>
		<title level="m">Latent Space Non-Linear Statistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangled Sequential Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding distinctive shape features for automatic hematoma classification in head CT images from traumatic brain injuries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fractional brownian motions, fractional noises and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Mandelbrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Van Ness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="437" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the theory of Brownian motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Ornstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Uhlenbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="823" to="841" />
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Deep Visual Analogy-Making. In: Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Riemannian Geometry of Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW)</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uppal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06964</idno>
		<title level="m">Geometry of Deep Generative Models for Disentangled Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Video Representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Independent Subspace Analysis for Unsupervised Learning of Disentangled Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuehmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics. Proceedings of Machine Learning Research</title>
		<editor>Chiappa, S., Calandra, R.</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics. Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR, Online</publisher>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decomposing Motion and Content for Natural Video Sequence Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
