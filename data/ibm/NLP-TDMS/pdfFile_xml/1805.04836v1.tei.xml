<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building Language Models for Text with Named Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Rizwan</forename><surname>Parvez</surname></persName>
							<email>rizwan@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California Los Angeles</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
							<email>rayb@cs.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
							<email>saikatc@virginia.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building Language Models for Text with Named Entities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a discriminative language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than the stateof-the-art language models. 7 http://dbpedia.org/page/Lionel Messi</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language model is a fundamental component in Natural Language Processing (NLP) and it supports various applications, including document generation <ref type="bibr" target="#b36">(Wiseman et al., 2017)</ref>, text autocompletion <ref type="bibr" target="#b1">(Arnold et al., 2017)</ref>, spelling correction <ref type="bibr" target="#b3">(Brill and Moore, 2000)</ref>, and many others. Recently, language models are also successfully used to generate software source code written in programming languages like Java, C, etc. <ref type="bibr" target="#b13">(Hindle et al., 2016;</ref><ref type="bibr" target="#b37">Yin and Neubig, 2017;</ref><ref type="bibr" target="#b12">Hellendoorn and Devanbu, 2017;</ref><ref type="bibr" target="#b30">Rabinovich et al., 2017)</ref>. These models have improved the language generation tasks to a great extent, e.g., <ref type="bibr" target="#b23">(Mikolov et al., 2010;</ref><ref type="bibr">Galley et al., 2015)</ref>. However, while generating text or code with a large number of named entities (e.g., different variable names in source code), these models often fail to predict the entity names properly due to their wide variations. For instance, consider building a language model for generating recipes. There are numerous similar, yet slightly different cooking ingredients (e.g., olive oil, canola oil, grape oil, etc.-all are different varieties of oil). Such diverse vocabularies of the ingredient names hinder the language model from predicting them properly.</p><p>To address this problem, we propose a novel language model for texts with many entity names. Our model learns the probability distribution over all the candidate words by leveraging the entity type information. For example, oil is the type for named entities like olive oil, canola oil, grape oil, etc. 1 Such type information is even more prevalent for source code corpus written in statically typed programming languages <ref type="bibr" target="#b5">(Bruce, 1993)</ref>, since all the variables are by construct associated with types like integer, float, string, etc.</p><p>Our model exploits such deterministic type information of the named entities and learns the probability distribution over the candidate words by decomposing it into two sub-components: (i) Type Model. Instead of distinguishing the individual names of the same type of entities, we first consider all of them equal and represent them by their type information. This reduces the vocab size to a great extent and enables to predict the type of each entity more accurately. (ii) Entity Composite Model. Using the entity type as a prior, we learn the conditional probability distribution of the actual entity names at inference time. We depict our model in <ref type="figure">Fig. 1</ref>.</p><p>To evaluate our model, we create two benchmark datasets that involve many named entities. One is a cooking recipe corpus 2 where each recipe contains a number of ingredients which are cate-1 Entity type information is often referred as category information or group information. In many applications, such information can be easily obtained by an ontology or by a pre-constructed entity gorized into 8 super-ingredients (i.e., type); e.g., "proteins", "vegetables", "fruits", "seasonings", "grains", etc. Our second dataset comprises a source code corpus of 500 open-source Android projects collected from GitHub. We use an Abstract Syntax Tree (AST) (Parsons, 1992) based approach to collect the type information of the code identifiers. Our experiments show that although state-ofthe-art language models are, in general, good to learn the frequent words with enough training instances, they perform poorly on the entity names. A simple addition of type information as an extra feature to a neural network does not guarantee to improve the performance because more features may overfit or need more model parameters on the same data. In contrast, our proposed method significantly outperforms state-of-the-art neural network based language models and also the models with type information added as an extra feature.</p><p>Overall, followings are our contributions:</p><p>• We analyze two benchmark language corpora where each consists of a reasonable number of entity names. While we leverage an existing corpus for recipe, we curated the code corpus. For both datasets, we created auxiliary corpora with entity type information. All the code and datasets are released. 3 • We design a language model for text consisting of many entity names. The model learns to mention entities names by leveraging the entity type information.</p><p>• We evaluate our model on our benchmark datasets and establish a new baseline performance which significantly outperforms stateof-the-art language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Background</head><p>Class Based Language Models. Building language models by leveraging the deterministic or probabilistic class properties of the words (a.k.a, class-based language models) is an old idea <ref type="bibr" target="#b4">(Brown et al., 1992;</ref><ref type="bibr" target="#b10">Goodman, 2001)</ref>. However, the objective of our model is different from the existing class-based language models. The key differences are two-folds: 1) Most existing class-based language models <ref type="bibr" target="#b4">(Brown et al., 1992;</ref><ref type="bibr" target="#b29">Pereira et al., 1993;</ref><ref type="bibr" target="#b25">Niesler et al., 1998;</ref><ref type="bibr" target="#b2">Baker and McCallum, 1998;</ref><ref type="bibr" target="#b10">Goodman, 2001;</ref><ref type="bibr" target="#b21">Maltese et al., 2001)</ref> are generative n-gram models whereas ours is a discriminative language model based on neural networks. The modeling principle and assumptions are very different. For example, we cannot calculate the conditional probability by statistical occurrence counting as these papers did. 2) Our approaches consider building two models and perform joint inference which makes our framework general and easy to extend. In Section 4, we demonstrate that our model can be easily incorporated with the state-of-art language model. The closest work in this line is hierarchical neural language models <ref type="bibr" target="#b24">(Morin and Bengio, 2005)</ref>, which model language with word clusters. However, their approaches do not focus on dealing with named entities as our model does. A recent work <ref type="bibr" target="#b15">(Ji et al., 2017)</ref> studied the problem of building up a dynamic representation of named entity by updating the representation for every contextualized mention of that entity. Nonetheless, their approach does not deal with the sparsity issue and their goal is different from ours.</p><p>Language Models for Named Entities. In some generation tasks, recently developed language models address the problem of predict-ing entity names by copying/matching the entity names from the reference corpus. For example, <ref type="bibr" target="#b35">Vinyals et al. (2015)</ref> calculates the conditional probability of discrete output token sequence corresponding to positions in an input sequence. <ref type="bibr" target="#b11">Gu et al. (2016)</ref> develops a seq2seq alignment mechanism which directly copies entity names or long phrases from the input sequence. <ref type="bibr" target="#b36">Wiseman et al. (2017)</ref> generates document from structured table like basketball statistics using copy and reconstruction method as well. Another related code generation model <ref type="bibr" target="#b37">(Yin and Neubig, 2017)</ref> parses natural language descriptions into source code considering the grammar and syntax in the target programming language (e.g., Python). <ref type="bibr" target="#b17">Kiddon et al. (2016)</ref> generates recipe for a given goal, and agenda by making use of items on the agenda. While generating the recipe it continuously monitors the agenda coverage and focus on increasing it. All of them are sequence-to-sequence learning or end-to-end systems which differ from our general purpose (free form) language generation task (e.g., text auto-completion, spelling correction).</p><p>Code Generation. The way developers write codes is not only just writing a bunch of instructions to run a machine, but also a form of communication to convey their thought. As observed by <ref type="bibr" target="#b20">Donald E. Knuth (Knuth, 1992)</ref>, "The practitioner of literate programming can be regarded as an essayist, whose main concern is exposition and excellence of style. Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what such variable means." Such comprehensible software corpora show surprising regularity <ref type="bibr" target="#b31">(Ray et al., 2015;</ref><ref type="bibr" target="#b7">Gabel and Su, 2010)</ref> that is quite similar to the statistical properties of natural language corpora and thus, amenable to large-scale statistical analysis <ref type="bibr" target="#b14">(Hindle et al., 2012)</ref>. <ref type="bibr" target="#b0">(Allamanis et al., 2017)</ref> presented a detailed survey.</p><p>Although similar, source code has some unique properties that differentiate it from natural language. For example, source code often shows more regularities in local context due to common development practices like copy-pasting <ref type="bibr" target="#b9">(Gharehyazie et al., 2017;</ref><ref type="bibr" target="#b18">Kim et al., 2005)</ref>. This property is successfully captured by cache based language models <ref type="bibr" target="#b12">(Hellendoorn and Devanbu, 2017;</ref><ref type="bibr" target="#b34">Tu et al., 2014)</ref>. Code is also less ambiguous than natural language so that it can be interpreted by a compiler. The constraints for generating cor-rect code is implemented by combining language model and program analysis technique <ref type="bibr" target="#b33">(Raychev et al., 2014)</ref>. Moreover, code contains open vocabulary-developers can coin new variable names without changing the semantics of the programs. Our model aims to addresses this property by leveraging variable types and scope.</p><p>LSTM Language Model. In this paper, we use LSTM language model as a running example to describe our approach. Our language model uses the LSTM cells to generate latent states for a given context which captures the necessary features from the text. At the output layer of our model, we use Softmax probability distribution to predict the next word based on the latent state. <ref type="bibr" target="#b22">Merity et al. (2017)</ref> is a LSTM-based language model which achieves the state-of-the-art performance on Penn Treebank (PTB) and WikiText-2 (WT2) datasets. To build our recipe language model we use this as a blackbox and for our code generation task we use the simple LSTM model both in forward and backward direction. A forward directional LSTM starts from the beginning of a sentence and goes from left to right sequentially until the sentence ends, and vice versa. However, our approach is general and can be applied with other types of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Probabilistic Model for Text with Named Entities</head><p>In this section, we present our approach to build a language model for text with name entities. Given previous contextw = {w 1 , w 2 , .., w t−1 }, the goal of a language model is to predict the probability of next word P (w t |w) at time step t, where w t ∈ V text and V text is a fixed vocabulary set. Because the size of vocabulary for named entities is large and named entities often occur less frequently in the training corpus, the language model cannot generate these named entities accurately. For example, in our recipe test corpus the word "apple" occurs only 720 times whereas any kind of "fruits" occur 27,726 times. Existing approaches often either only generate common named entities or omit entities when generating text <ref type="bibr" target="#b16">(Jozefowicz et al., 2016)</ref>.</p><p>To overcome this challenge, we propose to leverage the entity type information when modeling text with many entities. We assume each entity is associated with an entity type in a finite set of categories S = {s 1 , s 2 , .., s i , .., s k }. Given a word w, s(w) reflects its entity type. If the word is a named entity, then we denote s(w) ∈ S; otherwise the type function returns the words itself (i.e, s(w) = w). To simplify the notations, we use s(w) ∈ S to represent the case where the word is not an entity. The entity type information given by s(w) is an auxiliary information that we can use to improve the language model. We use s(w) to represent the entity type information of all the words in contextw and use w to represent the current word w t . Below, we show that a language model for text with typed information can be decomposed into the following two models: 1) a type model θ t that predicts the entity type of the next word and 2) an entity composite model θ v that predicts the next word based on a given entity type.</p><p>Our goal is to model the probability of next word w given previous contextw:</p><formula xml:id="formula_0">P (w|w; θ t , θ v ) ,<label>(1)</label></formula><p>where θ t and θ v are the parameters of the two aforementioned models. As we assume the typed information is given on the data, Eq. (1) is equivalent to</p><formula xml:id="formula_1">P (w, s(w)|w, s(w); θ t , θ v ) .<label>(2)</label></formula><p>A word can be either a named entity or not; therefore, we consider the following two cases.</p><p>Case 1: next word is a named entity. In this case, Eq. (2) can be rewritten as</p><formula xml:id="formula_2">P (s(w) = s|w, s(w); θ t , θ v ) × P (w|w, s(w), s(w) = s; θ v , θ t )<label>(3)</label></formula><p>based on the rules of conditional probability. We assume the type of the next token s(w) can be predicted by a model θ t using information of s(w), and we can approximate the first term in Eq.</p><p>(3)</p><formula xml:id="formula_3">P (s(w)|w, s(w); θ t , θ v ) ≈ P (s(w)|s(w), θ t )</formula><p>(4) Similarly, we can make a modeling assumption to simplify the second term as</p><formula xml:id="formula_4">P (w|w, s(w), s(w), θ v , θ t ) ≈ P (w|w, s(w), s(w), θ v ).<label>(5)</label></formula><p>Case 2: next word is not a named entity. In this case, we can rewrite Eq.</p><p>(2) to be</p><formula xml:id="formula_5">P (s(w) ∈ S|w, s(w), θ t ) × P (w|w, s(w), s(w) ∈ S, θ v ) .<label>(6)</label></formula><p>The first term in Eq. (6) can be modeled by</p><formula xml:id="formula_6">1 − s∈S P (s(w) = s|s(w), θ t ),</formula><p>which can be computed by the type model 4 . The second term can be again approximated by <ref type="formula" target="#formula_4">(5)</ref> and further estimated by an entity composition model.</p><p>Typed Language Model. Combine the aforementioned equations, the proposed language model estimates P (w|w; θ t , θ v ) by</p><formula xml:id="formula_7">P (w|w, s(w), s(w), θ v )× P (s(w)|s(w), θ t ) if s(w) ∈ S (1− s∈S P (s(w) = s|s(w), θ t )) if s(w) ∈ S<label>(7)</label></formula><p>The first term can be estimated by an entity composite model and the second term can be estimated by a type model as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Type model</head><p>The type model θ t estimates the probability of P (s(w)|s(w), θ t ). It can be viewed as a language model builds on a corpus with all entities replaced by their type. That is, assume the training corpus consists of x = {w 1 , w 2 , .., w n }. Using the type information provided in the auxiliary source, we can replace each word w with their corresponding type s(w) and generate a corpus of T = {s(w i ), s(w 2 ), .., s(w n )}. Note that if w i is not an named entity (i.e., s(w) ∈ S), s(w) = w and the vocabulary on T is V text ∪ S. 5 Any language modeling technique can be used in modeling the type model on the modified corpus T . In this paper, we use the state-of-the-art model for each individual task. The details will be discussed in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity Composite Model</head><p>The entity composite model predicts the next word based on modeling the conditional probability P (w|w, s(w), s(w), θ v ), which can be derived by</p><formula xml:id="formula_8">P (w|w, s(w); θ v ) ws∈Ω(s(w)) P (w s |w, s(w); θ v ) ,<label>(8)</label></formula><p>where Ω(s(w)) is the set of words of the same type with w.</p><p>To model the types of context word s(w) in P (w|w, s(w); θ v ), we consider learning a type embedding along with the word embedding by augmenting each word vector with a type vector when learning the underlying word representation. Specifically, we represent each word w as a vector of</p><formula xml:id="formula_9">[v w (w) T ; v t (s(w)) T ] T , where v w (·)</formula><p>and v t (·) are the word vectors and type vectors learned by the model from the training corpus, respectively. Finally, to estimate Eq. (8) using θ v , when computing the Softmax layer, we normalize over only words in Ω(s(w)). In this way, the conditional probability P (w|w, s(w), s(w), θ v ) can be derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Inference Strategies</head><p>We learn model parameters θ t and θ v independently by training two language models type model and entity composite model respectively. Given the context of type, type model predicts the type of the next word. Given the context and the type information of the all candidate words, entity composite model predicts the conditional actual word (e.g., entity name) as depicted in <ref type="figure">Fig  1.</ref> At inference time the generated probabilities from these two models are combined according to conditional probability (i.e., Eq. <ref type="formula" target="#formula_7">(7)</ref>) which gives the final probability distribution over all candidate words 6 .</p><p>Our proposed model is flexible to any language model, training strategy, and optimization. As per our experiments, we use ADAM stochastic minibatch optimization (Kingma and Ba, 2014). In Algorithm 1, we summarize the language generation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our proposed model on two different language generation tasks where there exist a lot of entity names in the text. In this paper, we release all the codes and datasets. The first task is recipe generation. For this task, we analyze a cooking recipe corpus. Each instance in this corpus is an individual recipe and consists of many ingredi-6 While calculating the final probability distribution over all candidate words, with our joint inference schema, a strong state-of-art language model, without the type information, itself can work sufficiently well and replace the entity composite model. Our experiments using <ref type="bibr" target="#b22">(Merity et al., 2017)</ref> in Section 4.1 validate this claim. Compute P (w|w, s(w), s(w), θ v ) 10 Compute P (w|w; θ t , θ v ) using Eq. <ref type="formula" target="#formula_7">(7)</ref> 11 end 12 W i ← argmax w P (w|w; θ t , θ v ) 13 end ents'. Our second task is code generation. We construct a Java code corpus where each instance is a Java method (i.e., function). These tasks are challenging because they have the abundance of entity names and state-of-the-art language models fail to predict them properly as a result of insufficient training observations. Although in this paper, we manually annotate the types of the recipe ingredients, in other applications it can be acquired automatically. For example: in our second task of code generation, the types are found using Eclipse JDT framework. In general, using DBpedia ontology (e.g., "Berlin" has an ontology "Location"), Wordnet hierarchy (e.g., "Dog" is an "Animal"), role in sports (e.g., "Messi" plays in "Forward"; also available in DBpedia 7 ), Thesaurus (e.g., "renal cortex", "renal pelvis", "renal vein", all are related to "kidney"), Medscape (e.g., "Advil" and "Motrin" are actually "Ibuprofen"), we can get the necessary type information. As for the applications where the entity types cannot be extracted automatically by these frameworks (e.g., recipe ingredients), although there is no exact strategy, any reasonable design can work. Heuristically, while annotating manually in our first task, we choose the total number of types in such a way that each type has somewhat balanced (similar) size. We use the same dimensional word embedding (400 for recipe corpus, 300 for code corpus) to represent both of the entity name (e.g., "apple") and their entity type (e.g., "fruits") in all the models. Note that in our approach, the type model only replaces named entities with entity type when it generates next word. If next word is not a named entity, it will behave like a regular language model. Therefore, we set both models with the same dimensionality. Accordingly, for the entity composite model which takes the concatenation of the entity name and the entity type, the concatenated input dimension is 800 and 600 respectively for recipe and code corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recipe Generation</head><p>Recipe Corpus Pre-processing: Our recipe corpus collection is inspired by <ref type="bibr" target="#b17">(Kiddon et al., 2016)</ref>. We crawl the recipes from "Now Youre Cooking! Recipe Software" 8 . Among more than 150,000 recipes in this dataset, we select similarly structured/formatted (e.g, title, blank line then ingredient lists followed by a recipe) 95,786 recipes.</p><p>We remove all the irrelevant information (e.g., author's name, data source) and keep only two information: ingredients and recipes. We set aside the randomly selected 20% of the recipes for testing and from the rest, we keep randomly selected 80% for the training and 20% for the development. Similar to <ref type="bibr" target="#b17">(Kiddon et al., 2016)</ref>, we preprocess the dataset and filter out the numerical values, special tokens, punctuation, and symbols. 9 Quantitatively, the data we filter out is negligible; in terms of words, we keep 9,994,365 words out of 10,231,106 and the number of filter out words is around ∼2%. We release both of the raw and cleaned data for future challenges. As the ingredients are the entity names in our dataset, we process it separately to get the type information.</p><p>Retrieving Ingredient Type: As per our type model, for each word w, we require its type s(w). We only consider ingredient type for our experiment. First, we tokenize the ingredients and consider each word as an ingredient. We manually classify the ingredients into 8 super-ingredients: "fruits", "proteins", "sides", "seasonings", "vegetables", "dairy", "drinks", and "grains". Some-times, ingredients are expressed using multiple words; for such ingredient phrase, we classify each word in the same group (e.g., for "boneless beef" both "boneless" and "beef" are classified as "proteins"). We classify the most frequent 1,224 unique ingredients, 10 which cover 944,753 out of 1,241,195 mentions (top 76%) in terms of frequency of the ingredients. In our experiments, we omit the remainder 14,881 unique ingredients which are less frequent and include some misspelled words. The number of unique ingredients in the 8 super ingredients is <ref type="bibr">110, 316, 140, 180, 156, 80, 84, and 158</ref> respectively. We prepare the modified type corpus by replacing each actual ingredient's name w in the original recipe corpus by the type (i.e., super ingredients s(w)) to train the type model.</p><p>Recipe Statistics: In our corpus, the total number of distinct words in vocabulary is 52,468; number of unique ingredients (considering splitting phrasal ingredients also) is 16,105; number of tokens is 8,716,664. In number of instances train/dev/test splits are <ref type="bibr">61,302/15,326/19,158</ref>. The average instance size of a meaningful recipe is 91 on the corpus.</p><p>Configuration: We consider the state-of-the art LSTM-based language model proposed in <ref type="bibr" target="#b22">(Merity et al., 2017)</ref> as the basic component for building the type model, and entity composite model. We use 400 dimensional word embedding as described in Section 4. We train the embedding for our dataset. We use a minibatch of 20 instances while training and back-propagation through time value is set to 70. Inside of this <ref type="bibr" target="#b22">(Merity et al., 2017)</ref> language model, it uses 3 layered LSTM architecture where the hidden layers are 1150 dimensional and has its own optimization and regularization mechanism. All the experiments are done using PyTorch and Python 3.5.</p><p>Baselines: Our first baseline is ASGD Weight-Dropped LSTM (AWD LSTM) <ref type="bibr" target="#b22">(Merity et al., 2017)</ref>, which we also use to train our models (see <ref type="bibr">'Configuration' in 4.1)</ref>. This model achieves the state-of-the-art performance on benchmark Penn Treebank (PTB), and WikiText-2 (WT2) language corpus. Our second baseline is the same language model (AWD LSTM) with the type information added as an additional feature (i.e., same as entity composite model).  <ref type="bibr" target="#b22">(Merity et al., 2017)</ref>. Our second baseline is the same language model (AWD LSTM) with the type information added as an additional feature for each word.</p><p>Results of Recipe Generation. We compare our model with the baselines using perplexity metric-lower perplexity means the better prediction. <ref type="table">Table 1</ref> summarizes the result. The 3 rd row shows that adding type as a simple feature does not guarantee a significant performance improvement while our proposed method significantly outperforms both baselines and achieves 52.2% improvement with respect to baseline in terms of perplexity. To illustrate more, we provide an example snippet of our test corpus: "place onion and ginger inside chicken . allow chicken to marinate for hour .". Here, for the last mention of the word "chicken", the standard language model assigns probability 0.23 to this word, while ours assigns probability 0.81.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Code Generation</head><p>Code Corpus Pre-processing. We crawl 500 Android open source projects from GitHub 11 . GitHub is the largest open source software forge where anyone can contribute <ref type="bibr" target="#b32">(Ray et al., 2014)</ref>. Thus, GitHub also contains trivial projects like student projects, etc. In our case, we want to study the coding practices of practitioners so that our model can learn to generate quality code. To ensure this, we choose only those Android projects from GitHub that are also present in Google Play Store 12 . We download the source code of these projects from GitHub using an off the shelf tool GitcProc <ref type="bibr" target="#b6">(Casalnuovo et al., 2017)</ref>.</p><p>Since real software continuously evolves to cater new requirements or bug fixes, to make our modeling task more realistic, we further study dif-ferent project versions. We partition the codebase of a project into multiple versions based on the code commit history retrieved from GitHub; each version is taken at an interval of 6 months. For example, anything committed within the first six months of a project will be in the first version, and so on. We then build our code suggestion task mimicking how a developer develops code in an evolving software-based on the past project history, developers add new code. To implement that we train our language model on past project versions and test it on the most recent version, at method granularity. However, it is quite difficult for any language model to generate a method from the scratch if the method is so new that even the method signature (i.e., method declaration statement consisting of method name and parameters) is not known. Thus, during testing, we only focus on the methods that the model has seen before but some new tokens are added to it. This is similar to the task when a developer edits a method to implement a new feature or bug-fix.</p><p>Since we focus on generating the code for every method, we train/test the code prediction task at method level-each method is similar to a sentence and each token in the method is equivalent to a word. Thus, we ignore the code outside the method scope like global variables, class declarations, etc. We further clean our dataset by removing user-defined "String" tokens as they increase the diversity of the vocabularies significantly, although having the same type. For example, the word sequences "Hello World!" and "Good wishes for ACL2018!!" have the same type java.lang.String.VAR.</p><p>Retrieving Token Type: For every token w in a method, we extract its type information s(w). A token type can be Java built-in data types (e.g., int, double, float, boolean etc.,) or user or framework defined classes (e.g., java.lang.String, io.segment.android.flush.FlushThread etc.). We extract such type information for each token by parsing the Abstract Syntax Tree (AST) of the source code 13 . We extract the AST type information of each token using Eclipse JDT framework 14 . Note that, language keywords like for, if, etc. are not associated with any type. Next, we prepare the type corpus by replacing the variable names with corresponding type information. For instance, if variable var is of type java.lang.Integer, in the type corpus we replace var by java.lang.Integer. Since multiple packages might contain classes of the same name, we retain the fully qualified name for each type 15 .</p><p>Code Corpus Statistics: In our corpus, the total number of distinct words in vocabulary is 38,297; the number of unique AST type (including all user-defined classes) is 14,177; the number of tokens is 1,440,993. The number of instances used for train and testing is 26,600 and 3,546. Among these 38,297 vocabulary words, 37,411 are seen at training time while the rests are new.</p><p>Configuration: To train both type model and entity composite model, we use forward and backward LSTM (See Section 2) and combine them at the inference/generation time. We train 300dimensional word embedding for each token as described in Section 4 initialized by GLOVE <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>. Our LSTM is single layered and the hidden size is 300. We implement our model on using PyTorch and Python 3.5. Our training corpus size 26,600 and we do not split it further into smaller train and development set; rather we use them all to train for one single epoch and record the result on the test set.</p><p>Baselines: Our first baseline is standard LSTM language model which we also use to train our modules (see <ref type="bibr">'Configuration' in 4.2)</ref>. Similar to our second baseline for recipe generation we also consider LSTM with the type information added as more features 16 as our another baseline. We further compare our model with state-of-the-art token-based language model for source code SLP-Core <ref type="bibr" target="#b12">(Hellendoorn and Devanbu, 2017)</ref>.</p><p>Results of Code Generation: <ref type="table" target="#tab_4">Table 2</ref> shows that adding type as simple features does not guarantee a significant performance improvement while our proposed method significantly outperforms both forward and backward LSTM baselines. Our approach with backward LSTM has 40.3% better perplexity than original backward LSTM and forward has 63.14% lower (i.e., better) perplexity than original forward LSTM. With respect to SLP-Core performance, our model is 22.06% better in perplexity. We compare our model with SLP-Core details in case study-2. <ref type="bibr">15</ref> Also the AST type of a very same variable may differ in two different methods. Hence, the context is limited to each method. <ref type="bibr">16</ref> LSTM with type is same as entity composite model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantitative Error Analysis</head><p>To understand the generation performance of our model and interpret the meaning of the numbers in <ref type="table">Table 1</ref> and 2, we further perform the following case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Case Study-1: Recipe Generation</head><p>As the reduction of the perplexity does not necessarily mean the improvement of the accuracy, we design a "fill in the blank task" task to evaluate our model. A blank place in this task will contain an ingredient and we check whether our model can predict it correctly. In particular, we choose six ingredients from different frequency range (low, mid, high) based on how many times they have appeared in the training corpus. Following <ref type="table">Table  shows</ref> two examples with four blanks (underlined with the true answer).</p><p>Example fill in the blank task 1. Sprinkle chicken pieces lightly with salt. 2. Mix egg and milk and pour over bread.</p><p>We further evaluate our model on a multiple choice questioning (MCQ) strategy where the fill in the blank problem remains same but the options for the correct answers are restricted to the six ingredients. Our intuition behind this case-study is to check when there is an ingredient whether our model can learn it. If yes, we then quantify the learning using standard accuracy metric and compare with the state-of-the-art model to evaluate how much it improves the performance. We also measure how much the accuracy improvement depends on the training frequency.  <ref type="table" target="#tab_5">Table 3</ref>: Performance of fill in the blank task.</p><p>i.e., without any options (free-form) and MCQ. Note that, the percentage of improvement is inversely proportional to the training frequencies of the ingredients-less-frequent ingredients achieve a higher accuracy improvement (e.g., "Apple" and "Tomato"). This validates our intuition of learning to predict the type first more accurately with lower vocabulary set and then use conditional probability to predict the actual entity considering the type as a prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Case Study-2: Code Generation</head><p>Programming language source code shows regularities both in local and global context (e.g., variables or methods used in one source file can also be created or referenced from another library file). SLP-Core (Hellendoorn and Devanbu, 2017) is a state-of-the-art code generation model that captures this global and local information using a nested cache based n-gram language model. They further show that considering such code structure into account, a simple n-gram based SLP-Core outperforms vanilla deep learning based models like RNN, LSTM, etc. In our case, as our example instance is a Java method, we only have the local context. Therefore, to evaluate the efficiency of our proposed model, we further analyze that exploiting only the type information are we even learning any global code pattern? If yes, then how much in comparison to the baseline (SLP-Core)? To investigate these questions, we provide all the full project information to SLP-Core (Hellendoorn and Devanbu, 2017) corresponding to our train set. However, at test-time, to establish a fair comparison, we consider the perplexity metric for the same methods. SLP-Core achieves a perplexity 3.40 where our backward LSTM achieves 2.65. This result shows that appropriate type information can actually capture many inherent attributes which can be exploited to build a good language model for programming language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Language model often lacks in performance to predict entity names correctly. Applications with lots of named entities, thus, obviously suffer. In this work, we propose to leverage the type information of such named entities to build an effective language model. Since similar entities have the same type, the vocabulary size of a type based language model reduces significantly. The prediction accuracy of the type model increases significantly with such reduced vocabulary size. Then, using the entity type information as prior we build another language model which predicts the true entity name according to the conditional probability distribution. Our evaluation and case studies confirm that the type information of the named entities captures inherent text features too which leads to learn intrinsic text pattern and improve the performance of overall language model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>table .</head><label>.</label><figDesc>Data is crawled from http://www.ffts.com/ recipes.htm. arXiv:1805.04836v1 [cs.CL] 13 May 2018 place proteins in center of a dish with vegetables on each side . place chicken in center of a dish with broccoli on each side .</figDesc><table><row><cell></cell><cell cols="4">Language Model (entity composite type model)</cell><cell></cell></row><row><cell cols="2">entity name w P(w|proteins)</cell><cell>P(w)</cell><cell cols="3">entity name w P(w|vegetables) P(w)</cell></row><row><cell>q chicken</cell><cell>0.43</cell><cell>0.35 x 0.43</cell><cell>q broccoli</cell><cell>0.26</cell><cell>0.52 x 0.26</cell></row><row><cell>q beef</cell><cell>0.19</cell><cell>0.35 x 0.19</cell><cell>q potatoes</cell><cell>0.21</cell><cell>0.52 x 0.21</cell></row><row><cell>q ..</cell><cell>..</cell><cell>..</cell><cell>q ..</cell><cell>..</cell><cell>..</cell></row><row><cell>type</cell><cell>P(type)</cell><cell></cell><cell></cell><cell>type</cell><cell>P(type)</cell></row><row><cell cols="2">q proteins 0.35 q vegetables 0.11</cell><cell cols="2">Language Model (type model)</cell><cell cols="2">q vegetables 0.52 q fruits 0.22</cell></row><row><cell>q ..</cell><cell>..</cell><cell></cell><cell></cell><cell>q ..</cell><cell>..</cell></row><row><cell cols="6">Figure 1: An example illustrates the proposed model.</cell></row><row><cell cols="6">For a given context (i.e., types of context words as input),</cell></row><row><cell cols="6">the type model (in bottom red block) generates the type</cell></row><row><cell cols="6">of the next word (i.e., the probability of the type of the</cell></row></table><note>2next word as output). Further, for a given context and type of each candidate (i.e., context words, correspond- ing types of the context words, and type of the next word generated by the type model as input), the entity compos- ite model (in upper green block) predicts the next word (actual entity name) by estimating the conditional proba- bility of the next word as output. The proposed approach conducts joint inference over both models to leverage type information for generating text.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1: Language Generation Input: Language corpus X = {w 1 , w 2 , .., w n }, type s(w) of the words, integer number m.Output: θ t , θ v , {W 1 , W 2 , .., W m } Generate T = { s(w 1 ), s(w 2 ), .., s(w n )} 3 Train type model θ t on T 4 Train entity composite model θ v on X using [w i ; s(w i )] as input</figDesc><table><row><cell cols="2">1 Training Phase:</cell></row><row><cell cols="2">2 5 Test Phase (Generation Phase):</cell></row><row><cell cols="2">6 for i = 1 to m do</cell></row><row><cell>7</cell><cell>for w ∈ V text do</cell></row><row><cell>8</cell><cell>Compute P (s(w)|s(w), θ t )</cell></row><row><cell>9</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparing the performance of code generation task. All the results are on the test set of the corre-</figDesc><table><row><cell>sponding corpus. fLSTM, bLSTM denotes forward and</cell></row><row><cell>backward LSTM respectively. SLP-Core refers to (Hel-</cell></row><row><cell>lendoorn and Devanbu, 2017).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>shows the result. Our model outperforms the fill in the blank task for both cases,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell cols="3">Ingredient Train Freq. #Blanks</cell><cell cols="2">Free-Form</cell><cell>MCQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AWD LSTM Our AWD LSTM Our</cell></row><row><cell>Milk</cell><cell>14, 136</cell><cell>4,001</cell><cell>26.94</cell><cell>59.34 80.83</cell><cell>94.90</cell></row><row><cell>Salt</cell><cell>33,906</cell><cell>9,888</cell><cell>37.12</cell><cell>62.47 89.29</cell><cell>95.75</cell></row><row><cell>Apple</cell><cell>7,205</cell><cell>720</cell><cell>1.94</cell><cell>30.28 37.65</cell><cell>89.86</cell></row><row><cell>Bread</cell><cell>11,673</cell><cell>3,074</cell><cell>32.43</cell><cell>52.64 78.85</cell><cell>94.53</cell></row><row><cell cols="2">Tomato 12,866</cell><cell>1,815</cell><cell>2.20</cell><cell>35.76 43.53</cell><cell>88.76</cell></row><row><cell cols="2">Chicken 19,875</cell><cell>6,072</cell><cell>22.50</cell><cell>45.24 77.70</cell><cell>94.63</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/uclanlp/NamedEntityLanguageModel</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Empirically for the non-entity words, s∈S P (s(w) = s|s(w) ≈ 0 5 In a preliminary experiment, we consider putting all words with s(w) ∈ S in a category "N/A". However, because most words on the training corpus are not named entities, the type "N/A" dominates others and hinder the type model to make accurate predictions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://www.ffts.com/recipes.htm 9 For example, in our crawled raw dataset, we find that some recipes have lines like "===MMMMM===" which are totally irrelevant to our task. For the words with numerical values like "100 ml", we only remove the "100" and keep the "ml" since our focus is not to predict the exact number.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We consider both singular and plural forms. The number of singular formed annotated ingredients are 797.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://github.com 12 https://play.google.com/store?hl=en</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">AST represents source code as a tree by capturing its abstract syntactic structure, where each node represents a construct in the source code. 14 https://www.eclipse.org/jdt/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments. We also thank Wasi Uddin Ahmad, Peter Kim, Shou-De Lin, and Paul Mineiro for helping us implement, annotate, and design the experiments. This work was supported in part by National Science Foundation Grants IIS-1760523, CCF-16-19123, CNS-16-18771  and an NVIDIA hardware grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counterfactual language model adaptation for suggesting phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional clustering of words for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Safe type checking in a statically-typed object-oriented programming language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGPLAN-SIGACT symposium on Principles of programming languages</title>
		<meeting>the 20th ACM SIGPLAN-SIGACT symposium on Principles of programming languages</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="285" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gitcproc: a tool for processing and classifying github commits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Casalnuovo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagnik</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cindy</forename><surname>Rubio-González</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="396" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A study of the uniqueness of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering</title>
		<meeting>the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06863</idno>
		<title level="m">Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015. deltableu: A discriminative metric for generation tasks with intrinsically diverse targets</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some from here, some from there: cross-project code reuse in github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Gharehyazie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Filkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Mining Software Repositories</title>
		<meeting>the 14th International Conference on Mining Software Repositories</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>cs.CL/0108006</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
		<ptr target="http://arxiv.org/abs/1603.06393" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3106237.3106290</idno>
		<ptr target="https://doi.org/10.1145/3106237.3106290" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2902362</idno>
		<ptr target="https://doi.org/10.1145/2902362" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="122" to="131" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering (ICSE), 2012 34th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Martschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00781</idno>
		<title level="m">Dynamic entity representations in neural language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical study of code clone genealogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibha</forename><surname>Sazawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Notkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Literate programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Center for the Study of Language and Information (CSLI)</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Combining word-and class-based language models: A comparative study in several languages using automatic and manual word-clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Maltese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Bravetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crépy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grainger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<title level="m">Regularizing and Optimizing LSTM Language Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Aistats. Citeseer</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparison of partof-speech and automatically derived category-based language models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas R Niesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip C</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Introduction to compiler construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Computer Science Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributional clustering of english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual meeting on Association for Computational Linguistics</title>
		<meeting>the 31st annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno>abs/1704.07535</idno>
		<ptr target="http://arxiv.org/abs/1704.07535" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The uniqueness of changes: Characteristics and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiyappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large scale study of programming languages and code quality in github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daryl</forename><surname>Posnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Filkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="155" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the localness of software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="269" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5866-pointer-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Challenges in data-todocument generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>abs/1707.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>abs/1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
