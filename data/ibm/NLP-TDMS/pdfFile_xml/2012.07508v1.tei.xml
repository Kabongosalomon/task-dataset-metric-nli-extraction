<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Relational Modeling with Self-Supervision for Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>nwpuwangdong@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
							<email>dihu@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods 4 Big Data Laboratory</orgName>
								<address>
									<country>Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
							<email>doudejing@baidu.com</email>
						</author>
						<title level="a" type="main">Temporal Relational Modeling with Self-Supervision for Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal relational modeling in video is essential for human action understanding, such as action recognition and action segmentation. Although Graph Convolution Networks (GCNs) have shown promising advantages in relation reasoning on many tasks, it is still a challenge to apply graph convolution networks on long video sequences effectively. The main reason is that large number of nodes (i.e., video frames) makes GCNs hard to capture and model temporal relations in videos. To tackle this problem, in this paper, we introduce an effective GCN module, Dilated Temporal Graph Reasoning Module (DTGRM), designed to model temporal relations and dependencies between video frames at various time spans. In particular, we capture and model temporal relations via constructing multi-level dilated temporal graphs where the nodes represent frames from different moments in video. Moreover, to enhance temporal reasoning ability of the proposed model, an auxiliary self-supervised task is proposed to encourage the dilated temporal graph reasoning module to find and correct wrong temporal relations in videos. Our DTGRM model outperforms state-of-the-art action segmentation models on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset. The code is available at https://github.com/redwang/DTGRM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Action understanding and prediction are fundamental to accomplishing effective communication and interaction between human beings. And the ability to reasoning the temporal relations between actions over time is crucial for human action understanding in daily life. Therefore, temporal relational reasoning in videos is of significant importance for action understanding algorithms, which is the key component in many artificial intelligence systems, such as robot vision <ref type="bibr" target="#b21">(Krüger et al. 2007;</ref><ref type="bibr" target="#b19">Koppula and Saxena 2015)</ref>, intelligent surveillance <ref type="bibr" target="#b4">(Danafar and Gheissari 2007)</ref>, and autonomous vehicles <ref type="bibr" target="#b34">(Rasouli and Tsotsos 2019;</ref><ref type="bibr" target="#b36">Sadigh et al. 2016)</ref>.</p><p>Video-based action segmentation <ref type="bibr" target="#b7">(Fathi, Farhadi, and Rehg 2011;</ref><ref type="bibr" target="#b8">Fathi and Rehg 2013;</ref><ref type="bibr" target="#b23">Kuehne, Gall, and Serre 2016;</ref><ref type="bibr" target="#b24">Lea et al. , 2017</ref> is the core task for human action understanding, which aims at temporally locating and recognizing human action segments (constituting by consecutive frames with same action labels) in long untrimmed videos, and is much more difficult than action recognition task. The temporal relations between sequential human actions play an important role in action segmentation, because the sequential human actions in daily life always constitute one meaningful event (e.g., making breakfast contains making salad, toasting bread, drinking milk, and etc.).</p><p>The topic of action segmentation has been studied by many researchers in the computer vision field. Earlier approaches <ref type="bibr" target="#b35">(Rohrbach et al. 2012;</ref><ref type="bibr" target="#b17">Karaman, Seidenari, and Del Bimbo 2014;</ref><ref type="bibr" target="#b3">Cheng et al. 2014)</ref> tried to improve the discriminability of the representations of single frame or video clip and predicted the action label based on learned representations, ignoring the temporal relations between actions. Segmental models <ref type="bibr" target="#b33">(Pirsiavash and Ramanan 2014;</ref>) and recurrent networks <ref type="bibr" target="#b14">(Huang, Fei-Fei, and Niebles 2016;</ref><ref type="bibr" target="#b38">Singh et al. 2016</ref>) paid attention to local temporal dependencies between consecutive actions in videos, and have been demonstrated to have difficulty in modeling longrange temporal relations. Recently, GCNs <ref type="bibr" target="#b15">(Huang, Sugano, and Sato 2020)</ref> were introduced to improve action segmentation results via modeling temporal relations between precomputed action segments, while it still focused on the temporal relations among local consecutive action segments. In fact, temporal relations in various timescales (i.e., short-term and long-term timescales) are all of importance to infer action label of each frame. For example, when cooking, people usually first turn on the rice cooker, then cut vegetables and stir fry a few dishes, and at last turn on the rice cooker. There are temporal relations occurring on different timescales, e.g., turn on/off rice cooker, cut different vegetables for one dish, and cut and stir fry actions for one dish. Therefore, capturing and modeling temporal relations in various timescales effectively are at the core of action segmentation and remain difficult for existing methods.</p><p>In this work, we propose a Dilated Temporal Graph Reasoning Module (DTGRM) to capture and model the temporal relations and dependencies among actions in different timescales. Further, to enhance temporal reasoning ability of the proposed model, an auxiliary self-supervised task is introduced to identify the wrong-ordered frames in video and predict the correct action labels for them. Specifically, we construct multi-level dilated temporal graphs to effectively capture temporal relations in different timescales, and conduct temporal relational reasoning on the dilated temporal graphs with two complementary edge weights. In the multilevel dilated temporal graphs, we view each video frame as a graph node and update the frame-wise feature representations via the proposed dilated graph reasoning module. Moreover, the auxiliary self-supervision signals are automatically generated by randomly exchanging a fraction of frames in video. By jointly optimizing the auxiliary selfsupervised objective function and traditional classification loss function (i.e., cross-entropy loss), the proposed model can effectively learn temporal relations of actions from different time spans, resulting in an improvement on the action segmentation predictions.</p><p>The proposed model is evaluated on three challenging benchmark datasets. The experimental results demonstrate the proposed DTGRM is capable of capturing temporal actions and dependencies between video frames in different timescales. Especially, the proposed model outperforms the state-of-the-arts on structure evaluation metrics, i.e., segmental edit score and segmental overlap F1 score. To summarize, the main contributions of this work include:</p><p>• The proposed DTGRM construct multi-level dilated temporal graphs on video frames to effectively model temporal relations in various timescale, and compute two complementary edge weights to conduct temporal relational reasoning with GCNs. • An auxiliary self-supervised task is proposed to enforce the proposed model focus on temporal relational reasoning, which improves the accuracy of the prediction and alleviates the over-fitting problem. • Experiments on multiple benchmark datasets demonstrate the effectiveness of the proposed DTGRM for addressing action segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Action Segmentation Action segmentation aims at temporally locating and recognizing multiple action segments in long untrimmed videos. To address this problem, earlier approaches <ref type="bibr" target="#b35">(Rohrbach et al. 2012;</ref><ref type="bibr" target="#b17">Karaman, Seidenari, and Del Bimbo 2014)</ref> employed the temporal sliding windows to detect the action segments with different lengths. Fathi et al. <ref type="bibr" target="#b7">(Fathi, Farhadi, and Rehg 2011;</ref><ref type="bibr" target="#b9">Fathi, Ren, and Rehg 2011;</ref><ref type="bibr" target="#b8">Fathi and Rehg 2013)</ref> attempted to use a segmental model to predict the temporally consistent action segments. <ref type="bibr" target="#b3">Cheng et al. (Cheng et al. 2014</ref>) adopted a hierarchical Bayesian nonparametric model to model the temporal dependency between action segments. However, the optimization of these temporal models are mostly time-consuming. Other approaches tried to accomplish action segmentation task by predicting action label for every frame in the video. <ref type="bibr" target="#b24">Lea et al. (Lea et al. 2017</ref>) first proposed to use temporal convolution networks (TCN) to predict frame-wise action labels. <ref type="bibr" target="#b28">Lei and Todorovic (Lei and Todorovic 2018)</ref> further proposed deformable temporal convolutions equipped with residual connections to replace the regular temporal convolutions. In addition, Farha et al. <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref> pro-posed to use dilated TCN to model the long-range temporal dependencies in videos, and refine the prediction via a multi-stage framework. Recently, Huang et al. <ref type="bibr" target="#b15">(Huang, Sugano, and Sato 2020)</ref> exploited the temporal relations among multiple action segments with graph convolution networks. However, this method constructed the graph by viewing single action segment from backbone model as one node in graph, which may be very noisy for modeling temporal relations since the prediction from backbone model are mostly inaccurate, resulting in inefficient optimization for GCNs. Relational Reasoning with GCNs The graph convolution network (GCN) was proposed by <ref type="bibr" target="#b18">Kipf et al. (Kipf and Welling 2017)</ref> and has been proved to be effective in modeling the relations in data <ref type="bibr" target="#b30">(Li and Gupta 2018;</ref><ref type="bibr" target="#b31">Liang et al. 2018)</ref>. Recently, GCNs have been widely applied to several research topic in computer vision filed, such as person re-identification <ref type="bibr" target="#b37">(Shen et al. 2018)</ref>, skeleton-based action recognition <ref type="bibr" target="#b42">(Yan, Xiong, and Lin 2018)</ref> and video action recognition and detection <ref type="bibr" target="#b40">(Wang and Gupta 2018;</ref><ref type="bibr" target="#b44">Zhang et al. 2020</ref><ref type="bibr" target="#b45">Zhang et al. , 2019</ref><ref type="bibr" target="#b43">Zeng et al. 2019)</ref>. For instance, <ref type="bibr" target="#b43">Zeng et al. (Zeng et al. 2019)</ref> proposed to exploit the temporal action proposal-proposal relations using graph convolutional networks. Huang et al. <ref type="bibr" target="#b15">(Huang, Sugano, and Sato 2020)</ref> improved action segmentation result via modeling temporal relations with GCNs. However, these methods constructed relative small graph based on pre-computed proposals or predicted segments rather than frames. As we all know, the precomputed proposals and predicted segments are mostly inaccurate and the constructed graphs are noisy. To avoid this problem, in this work, we construct the graphs upon individual frames to achieve more effective relation reasoning. Self-Supervision for Video Representation The selfsupervised pre-trained models and auxiliary self-supervision signals have been proved to be beneficial to many computer vision tasks <ref type="bibr" target="#b5">(Doersch, Gupta, and Efros 2015;</ref><ref type="bibr" target="#b11">Gidaris, Singh, and Komodakis 2018;</ref><ref type="bibr" target="#b12">Hu, Nie, and Li 2019;</ref><ref type="bibr" target="#b13">Hu et al. 2020)</ref>. For learning effective video representations with self-supervision, several methods <ref type="bibr" target="#b32">(Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b27">Lee et al. 2017;</ref><ref type="bibr" target="#b10">Fernando et al. 2017)</ref> designed auxiliary tasks to verify the input short video clips (i.e., several seconds) is in the correct order or not. These pre-trained models were usually fine-tuned to recognize action on short trimmed videos, while the self-supervised task in this work is specifically designed for action segmentation in long untrimmed videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach</head><p>We introduce a dilated temporal graph reasoning module (DTGRM) for capturing temporal relations from various timescales in videos, which is essential for the action segmentation task. Given a video of a total T frames, the action segmentation methods need to infer the action class label for each frame c 1:T = (c 1 , ..., c T ), whose ground-truth is given by y gt 1:T = (y gt 1 , ..., y gt T ), where y gt t ∈ {0, 1} C is a one-hot vector indicating the true action label. C is the number of action classes including the background class (i.e., no action). Our DTGRM is used to refine the predicted result in an iterative manner, which is built upon a backbone prediction ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Video Sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exchanged Video Sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Model</head><formula xml:id="formula_0">… … 1-level DRGC Layer ℎ " " ℎ # " … ℎ $ " ℎ % " … DRGC Layer ℎ " # ℎ # # … ℎ $ # ℎ % # … … … 2-level …… DRGC Layer ℎ " &amp; ℎ # &amp; … ℎ $ &amp; ℎ % &amp; … … K-level … … Dilated Temporal Graph Reasoning Module ' ' ℒ )*+ + ℒ )*-.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DRGC Layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Graph</head><formula xml:id="formula_1">L-Graph ℎ " / ℎ # / ℎ $ / ℎ % / … 0 = 2 0 = 3 0 = 3 452</formula><p>Conv Layer <ref type="figure">Figure 1</ref>: The pipeline of the proposed DTGRM model. The frame-wise features are fed into the backbone model, and the action segmentation results are refined by our DTGRM model. Note that the dilated factor τ is doubled at each level in DTGRM. L seg and L self represent the action segmentation loss and auxiliary self-supervision loss respectively.</p><p>model. In the rest of this section, we first give an overview of the proposed model. Then, the details of our DTGRM and auxiliary self-supervised task are carefully explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>The architecture of the proposed model is illustrated in <ref type="figure">Fig.  1</ref>. We take the dilated TCN in MS-TCN <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref> as the backbone model. The backbone model takes frame-wise feature representations x 1:T = (x 1 , ..., x T ), which are extracted using pre-trained I3D network <ref type="bibr" target="#b0">(Carreira and Zisserman 2017)</ref>, and outputs the predicted action class likelihoods y 1:T = (y 1 , ..., y T ), where y t ∈ R C are obtained through softmax function. The prediction y 1:T is the only input to our DTGRM, which refines the input prediction with GCNs by modeling temporal relations between actions. In addition, inspired by the success on multi-stage refinement <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref> in action segmentation, we also iteratively refine the prediction using our DTGRM S times to obtain the final prediction result.</p><p>In the proposed DTGRM, we view each frame in video as one node and construct multi-level dilated temporal graphs on frames to capture temporal relations in various timescales. Along the constructed multi-level graphs, DT-GRM stacks K Dilated Residual Graph Convolution layer (DRGC layer) to conduct temporal relational reasoning on various timescales. Specifically, for each frame in video at each level, we construct two graphs, called S-Graph and L-Graph, on its dilated neighborhood frames. The dilation factor is increasing exponentially while stacking DRGC layers in DTGRM. Note that the edges of dilated graphs represent the relations between frames from various timescales. In the following, a graph with N nodes in GCNs are denoted as G(V, E), where V is the set of the node v i and e(i, j) ∈ E represents the edge weight between node v i and v j .</p><p>Moreover, the over-segmentation problem  is one of the key factors affecting action segmentation accuracy. To reduce over-segmentation errors in action segmentation results, we introduce an auxiliary self-supervised task to simulate the over-segmentation errors manually. In detail, we first random choose some frames from videos and pairwise exchange them. The goal of the self-supervised task is to identify the exchanged frame and predict the correct action label by temporal relational reasoning in various timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dilated Temporal Graph Reasoning Module</head><p>GCNs have shown promising ability on relational reasoning <ref type="bibr" target="#b2">(Chen et al. 2019;</ref><ref type="bibr" target="#b16">Hussein, Gavves, and Smeulders 2019;</ref><ref type="bibr" target="#b43">Zeng et al. 2019</ref>). The key step in GCNs is to construct the graphs and compute the edge weights. Previous works usually construct graphs based on action proposals or action segments, which are pre-computed by other models and mostly inaccurate. In contrast, we directly construct graphs on frames and address large graph problem with the proposed multi-level dilated temporal graphs.</p><p>Multi-Level Dilated Temporal Graphs Temporal relations from various time spans are very useful to infer action label on single frame, i.e., successive frames always belong to the same action class and long-range temporal relations always capture the relationship between different action classes. But, it is hard to train and optimize GCNs with one large graph containing all frames (i.e., nodes) in videos. To address this problem, we propose to construct multi-level dilated temporal graphs to capture temporal relations between all the frames in videos.</p><p>Suppose we have a total of T frames in video and the dilated temporal graphs at k-th level are constructed based on dilation factor τ k . To be specific, for the frame at timestep t, its dilated neighborhood frames is {t − τ k , t + τ k }. Then, the frames at time {t − τ k , t, t + τ k } are viewed as nodes and the dilated temporal graph G k t is constructed upon them. We denote the order of the graph (its number of vertices) as O k t , i.e., O k t = 3. As shown in <ref type="figure">Fig. 1</ref>, to capture temporal relations in various time spans, we construct K levels of dilated temporal graphs and apply the proposed DRGC layer at each level, where the dilation factor τ k is doubled at each level, i.e.,τ k = 2 k−1 , k ∈ {1, 2, .., K}. Note that all the dilated temporal graphs contain three nodes (i.e., node v t−τ k , v t , v t+τ k ). At k-th level, to alleviate the noise problem in single constructed graph, we compute two complementary edge weights for dilated temporal graph G</p><formula xml:id="formula_2">(k) t and name them as S-Graph G s,(k) t and L-Graph G l,(k) t . S-Graph The motivation of constructing S-Graph (Simi- larity Graph) G s,(k) t</formula><p>is that the nodes with similar action class likelihoods y should have larger edge weights. Therefore, we first apply one 1 × 1 convolution layer to transfer action class likelihoods y ∈ R C into d-dimensional hidden representations h 1:T = (h 1 , ..., h T ). Then, for S-Graph G s,(k) t , the edge weight e s (i, j) between node v i and v j are defined by the cosine similarity between their hidden representations h i , h j , i.e.,</p><formula xml:id="formula_3">e s (i, j) = h i · h j max( h i 2 · h j 2 , ) ,<label>(1)</label></formula><p>where is a small constant avoiding divide-by-zero. We gather all edge weights in G L-Graph Since there mostly are some wrong predictions in action class likelihood y that make the edge weight e s (i, j) inaccurate, we propose to construct L-Graph (Learned Graph) G l,(k) t whose edge weights are generated by one sub-network, which can capture the important temporal relations that are complementary to S-Graph after training. Specifically, we apply one dilated 1D convolution on hidden representations h 1:T , and the dilation factor of this 1D convolution layer equals to corresponding dilated temporal graph, i.e., dilation = τ k . The outputs of this layer is the adjacency matrix of graph G l,(k) t , where the value with index i, j represent the edge weight between node v i and v j . Formally, the adjacency matrix A l,(k) t of the graph G l,(k) t are defined as</p><formula xml:id="formula_4">A l,(k) t = Conv(h[t − τ k , t, t + τ k ], W, dilation = τ k ),<label>(2)</label></formula><p>where W ∈ R ks×(Ot×Ot)×d are the weights of the dilated convolution filter with kernel size ks = 3. O t = 3 is the number of vertices of the graph G Reasoning on Dilated Temporal Graph Given the constructed dilated temporal graphs at each frame t and level k, G s,(k) t and G l,(k) t , we apply the proposed DRGC layer on them to conduct temporal relational reasoning in various timescales. To be specific, we first normalize the adjacency matrixes A s,(k) t and A l,(k) t with softmax function. Then, for relational reasoning on constructed graphs, our DRGC layers employ the graph convolution layer proposed in <ref type="bibr" target="#b18">(Kipf and Welling 2017)</ref>:</p><formula xml:id="formula_5">X = σ(AXW ),<label>(3)</label></formula><p>where A ∈ R N ×N is the adjacency matrix of the graph, X ∈ R N ×d are the hidden representation of all nodes in the graph, and W ∈ R d×d is the parameter matrix to be learned. σ is the ReLU activation function. Based on the graph convolution layer and constructed dilated temporal graphs, our DTGRM stacks K-level DRGC layers to model temporal relations in various timescales. Specifically, at k-th DRGC layer (k ∈ {0, 1, ..., K − 1}), the dilated temporal graphs are constructed with dilation factor τ = 2 k . As illustrated in <ref type="figure">Fig. 1</ref>, at t-th frame, we first separately apply graph convolution on G s,(k) t and G l,(k) t , and then fuse their output with addition operation, i.e.,</p><formula xml:id="formula_6">X t = h (k) [t−τ k ,t,t+τ k ] , O (k) t = GCN (k) (X t , A s,(k) t , W s,(k) t )+ GCN (k) (X t , A l,(k) t , W l,(k) t ), O (k) t = o (k) [t−τ k ,t,t+τ k ] , h (k+1) t = Conv(o (k) t , W (k) ) + h (k) t ,<label>(4)</label></formula><p>where GCN is the graph convolution operation defined in</p><formula xml:id="formula_7">Eq. 3. A s,(k) t , A l,(k) t ∈ R N ×N are the adjacency matrix of the graph G s,(k) t and G l,(k) t . W s,(k) t , W l,(k) t</formula><p>∈ R d×d are the parameter matrix of the graph convolution layer for t-th frame at k-th layer. W (k) ∈ R 1×d×d is the weights of the 1D convolution filter with kernel size 1, which is shared with each timestep in video. With stacking the DRGC layer K times, our DTGRMs can efficiently capture short and longrange temporal relations in videos and avoid the large graph problem. In this way, our DTGRMs conduct temporal realtional reasoning in various timescales, which is essential for action segmentation.</p><p>To get the action class likelihoods y 1:T for each frame, we apply a fully-connected layer over the outputs of the last DRGC layer followed by a softmax activation, i.e.,</p><formula xml:id="formula_8">y 1:T = sof tmax(W h (K) 1:T + b),<label>(5)</label></formula><p>Where W ∈ R C×d and b ∈ R C are the weights and bias for the FC layer. h</p><p>1:T is the output of the K-th DRGC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Self-Supervision</head><p>Self-supervision signals have been used for video representation learning <ref type="bibr" target="#b32">(Misra, Zitnick, and Hebert 2016;</ref><ref type="bibr" target="#b27">Lee et al. 2017;</ref><ref type="bibr" target="#b10">Fernando et al. 2017;</ref><ref type="bibr" target="#b20">Korbar, Tran, and Torresani 2018)</ref> and improved the downstream tasks, such as action recognition and action detection. Compared to supervised learning methods, self-supervised methods automatically generate the supervisory signals (i.e., pseudo label) that are inferred from the structure of the data, without involving any human annotation. In this work, different from previous works that only provide the self-supervision signals on video-level, we obtain the frame-wise self-supervision signals in the context of the pairwise exchanging frames in video, which simulates the over-segmentation errors in the action segmentation results. Specifically, given the input video sequence x 1:T = (x 1 , ..., x T ) with correct temporal order. We select η% frames and randomly form them as frame pairs {x ti , x tj }, then the frames in each pair are exchanged. The resulting video sequence x ex 1:T = (..., x tj , ..., x ti , ...) contains some wrong ordered frame and is fed into the proposed model along with original video sequence x 1:T . The outputs corresponding to x ex 1:T consist of action class likelihoods y ex 1:T and exchange likelihood e ex 1:T , which are obtained by feeding the hidden representation h K,ex 1:T into a fully-connected layer. The goal of the auxiliary self-supervised task is to identify the exchanged frames and predict the correct action labels that should be at their moments. Formally, we generate a binary self-supervised signal p 1:T = (p 1 , ..., p T ) to label the exchanged frames, where p t ∈ {0, 1} 2 is the one-hot vector indicating whether t-th frame is exchanged or not. Moreover, exchanged frames are the prefect simulation of oversegmentation errors in action segmentation task. Therefore, except the binary training label p 1:T , we directly take the ordered ground-truth action label y gt 1:T as another training label. The overall loss function of self-supervision is</p><formula xml:id="formula_10">L self = L ex (e ex , p) + L corr (y ex , y gt ),<label>(6)</label></formula><p>where e ex ∈ R T ×2 and y ex ∈ R T ×C (for simplicity, we drop the timestep notation). With the above self-supervised objective function, our DTGRM learns to do accurate temporal relational reasoning about the temporal relation structure, leading to better action segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Loss Function</head><p>We train the backbone model and our DTGRM in an endto-end manner with a combination of the multiple loss functions. The inputs of the whole network is the ordered video sequence x 1:T and exchanged video sequence x ex 1:T , and the outputs is the action class likelihood y 1:T , y ex 1:T and exchange likelihood e ex 1:T . As for the action class likelihood y 1:T and y ex 1:T , we apply the typical cross entropy loss</p><formula xml:id="formula_11">L cls (y, y gt ) = 1 T T t C c −y gt t,c log(y t,c ).<label>(7)</label></formula><p>And we adopt the truncated mean squared error L t−mse proposed in <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref> to punish local inconsistency in action class likelihood. Based on these loss functions, the action segmentation loss for ordered video sequence and auxiliary self-supervised task loss function are defined as follows, L seg = L cls (y, y gt ) + ωL t−mse , L ex (e ex , p) = λ e L cls (e ex , p),</p><formula xml:id="formula_12">L corr (y ex , y gt ) = λ c L cls (y ex , y gt ) + ωL t−mse , L = L seg + L self ,<label>(8)</label></formula><p>where ω, λ c , λ e are hyper-parameters that balance the components in loss function. Since we apply our DTGRM S times sequentially, the loss function L is applied on the outputs from the each DTGRM and backbone model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Implementation Details The whole model proposed in this paper consists of one backbone network and three DT-GRMs (i.e., S = 3) that are implemented with Pytorch library on Nvidia 2080Ti GPU. We set the dimension of hidden representation d as 64 for backbone network and our DTGRMs. The proposed DTGRM constructs K = 10 dilated temporal graphs and apply DRGC layer on each level, where the dilation factor is doubled at each level. For hyperparameter η in auxiliary self-supervised task, we set it as η = 20. For the loss function, we set ω = 0.15, λ e = 2 and λ c = 0.5. In all experiments, the network is trained using Adam optimizer with a learning rate of 5e-4.</p><p>Datasets The 50Salads <ref type="bibr" target="#b39">(Stein and McKenna 2013)</ref> dataset consists of 50 videos of 17 action classes, which averagely contains 20 action instances and is 6.4 minutes long. The videos capture the salad preparation activities performed by 25 actors where each actor prepares two different salads. The GTEA (Fathi, Ren, and Rehg 2011) dataset contains 28 videos with 7 different activities performed by 4 subjects, such as preparing coffee and cheese sandwich. Each video is annotated with 11 fine-grained action classes and averagely has 20 action instances. The Breakfast (Kuehne, Arslan, and Serre 2014) dataset is the largest among the three datasets with 1712 videos, recording the breakfast related activities in 18 different kitchens. The videos are annotated with 48 different actions and contain 6 action instances on average. In all datasets, we sample the videos with fixed fps rather than fixed number of frames and extract I3D (Carreira and Zisserman 2017) features for the video frames, which are input to the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Metrics For evaluating our model, we adopt the following evaluation metrics as in <ref type="bibr" target="#b24">(Lea et al. 2017;</ref><ref type="bibr" target="#b6">Farha and Gall 2019;</ref><ref type="bibr" target="#b15">Huang, Sugano, and Sato 2020)</ref>: frame-wise accuracy (Acc), segmental edit distance (Edit) and segmental F1 score at overlapping thresholds 10%, 25% and 50%, denoted by F1@{10,25,50}. The overlapping ratio is the intersection over union (IoU) ratio between predicted and ground-truth action segments. Frame-wise accuracy is the most commonly used metric for action segmentation. However, actions with long duration tend to have a higher impact than actions with short duration on this metric, and there is no explicit penalty on over-segmentation errors. In contrast, segmental edit score and F1 score presented in <ref type="bibr" target="#b24">(Lea et al. 2017</ref> are used to penalizes the over-segmentation errors and measure the quality of the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-Art</head><p>In this section, we compare the proposed model with several state-of-the-art models on three datasets: 50Salads, GTEA, and the Breakfast dataset. The results are presented in <ref type="table">Table.</ref> 1. Specifically, the comparison methods consists of five closely related state-of-the-art models, including MSTCN <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref>, MSTCN++(Li et al.  <ref type="bibr" target="#b15">(Huang, Sugano, and Sato 2020)</ref>. MSTCN are the recent temporal convolution based model that adopted the similar multi-stage framework as our approach (i.e., iteratively refining the prediction from backbone model several times), which is the baseline method of the proposed model. We use the same backbone model and the same num of layers and stages with the MSTCN. MSTCN++ and MTDA are the extended works of MSTCN. BCN improves the smoothness of frame-wise predictions by cooperating action boundary information. MSTCN+GTRM is the most related to our models, where the GCNs are used to model relations between action segments upon the results of MSTCN model. As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, the proposed DTGRM model outperforms the baseline method MSTCN on the three datasets and by a large margin with respect to three evaluation metrics. Specifically, our DTGRM model achieves a moderate improvement on 50Salads and GTEA dataset, i.e., around 2-5% in all evaluation metrics, except the frame-wise accuracy on the 50salads. As for the Breakfast dataset, our approach outperforms MSTCN and MSTCN+GTRM with a larger margin, i.e., near 10% improvement on F1 score and segmental edit score. This shows that our DTGRM is capable of reducing over-segmentation errors in prediction. In addition, the improvements over MSTCN demonstrate that dilated temporal convolution in MSTCN is inefficient in temporal relations reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>The Effectiveness of DTGRM model To verify the effect of each constructed graph in our DTGRM, we conduct ablation studies by changing or deleting part of DRGC layer in our DTGRM. All these models are implemented based on the same backbone model and trained without auxiliary self-supervision on GTEA dataset. As shown in upper part of <ref type="table" target="#tab_1">Table 2</ref>, "DTCN" is the case where GCNs in DRGC layers are replaced by the dilated temporal convolution layer presented in <ref type="bibr" target="#b6">(Farha and Gall 2019)</ref>. Our DTGRM outperforms this approach by 1-3% in all metrics, which validates that our method can effectively capture temporal relations from various time spans to improve action segmentation. "S-Graph" indicates the model that only applies GCNs on S-Graph while ignoring the L-Graph. The results of this model suggest that the S-Graph may be very noisy due to the errors in prediction from backbone model. "L-Graph" represents the model that only applies GCNs on L-Graph while ignoring the S-Graph and achieves comparable performance, which shows the learned graph weights are more appropriate and useful to capture the temporal relations in videos.</p><p>In addition, we compare the results from backbone model (denoted as "BK") and models with different number of DT-GRMs (denoted as "BK+*-DTGRM") that are trained with auxiliary self-supervision on 50Salads dataset. As shown in middle part of <ref type="table" target="#tab_1">Table 2</ref>, the performance is significantly improved after using only one DTGRM and stacking more DT-GRMs can improve the predictions performance on segmental edit distance and segmental F1 score progressively, which demonstrates the effectiveness of our DTGRM on improving the quality of the predictions. The results on Breakfast dataset also prove the effectiveness of the proposed method.</p><p>The Effectiveness of Auxiliary Self-Supervision To demonstrate the necessity and superiority of the auxiliary self-supervision signals, we report the performance of our DTGRM model and its variants with or without auxiliary self-supervision signal during training stage on 50Salads dataset. As we can see in <ref type="table" target="#tab_2">Table 3</ref>, the models trained with self-supervision signals outperforms their duplicates, which are trained only with ground-truth action segmentation labels. Specifically, the "S-Graph" performs very bad because the constructed S-Graph usually is very noisy, while its performance is improved by a large margin after trained with self-supervision signal. This shows that the proposed self-  supervised task can improve the model's generalization ability and make it more robust to the noise in the constructed graphs. Moreover, besides our DTGRM, the self-supervision task is also helpful for temporal convolution (DTCN), which indicates that the self-supervision task can be widely used to other action segmentation models. From the qualitative comparison in <ref type="figure" target="#fig_2">Fig. 2</ref>, we can see that the auxiliary selfsupervised task is very useful to improve the quality of action segmentation results. Especially, the model trained with self-supervision is able to correct the wrong action segments with considerable duration and reduce the oversegmentation errors at the boundaries of action segments.</p><p>The Impact of Hyper-Parameters The effect of the proposed auxiliary self-supervised task is controlled by three hyper-parameters: λ c , λ e and η. As shown in <ref type="table">Table 4</ref>, in this section, we study the impact of these parameters and see how they affect the performance of the proposed model. In all experiments in <ref type="table">Table 4</ref>, we set ω = 0.15, whose impact has been fully analyzed in MSTCN.</p><p>To analyze the effect of the parameter λ c , we train different models with different values of λ c and λ e = 2. As we can see in <ref type="table">Table 4</ref>, the impact of λ c is relatively small on the performance when λ c &gt; 0. Compared the model without self-supervision, i.e., DTGRM(w/o self) in <ref type="table" target="#tab_2">Table 3</ref>, increasing λ c to 1.0 still improves the performance but not as good as the value of λ c = 0.5. However, there is a huge degradation in performance when we reduce λ c to 0, which indicates that the L corr is an essential factor for action segmentation task. The hyper-parameter λ e is another parameter that balance the multiple components in the loss function of the self-supervised task. Our default value is λ e = 2.</p><p>While the values λ e = 1, 3 still gives an improvement over the baseline without self-supervision, setting λ e = 4 results in a huge drop in performance. This may be because large λ e makes the loss function focus on finding the exchanged frames while ignoring correcting them.</p><p>In addition, the hyper-parameter η defines the number of the exchanged frames in video, which play a vital role in auxiliary self-supervised task. As shown in <ref type="table">Table 4</ref>, increasing η from 5 to 20 significantly improves the performance. This is mainly because the exchanged frames perfectly simulate the over-segmentation errors and make the model explicitly penalizes them. However, when there are too many exchanged frames (i.e., η = 30), the model performs worse since the correct temporal relations in video have been disturbed heavily. More ablation studies on hyperparameter number of level K and the order of graph O t are included in the supplementary materials.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose to model the short and longrange temporal relations in action segmentation. We construct multi-level dilated temporal graphs to capture the temporal relations in various time spans and propose DRGC layers to perform relational reasoning. Further, an auxiliary self-supervision is introduced to explicitly simulate the oversegmentation errors in predictions. Extensive experiments showed that our model can effectively conduct temporal relational reasoning in different timescales, and outperform the state-of-the-art methods on three challenging datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>.</head><label></label><figDesc>The graph convolution operation is used to update the hidden representation h t of each frame according to its S-Graph G s,(k) t at each DRGC layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t × O t -dimensional vector and reshaped to the adjacency matrix with size (O t , O t ). Note that the adjacency matrix A l,(k) t is asymmetric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison of the action segmentation results on (a)50Salads, and (b)GTEA dataset. Only few frames of the whole video are shown for clarity. We can see that the model trained with self-supervision generate better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>4: Impact of hyper-parameters λ c , λ e and η on the 50Salads dataset. More results are in the Supp. Materials. Impact of λ c F1@{10,25,50} Edit Acc (λ c = 0.0, λ e = 2) 58.7 55.3 46.0 53.0 72.1 (λ c = 0.5, λ e = 2) 79.1 75.9 66.1 72.0 80.0 (λ c = 1.0, λ e = 2) 77.7 74.7 64.4 71.4 78.3Impact of λ e F1@{10,25,50} Edit Acc (λ c = 0.5, λ e = 1) 77.2 74.7 64.6 70.4 78.7 (λ c = 0.5, λ e = 2) 79.1 75.9 66.1 72.0 80.0 (λ c = 0.5, λ e = 3) 74.3 71.4 62.5 66.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparisons with the state-of-the-art methods on 50Salads, GTEA, and the Breakfast dataset.</figDesc><table><row><cell>50Salads</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MSTCN</cell><cell cols="2">76.3 74.0 64.5 67.9 80.7</cell></row><row><cell>MSTCN++</cell><cell cols="2">80.7 78.5 70.1 74.3 83.7</cell></row><row><cell>BCN</cell><cell cols="2">82.3 81.3 74.0 74.3 84.4</cell></row><row><cell cols="3">MSTCN+GTRM 75.4 72.8 63.9 67.5 82.6</cell></row><row><cell>DTGRM</cell><cell cols="2">79.1 75.9 66.1 72.0 80.0</cell></row><row><cell>GTEA</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MSTCN</cell><cell cols="2">85.8 83.4 69.8 79.0 76.3</cell></row><row><cell>MSTCN++</cell><cell cols="2">88.8 85.7 76.0 83.5 80.1</cell></row><row><cell>BCN</cell><cell cols="2">88.5 87.1 77.3 84.4 79.8</cell></row><row><cell>MTDA</cell><cell cols="2">82.0 80.1 72.5 75.2 83.2</cell></row><row><cell>DTGRM</cell><cell cols="2">87.8 86.6 72.9 83.0 77.6</cell></row><row><cell>Breakfast</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MSTCN</cell><cell cols="2">52.6 48.1 37.9 61.7 66.3</cell></row><row><cell>MSTCN++</cell><cell cols="2">64.1 58.6 45.9 65.6 67.6</cell></row><row><cell>BCN</cell><cell cols="2">68.7 65.5 55.0 66.2 70.4</cell></row><row><cell cols="3">MSTCN+GTRM 57.5 54.0 43.3 58.7 65.0</cell></row><row><cell>DTGRM</cell><cell cols="2">68.7 61.9 46.6 68.9 68.3</cell></row><row><cell cols="3">2020), MTDA(Chen et al. 2020), BCN (Wang et al. 2020),</cell></row><row><cell>and MSTCN+GTRM</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of performance by our DTGRM and its variants on the GTEA, 50 Salads and Breakfast dataset.</figDesc><table><row><cell>GTEA</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MSTCN</cell><cell cols="2">85.8 83.4 69.8 79.0 76.3</cell></row><row><cell>DTCN(w/o self)</cell><cell cols="2">86.3 83.6 70.6 80.8 76.1</cell></row><row><cell cols="3">S-Graph(w/o self) 48.2 44.9 37.4 38.4 71.3</cell></row><row><cell cols="3">L-Graph(w/o self) 85.6 83.7 70.3 78.8 76.4</cell></row><row><cell cols="3">DTGRM(w/o self) 87.3 85.5 72.3 80.7 77.5</cell></row><row><cell>50Salads</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>BK</cell><cell cols="2">52.7 47.8 40.0 42.0 78.0</cell></row><row><cell>BK+1-DTGRM</cell><cell cols="2">69.0 65.3 56.2 60.3 79.3</cell></row><row><cell>BK+2-DTGRM</cell><cell cols="2">75.2 71.6 62.6 66.6 79.7</cell></row><row><cell>BK+3-DTGRM</cell><cell cols="2">79.1 75.9 66.1 72.0 80.0</cell></row><row><cell>Breakfast</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MSTCN(IDT)</cell><cell cols="2">58.2 52.9 40.8 61.4 65.1</cell></row><row><cell>S-Graph(w/ self)</cell><cell cols="2">49.6 43.7 31.9 54.6 66.3</cell></row><row><cell>L-Graph(w/ self)</cell><cell cols="2">67.5 60.7 45.3 68.2 68.0</cell></row><row><cell>DTGRM</cell><cell cols="2">68.7 61.9 46.6 68.9 68.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of performance by our DTGRM and its variants with or without auxiliary self-supervision on the 50Salads dataset.</figDesc><table><row><cell>50Salads</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>DTCN(w/o self)</cell><cell cols="2">74.7 71.8 63.9 66.7 80.3</cell></row><row><cell>DTCN(w/ self)</cell><cell cols="2">79.0 76.2 66.4 71.4 78.1</cell></row><row><cell cols="3">S-Graph(w/o self) 52.6 48.1 39.9 41.9 77.5</cell></row><row><cell>S-Graph(w/ self)</cell><cell cols="2">59.3 54.7 46.5 49.5 78.9</cell></row><row><cell cols="3">L-Graph(w/o self) 73.5 71.4 60.8 65.3 77.3</cell></row><row><cell>L-Graph(w/ self)</cell><cell cols="2">78.6 75.6 66.4 70.9 79.5</cell></row><row><cell cols="3">DTGRM(w/o self) 74.0 71.0 60.8 67.9 77.9</cell></row><row><cell cols="3">DTGRM(w/ self) 79.1 75.9 66.1 72.0 80.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action Segmentation with Mixed Temporal Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2227" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition for surveillance applications using optic flow and SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Danafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling actions through state changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2579" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-oneout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multimodal clustering for unsupervised audiovisual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9248" to="9257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Action Segmentation via Graph-Based Temporal Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05143</idno>
		<title level="m">Videograph: Recognizing minutes-long human activities in videos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The meaning of action: A review on action recognition and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Robotics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1473" to="1501" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional action primitives for fine-grained action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2020.3021756</idno>
		<title level="m">MS-TCN++: Multi-Stage Temporal Convolutional Network for Action Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond grids: Learning graph representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9225" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symbolic graph reasoning meets convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1853" to="1863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsing videos of actions with segmental grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autonomous vehicles that interact with pedestrians: A survey of theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="900" to="918" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Planning for autonomous cars that leverage effects on human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Boundary-Aware Cascade Networks for Temporal Action Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal reasoning graph for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5491" to="5506" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A structured model for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9975" to="9984" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
