<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HIERARCHICAL GRAPH-TO-GRAPH TRANSLATION FOR MOLECULES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HIERARCHICAL GRAPH-TO-GRAPH TRANSLATION FOR MOLECULES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of accelerating drug discovery relies heavily on automatic tools to optimize precursor molecules to afford them with better biochemical properties. Our work in this paper substantially extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization. In particular, we realize coherent multi-resolution representations by interweaving the encoding of substructure components with the atom-level encoding of the original molecular graph. Moreover, our graph decoder is fully autoregressive, and interleaves each step of adding a new substructure with the process of resolving its attachment to the emerging molecule. We evaluate our model on multiple molecular optimization tasks and show that our model significantly outperforms previous state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Molecular optimization seeks to modify compounds in order to improve their biochemical properties. This task can be formulated as a graph-to-graph translation problem analogous to machine translation. Given a corpus of molecular pairs {(X, Y )}, where Y is a paraphrase of X with better chemical properties, the model is trained to translate an input molecular graph into its better form. The task is difficult since the space of potential candidates is vast, and molecular properties can be complex functions of structural features. Moreover, graph generation is computationally challenging due to complex dependencies involved in the joint distribution over nodes and edges. Similar to machine translation, success in this task is predicated on the inductive biases built into the encoder-decoder architecture, in particular the process of generating molecular graphs.</p><p>Prior work <ref type="bibr" target="#b14">(Jin et al., 2019)</ref> proposed a junction tree encoder-decoder that utilized valid chemical substructures (e.g., aromatic rings) as building blocks to generate graphs. Each molecule was represented as a junction tree over chemical substructures in addition to the original atom-level graph. While successful, the approach remains limited in several ways. The tree and graph encoding were carried out separately, and decoding proceeded in strictly successive steps: first generating the junction tree for the new molecule, and then attaching its substructures together. This means the predicted attachments do not impact the subsequent substructure choices. Moreover, the attachment prediction process involved complex combinatorial enumeration which made the decoding process slow and hard to parallelize.</p><p>We propose a multi-resolution, hierarchically coupled encoder-decoder for graph generation. Our auto-regressive decoder interleaves the prediction of substructure components with their attachments to the molecule being generated. In particular, a target graph is unraveled as a sequence of triplet predictions (where to expand the graph, new substructure type, its attachment). This enables us to model strong dependencies between successive attachments and substructure choices. The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process. Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step. The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures at the highest level supports the prediction of successive substructures. Compared to prior work, our decoding process is much more efficient because it decomposes each generation step into a hierarchy of smaller steps in order to avoid combinatorial explosion. We also extend the method to handle conditional translation where desired criteria are fed as input to the translation process. This enables our method to handle different combinations of criteria at test time.</p><p>We evaluate our new model on multiple molecular optimization tasks. Our baselines include previous state-of-the-art graph generation methods <ref type="bibr" target="#b39">(You et al., 2018a;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b14">Jin et al., 2019)</ref> and an atom-based translation model we implemented for a more comprehensive comparison. Our model significantly outperforms these methods in discovering molecules with desired properties, yielding 3.3% and 8.1% improvement on QED and DRD2 optimization tasks. During decoding, our model runs 6.3 times faster than previous substructure-based generation methods. We further conduct ablation studies to validate the advantage of our hierarchical decoding and multi-resolution encoding. Finally, we show that conditional translation can succeed (generalize) even when trained on molecular pairs with only 1.6% of them having desired target property combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Molecular Graph Generation Previous work have adopted various approaches for generating molecular graphs. Methods <ref type="bibr" target="#b10">(Gómez-Bombarelli et al., 2018;</ref><ref type="bibr" target="#b33">Segler et al., 2017;</ref><ref type="bibr" target="#b20">Kusner et al., 2017;</ref><ref type="bibr" target="#b3">Dai et al., 2018;</ref><ref type="bibr" target="#b11">Guimaraes et al., 2017;</ref><ref type="bibr" target="#b27">Olivecrona et al., 2017;</ref><ref type="bibr" target="#b28">Popova et al., 2018;</ref><ref type="bibr" target="#b16">Kang &amp; Cho, 2018)</ref> generate molecules based on their SMILES strings <ref type="bibr" target="#b36">(Weininger, 1988)</ref>. <ref type="bibr" target="#b34">Simonovsky &amp; Komodakis (2018);</ref><ref type="bibr" target="#b5">De Cao &amp; Kipf (2018)</ref>; <ref type="bibr" target="#b25">Ma et al. (2018)</ref> developed generative models which output the adjacency matrices and node labels of the graphs at once. <ref type="bibr" target="#b40">You et al. (2018b)</ref>; ; <ref type="bibr" target="#b30">Samanta et al. (2018)</ref>; <ref type="bibr" target="#b24">Liu et al. (2018)</ref> proposed generative models decoding molecules sequentially node by node. <ref type="bibr" target="#b39">You et al. (2018a)</ref>; <ref type="bibr" target="#b41">Zhou et al. (2018)</ref> adopted similar node-by-node approaches in the context of reinforcement learning. <ref type="bibr" target="#b15">Kajino (2018)</ref> developed a hypergraph grammar based method for molecule generation.</p><p>Our work is most closely related to <ref type="bibr" target="#b13">Jin et al. (2018;</ref> that generate molecules based on substructures. They adopted a two-stage procedure for realizing graphs. The first step generates a junction tree with substructures as nodes, capturing their coarse relative arrangements. The second step resolves the full graph by specifying how the substructures should be attached to each other. Their major drawbacks are 1) The second step introduced local independence assumptions and therefore the decoder is not autoregressive. 2) These two steps are applied stage-wise during decoding -first realizing the junction tree and then reconciling attachments without feedback. In contrast, our method jointly predicts the substructures and their attachments with an autoregressive decoder.</p><p>Graph Encoders Graph neural networks have been extensively studied for graph encoding <ref type="bibr" target="#b31">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b1">Bruna et al., 2013;</ref><ref type="bibr" target="#b22">Li et al., 2015;</ref><ref type="bibr" target="#b26">Niepert et al., 2016;</ref><ref type="bibr" target="#b19">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b12">Hamilton et al., 2017;</ref><ref type="bibr" target="#b21">Lei et al., 2017;</ref><ref type="bibr" target="#b35">Velickovic et al., 2017;</ref><ref type="bibr" target="#b37">Xu et al., 2018)</ref>. Our method is related to graph encoders for molecules <ref type="bibr" target="#b7">(Duvenaud et al., 2015;</ref><ref type="bibr" target="#b17">Kearnes et al., 2016;</ref><ref type="bibr" target="#b2">Dai et al., 2016;</ref><ref type="bibr" target="#b9">Gilmer et al., 2017;</ref><ref type="bibr" target="#b32">Schütt et al., 2017)</ref>. Different to these approaches, our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees.</p><p>Our work is most closely related to <ref type="bibr" target="#b6">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b38">Ying et al., 2018;</ref><ref type="bibr" target="#b8">Gao &amp; Ji, 2019</ref>) that learn to represent graphs in a hierarchical manner. In particular, <ref type="bibr" target="#b6">Defferrard et al. (2016)</ref> utilized graph coarsening algorithms to construct multiple layers of graph hierarchy and <ref type="bibr" target="#b38">Ying et al. (2018)</ref>; <ref type="bibr" target="#b8">Gao &amp; Ji (2019)</ref> proposed to learn the graph hierarchy jointly with the encoding process. Despite some differences, all of these methods seek to represent graphs as a single vector for regression or classification tasks. In contrast, our focus is graph generation and a molecule is encoded into multiple sets of vectors, each representing the input at different resolutions. Those vectors are dynamically aggregated by decoder attention modules in each graph generation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HIERARCHICAL GENERATION OF MOLECULAR GRAPHS</head><p>The graph translation task seeks to learn a function F that maps a molecule X into another molecule G with better chemical properties. F is parameterized as an encoder-decoder with neural attention. Both our encoder and decoder are illustrated in <ref type="figure">Figure 1</ref>. In each generation step, our decoder adds a new substructure (substructure prediction) and decides how it should be attached to the current graph. The attachment prediction proceeds in two steps: predicting attaching points in the new substructure and their corresponding attaching points in the current graph (attachment prediction 1-2).</p><p>To support the above hierarchical generation, we need to design a matching encoder representing molecules at multiple resolutions in order to provide necessary information for each decoding step. <ref type="figure">Figure 1</ref>: Overview of our approach. Each substructure S i is a subgraph of a molecule (e.g., rings). In each step, our decoder adds a new substructure and predicts its attachment to current graph. Our encoder represents each molecule across three levels (atom layer, attachment layer and substructure layer), with each layer capturing relevant information for the corresponding decoding step. Therefore, we propose to represent a molecule X by a hierarchical graph H X with three components: 1) substructure layer representing how substructures are coarsely connected; 2) attachment layer showing the attachment configuration of each substructure; 3) atom layer showing how atoms are connected in the graph. Our model encodes nodes in H X into substructure vectors c S X , attachment vectors c A X and atom vectors c G X , which are fed to the decoder for corresponding prediction steps. As our encoder is tailored for the decoder, we first describe our decoder to clarify relevant concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HIERARCHICAL GRAPH DECODER</head><p>Notations We denote the sigmoid function as σ(·). MLP(a, b) represents a multi-layer neural network whose input is the concatenation of a and b. attention θ (h * , c X ) stands for a bi-linear attention over vectors c X with query vector h * .</p><p>Substructures We define a substructure S i = (V i , E i ) as subgraph of molecule G induced by atoms in V i and bonds in E i . Given a molecule, we extract its substructures S 1 , · · · , S n such that their union covers the entire molecular graph: V = i V i and E = i E i . In this paper, we consider two types of substructures: rings and bonds. We denote the vocabulary of substructures as S, which is constructed from the training set. In our experiments, |S| &lt; 500 and it has over 99.5% coverage on test sets.</p><p>Substructure Tree To characterize how substructures are connected in the molecule G, we construct its corresponding substructure tree T , whose nodes are substructures S 1 , · · · , S n . Specifically, we construct the tree by first drawing edges between S i and S j if they share common atoms, and then applying tree decomposition over T to ensure it is tree-structured. This allows us to significantly simplify the graph generation process.</p><p>Generation Our graph decoder generates a molecule G by incrementally expanding its substructure tree in its depth-first order. Suppose the model is currently visiting substructure node S k . It makes the following predictions conditioned on encoding of input X (see <ref type="figure">Figure 2</ref>):</p><p>1. Topological Prediction: It first predicts whether there will be a new substructure attached to S k .</p><p>If not, the model backtracks to its parent node S d k in the tree. Let h S k be the hidden representation of S k learned by the decoder (which will be elaborated in §3.2). This probability is predicted by a MLP with attention over substructure vectors c S X of X:</p><formula xml:id="formula_0">p k = σ(MLP(h S k , α d k )) α d k = attention d h S k , c S X<label>(1)</label></formula><p>2. Substructure Prediction: If p k &gt; 0.5, the model decides to create a new substructure S t from S k and sets its parent d t = k. It then predicts the substructure type of S t using another MLP that outputs a distribution over the vocabulary S:</p><formula xml:id="formula_1">p St = softmax(MLP(h S k , α s k )) α s k = attention s h S k , c S X<label>(2)</label></formula><p>3. Attachment Prediction: Now the model needs to decide how S t should be attached to S k . The attachment between S t and S k is defined as atom pairs</p><formula xml:id="formula_2">M t = {(u j , v j )|u j ∈ S t , v j ∈ S k }</formula><p>where atom u j and v j are attached together. We predict those atom pairs in two steps:</p><p>Figure 2: Illustration of hierarchical graph decoding. Suppose the decoder is visiting the substructure S k . 1) It decides to add a new substructure (topological prediction).</p><p>2) It predicts that new substructure S t should be a ring (substructure prediction) 3) It predicts how this new ring should be attached to the graph (attachment prediction). Finally, the decoder moves to S t and repeats the process. Dashed arrows connect each atom to the substructures it belongs. In the attachment layer, each node A i is a particular attachment configuration of substructure S i . Right: Attachment vocabulary for a ring. The attaching points in each configuration (highlighted in red) must be consecutive.</p><p>1) We first predict the atoms {v j } ⊂ S t that will be attached to S k . Since the graph S t is always fixed and the number of attaching atoms between two substructures is usually small, we can enumerate all possible configurations {v j } to form a vocabulary A(S t ) for each substructure S t . This allows us to formulate the prediction of {v j } as a classification task -predicting the correct configuration A t = (S t , {v j }) from the vocabulary A(S t ):</p><formula xml:id="formula_3">p At = softmax(MLP(h S k , α a k )) α a k = attention a h S k , c A X<label>(3)</label></formula><p>2) Given the predicted attaching points {v j }, we need to find the corresponding atoms {u j } in the substructure S k . As the attaching points are always consecutive, there exist at most</p><formula xml:id="formula_4">2|S k | different attachments M = {(u j , v j )}.</formula><p>The probability of a candidate attachment M is computed based on the atom representations h uj and h vj learned by the decoder:</p><formula xml:id="formula_5">p M = softmax h M · attention m (h M , c G X ) h M = j MLP(h uj , h vj )<label>(4)</label></formula><p>The above three predictions together give an autoregressive factorization of the distribution over the next substructure and its attachment. Each of the three decoding steps depends on the outcome of previous step, and predicted attachments will in turn affect the prediction of subsequent substructures. During training, we apply teacher forcing to the above generation process, where the generation order is determined by a depth-first traversal over the ground truth substructure tree. The attachment enumeration is tractable because most of the substructures are small. In our experiments, the average size of attachment vocabulary |A(S t )| &lt; 5 and the number of candidate attachments is less than 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">HIERARCHICAL GRAPH ENCODER</head><p>Our encoder represents a molecule X by a hierarchical graph H X in order to support the above decoding process. The hierarchical graph has three components (see <ref type="figure" target="#fig_0">Figure 3</ref>):</p><p>1. Atom layer: The atom layer is the molecular graph of X representing how its atoms are connected. Each atom node v is associated with a label a v indicating its atom type and charge. Each edge <ref type="bibr">(u, v)</ref> in the atom layer is labeled with b uv indicating its bond type. 2. Attachment layer: This layer is derived from the substructure tree of molecule X. Each node A i in this layer represents a particular attachment configuration of substructure S i in the vocabulary</p><formula xml:id="formula_6">A(S i ). Specifically, A i = (S i , {v j })</formula><p>where {v j } are the attaching atoms between S i and its parent S di in the tree. This layer provides necessary information for the attachment prediction (step 1). <ref type="figure" target="#fig_0">Figure 3</ref> illustrates how A i and the vocabulary A(S i ) look like. 3. Substructure layer: This layer is the same as the substructure tree. This layer provides essential information for the substructure prediction in the decoding process.</p><p>We further introduce edges that connect the atoms and substructures between different layers in order to propagate information in between. In particular, we draw a directed edge from atom v in the atom layer to node A i in the attachment layer if v ∈ S i . We also draw edges from node A i to node S i in the substructure layer. This gives us the hierarchical graph H X for molecule X, which will be encoded by a hierarchical message passing network (MPN) (see <ref type="figure" target="#fig_0">Figure 3</ref>). The encoder contains three MPNs that encode each of the three layer. We use the MPN architecture from <ref type="bibr" target="#b14">Jin et al. (2019)</ref>. 1 For simplicity, we denote the MPN encoding process as MPN ψ (·) with parameter ψ.</p><p>Atom Layer MPN We first encode the atom layer of H X (denoted as H g X ). The inputs to this MPN are the embedding vectors {e(a u )}, {e(b uv )} of all the atoms and bonds in X. During encoding, the network propagates the message vectors between different atoms for T iterations and then outputs the atom representation h v for each atom v:</p><formula xml:id="formula_7">c G X = {h v } = MPN ψ1 (H g X , {e(a u )}, {e(b uv )})<label>(5)</label></formula><p>Attachment Layer MPN The input feature of each node A i in the attachment layer H a X is an concatenation of the embedding e(A i ) and the sum of its atom vectors {h v | v ∈ S i }:</p><formula xml:id="formula_8">f Ai = MLP e(A i ), v∈Si h v<label>(6)</label></formula><p>The input feature for each edge (A i , A j ) in this layer is an embedding vector e(d ij ), where d ij describes the relative ordering between node A i and A j during decoding. Specifically, we set d ij = k if node A i is the k-th child of node A j and d ij = 0 if A i is the parent. We then run T iterations of message passing over H a X to compute the substructure representations:</p><formula xml:id="formula_9">c A X = {h Ai } = MPN ψ2 (H a X , {f Ai }, {e(d ij )})<label>(7)</label></formula><p>Substructure Layer MPN Similarly, the input feature of node S i in this layer is computed as the concatenation of embedding e(S i ) and the node vector h Ai from the previous layer. Finally, we run message passing over the substructure layer H s X to obtain the substructure representations:</p><formula xml:id="formula_10">f Si = MLP (e(S i ), h Ai ) c S X = {h Si } = MPN ψ3 (H s X , {f Si }, {e(d ij )})<label>(8)</label></formula><p>In summary, the output of our hierarchical encoder is a set of vectors c X = c S X ∪ c A X ∪ c G X that represent a molecule X at multiple resolutions. These vectors are input to the decoder attention.</p><p>Decoder MPN During decoding, we use the same hierarchical MPN architecture to encode the hierarchical graph H G at each step t. This gives us the substructure vectors h S k and atom vectors h vj in §3.1. All future nodes and edges are masked to ensure the prediction of current substructure and attachment only depends on previously generated outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">TRAINING</head><p>Our training set contains molecular pairs (X, Y ) where each compound X can be associated with multiple outputs Y since there are many ways to modify X to improve its properties. In order to generate diverse outputs, we follow <ref type="bibr" target="#b14">Jin et al. (2019)</ref> and extend our method to a variational translation model F : (X, z) → Y with an additional input z. The latent vector z indicates the intended mode of translation which is sampled from a Gaussian prior P (z) during testing. We train our model using variational inference <ref type="bibr" target="#b18">(Kingma &amp; Welling, 2013)</ref>. Given a training example (X, Y ), we sample z from the posterior Q(z|X, Y ) = N (µ X,Y , σ X,Y ). To compute Q(z|X, Y ), we first encode X and Y into their representations c X and c Y and then compute vector δ X,Y that summarizes the structural changes from molecule X to Y at both atom and substructure level:</p><formula xml:id="formula_11">δ S X,Y = c S Y − c S X δ G X,Y = c G Y − c G X<label>(9)</label></formula><p>Finally, we compute [µ X,Y , σ X,Y ] = MLP(δ S X,Y , δ G X,Y ) and sample z using reparameterization trick. The latent code z is passed to the decoder along with the input representation c X to reconstruct output Y . The overall training objective follows a standard conditional VAE:</p><formula xml:id="formula_12">L(X, Y ) = −E z∼Q [log P (Y |z, X)] + λ KL D KL [Q(z|X, Y )||P (z)]<label>(10)</label></formula><p>Algorithm 1 Variational Translation (unconditional setting, without target criteria g) 1: for (X, Y ) in the training set do 2:</p><formula xml:id="formula_13">Encode molecule X, Y into vectors c X , c Y 3: Compute µ X,Y , σ X,Y from δ X,Y 4: Sample latent code z ∼ Q(z|X, Y ) 5:</formula><p>Generate molecule Y given c X and z 6: end for <ref type="figure">Figure 4</ref>: Conditional translation.</p><p>Conditional Translation In the above formulation, the model does not know what properties are being optimized during translation. During testing, users cannot change the behavior of a trained model (i.e., what properties should be changed). This may become a limitation of our method in a multi-property optimization setting. Therefore, we extend our method to handle conditional translation where the desired criteria are also fed as input to the translation process. In particular, let g X,Y be a translation criteria indicating what properties should be changed. During variational inference, we compute µ X,Y and σ X,Y with an additional input g X,Y :</p><formula xml:id="formula_14">[µ X,Y , σ X,Y ] = MLP(δ S X,Y , δ G X,Y , g X,Y )<label>(11)</label></formula><p>We then augment the latent code as [z, g X,Y ] and pass it to the decoder. During testing, the user can specify their criteria in g X,Y to control the outcome (e.g., Y should be drug-like and bioactive).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We follow the experimental design by <ref type="bibr" target="#b14">Jin et al. (2019)</ref> and evaluate our translation model on their single-property optimization tasks. As molecular optimization in the real-world often involves different property criteria, we further construct a novel conditional optimization task where the desired criteria is fed as input to the translation process. To prevent the model from ignoring input X and translating it into arbitrary compound, we require the molecular similarity between X and output Y to be above certain threshold sim(X, Y ) ≥ δ at test time. The molecular similarity is defined as the Tanimoto similarity over Morgan fingerprints <ref type="bibr" target="#b29">(Rogers &amp; Hahn, 2010)</ref> of two molecules.</p><p>Single-property Optimization This dataset consists of four different tasks. For each task, we train and evaluate our model on their provided training and test sets. For these tasks, our model is trained under an unconditional setting (without g X,Y as input).</p><p>• LogP Optimization: The penalized logP score (Kusner et al., 2017) measures the solubility and synthetic accessibility of a compound. In this task, the model needs to translate input X into output Y such that logP(Y ) &gt; logP(X). We experiment with two similarity thresholds δ = {0.4, 0.6}. • QED Optimization: The QED score (Bickerton et al., 2012) quantifies a compound's druglikeness. In this task, the model is required to translate molecules with QED scores from the lower range [0.7, 0.8] into the higher range [0.9, 1.0]. The similarity constraint is sim(X, Y ) ≥ 0.4. • DRD2 Optimization: This task involves the optimization of a compound's biological activity against dopamine type 2 receptor (DRD2). The model needs to translate inactive compounds (p &lt; 0.05) into active compounds (p ≥ 0.5), where the bioactivity is assessed by a property prediction model from <ref type="bibr" target="#b27">Olivecrona et al. (2017)</ref>. The similarity constraint is sim(X, Y ) ≥ 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Optimization</head><p>This new task requires the model to translate input X into output Y to satisfy different combination of constraints over its QED and DRD2 scores. We define a molecule Y as drug-like if QED(Y ) ≥ 0.9 and as DRD2-active if its predicted bioactivity DRD2(Y ) ≥ 0.5. At test time, our model needs to handle the following two criteria over output molecule Y :</p><p>1. Y is both drug-like and DRD2-active. Here both properties need to be improved after translation. 2. Y is drug-like but DRD2-inactive. In this case, DRD2 is an off-target that may cause side effects. Therefore only the drug-likeness should be improved after translation.</p><p>As different users may be interested in different settings, we encode the desired criteria as vector g and train our model under the conditional translation setup in §3.3. Like single-property tasks, we impose a similarity constraint sim(X, Y ) ≥ 0.4 for both settings. Evaluation Metrics Our evaluation metrics include translation accuracy and diversity. Each test molecule X i is translated K = 20 times with different latent codes sampled from the prior distribution. On the logP optimization, we select compound Y i as the final translation of X i that gives the highest property improvement and satisfies sim(X i , Y i ) ≥ δ. We then report the average property improvement 1</p><formula xml:id="formula_15">D i logP(Y i ) − logP(X i ) over test set D.</formula><p>For other tasks, we report the translation success rate. A compound is successfully translated if one of its K translation candidates satisfies all the similarity and property constraints of the task. To measure the diversity, for each molecule we compute the average pairwise Tanimoto distance between all its successfully translated compounds.</p><p>Here the Tanimoto distance is defined as dist(X, Y ) = 1 − sim(X, Y ).</p><p>Baselines We compare our method (HierG2G) against the baselines including GCPN <ref type="bibr" target="#b39">(You et al., 2018a)</ref>, MMPA <ref type="bibr" target="#b4">(Dalke et al., 2018)</ref> and translation based methods Seq2Seq and JTNN <ref type="bibr" target="#b14">(Jin et al., 2019)</ref>. Seq2Seq is a sequence-to-sequence model that generates molecules by their SMILES strings. JTNN is a graph-to-graph architecture that generates molecules structure by structure, but its decoder is not fully autoregressive. We also compare with CG-VAE <ref type="bibr" target="#b24">(Liu et al., 2018</ref>), a generative model that decodes molecules atom by atom and optimizes properties in the latent space using gradient ascent.</p><p>To make a direct comparison possible between our method and atom-based generation, we further developed an atom-based translation model (AtomG2G) as baseline. It makes three predictions in each generation step. First, it predicts whether the decoding process has completed (no more new atoms). If not, it creates a new atom a t and predicts its atom type. Lastly, it predicts the bond type between a t and other atoms autoregressively to fully capture edge dependencies <ref type="bibr" target="#b40">(You et al., 2018b)</ref>. The encoder of AtomG2G encodes only the atom-layer graph and the decoder attention only sees the atom vectors c G X . All translation models (Seq2Seq, JTNN, AtomG2G and HierG2G) are trained under the same variational objective ( §3.3). Details of baseline architectures are in the appendix. <ref type="table" target="#tab_0">Table 1</ref>, our model achieves the new state-of-the-art on the four translation tasks. In particular, our model significantly outperforms JTNN in both translation accuracy (e.g., 76.9% versus 59.9% on the QED task) and output diversity (e.g., 0.564 versus 0.480 on the logP task). While both methods generate molecules by structures, our decoder is autoregressive which can learn more expressive mappings. More importantly, our model runs 6.3 times faster than JTNN during decoding. Our model also outperforms AtomG2G on three datasets, with over 10% improvement on the DRD2 task. This shows the advantage of our hierarchical encoding and decoding.  <ref type="figure">Figure 5</ref> illustrates how the input criteria g affects the generated output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-property Optimization As shown in</head><p>Ablation Study To understand the importance of different architecture choices, we report ablation studies over the QED and DRD2 tasks in <ref type="table" target="#tab_1">Table 2b</ref>. We first replace our hierarchical decoder with atom-based decoder of AtomG2G to see how much the structure-based decoding benefits us. We keep the same hierarchical encoder but modified the input of the decoder attention to include both atom and substructure vectors. Using this setup, the model performance decreases by 0.8% and 10.9% on the two tasks. We suspect the DRD2 task benefits more from structure-based decoding because biological target binding often depends on the presence of specific functional groups.</p><p>Our second experiment reduces the number of hierarchies in our encoder and decoder MPN, while keeping the same hierarchical decoding process. When the top substructure layer is removed, the translation accuracy drops slightly by 0.8% and 2.4%. When we further remove the attachment layer, the performance degrades significantly on both datasets. This is because all the substructure information is lost and the model needs to infer what substructures are and how substructure layers are constructed for each molecule. Implementation details of those ablations are shown in the appendix.</p><p>Lastly, we replaced our LSTM MPN with the original GRU MPN used in JTNN. While the translation performance decreased by 4% and 2.2%, our method still outperforms JTNN by a wide margin. Therefore we use the LSTM MPN architecture for both HierG2G and AtomG2G baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we developed a hierarchical graph-to-graph translation model that generates molecular graphs using chemical substructures as building blocks. In contrast to previous work, our model is fully autoregressive and learns coherent multi-resolution representations. The experimental results show that our method outperforms previous models under various settings. <ref type="figure">Figure 6</ref>: Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue Q. In each step, the model picks the first node v t from Q and predict whether there will be new atoms attached to v t . If so, it predicts the atom type of new node u t (atom prediction). Then the model predicts the bond type between u t and other nodes in Q sequentially for |Q| steps (bond prediction, |Q| = 2). Finally, it adds the new atom to the queue Q.</p><p>2. Otherwise, it creates a new atom u t and predicts its atom type. 3. Lastly, it predicts the bond type between u t and other frontier nodes in Q autoregressively to fully capture edge dependencies <ref type="bibr" target="#b40">(You et al., 2018b)</ref>. Since nodes are generated in breath-first order, there will be no edges between u t and nodes outside of Q.</p><p>To make those predictions, we use the same LSTM MPN to encode the current graph G t . Let h vt be the atom representation of v t . We represent G t as the sum of all its atom vectors h Gt = v∈Gt h v . In the first step, we model the probability of expanding a new node from v t as:</p><formula xml:id="formula_16">p t = σ(MLP(h vt , h Gt , α d t )) α d t = attention d [h vt , h Gt ], c G X<label>(13)</label></formula><p>In the second step, the atom type of the new node u t is predicted using another MLP:</p><formula xml:id="formula_17">q t = softmax(MLP(h vt , h Gt , α s t )) α s t = attention s [h vt , h Gt ], c G X<label>(14)</label></formula><p>In the last step, we predict the bonds between u t and nodes in Q = a 1 , · · · , a n sequentially starting with a 1 = v t . Specifically, for each atom pair (u t , a k ), we predict their bond type (single, double, triple or none) as the following:</p><formula xml:id="formula_18">b ut,a k = softmax(MLP(h Gt , h k ut , h a k , α b t ))<label>(15)</label></formula><formula xml:id="formula_19">α b t = attention b [h Gt , h k ut , h a k ], c G X<label>(16)</label></formula><p>where h a k is the atom representation of node a k and h k ut is the representation of node u t at the k th bond prediction. Let N k (u t ) be node u t 's current neighbor predicted in the first k steps. h k ut is computed as follows to reflect its local graph structure after k th bond prediction:</p><formula xml:id="formula_20">h k ut = MLP x ut , w∈N k (ut) ν w,ut ν w,ut = MLP(h w , x w,ut )<label>(17)</label></formula><p>where x ut is the atom feature of u t (i.e., predicted atom type) and x w,ut is the bond feature between w and u t (i.e., predicted bond type). Intuitively, this can be viewed as running one-step message passing at each bond prediction step (i.e., passing the message ν w,ut from w to u t ).</p><p>AtomG2G is trained under the same variational objective as HierG2G, with the latent code z sampled from the posterior</p><formula xml:id="formula_21">Q(z|X, Y ) = N (µ X,Y , σ X,Y ) and [µ X,Y , σ X,Y ] = MLP( c G Y − c G X ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS</head><p>Data The single-property optimization datasets are directly downloaded from the link provided in <ref type="bibr" target="#b14">Jin et al. (2019)</ref>. The training set and substructure vocabulary size for each dataset is listed in <ref type="table">Table 3</ref>. We constructed the multi-property optimization by combining the training set of QED and DRD2 optimization task. The test set contains 780 compounds that are not drug-like and DRD2-inactive. The training and test set is attached as part of the supplementary material.</p><p>Hyperparameters For HierG2G, we set the hidden layer dimension to be 270 and the embedding layer dimension 200. We set the latent code dimension |z| = 8 and KL regularization weight λ KL = 0.3. We run T = 20 iterations of message passing in each layer of the encoder. For AtomG2G, we set the hidden layer and embedding layer dimension to be 400 so that both models have roughly logP (δ = 0.6) logP (δ = 0.4) QED <ref type="table" target="#tab_0">DRD2  Training set size  75K  99K  88K  34K  Test set size  800  800  800  1000  Substructure vocabulary |S|  478  462  307</ref> 307 Average attachment vocabulary |A(S t )| 3.68 3.50 3.62 3.30 <ref type="table">Table 3</ref>: Training set size and substructure vocabulary size for each dataset. the same number of parameters. We also set λ KL = 0.3 and number of message passing iterations to be T = 20. We train both models with Adam optimizer with default parameters.</p><p>For CG-VAE <ref type="bibr" target="#b24">(Liu et al., 2018)</ref>, we used their official implementation for our experiments. Specifically, for each dataset, we trained a CG-VAE to generate molecules and predict property from the latent space. This gives us three CG-VAE models for logP, QED and DRD2 optimization tasks, respectively. At test time, each compound X is translated following the same procedure as in <ref type="bibr" target="#b13">Jin et al. (2018)</ref>. First, we embed X into its latent representation z and perform gradient ascent over z to maximize the predicted property score. This gives us z 1 , · · · , z K vectors for K gradient steps. Then we decode K molecules from z 1 , · · · , z K and select the one with the best property improvement within similarity constraint. We found that it is necessary to keep the KL regularization weight low (λ KL = 0.005) to achieve meaningful results. When λ KL = 1.0, the above gradient ascent procedure always generate molecules very dissimilar to the input X.</p><p>Ablation Study Our ablation studies are illustrated in <ref type="figure" target="#fig_1">Figure 7</ref>. In our first experiment, we changed our decoder to the atom-based decoder of AtomG2G. As the encoder is still hierarchical, we modified the input of the decoder attention to include both atom and substructure vectors. We set the hidden layer and embedding layer dimension to be 300 to match the original model size.</p><p>Our next two experiments reduces the number of hierarchies in both our encoder and decoder MPN.</p><p>In the two-layer model, molecules are represented by c X = c G X ∪ c A X . We make topological and substructure predictions based on hidden vector h A k instead of h S k because the substructure layer is removed. In the one-layer model, molecules are represented by c X = c G X and we make topological and substructure predictions based on atom vectors v∈S k h v . The hidden layer dimension is adjusted accordingly to match the original model size.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Left: Hierarchical graph encoder. Solid arrows illustrate message passing in each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Illustration of three different ablation studies. Left: Atom-based decoder; Middle: Twolayer encoder; Right: One-layer encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on single-property translation tasks. "Div." stands for diversity. "Succ." stands for success rate. "Improve." stands for average property improvement.</figDesc><table><row><cell>Method</cell><cell cols="2">logP (sim ≥ 0.6)</cell><cell cols="2">logP (sim ≥ 0.4)</cell><cell cols="2">QED</cell><cell cols="2">DRD2</cell></row><row><cell></cell><cell>Improve.</cell><cell>Div.</cell><cell>Improve.</cell><cell>Div.</cell><cell>Succ.</cell><cell>Div.</cell><cell>Succ.</cell><cell>Div.</cell></row><row><cell>JT-VAE</cell><cell>0.28 ± 0.79</cell><cell>-</cell><cell>1.03 ± 1.39</cell><cell>-</cell><cell>8.8%</cell><cell>-</cell><cell>3.4%</cell><cell>-</cell></row><row><cell>CG-VAE</cell><cell>0.25 ± 0.74</cell><cell>-</cell><cell>0.61 ± 1.09</cell><cell>-</cell><cell>4.8%</cell><cell>-</cell><cell>2.3%</cell><cell>-</cell></row><row><cell>GCPN</cell><cell>0.79 ± 0.63</cell><cell>-</cell><cell>2.49 ± 1.30</cell><cell>-</cell><cell>9.4%</cell><cell>0.216</cell><cell>4.4%</cell><cell>0.152</cell></row><row><cell>MMPA</cell><cell cols="8">1.65 ± 1.44 0.329 3.29 ± 1.12 0.496 32.9% 0.236 46.4% 0.275</cell></row><row><cell>Seq2Seq</cell><cell cols="8">2.33 ± 1.17 0.331 3.37 ± 1.75 0.471 58.5% 0.331 75.9% 0.176</cell></row><row><cell>JTNN</cell><cell cols="8">2.33 ± 1.24 0.333 3.55 ± 1.67 0.480 59.9% 0.373 77.8% 0.156</cell></row><row><cell cols="9">AtomG2G 2.41 ± 1.19 0.379 3.98 ± 1.54 0.563 73.6% 0.421 75.8% 0.128</cell></row><row><cell>HierG2G</cell><cell cols="8">2.49 ± 1.09 0.381 3.98 ± 1.46 0.564 76.9% 0.477 85.9% 0.192</cell></row></table><note>Our training set contains 120K molecular pairs and the test set has 780 compounds. For each pair (X, Y ), we set g X,Y = (I[Y is drug-like], I[Y is DRD2-active]). During testing, we translate each compound with g = [1, 1], [1, 0] for each setting. We note that the first criteria (g = [1, 1]) is the most challenging because there are only 1.6% of the training pairs with target Y being both drug-like and DRD2-active. To achieve good performance, the model must learn to transfer the knowledge from other pairs with g X,Y = [1, 0], [0, 1]) that partially satisfy the criteria.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on conditional optimization tasks and ablation studies over architecture choices.(a) Conditional optimization results: g = [1, * ] means the output Y needs to be drug-like and g = [ * , 1] means it needs to be DRD2-active. Conditional Optimization For this task, we compare our method with other translation methods: Seq2Seq, JTNN and AtomG2G. All these models are trained under the conditional translation setup where feed the desired criteria g X,Y as input. As shown inTable 2a, our model outperforms other models in both translation accuracy and output diversity. Notably, all models achieved very low success rate on c = [1, 1] because it has the strongest constraints and only 1.6K of the training pairs satisfy this criteria. In fact, training our model on the 1.6K examples only gives 4.2% success rate as compared to 13.0% when trained with other pairs. This shows our conditional translation setup can transfer the knowledge from other pairs with g X,Y = [1, 0], [0, 1].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Ablation study: the importance of hierarchi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">cal graph encoding, LSTM MPN architecture and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>structure-based decoding.</cell></row><row><cell>Method</cell><cell cols="2">g = [1, 1]</cell><cell cols="2">g = [1, 0]</cell><cell>Method</cell><cell>QED</cell><cell>DRD2</cell></row><row><cell></cell><cell>Succ.</cell><cell>Div.</cell><cell>Succ.</cell><cell>Div.</cell><cell>HierG2G</cell><cell>76.9% 85.9%</cell></row><row><cell>Seq2Seq</cell><cell>5.0%</cell><cell cols="3">0.078 67.8% 0.380</cell><cell cols="2">· atom-based decoder 76.1% 75.0%</cell></row><row><cell>JTNN</cell><cell cols="4">11.1% 0.064 71.4% 0.405</cell><cell>· two-layer encoder</cell><cell>75.8% 83.5%</cell></row><row><cell cols="5">AtomG2G 12.5% 0.031 74.5% 0.443</cell><cell>· one-layer encoder</cell><cell>67.8% 74.1%</cell></row><row><cell>HierG2G</cell><cell cols="4">13.0% 0.094 78.5% 0.480</cell><cell>· GRU MPN</cell><cell>72.6% 83.7%</cell></row><row><cell cols="7">Figure 5: Illustration of conditional translation. Our model generates different molecules when the</cell></row><row><cell cols="7">translation criteria changes. When g = [1, 1], the model indeed generates a compound with high</cell></row><row><cell cols="7">QED and DRD2 scores. When g = [1, 0], the model predicts another compound inactive to DRD2.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We slightly modified their architecture by using LSTM instead of GRU for message propagation due to its better empirical performance. The details are shown in the appendix.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A NETWORK ARCHITECTURE LSTM MPN Architecture The LSTM MPN is a slight modification from the MPN architecture used in <ref type="bibr" target="#b14">Jin et al. (2019)</ref>. Let N (v) be the neighbors of node v, x v the node feature of v and x uv be the feature of edge <ref type="bibr">(u, v)</ref>. During encoding, each edge (u, v) is associated with two messages ν uv and ν vu , representing the message from u to v and vice versa. The messages are updated by an LSTM cell with parameters ψ = {W z ψ , W o ψ , W r ψ , W ψ } defined as follows:</p><p>Attention Layer Our attention layer is a bilinear attention function with parameter θ = {A θ }:</p><p>AtomG2G Architecture AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with substructures. The encoder of AtomG2G is the same LSTM MPN over molecular graph. This gives us a set of atom vectors c G X representing molecule X only at the atom level. The decoder of AtomG2G is illustrated in <ref type="figure">Figure 6</ref>. Following <ref type="bibr" target="#b40">You et al. (2018b)</ref>; <ref type="bibr" target="#b24">Liu et al. (2018)</ref>, the model generates molecule G atom by atom following their breath-first order. During generation, it maintains a FIFO queue Q that contains the frontier nodes in the graph (i.e., nodes who still have neighbors to be generated). Let v t be the first node in Q and G t be the current graph at step t. In each step, the model makes three predictions to expand the graph G t :</p><p>1. It predicts whether there will be new atoms attached to v t . If not, the model discards v and move on to the next node in Q. The generation stops if Q is empty.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G Richard Bickerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémy</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorel</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew L</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature chemistry</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08786</idno>
		<title level="m">Syntax-directed variational autoencoder for structured data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">mmpdb: An open-source matched molecular pair platform for large multiproperty data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dalke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Hert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<title level="m">Graph u-net. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamín</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscentsci.7b00572</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Objective-reinforced generative adversarial networks (organ) for sequence generation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Gabriel Lima Guimaraes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro Luis Cunha</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10843</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multimodal graph-tograph translation for molecular optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Molecular hypergraph grammar with its application to molecular optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kajino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02745</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional molecular design with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Miguel Hernández-Lobato</forename><surname>Paige</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<title level="m">Grammar variational autoencoder</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constrained generation of semantically valid graphs via regularizing variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7113" to="7124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for de novo drug design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariya</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olexandr</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tropsha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7885</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bidisha</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gourhari</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratim</forename><surname>Kumar Chattaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<title level="m">Nevae: A deep generative model for molecular graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generating focussed molecule libraries for drug discovery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Marwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kogej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">P</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01329</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02473</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08773</idno>
		<title level="m">Graphrnn: A deep generative model for graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Optimization of molecules via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Richard N Zare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08678</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
