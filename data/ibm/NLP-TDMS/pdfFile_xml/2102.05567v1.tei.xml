<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Generative Adversarial Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Lazcano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolás</forename><surname>Fredes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Creixell</surname></persName>
						</author>
						<title level="a" type="main">Hyperbolic Generative Adversarial Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-GAN</term>
					<term>HGAN</term>
					<term>Hyperbolic Neural Networks</term>
					<term>Non-Euclidean</term>
					<term>Hyperbolic Spaces</term>
					<term>Poincaré Ball</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Hyperbolic Spaces in the context of Non-Euclidean Deep Learning have gained popularity because of their ability to represent hierarchical data. We propose that it is possible to take advantage of the hierarchical characteristic present in the images by using hyperbolic neural networks in a GAN architecture. In this study, different configurations using fully connected hyperbolic layers in the GAN, CGAN, and WGAN are tested, in what we call the HGAN, HCGAN, and HWGAN, respectively. The results are measured using the Inception Score (IS) and the Fréchet Inception Distance (FID) on the MNIST dataset. Depending on the configuration and space curvature, better results are achieved for each proposed hyperbolic versions than their euclidean counterpart.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: Triangle on a hyperbolic paraboloid.</p><p>Hierarchy o cyclicity in the data can be better exploited through embeddings in a Riemannian manifold. A Riemannian manifold is a "curved" space. Intuitively, the curvature concept can be understood by analyzing a surface. The surface curvature will be represented by how much any point in the surface deviates from a tangent plane. For spaces that present constant curvature, there are three families <ref type="bibr" target="#b4">[5]</ref>, those with positive curvature or elliptic spaces, negative curvature also known as hyperbolic spaces, and the Euclidean space with zero curvature <ref type="bibr" target="#b5">[6]</ref>. In a Riemannian Manifold, the fifth axiom of Euclid (parallel postulate) does not hold <ref type="bibr" target="#b6">[7]</ref>, this postulate is equivalent to assert that in a triangle, the internal angles add 180° <ref type="bibr" target="#b4">[5]</ref>. Instead, depending on the intrinsic space curvature, the sum of the angles in a triangle can be bigger than 180°f or elliptic spaces and less than 180°for hyperbolic spaces, as shown in <ref type="figure">figure 1</ref> for a triangle in a hyperbolic paraboloid.</p><p>Non-Euclidean spaces with constant curvature have properties useful for providing better representations for a certain type of structured data. The elliptical or spherical spaces are well suited for representing data with a cyclical structure <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b8">[9]</ref>. On the other hand, for hierarchically structured data, Hyperbolic spaces, particularly the Poincaré ball shares the same metric structure as trees <ref type="bibr" target="#b9">[10]</ref>. Hierarchical structural properties are strongly present in text and graphs <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref>, motivating different works using Poincaré Ball space. Tifreaa et al. proposed an adaptation of the GloVe <ref type="bibr" target="#b12">[13]</ref> embedding algorithm in a Poincaré Ball <ref type="bibr" target="#b13">[14]</ref> and Dhingra et al. developed a text embedding, using neural networks, in a hyperbolic space <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, a method is proposed to make link prediction over a graphs of words, embedded on a low dimension Poincaré ball, to achieve better results in word similarity and lexical entailment. However, besides text, other data types have underlying or latent tree structures, where Poincaré spaces have been less used. Images do not present an apparent hierarchical behavior; however, they can arXiv:2102.05567v1 <ref type="bibr">[cs.</ref>LG] 10 Feb 2021 be grouped in classes because of their similarities. These classes can also be grouped in other classes, increasing the abstraction level or going up in a tree structure. An example of the hierarchy in images is the WordNet-ImageNet <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b17">[18]</ref> dataset. The WordNet dataset is a compound of words (nouns and adjectives) organized in a tree, and in the ImageNet dataset, the images are organized following the WordNet tree. The Hyperbolic space can leverage the hierarchy of ImageNet as shown in <ref type="figure" target="#fig_0">figure 2</ref>, where WordNet-ImageNet mammals tree embedded in a two-dimensional Poincaré Ball using the method proposed in <ref type="bibr" target="#b15">[16]</ref>. This argument was first wielded in <ref type="bibr" target="#b18">[19]</ref> where they claim that the hierarchical semantic structure of language concepts can also be present in the images of those concepts, as in our example with mammals in <ref type="figure" target="#fig_0">figure 2</ref>. In that work, they modify the last section of three network architectures adding hyperbolic embedding to a Poincaré space followed by hyperbolic layers for the few-shot learning task. The hyperbolic layers were proposed in <ref type="bibr" target="#b19">[20]</ref> adapting the MLP, RNN, GRU, MLR to the hyperbolic space plus two layers for mapping from euclidean space to hyperbolic, called exponential map, and vice versa called logarithmic map. Hyperbolic Neural Networks (HNN) have been useful for the data exhibiting hierarchical latent anatomy because they increase its representation fidelity, with less distortion and dimensional requirements <ref type="bibr" target="#b7">[8]</ref>.</p><p>Similar to the embeddings, hyperbolic spaces have been employed to improve generation tasks on deep learning, both on text and images. Shuyang Dai et al. <ref type="bibr" target="#b20">[21]</ref> developed text generation in hyperbolic space, using a hyperbolic version of a Variational Auto Encoder (VAE). Hyperbolic VAE's also have been used to hierarchically represent images on the Poincaré ball <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b22">[23]</ref>. The VAE can be used to learn efficient representations and also for generation. However, the most popular architecture for the image generation tasks is the Generative Adversarial Network (GAN) <ref type="bibr" target="#b23">[24]</ref>. GAN consists of two networks that participate in an adversarial game during training. This architecture comprises a generator network trying to generate data similar to the training dataset and a discriminator network trying to differentiate between real and generated data. The generator network is trained in an unsupervised way evaluating its output against the discriminator. The discriminator is trained in a supervised way for classifying real and fake images. After the networks have been trained, the generator can produce data similar to the dataset, but with original variations, only from a noise input. There are multiple variations and modifications to the original GAN, but, to the best of our knowledge, there is no application of hyperbolic space to GANs architectures that aim to exploit the data's hierarchical characteristics, as in the VAE architecture.</p><p>We propose that the GAN architecture can take advantage of the hierarchical characteristics present in the images by using hyperbolic neural networks. Thereby, they can achieve better quality and diversity of the generated images. In this work, we use the GAN <ref type="bibr" target="#b23">[24]</ref>, WGAN <ref type="bibr" target="#b24">[25]</ref>, and CGAN <ref type="bibr" target="#b25">[26]</ref> architectures, conforming to their hyperbolic versions HGAN, HWGAN, and HCGAN. In each case, experiments with different arrangements and combinations of euclidean with hyperbolic layers and different curvature values were conducted. The performance over the MNIST dataset was measured by the Inception Score (IS) <ref type="bibr" target="#b26">[27]</ref>, and Fréchet Inception Distance (FID) <ref type="bibr" target="#b27">[28]</ref>, achieving better results for some layers arrangements for each architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND A. The Poincaré Ball</head><p>The hyperbolic spaces H n are n-dimensional Riemannian manifolds homogeneous and simply connected with a constant negative curvature. There are multiple models of hyperbolic space, but this research focuses on The Poincaré Ball. The Poincaré Ball manifold (D n c , g D n c u ) is a n-ball over R n of radius 1/ √ c. In this space, the ball center is the hyperbolic version of euclidean zero, and the ball perimeter is the infinity.</p><formula xml:id="formula_0">The Poincaré Ball (D n c , g D n c u ) is defined as: D n c = {u ∈ R n : c ||u|| 2 &lt; 1, c ≥ 0}<label>(1)</label></formula><p>With the Riemannian metric tensor:</p><formula xml:id="formula_1">g D n c u = λ c u g E (2) λ c u := 2 1 − c ||u|| 2<label>(3)</label></formula><p>λ c u is called the conformal factor of this space, and g E = I n is the Euclidean metric tensor in the cannonical base. With T u the tangent space operator, the exponential map takes a vector x ∈ T u D n c ∼ = R n and assign it to a vector v ∈ D n c . Its equation is given by: where the Möbius addition ⊕ c , for u, v ∈ D n c , is defined as:</p><formula xml:id="formula_2">exp c u (x) := (u) ⊕ c tanh √ c ||x||λ c u 2 x √ c||x|| (4)</formula><formula xml:id="formula_3">u ⊕ c v := 1 + 2c u, v + c v 2 u + 1 − c u 2 v 1 + 2c u, v + c 2 u 2 v 2<label>(5)</label></formula><p>The inverse of exponential map, the logarithmic map, takes a vector v ∈ D n c and assign it to a vector</p><formula xml:id="formula_4">x ∈ T u D n c ∼ = R n . log c u (v) := 2 √ c λ c u tanh −1 ( √ c||−u⊕ c v||) −u ⊕ c v || − u ⊕ c v||<label>(6)</label></formula><p>For simplicity we choose u as the origin of R n and the equations (4) and <ref type="bibr" target="#b5">(6)</ref> </p><formula xml:id="formula_5">result in: v = exp c 0 (x) := tanh( √ c||x||) x √ c||x|| (7) x = log c 0 (v) := tanh −1 ( √ c||v||) v √ c||v|| (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperbolic Deep Learning</head><p>Hyperbolic spaces have a high capacity to represent data with tree-like structures. <ref type="figure" target="#fig_1">Figure 3</ref> shows how a square grid shapes into a tree like-structure in a 2D Poincaré ball. When applied to neural network layers, this property allows a better representation of hierarchical data, which is exploited in Hyperbolic Neural Networks (HNN's) <ref type="bibr" target="#b19">[20]</ref>. The HNN uses the gyrovectors to implement the basic neural network operations in the Poincaré Ball. The gyrovector spaces allows create a simil of a vector space in a non-eculidean space like the hyperbolic. The gyrovector space used to operate in the Poincaré Ball is the Möbius, with two binary operations the Möbius addition <ref type="bibr" target="#b4">(5)</ref>, and the Möbius scalar multiplication of k ∈ R by vector v ∈ D n c , defined as:</p><formula xml:id="formula_6">k ⊗ c v := exp c 0 (r · log c 0 (v))<label>(9)</label></formula><p>Equation <ref type="formula" target="#formula_6">(9)</ref> together with equation <ref type="formula" target="#formula_3">(5)</ref> allow us to build feedfordward networks. A Feedforward network consists of an affine transformation and a non-linear function as activation.</p><p>The hyperbolic feedforward network <ref type="bibr" target="#b19">[20]</ref> has a gyrovectorbased affine transformation with Möbius matrix-vector multiplication and Möbius addition the bias vector. The Möbius matrix-vector multiplication was defined in <ref type="bibr" target="#b19">[20]</ref>; the procedure implementation is similar to Möbius scalar multiplication <ref type="bibr" target="#b8">(9)</ref>, it uses the exponential and logarithmic mappings. For v ∈ D n c to be multiplied by a Matrix M ∈ R m×n , v is taken to the Euclidean space by a logarithmic map, and multiplied by M . The exponential map is then applied to the resulting vector. This operation (D n c → D m c ) is given by:</p><formula xml:id="formula_7">M ⊗ c v := exp c 0 (M · log c 0 (v))<label>(10)</label></formula><p>The bias Möbius addition is a translation of gyrovector v ∈ D n c by bias b ∈ D n c , that is given by:</p><formula xml:id="formula_8">v ⊕ c b = exp c v λ c 0 λ c v log c 0 (b) ,<label>(11)</label></formula><p>Additionally, in order to apply any function to a gyrovector, the Möbius version of the function is required. Similarly to Möbius matrix-vector multiplication, let f :</p><formula xml:id="formula_9">R n → R m , the Möbius version f ⊗c that map from D n c to D m c is defined by: f ⊗c (v) := exp c 0 (f (log c 0 (v)))<label>(12)</label></formula><p>C. GAN Generative Adversarial Networks <ref type="bibr" target="#b23">[24]</ref> are an architecture of two networks that participate in an adversarial game during training. One, the Generator, is in charge of producing artificial images from noise input. The other, the discriminator, is in charge of classifying between real images from the artificially generated ones. Therefore, during training, the generated images are passed to the discriminator input, which in turn alternates artificial and real images. The roles in the adversarial game are as follows: the generator G is trying to produce images similar to the real ones in order to fool the discriminator D; on the other hand, the discriminator is trying to detect whether a particular image is real or artificially generated by G. In this way, the generator G(z; θ g ) learn the distribution p data of the images x from the noise input z, and the discriminator D(x; θ d ) estimates the probability of x being real or artificial. Equation <ref type="formula" target="#formula_0">(13)</ref> shows the loss function for the GAN, a minmax game, as proposed by <ref type="bibr" target="#b23">[24]</ref>.</p><formula xml:id="formula_10">min G max D V (G, D),<label>(13)</label></formula><p>Where</p><formula xml:id="formula_11">V (G, D) =E x∼p data (x) [log(D(x))] + E z∼pz(z) [log(1 − D(G(z)))]<label>(14)</label></formula><p>The GAN's training finishes when the discriminator is unable to distinguish between real images from generated images. Once trained, the generator network can be used independently for generating images. However, it is impossible to control the output in any way. Furthermore, the GAN network can have challenges to train because of vanishing gradients, and mode collapse; more details about these issues can be found in <ref type="bibr" target="#b28">[29]</ref>. In this work, in addtition to the GAN, we use two modifications to the original GAN architecture, the Conditional GAN (CGAN) <ref type="bibr" target="#b25">[26]</ref>, and the Wasserstein GAN (WGAN) <ref type="bibr" target="#b24">[25]</ref>.</p><p>The CGAN has additional class label information concatenated in the input layer of the generator and discriminator network. This additional information condition both networks, like a digit label, when working with the MNIST dataset. Equation <ref type="bibr" target="#b14">(15)</ref> shows the loss function for the CGAN, which includes the label information y for conditioning at the discriminator and generator.</p><formula xml:id="formula_12">V (G, D) =E x∼p data (x) [log(D(x|y))] + E z∼pz(z) [log(1 − D(G(z|y)|y))]<label>(15)</label></formula><p>The WGAN addresses the issues of vanishing gradients and mode collapse of the original GAN architecture. To achieve this goal, they use Wasserstein distance to have a smoother gradient everywhere, but the distance equation is intractable. The Kantorovich-Rubinstein duality simplifies the equation of distance as <ref type="formula" target="#formula_0">(16)</ref>:</p><formula xml:id="formula_13">W (P r , P θ ) = sup f L ≤1 E x∼Pr [f (x)] − E x∼P θ [f (x)]<label>(16)</label></formula><p>Where sup indicate the supremum, P r and P θ two distributions, and f is a 1-Lipschitz function that meet the following constraint:</p><formula xml:id="formula_14">|f (x 1 ) − f (x 2 )| ≤ |x 1 − x 2 |.<label>(17)</label></formula><p>To calculate the Wasseterin distance is necessary to find a 1-Lipschitz function. However, in practice this function can be learned by a neural network, and the discriminator network D, without sigmoid function in the output layer, is an ideal candidate for doing this task. Under this configuration, the output of the discriminator can be any real number, and the bigger this score, the closest to a real image the input image is. To enforce D to comply with 1-Lipschitz restrictions, WGAN can apply a gradient penalty <ref type="bibr" target="#b29">[30]</ref>. The WGAN with Gradient Penalty (WGAN-GP) works since any differentiable function f is 1-Lipschitz if and only if its gradient have norm less or equal to one in the whole space (check the demonstration in <ref type="bibr" target="#b29">[30]</ref>). The loss function of the WGAN-GP is given by:</p><formula xml:id="formula_15">min D E z∼pz(z) [D(G(z))] − E x∼p data (x) [D(x)] + λ Ex ∼px(x) [(||∇xD(x)|| 2 − 1) 2 ]<label>(18)</label></formula><formula xml:id="formula_16">max G E z∼pz(z) [D(G(z))]<label>(19)</label></formula><p>Wherex sampled from fake images G(z), and real images x with uniformly sampled between 0 and 1.</p><formula xml:id="formula_17">x = x + (1 − ) G(z)<label>(20)</label></formula><p>III. HYPERBOLIC GAN (HGAN)</p><p>The central assumption in this work is that the hyperbolic neural networks can improve GAN's performance, either in the generation or discrimination process, because the hyperbolic layers can leverage the hierarchical characteristics of the images <ref type="bibr" target="#b18">[19]</ref>. This hyperbolic version of GAN, the HGAN, mix the hyperbolic and euclidean space in either the generator and discriminator, replacing some of the euclidean linear layers by the hyperbolic linear layers implemented in <ref type="bibr" target="#b18">[19]</ref>; therefore, the use of exponential and logarithmic mapping is necessary to move between euclidean and hyperbolic spaces. The use of different spaces in the neural networks allows the creation of abstract features representing different implicit data structures, like the hierarchical structure as already mentioned. This approach adds more degree of freedom to the HGAN design by the number of hyperbolic layers, their location, and the hyperbolic space curvature (represented by c). The HGAN is a family of architectures derived from modifying the original GAN architecture proposed by Goodfellow et al. <ref type="bibr" target="#b23">[24]</ref>, <ref type="figure">figure 4</ref> shows the general architecture that we implemented. Similarly to the HGAN, the HWGAN, and HCGAN, correspond to modifications of the original WGAN-GP <ref type="bibr" target="#b29">[30]</ref>, and CGAN <ref type="bibr" target="#b25">[26]</ref> architectures by replacing some euclidean by hyperbolic layers.</p><p>The HGAN family architectures can have both euclidean layers and hyperbolic layers in different arrangements, with the corresponding exponential and logarithm mapping, center on zero, necessary to pass from the euclidean space to hyperbolic space and vice versa. The notation of this architecture's different configurations are D eehh , for the discriminator, and G hhee , for the generator, the subscripts denote which layers they are composed of. For example, G hhee means that the generator network comprises an exponential map, two hyperbolic layers, a logarithmic map, and two euclidean layers, the reading order from left to right correspond the order from input to output in the network. For simplicity, both exponential and logarithmic mappings are omitted from the notation. With this notation, the GAN architecture is represented by D eeee G eeee .</p><p>The place of hyperbolic layers in the network has a significant influence on the HGAN performance. Three different general configurations were tested. First, the HGAN with a euclidean-hyperbolic (EH) configuration, consisting of euclidean layers, an exponential map, and finalize with hyperbolic layers. The EH configuration first generates a feature vector in euclidean space, and then the abstract representations are processed in the Poincaré Ball. We believe that the EH configurations could improve the discriminator network's performance because the image hierarchical structure should be relevant in deep process layers where there is a more abstract representation. The second configuration is the hyperbolic-euclidean (HE), which consists of the exponential map in the network input, followed by hyperbolic layers, logarithmic map, and euclidean layers in the network output. This sequence, HE, could improve performance by creating hierarchical representations first in  <ref type="figure">Fig. 4</ref>: HGAN Architecture the hyperbolic space that can help the euclidean layers task. Finally, we have the Euclidean-Hyperbolic-Euclidean (EHE) configurations that consist of the network start with euclidean layers, followed by the exponential map, hyperbolic layers, logarithmic map, and euclidean layers again to the network end. The EHE configuration first creates a representation in the euclidean space enriched with abstract features that can then be exploited by the hyperbolic layers, and the final layers map it to euclidean space. The generator can better exploit this characteristic since it has to create an entire image from a complete non hierarchical noise input.</p><p>Another degree of freedom is the c value, which has a significant influence on HGAN performance. The Poincaré Ball radius r is related to c by r = 1/ √ c. Therefore, c represents how big the Poincaré ball is, and its effect when mapping the MINIST dataset can be seen in <ref type="figure" target="#fig_3">figure 5</ref>. The distribution of the magnitude of the images in the hyperbolic space change as c varies, for c = 10 −5 (r ≈ 316) the distribution is near 0; on the other hand, for c = 10 −2 (r = 10) the distribution is squashed to the r value. A wrong selection of c causes a poor behavior or fault in the convergence because the data can collapse into the ball's boundary or to the origin and it can also produce numeric instability. The study of the appropriate c values for a specific data set is a crucial task.</p><p>Finally, the HGANs architectures used Leaky ReLu directly in hyperbolic space as an activation function, without a logarithmic and exponential map, which differs from the standard procedure to implement functions in the hyperbolic space. Furthermore, the HGAN has standard dropout layers for regularization, and the method used for optimization was Adam <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The experiments consist of training the architectures with the MNIST dataset. The performance of each architecture is measure with the Inception Score (IS) <ref type="bibr" target="#b31">[32]</ref> and the Fréchet Inception Distance (FID) <ref type="bibr" target="#b27">[28]</ref>.</p><p>The GAN implementation has four linear layers on each network. The discriminator input is a vectorized image with 784 dimensions, followed by layers of 1024, 512, 256, and 1 hidden units respectively. For the generator, the input layer has 128 units, which corresponds to Gaussian noise, and the hidden layers are of size 256, 512, 1024, and 784, respectively. The discriminator contains dropout layers with a rate of 0.1. Both networks have a leaky ReLU with a leak factor equal a 0.2 for activation, except the network end, where the discriminator has no activation function, and the generator has an hyperbolic tangent. The loss function was the binary cross-entropy with logistic regression, and the training pipeline used the Adam <ref type="bibr" target="#b30">[31]</ref> optimizer with a learning rate equal to 0.001, β 1 = 0.5, β 2 = 0.999. The WGAN-GP used the same structure as the GAN. However, it uses the Wasserstein loss with gradient penalty <ref type="bibr" target="#b24">[25]</ref> and Adam optimizer with a learning rate equal to 0.0001 and the same beta values. The version of CGAN follows almost the same structure, but with the discriminator input of 794 and the generator input of 138, ten more nodes on both network in order to allocate the one-hot encoding for the class labels. The optimizer uses the same set of parameters than the WGAN-GP with binary cross entropy with logistic regression as loss function. The tested networks were compound of blocks of hyperbolic networks at the begging (HE), in the middle (EHE), and to the end (EH) for both generator and discriminator with different fixed c values. For configurations where both discriminator and generator have hyperbolic linear layers, combinations of the architectures in the previous step were combined into one, with the sole exception of HWGAN, where the discriminator never showed better performance than the euclidean version. Since the different nature of discriminator and generator tasks, the curvature for each was not always the same, c d was for the discriminator and c g for the generator. This distinction produced a remarkable improvement in the FID score from 67.291 for the euclidean GAN to 18.697 for the HGAN with c d = 10 −5 and c g = 10 −3 , as showed in table I. The best performing architectures are summarized in tables I for the HGAN, II for the HWGAN, and III HCGAN. The broader exploratory study obtained in this paper are shown in the annex figures 7, 8, 9, and 10.    V. CONCLUSION This work shows that the HGAN, HCGAN, and HWGAN architectures can perform as well as or, in some cases, much better than the original euclidean architectures. The performance depends heavily on two main factors: the curvature through the c parameter, and the architecture configuration. The best performance was achieved for the HGAN with c = 10 −3 , HCGAN with c = 10 −2 , and for the HWGAN with c = 10 −1 . For smaller values of c the radius growths and the hyperbolic layers behave as euclidean layers, consequently there is no improvement over the euclidean version. And for c &lt; 10 −6 the networks did not converge because of numerical instability. One last consideration for the c value was to make a distinction between the c used in the discriminator that can be different from the c used in the generator. This was because the different nature of the tasks performed by discriminator and generator, and it had as a consequence the best improvement of performance in the HGAN as showed in table I. For the configurations, the experiments show that the generator's EHE and the HE configurations had better performance. For the discriminator the HE and the EH configurations worked better. However, the HWGAN never showed a performance improvement with hyperbolic layers in the discriminator. When the discriminator was in the EH configuration, the logarithmic map at the network output was not applied, this because it became unstable for one dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. VISUALIZATION OF EXPERIMENTS</head><p>We implement a method to visualize all experiments for each architecture, HGAN, HCGAN, and HWGAN.</p><p>Also, the visualization shows each configuration and c value. The rows show the configurations, for example D hhhh G eehh , and the columns indicate the c value. The IS and FID measurements are in the x-axis. The bigger values of IS represent better performance (showed by the → arrow), on the other hand smaller values on the FID score represent better values (showed by the ← arrow). Therefore, when a network has good performance its IS and FID markers should be placed to the center of the column near each other, in contrast when the markers are far from each other and from the center it is an indicative of poor performance. Aditionally, the perfomance of the fully euclidean version is depicted by a vertical segment in each graph line. <ref type="figure">Figure 8</ref> shows the results for the HGAN, the best performing architectures can be seen for c = 10 −3 . The results for the HCGAN are displayed in <ref type="figure">figure 8</ref>, it is possible to observe that there are many configurations that presents better performance than the euclidean CGAN for all the tested values of c. Finally, <ref type="figure">figure 9 and 10</ref> show the results obtained for the HWGAN. The best perfoming architecture was found for c = 0.1.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>2D Poincaré embedding applied to WordNet database correlated with ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Tesselation of squares (sides of equal length) in a 2D Poincaré ball. The squares sides are in black, and the squares diagonals are in white.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Poincaré Ball embedded vectorized MNIST (x) distribution, from equation (7) normalized respect the Poincaré ball radius r using the function R(x) = || exp c 0 (x)||/r = tanh( √ c||x||) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Images of the best results generated by each architecture, on MNIST and measured by the FID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIDFig. 7 :</head><label>7</label><figDesc>HGAN experiments visualization with only hyperbolic discriminator or hyperbolic generator measure with FID and IS. The segment line indicate the euclidean GAN performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FID 20 FIDc = 1 .0e 5 Fig. 8 :</head><label>20158</label><figDesc>HCGAN experiments visualization with only hyperbolic discriminator or hyperbolic generator, and mixed hyperbolic configurations measure with FID and IS. The segment line indicate the euclidean CGAN performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :FIDc = 1 .0e 5 Fig. 10 :</head><label>91510</label><figDesc>HWGAN experiments visualization measure with FID and IS. The segment line indicate the euclidean WGAN performance. HWGAN experiments visualization with hyperbolic generator measure with FID and IS. The segment line indicate the euclidean WGAN performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The best HGAN results, measured by FID and IS with ten seeds for each configuration.</figDesc><table><row><cell>Architecture HWGAN</cell><cell>c d</cell><cell>cg</cell><cell>FID ↓</cell><cell>IS ↑</cell></row><row><cell>DeeeeGeeee</cell><cell>-</cell><cell>-</cell><cell>16.130</cell><cell>9.284</cell></row><row><cell>DeeeeG hhhe</cell><cell>-</cell><cell>0.1</cell><cell>12.969</cell><cell>9.424</cell></row><row><cell>DeeeeG ehhe</cell><cell>-</cell><cell>0.1</cell><cell>12.884</cell><cell>9.291</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The best HWGAN results, measured by FID and IS.</figDesc><table><row><cell>Architecture HCGAN</cell><cell>c d</cell><cell>cg</cell><cell>FID ↓</cell><cell>IS ↑</cell></row><row><cell>DeeeeGeeee</cell><cell>-</cell><cell>-</cell><cell>11.588</cell><cell>9.910</cell></row><row><cell>D hhhh G hhee</cell><cell>0.01</cell><cell>0.01</cell><cell>9.171</cell><cell>9.899</cell></row><row><cell>D hhee G hhee</cell><cell>0.01</cell><cell>0.01</cell><cell>9.157</cell><cell>9.903</cell></row><row><cell>D hhhe G hhee</cell><cell>1e-5</cell><cell>1e-5</cell><cell>10.940</cell><cell>9.919</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The best HCGAN results, measured by FID and IS.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spherical and hyperbolic embeddings of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Weisstein</surname></persName>
		</author>
		<title level="m">CRC concise encyclopedia of mathematics</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Introduction to non-Euclidean geometry. Courier Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riemann</forename><surname>Surfaces</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning mixed-curvature representations in product spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spherical and hyperbolic embeddings of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pekalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2255" to="2269" />
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application of hyperbolic geometry in link prediction of multiplex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Samei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4868" to="4879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Poincaré glove: Hyperbolic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2019)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embedding text in hyperbolic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing</title>
		<meeting>the Twelfth Workshop on Graph-Based Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
	<note>TextGraphs-12)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchicals representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyperbolic image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mirvakhabova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Apo-vae: Text generation in hyperbolic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00054</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous hierarchical representations with poincaré variational auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Le</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12565" to="12576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A wrapped normal distribution on hyperbolic space for gradient-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02992</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.04862</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980v9</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Alice: Towards understanding adversarial learning for joint distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
