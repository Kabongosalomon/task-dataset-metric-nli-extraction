<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DMCNN: DUAL-DOMAIN MULTI-SCALE CONVOLUTIONAL NEURAL NETWORK FOR COMPRESSION ARTIFACTS REMOVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DMCNN: DUAL-DOMAIN MULTI-SCALE CONVOLUTIONAL NEURAL NETWORK FOR COMPRESSION ARTIFACTS REMOVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Compression Artifacts Removal</term>
					<term>Image Restoration</term>
					<term>JPEG</term>
					<term>Convolutional Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>JPEG is one of the most commonly used standards among lossy image compression methods. However, JPEG compression inevitably introduces various kinds of artifacts, especially at high compression rates, which could greatly affect the Quality of Experience (QoE). Recently, convolutional neural network (CNN) based methods have shown excellent performance for removing the JPEG artifacts. Lots of efforts have been made to deepen the CNNs and extract deeper features, while relatively few works pay attention to the receptive field of the network. In this paper, we illustrate that the quality of output images can be significantly improved by enlarging the receptive fields in many cases. One step further, we propose a Dual-domain Multi-scale CNN (DMCNN) to take full advantage of redundancies on both the pixel and DCT domains. Experiments show that DMCNN sets a new state-of-the-art for the task of JPEG artifact removal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Nowadays, lossy image compression methods (e.g. JPEG, HEVC-MSP and WebP) have been used extensively for image storage and transmission. These methods typically shrink parts of image information by quantization and approximation, so that higher compression rates can be reached. These methods can usually reduce the bit-rate greatly but still maintain satisfactory visual quality by taking advantage of the limitation of the human visual system. But as the compression rate increases, these methods tend to introduce undesirable artifacts such as blocking, ringing, and banding. These artifacts severely degrade the user experience.</p><p>In this paper, we examine the degradation of JPEGcompressed images. Typically, a JPEG compressor converts an image of RGB color space into the YCbCr color space. The chroma channels (namely Cb and Cr) are downsampled by the factor of 2. Then, the image is partitioned into 8 × 8</p><p>blocks and the block-wise 2D Discrete Cosine Transform (DCT) is performed. After DCT, the top left items in each 8 × 8 block are low-frequency components, representing the overall features such as the average luminance. The bottom right items are high-frequency components, representing local features such as textures and details. Next, quantization is applied on each of 64 DCT coefficients. As human eyes are not so good at distinguishing high frequency brightness variation, quantization intervals are typically much larger on high-frequency components than low-frequency ones. Noting that the quantization step is the culprit for various kinds of artifacts such as the blocking artifacts within the boundaries of each 8 × 8 DCT block, the ringing artifacts around sharp edges, and the noticable banding effects over the image. As a matter of fact, these kinds of artifacts can be commonly seen on other transform-based methods.</p><p>Many methods have been proposed to improve the quality of JPEG-compressed images. Traditional filter-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> pay attention to general images denoising. Others apply the sparse coding (SC) to restore the compressed images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. These methods generally produce sharper images given a compressed input, but they are usually too slow and their results are often accompanied with additional artifacts.</p><p>With the rapid development of deep neural networks, multiple CNN-based methods have been used in low-level image processing, including denoising <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, super-resolution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, video compression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, rain removal <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Specifically for compression artifacts removal, Dong et al. <ref type="bibr" target="#b14">[15]</ref> first introduce a CNN-based method and the proposed ARCNN set a good practice for the following low-level CNNbased methods including DnCNN <ref type="bibr" target="#b4">[5]</ref>, CAS-CNN <ref type="bibr" target="#b15">[16]</ref>, and MemNet <ref type="bibr" target="#b16">[17]</ref>. However, these methods usually work on pixel domain only and do not incorporate much JPEG prior knowledge. More recently, a dual-domain CNN-based model DDCN is proposed in <ref type="bibr" target="#b17">[18]</ref>. The model sucessfully combines DCT-domain prior and the power of the CNN, thus achieves impressive performance.</p><p>However, a common weakness of all these CNN-based methods is that the receptive fields of their models are too small, and these models are usually trained on mini image patches (e.g. 35 × 35 for ARCNN, 49 × 49 for DnCNN, 55 × 55 for DDCN), so that only a small range of information (i.e. neighbor pixels) are taken into consideration when performing the restoration. Noting that local information is sometimes not enough to remove all the artifacts, especially the banding effects, which can appear at a large scale on the image. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, due to the quantization step of JPEG, the sky in the image is split into multiple bandings, and ARCNN fails to recover the bandings because of small receptive fields. To eliminate the banding effects, three efforts have been made to enlarge the receptive fields and enable our model with the ability of extracting global features: (1) Auto-encoder style architecture; (2) Dilated convolutions; (3) Multi-scale loss. Moreover, as validated by DDCN, redundancies in the DCT domain can be effectively utilized. We also adopt a DCT domain branch to enhance the performance of the proposed model.</p><p>Our major contribution is to propose an end-to-end CNN with large receptive fields to exploit dual-domain multi-scale features. In order to train the proposed DMCNN more effectively, a modified version of residual learning as well as other training techniques have been utilized. The evaluations on the BSDS500 and the LIVE1 dataset have shown that our work is the current state-of-the-art among all CNN-based JPEG artifact removal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DUAL-DOMAIN MULTI-SCALE CONVOLUTIONAL NEURAL NETWORK (DMCNN)</head><p>The architecture of our proposed DMCNN is in <ref type="figure" target="#fig_1">Fig. 2</ref>. The model is mainly composed of two similar auto-encoder style networks working on pixel and DCT domains, respectively. The input image is processed by DCT branch first, then passed into the pixel domain branch. The final restoration result is the weighted sum of the input, the DCT branch estimation and the pixel branch estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Auto-Encoder</head><p>The auto-encoder is an efficient way to learn a representation for given data, and typically used for the purpose of dimensionality reduction. An auto-encoder consists of two parts, the encoder E and the decoder D, so that:</p><formula xml:id="formula_0">E : X → F, D : F → X , E, D = arg min E,D x − (D • E)x 2 ,<label>(1)</label></formula><p>where X denotes the set of the to-be-compressed data, F denotes the feature space and • is the composition operator. Typically, an auto-encoder is trained with identical image pairs (x, x) ∈ X × X , so that E could learn to extract the best representation of the data in F.</p><p>We adopt an auto-encoder style network here as a generative model. Given a compressed image, the encoder E is expected to extract artifact-free features robustly, and then the image is restored from these clean features by decoder D.</p><p>In order to learn both local and global features, shortcuts are linked between symmetric layers. Also, residual learning strategy is employed. To stress the information learnt form the DCT domain, we add a shortcut with parameter r from the DCT-branch into the final result. Given an input image C, the intermediate estimation of the DCT branchÕ D and the final outputÕ 0 can be fomulated as:</p><formula xml:id="formula_1">O D = D −1 ([f (D(C))] DRU ), O 0 = g(C,Õ D ) + rÕ D + (1 − r)C,<label>(2)</label></formula><p>where D and D −1 stand for the process of 8 × 8 block-wise DCT and inverse DCT (IDCT) respectively. f (·) and g(·) denote the processes of the DCT domain auto-encoder and the pixel domain auto-encoder, respectively.</p><p>[·] DRU denotes the DCT Rectify Unit stated later, and r is a learnable parameter of the residual addition module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dilated Convolution</head><p>The dilated convolution is a kind of convolution with predefined gaps, it is first named in <ref type="bibr" target="#b18">[19]</ref>. Consider an input image I as a discrete function I : Z 2 → R and a convolution kernel k shaped (2r+1)×(2r+1) as a discrete function k : Ω r → R,</p><p>where Ω r = [−r, r] 2 ∩ Z 2 . The discrete convolution operator * d with dilation factor d can be defined as:</p><formula xml:id="formula_2">(I * d k)(p) = s+dt=p I(s)k(t),<label>(3)</label></formula><p>where p, s, t ∈ Z 2 are 2D indices. Unlike normal convolutions, the receptive field of n combined dilated convolutions can reach (2 n−1 − 1) × (2 n−1 − 1) when dilation factors are set to 1, 2, 4, ..., 2 n−1 , respectively. The dilated convolution has been widely used in other vision tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and shown considerable gain, but to the best of our knowledge, has not been used in the task of artifacts removal. In our model, the dilated convolutional layers with dilation factors 2, 4, 8 are used in the middle of the autoencoder, which aim to enlarge the receptive field further. With the combination of auto-encoder style architecture and dilated convolutions, the receptive field of our proposed DMCNN reaches 145 × 145, which is about 58 times larger than the ARCNN <ref type="bibr">(19 × 19)</ref>, and 13 times larger than the DnCNN and the DDCN (41 × 41).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">DCT Rectify Unit (DRU)</head><p>As stated earlier in the introduction, the main cause of the JPEG compression artifacts is the step of quantization. The quantization step in each of the 8 × 8 compression blocks can be formulated as follows:</p><formula xml:id="formula_3">C DCT (p) = round(O DCT (p)/Q(p)) * Q(p),<label>(4)</label></formula><p>where C DCT is the quantized DCT block, O DCT is the original DCT block, Q is the quantization table, and p ∈ Z 2 is a 2D index. / here denotes the element-wise division. From (4), it can be easily seen that the estimatedÕ DCT should meet the requirement:</p><formula xml:id="formula_4">C DCT − Q/2 ≤Õ DCT ≤ C DCT + Q/2.<label>(5)</label></formula><p>So like <ref type="bibr" target="#b17">[18]</ref> we employ a DCT Rectify Unit (DRU) to constraint the value of DCT block elements, where values out of the range will be cropped. A slight difference is that we drop the leaky slope α in their proposed unit, as no gain can be observed with it. Our DRU can be formulated as:</p><formula xml:id="formula_5">[X] DRU (p) =      C DCT (p) − Q(p)/2, X(p) &lt; C DCT (p) − Q(p)/2, C DCT (p) + Q(p)/2, X(p) &gt; C DCT (p) + Q(p)/2, X(p), otherwise.<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Multi-scale DCT-Embedded Loss</head><p>As is pointed out by previous works <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b14">[15]</ref>, "deeper is not better" in certain low-level tasks. The reason is that deeper neural networks are usually harder to train due to the gradient vanishing. We try to address this problem by redesigning the loss function of the model. A multi-scale loss is adopted to extract features at different scales. More specifically, features are extracted from different deconvolutional layers of the pixel domain decoder, and scaled images are expected to be reconstructed from these features. By adopting the multi-scale loss, we explicitly guide our network to learn features at different scales.</p><p>We also add a DCT loss to train the DCT branch more effectively. Finally, our loss function can be stated as:</p><formula xml:id="formula_6">L({Õ} 2 i=0 ,Õ D , {O} 2 i=0 ) = L MMSE ({Õ} 2 i=0 , {O} 2 i=0 ) + λL DCT (Õ D , O 0 ) = 2 i=0 θ i MSE(Õ i , O i ) + λMSE(Õ D , O 0 ),<label>(7)</label></formula><p>where {Õ} 2 i=0 are estimations from pixel domain autoencoder at different scales, {O} 2 i=0 are original images at different scales, andÕ D is the intermeidate estimation of the DCT branch. Hyper-parameters λ and θ are used to adjust the weights of each loss, they should typically be in the range of [0, 1]. MSE(·,·) denotes the mean squared error loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implement Details</head><p>Datasets. In all experiments, we employ the ImageNet <ref type="bibr" target="#b22">[23]</ref> for training. The LIVE1 dataset and the testing set of BSDS500 are used for evaluation. All training and evaluation processes are done on gray-scale images (the Y channel of YCbCr space). The PIL module of python is applied to generate JPEG-compressed images. The module produces numerically identical images as the commonly used MATLAB JPEG encoder after setting the quantization tables manually. Training Details. Adam optimizer with initial learning rate 0.001 is used for training. The learning rate is scaled down by the factor of 3 when the validation loss stops decreasing. The batch size is set to 80. Training pairs are dynamically generated from the training set. The sizes of training patches are not fixed. As an easy-to-hard transfer, we first train our model on pairs generated with quality factor (QF) of 20 and patch size of 56 × 56. Then we gradually increase the patch size till 224 × 224. After full convergence, the model dedicated to QF10 is trained based on the previous QF20 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Objective Comparisons</head><p>To have overall comparisons, we calculate the mean PSNR, SSIM <ref type="bibr" target="#b23">[24]</ref>, and PSNR-B [25] on the two datasets. We compare to recent state-of-the-arts including pixel domain methods -ARCNN and CAS-CNN, dual domain method -DDCN and general frameworks -TNRD <ref type="bibr" target="#b25">[26]</ref> and DnCNN <ref type="bibr" target="#b4">[5]</ref>.</p><p>The quantitative results are shown in <ref type="table">Table.1 and Table.</ref>2. Generally, our proposed model DMCNN outperforms all the other methods on all evaluated datasets, QFs and metrics. Specifically, our model far surpasses all pixel domain methods and general frameworks. Also, considerable gains can be observed compared to the dual domain method DDCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Subjective Comparisons</head><p>For subjective comparisons, some restored images of different approches on the LIVE1 dataset have been presented. As can be seen in <ref type="figure" target="#fig_2">Fig. 3</ref>, the results of DMCNN are more visually pleasing. Due to the large receptive fields, our model is able to handle quantization banding effects as well as recover lost details using regional patterns. More experiment results are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS</head><p>In this paper, we introduce a novel network based on dualdomain auto-encoders, named DMCNN. By applying dilated convolutional layers and multi-scale loss, our model is able to extract global information and eliminate JPEG compression artifacts effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The banding effects can still be clearly seen after the process of ARCNN (QF=10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of Dual-domain Multi-scale Convolutional Network (DMCNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Parameter Settings. The hyper-parameters λ and θ are Visual comparisons between different algorithms with QF=10. Zooming-in the figure will provide a better look at the restoration quality. set to 0.9 and 0.618, respectively. The parameter r of final residual links is initialized to 0.5. The DCT and IDCT layers are fixed and initialized with the corresponding DCT matrix coefficients. Leaky slopes are initialized to 0.1 for PReLUs. The depth of the pixel and DCT domain auto-encoders are 15 and 9, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>The quantitative results on LIVE1. The quantitative results on BSDS500 testing set.</figDesc><table><row><cell>QF</cell><cell>Method</cell><cell cols="3">PSNR(dB) SSIM PSNR-B(dB)</cell></row><row><cell></cell><cell>JPEG</cell><cell>27.77</cell><cell>0.791</cell><cell>25.33</cell></row><row><cell></cell><cell>ARCNN [15]</cell><cell>29.13</cell><cell>0.823</cell><cell>28.74</cell></row><row><cell>10</cell><cell>TNRD [26] DnCNN-3 [5]</cell><cell>29.24 29.27</cell><cell>0.825 0.825</cell><cell>28.90 28.98</cell></row><row><cell></cell><cell>CAS-CNN [16]</cell><cell>29.44</cell><cell>0.833</cell><cell>29.19</cell></row><row><cell></cell><cell>DMCNN</cell><cell>29.73</cell><cell>0.842</cell><cell>29.55</cell></row><row><cell></cell><cell>JPEG</cell><cell>30.07</cell><cell>0.868</cell><cell>27.57</cell></row><row><cell></cell><cell>ARCNN [15]</cell><cell>31.40</cell><cell>0.890</cell><cell>30.69</cell></row><row><cell>20</cell><cell>TNRD [26] DnCNN-3 [5]</cell><cell>31.52 31.62</cell><cell>0.892 0.894</cell><cell>30.88 30.89</cell></row><row><cell></cell><cell>CAS-CNN [16]</cell><cell>31.70</cell><cell>0.895</cell><cell>30.88</cell></row><row><cell></cell><cell>DMCNN</cell><cell>32.09</cell><cell>0.905</cell><cell>31.32</cell></row><row><cell>QF</cell><cell>Method</cell><cell cols="3">PSNR(dB) SSIM PSNR-B(dB)</cell></row><row><cell></cell><cell>JPEG</cell><cell>27.80</cell><cell>0.788</cell><cell>25.10</cell></row><row><cell></cell><cell>ARCNN [15]</cell><cell>29.10</cell><cell>0.820</cell><cell>28.73</cell></row><row><cell>10</cell><cell>TNRD [26] DnCNN-3 [5]</cell><cell>29.16 29.17</cell><cell>0.823 0.823</cell><cell>28.81 28.91</cell></row><row><cell></cell><cell>DDCN [18]</cell><cell>29.59</cell><cell>0.838</cell><cell>29.18</cell></row><row><cell></cell><cell>DMCNN</cell><cell>29.67</cell><cell>0.840</cell><cell>29.33</cell></row><row><cell></cell><cell>JPEG</cell><cell>30.05</cell><cell>0.867</cell><cell>27.22</cell></row><row><cell></cell><cell>ARCNN [15]</cell><cell>31.28</cell><cell>0.885</cell><cell>30.55</cell></row><row><cell>20</cell><cell>TNRD [26] DnCNN-3 [5]</cell><cell>31.41 31.50</cell><cell>0.889 0.891</cell><cell>30.83 30.85</cell></row><row><cell></cell><cell>DDCN [18]</cell><cell>31.88</cell><cell>0.900</cell><cell>31.10</cell></row><row><cell></cell><cell>DMCNN</cell><cell>31.98</cell><cell>0.904</cell><cell>31.29</cell></row><row><cell cols="3">available on our project page 1 .</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://i.buriedjet.com/projects/DMCNN/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing artifacts in jpeg decompression via a learned dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huibin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven sparsity-based restoration of jpeg-compressed images in dual transform-pixel domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dynamically unfolding recurrent restorer: A moving endpoint control method for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07709</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reference guided deep super-resolution via manifold localized external compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifeng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video super-resolution based on spatial-temporal recurrent residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced intra prediction with recurrent neural network in video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifeng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Data Compression Conference</title>
		<meeting>Data Compression Conference</meeting>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A group variational transformation neural network for fractional interpolation of video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifeng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Data Compression Conference</title>
		<meeting>Data Compression Conference</meeting>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Erase or fill? deep joint recurrent rain removal and reconstruction in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cas-cnn: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large receptive field convolutional neural network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE International Conference on Image Processing</title>
		<meeting>2017 IEEE International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quality assessment of deblocked images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhoon</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Conrad</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
