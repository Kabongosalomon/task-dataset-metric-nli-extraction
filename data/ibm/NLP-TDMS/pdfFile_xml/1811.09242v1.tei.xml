<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoSense Model for Word Sense Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amplayo Seung-Won</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Song</surname></persName>
							<email>min.song@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoSense Model for Word Sense Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Word sense induction (WSI), or the task of automatically discovering multiple senses or meanings of a word, has three main challenges: domain adaptability, novel sense detection, and sense granularity flexibility. While current latent variable models are known to solve the first two challenges, they are not flexible to different word sense granularities, which differ very much among words, from aardvark with one sense, to play with over 50 senses. Current models either require hyperparameter tuning or nonparametric induction of the number of senses, which we find both to be ineffective. Thus, we aim to eliminate these requirements and solve the sense granularity problem by proposing AutoSense, a latent variable model based on two observations: (1) senses are represented as a distribution over topics, and (2) senses generate pairings between the target word and its neighboring word. These observations alleviate the problem by (a) throwing garbage senses and (b) additionally inducing fine-grained word senses. Results show great improvements over the stateof-the-art models on popular WSI datasets. We also show that AutoSense is able to learn the appropriate sense granularity of a word. Finally, we apply AutoSense to the unsupervised author name disambiguation task where the sense granularity problem is more evident and show that AutoSense is evidently better than competing models. We share our data and code here: https://github.com/rktamplayo/ AutoSense.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Word sense induction (WSI) is the task where given an ambiguous target word (e.g. cold) and texts where the word is used, we automatically discover its multiple senses or meanings (e.g. (1) nose infection, (2) absence of heat, etc.). We show examples of words with multiple senses and example usage in a text 1 in <ref type="figure" target="#fig_0">Figure 1</ref>. It is distinct from its similar supervised counterpart, word sense disambiguation (WSD) <ref type="bibr" target="#b12">(Stevenson and Wilks 2003)</ref>, because WSI models should consider the following challenges due to its unsupervised nature: (C1) adaptability to new domains, (C2) ability to detect novel senses, and (C3) flexibility to different word sense granularities <ref type="bibr" target="#b7">(Jurgens and Klapaftis 2013)</ref>. Another Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. <ref type="bibr">1</ref> All sense meanings are copied from WordNet: http:// wordnetweb.princeton.edu/perl/webwn task similar to the WSI is the unsupervised author name disambiguation (UAND) task <ref type="bibr" target="#b12">(Song et al. 2007)</ref>, where it aims to automatically find different authors, instead of words, with the same name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senses of play</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senses of cold</head><p>In this paper, we consider a latent variable modeling approach to WSI problem as it is proven to be more effective than other approaches <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014;</ref><ref type="bibr" target="#b7">Komninos and Manandhar 2016)</ref>. Specifically, we look into methods based on Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref>, a topic modeling method that automatically discovers the topics underlying a set of documents using Dirichlet priors to infer the multinomial distribution over words and topics. LDA naturally answers two of the three main problems mentioned above, i.e. (C1) and (C2), of the WSI task <ref type="bibr" target="#b3">(Brody and Lapata 2009</ref>). However, it is not flexible with regards to (C3), or the sense granularity problem, as it requires the users to specify the number of senses: Current systems <ref type="bibr" target="#b14">(Wang et al. 2015;</ref><ref type="bibr" target="#b5">Chang, Pei, and Chen 2014)</ref> required to set the number of senses to a small number (set to 3 or 5 in the literature) to get a good accuracy, however many words may have a large number of senses, e.g. play in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDA: !(#|%)</head><p>AutoSense: !('|#, (% ) , %)) target word: cold t 0 : medical t 1 : temperature t 2 : science t 3 : weather <ref type="bibr">(cold, common)</ref> (cold, sick) (cold, sneeze) … t 0 : medical, t 2 : science, t 1 : temperature, …  <ref type="figure">Figure 2</ref>: Example induced senses when the target word is cold from LDA and AutoSense. Applying our observations to LDA introduces both garbage and fine-grained senses.</p><p>To this end, we propose a latent variable model called AutoSense that solves all the challenges of WSI, including overcoming the sense granularity problem. Consider <ref type="figure">Figure  2</ref> on finding the senses of the target word cold. An LDA model naively considers the topics as senses and thus differentiates the usage of cold in the medical and science domains, even though the same sense is commonly used in the two domains. This results in too many senses induced by the model. We extend LDA using two observations. First, we introduce a separate latent variable for senses, which can be represented as a distribution over topics. This introduces more accurate induced senses (e.g. the cold: nose infection sense can be from a mixture of medical, science, and temperature topics), as well as garbage senses (colored red in the figure) as most topic distributions will not be assigned to any instance. Second, we enforce senses to generate target-neighbor pairs, a pair (w t , w) which consists of the target word w t and one of its neighboring word w, at once. This separates the topic distributions into fine-grained senses based on lexical semantic features easily captured by the target-neighbor pairs. For example, the cold: absence of heat and the cold: sensation from low temperature senses are both related to temperature, but have different syntactic and semantic usage.</p><p>By applying the two observations above, AutoSense removes the strict requirement on correctly setting the number of senses by throwing garbage senses and introducing fine-grained senses. Nonparametric models <ref type="bibr" target="#b14">(Teh et al. 2004;</ref> have also been used to solve this problem by automatically inducing the number of senses, however our experiments show that these models are less effective than parametric models and induce incorrect number of senses. Our proposed model is parametric, and is also able to adapt to the different number of senses of different words, even when the number of senses is set to an arbitrarily large number. Moreover, the model can also be used in other tasks such as UAND where the variance in the number of senses is large. To the best of our knowledge, we are the first to experiment extensively on the sense granularity problem of parametric latent variable models.</p><p>In our experiments, we estimate the parameters of the model using collapsed Gibbs sampling and get the sense distribution of each instance as the WSI solution. We evaluate our model using the SemEval 2010 and 2013 WSI datasets <ref type="bibr" target="#b9">(Manandhar et al. 2010;</ref><ref type="bibr" target="#b7">Jurgens and Klapaftis 2013)</ref>. Results show that AutoSense performs superior than previous state-of-the-art models. We also provide analyses and experiments that shows how AutoSense overcomes the issue on sense granularity. Finally, we show that our model performs the best on unsupervised author name disambiguation (UAND), where the sense granularities are extremely varied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Previous works on WSI used context vectors and attributes <ref type="bibr" target="#b0">(Almuhareb, Poesio, and others 2006)</ref>, pretrained classification systems <ref type="bibr" target="#b14">(Tsvetkov et al. 2014)</ref>, and alignment of parallel corpus <ref type="bibr" target="#b15">(Yao, Van Durme, and Callison-Burch 2012)</ref>.</p><p>In the most recent shared task on WSI <ref type="bibr" target="#b7">(Jurgens and Klapaftis 2013)</ref>, top models used lexical substitution method (AI-KU) <ref type="bibr" target="#b1">(Baskaya et al. 2013)</ref> and Hierarchical Dirichlet Process trained with additional instances (Unimelb) .</p><p>Latent variable models such as LDA <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref> are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (LDA, Spectral) (Goyal and Hovy 2014). More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (HC, HC+Zipf) <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref> and that topics and senses should be inferred jointly (STM) <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>. In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of targetneighbor pairs. HC was also extended to a nonparametric model (BNP-HC) <ref type="bibr" target="#b14">(Teh et al. 2004</ref>) in order to automatically set the number of senses of a word, providing flexibility to the sense granularity <ref type="bibr" target="#b15">(Yao and Van Durme 2011;</ref><ref type="bibr" target="#b7">Lau et al. 2012;</ref>. In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.</p><p>Recent inclusions to the WSI models are neural-based dense distributional representation models. STM also used word embeddings <ref type="bibr" target="#b10">(Mikolov et al. 2013)</ref> to assign similarity weights during inference (STM+w2v) <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>. Existing sense embeddings are also used to perform word sense induction (CRP-PPMI, SE-WSI-fix, WG, DIVE) (Song 2016; <ref type="bibr" target="#b11">Pelevina et al. 2016;</ref><ref type="bibr" target="#b4">Chang et al. 2018</ref>). These models, on their own, do not perform well on the WSI task until recently when embeddings of words and their dependencies are used to construct a probabilistic model (MCC) (Komninos and <ref type="bibr" target="#b7">Manandhar 2016)</ref>. We show that neuralbased embeddings are still ineffective for this task and that our model performs better than these models as well.</p><p>In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used <ref type="bibr" target="#b12">(Shu, Long, and Meng 2009)</ref> to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features <ref type="bibr" target="#b13">(Tang et al. 2012)</ref>. In this paper, we study on how to improve the performance of text features for UAND using latent variable models, which can later be combined with non-text features in the future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Model</head><p>There are two reasons why Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref> is not effective for WSI. First, LDA tries to give instance assignments to all senses even when it is unnecessary. For example, when the number of senses S is set to 10, the model tries to assign all the senses to all instances even when the original number of senses of a target word is 3. LDA extensions (Wang et al. 2015; Chang, Pei, and Chen 2014) mitigated this problem by setting S to a small number (e.g. 3 or 5). However, this is not a good solution because there are many words with more than five senses. Second, LDA and its extensions do not consider the existence of fine-grained senses. For example, the cold: absence of heat and the cold: sensation from low temperature senses are fine-grained senses because they are similarly related to temperature yet have different usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoSense Model</head><p>To solve the problems above, we propose to extend LDA in two parts. First, we introduce a new latent variable, apart from the topic latent variable, to represent word senses. Previous works also attempted to introduce a separate sense latent variable to generate all the words <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref>, or to generate only the neighboring words within a local context, decided by a strict user-specified window <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>. We improve by softening the strict local context assumption by introducing a switch variable which decides whether a word not in a local context should be generated by conditioning also on the sense latent variable. Our experiments show that our sense representation provides superior improvements from previous models.</p><p>Second, we force the model to generate target-neighbor   <ref type="figure">)</ref>. These pairs give explicit information on the lexical semantics of the target word given the neighboring words. In our running example <ref type="figure">(Figure 2</ref>), the cold: absence of heat and the cold: sensation from low temperature senses can be easily differentiated when we are given the target-neighbor pairs (cold, weather) and (cold, climate) for the former, and (cold, water) and (cold, f resh) for the latter sense, rather than the individual words. These extensions bring us to our proposed model called AutoSense. The graphical representation of AutoSense is shown in <ref type="figure" target="#fig_1">Figure 3</ref>, while the meaning of the notations used in this paper is shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Generative process For each instance, we divide the text into two contexts: the local context L which includes the target word w t and its neighboring words w l , and the global context M which contains the other remaining words w m . Words from different contexts are generated separately.</p><p>In the global context M , words w m are generated from either a sense s or a topic t latent variable. The selection is done by a switch variable x. If x = 1, then the word generation is done by using the sense variable s. Otherwise, it is done by using the topic variable t. The probability of a global context word w m in document d is given below.</p><formula xml:id="formula_0">P (w m |d) = P (w m |x = 1) s P (w m |s)P (s|d)+ P (w m |x = 2) t P (w m |t)P (t|d) = θ x=1 s θ (d) s φ (s) wm + θ x=2 t θ (d) t φ (t) wm</formula><p>In the local context L, words w l are generated from both sense s and topic t variables. Also, the target word w t is generated along with w l as target-neighbor pairs (w t , w l ) using the sense variable s. Sense and topic variables are dependent to each other, so we generate them using the joint probability p(s, t|d). We factorize p(s, t|d) approximately using ideas from dependency networks <ref type="bibr" target="#b6">(Heckerman et al. 2000)</ref> to avoid independency assumptions, i.e. p(a, b|c) = p(a|b, c)p(b|a, c), and deficient modeling <ref type="bibr" target="#b3">(Brown et al. 1993)</ref> to ignore redundancies, i.e. p(a|b, c)p(b|a, c) = p(a|b)p(a|c)p(b|a)p(b|c)p(a, b). The probability of a local context word w l in document d given below.</p><formula xml:id="formula_1">P (w t , w l |d) = s t p(w t |s)p(w l |s, t)p(s, t|d) ≈ s t p(w t |s)p(w l |s, t)p(s|d, t)p(t|d, s) ≈ s t p(w t |s)p(w l |s)p(w l |t) p(s|d)p(s|t)p(t|d)p(t|s)p(s, t) = s t φ (s) wt φ (s) w l φ (t) w l θ (d) s θ (d) t θ s|t θ t|s θ st</formula><p>Inference We use collapsed Gibbs sampling <ref type="bibr" target="#b6">(Griffiths and Steyvers 2004)</ref> to estimate the latent variables. At each transition step of the Markov chain, for each word w m in the global context, we draw the switch x ∼ {1, 2}, and the sense s = k or the topic t = j variables using the conditional probabilities given below. The variable C AB ab represents the number of a ∈ A and b ∈ B assignments, excluding the current word. The rest corresponds to the other remaining variables, such as the instance d, the current word w m , the θ and φ distributions, and the α, β, and γ Dirichlet priors.</p><formula xml:id="formula_2">P (x = 1, s = k|rest) = C DX d1 + γ 2 x =1 C DX dx + 2γ C DS dk + α S k =1 C DS dk + Sα C SW kwm + β V w =1 C SW kw + V m β P (x = 2, t = j|rest) = C DX d2 + γ 2 x =1 C DX dx + 2γ C DT dj + α T j =1 C DT dj + T α C T W jwm + β V w =1 C T W jw + V m β<label>Subsequently</label></formula><p>, for each word w l and the target word w t (forming the target-neighbor pair (w t , w l )) in the local context, we draw the sense s = k and the topic t = j variables using the conditional probability given below.</p><formula xml:id="formula_3">P (t i = j, s i = k|rest) = C DT dj + α T j =1 C DT dj + T α C DS dk + α S k =1 C DS dk + Sα C T W jw l + β V w =1 C T W jw + V l β C SW kw l + β V w =1 C SW kw + V l β C SW kwt + β V w =1 C SW kw + V l β + 1 C ST kj + α T j =1 C ST kj + T α C T S jk + α S k =1 C T S jk + Sα C ST kj + α S k =1 T j =1 C ST k j + ST α j</formula><p>Word sense induction After inference is done, the approximate probability of the sense s of the target word in a given document d is induced using the sense distribution of the document as shown in the equation below, where C AB ab represents the number of a ∈ A and b ∈ B assignments. We also calculate the word distribution of each sense using the second equation below to inspect the meaning of sense.</p><formula xml:id="formula_4">θ s|d = C DS ds S s =1 C DS ds θ w|s = C SW sw V w =1 C SW sw (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>Datasets and preprocessing We use two publicly available datasets: SemEval 2010 Task 14 <ref type="bibr" target="#b9">(Manandhar et al. 2010)</ref> and SemEval 2013 Task 13 <ref type="bibr" target="#b7">(Jurgens and Klapaftis 2013)</ref>. The SemEval 2010 dataset 2 consists of 50 verbs and 50 nouns, each with different number of instances for a total of 8915 instances. SemEval 2013 dataset 3 consists of 20 verbs, 20 nouns, and 10 adjectives, with a total of 4664 instances.</p><p>For preprocessing, we do tokenization, lemmatization, and removing of symbols to build the word lists using Stanford CoreNLP <ref type="bibr" target="#b9">(Manning et al. 2014)</ref>. We divide the word lists into two contexts: the local and global context. Following (Wang et al. 2015), we set the local context window to 10, with a maximum number of words of 21 (i.e. 10 words before and 10 words after). Other words are put into the global context. Note however that AutoSense has a less strict global/local context assumption as it treats some words in the global context as local depending on the switch variable.</p><p>Parameter setting We set the hyperparameters to α = 0.1, β = 0.01, γ = 0.3, following the conventional setup <ref type="bibr" target="#b6">(Griffiths and Steyvers 2004;</ref><ref type="bibr" target="#b5">Chemudugunta, Smyth, and Steyvers 2006)</ref>. We arbitrarily set the number of senses to S = 15, and the number of topics T = 2S = 30, following <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>. We also include four other versions of our model: AutoSense −wp removes the target-neighbor pair constraint and transforms the local context to that of STM, AutoSense −sw removes the switch variable and transforms the global context to that of LDA, AutoSense s=X is a tuned and best version of the model, where the number of senses is tuned over a separate development set provided by the shared tasks and X is the tuned number of sense, different for each dataset, and AutoSense s=100 is the overestimated and worst version of the model, where we set the number of senses to an arbitrary large number, i.e. 100.</p><p>We set the number of iterations to 2000 and run the Gibbs sampler. Following the convention of previous works <ref type="bibr" target="#b7">(Lau et al. 2012;</ref><ref type="bibr" target="#b5">Goyal and Hovy 2014;</ref><ref type="bibr" target="#b14">Wang et al. 2015)</ref>, we assume convergence when the number of iterations is high. However, due to the randomized nature of Gibbs sampling, we report the average scores over 5 runs of Gibbs sampling. We then use the distribution θ s|d as shown in Equation 1 as the solution of the WSI problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Word sense induction</head><p>SemEval 2010 For the SemEval 2010 dataset, we compare models using two unsupervised metrics: V-measure (V-M) and paired F-score (F-S). V-M favors a high number of senses (e.g. assigning one cluster per instance), while F-S favors a small number of senses (e.g. all instances in one cluster) <ref type="bibr" target="#b9">(Manandhar et al. 2010)</ref>. In order to get a common ground for comparison, we do a geometric average AVG of both metrics, following <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>. Finally, we also report the absolute difference between the actual (3.85) and induced number of senses as δ(#S).</p><p>We compare with seven other models: a) LDA on cooccurrence graphs (LDA) and b) spectral clustering on cooccurrence graphs (Spectral) as reported in (Goyal and Hovy 2014), c) Hidden Concept (HC), d) HC using Zipf's law (HC+Zipf), and e) Bayesian nonparametric version of HC (BNP-HC) as reported in <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref>, f) CRP-based sense embeddings with positive PMI vectors as pre-trained vectors (CRP-PPMI), and g) Multi-Sense Skip-gram Model (SE-WSI-fix) as reported in <ref type="bibr">(Song 2016)</ref>. <ref type="table" target="#tab_4">Table 2a</ref>, where AutoSense outperforms other competing models on AVG. Among the Au-toSense models, the AutoSense −wp and AutoSense −sw version perform the worst, emphasizing the necessity of the target-neighbor pairs and the switch variable. The overestimated AutoSense s=100 performs better than previously proposed models, proving the robustness of our model on the different word sense granularities. On the δ(#S) metric, the untuned AutoSense and AutoSense s=5 perform the best. The V-M metric needs to be interpreted carefully, because it can easily be maximized by separating all instances into different sense clusters and thus overestimating the actual number of senses #S and decreasing the F-S metric. The model BNP-HC is an example of such: Though its V-M metric is the highest, it scores the lowest on the F-S metric and greatly overestimates #S, thus having a very high δ(#S). The goal is thus a good balance of V-M and F-S (i.e. highest AVG), and a close estimation of #S (i.e. lowest δ(#S), which is successfully achieved by our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results are shown in</head><p>SemEval 2013 Two metrics are used for the SemEval 2013 dataset: fuzzy B-cubed (F-BC) and fuzzy normalized mutual information (F-NMI). F-BC gives preference to labelling all instances with the same sense, while F-NMI gives preference to labelling all instances with distinct senses. Therefore, computing the AVG of both metrics is also necessary in this experiment, for ease of comparison, as also suggested in <ref type="bibr" target="#b14">(Wang et al. 2015)</ref>.</p><p>We use seven baselines: a) lexical substitution method (AI-KU) and b) nonparametric HDP model <ref type="formula">(</ref> Results are shown in <ref type="table" target="#tab_4">Table 2b</ref>. Among the models, all versions of AutoSense perform better than other models on AVG. The untuned AutoSense and AutoSense s=7 especially garner noticeable increase of 6.1% on fuzzy B-cubed metric from MCC, the previous best model. We also notice a big 6.0% decrease on the fuzzy B-cubed of AutoSense when the target-neighbor pair context is removed. This means that introducing the target-neighbor pair is crucial to the improvement of the model. Finally, the overestimated AutoSense model performs as well as the other AutoSense models, even outperforming all previous models on AVG, which proves the effectiveness of AutoSense even when s is set to a large value.</p><p>For completeness, we also report STM with additional contexts, <ref type="bibr">STM+actual and STM+ukWac (Wang et al. 2015)</ref>, where they used the actual additional contexts from the original data and semantically similar contexts from ukWac, respectively, as additional global context. With the performance gain we achieved, AutoSense without additional context can perform comparably to models with additional contexts: Our model greatly outperforms these models on the <ref type="table" target="#tab_2">Sense Word distribution   #Docs  1  hotel tour tourist summer flight  22  2  month ticket available performance  3  3</ref> guest office stateroom class suite 3 * advance overseas line popular japan 0 * email day buy unable tour 0 * sort basic tour time 0 <ref type="table">Table 3</ref>: Six of the 15 senses of the target verb book using AutoSense with S = 15. The word lists shown are preprocessed to remove stopwords and the target word. The first three senses are senses which are assigned at least once to an instance document. The last three are garbage senses.</p><p>F-BC metric by at least 2%. Also, considering that both Au-toSense and STM are LDA-based models, the same data enhancements can straightforwardly be applied when the needs arise. We similarly apply the actual additional contexts to AutoSense and find that we achieve state-of-the-art performance on AVG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sense granularity problem</head><p>The main weakness of LDA when used on WSI task is the sense granularity problem. Recent models such as HC <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref> and <ref type="bibr">STM (Wang et al. 2015)</ref> mitigated this problem by tuning the number of senses hyperparameter S to minimize the error. However, such tuning, often empirically set to a small number such as S = 3 (Wang et al. 2015), fails to infer varying number of senses of words, especially for words with a higher number of senses. Nonparametric models such as HDP and BNP-HC <ref type="bibr" target="#b5">Chang, Pei, and Chen 2014)</ref> claim to automatically induce different S for each word. However, as shown in the results in <ref type="table" target="#tab_4">Table 2</ref>, the estimated S is far from the actual number of senses and both models are ineffective. On the other hand, <ref type="table" target="#tab_4">Table 2</ref> also shows that AutoSense is effective even when S is overestimated. We explain why through an example result shown in <ref type="table">Table 3</ref>, where the target word is the verb book, the actual number of senses is three, and S is set to 15. First, we see that there are senses which are not assigned to any instance document, signified by * , which we call garbage senses. We notice that effectively representing a new latent variable for sense as a distribution over topics forces the model to throw garbage senses. Second, while it is easy to distinguish the third sense (i.e., book: register in a booker) to the two other senses, the first and second senses both refer to planning or arranging for an event in advance. Incorporating the target-neighbor pairs helps the model differentiates both into fine-grained senses book: arrange for and reserve in advance and book: engage for a performance.</p><p>We compare the competing models quantitatively on how they correctly detect the actual number of sense clusters using cluster error, which is the mean absolute error between the detected number and the actual number of sense clusters. We compare the cluster errors of LDA <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref>, <ref type="bibr">STM (Wang et al. 2015)</ref>, HC <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref>, and a nonparametric model HDP <ref type="bibr" target="#b14">(Teh et al. 2004</ref>), with AutoSense. We report the results in <ref type="figure" target="#fig_3">Figure 4</ref>. Results show that the cluster error of LDA increases sharply as the number of senses exceeds the actual mean number of senses. HC and STM also throw garbage senses since they also introduce in some way a new sense variable, however the cluster errors of both models still increase when S is set beyond the maximum number of senses. We argue that this is because first, the sense representation is not optimal as they assume strict local/global context assumption, and second and most importantly, the models do not produce fine-grained senses. AutoSense does both garbage sense throwing and fine-grained sense induction, which helps in the detection of the actual word granularity. Finally, the cluster error of AutoSense is always better than that of HDP. This shows that AutoSense, despite being a parametric model, automatically detects the number of sense clusters without parameter tuning and is more accurate than the automatic detection of nonparametric models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised author name disambiguation</head><p>Unsupervised author name disambiguation (UAND) is a task very similar to the WSI task, where ambiguous author names are the target words. However, one additional challenge of UAND is that there can be as many as 100 authors  with the same name, whereas words can have at most 20 different senses, at least in our datasets, as shown in the dataset statistics in <ref type="table" target="#tab_6">Table 4</ref>. Moreover, the standard deviations of the author name disambiguation datasets are also higher, which means that there is more variation on the number of senses per target author name. Thus, in this task, the sense granularity problem is more difficult and needs to be addressed properly. Current state-of-the-art models use non-text features such as publication venue and citations <ref type="bibr" target="#b13">(Tang et al. 2012)</ref>. We argue that text features also provide informative clues to disambiguate author names. In this experiment, we make use of text features such as the title and abstract of research papers as data instance of the task. In addition, we also include in our dataset author names and the publication venue as pseudo-words. In this way, we can reformulate the UAND task as a WSI task, and exploit text features not used in current techniques.</p><p>Experimental setup We use two publicly available datasets for the UAND task: Arnet 4 and PubMed 5 . The Arnet dataset contains 100 ambiguous author names and a total of 7528 papers as data instance. Each instance includes the title, author list, and publication venue of a research paper authored by the given author name. In addition, we also manually extract the abstracts of the research papers for additional context. The PubMed dataset contains 37 author names with a total of 2875 research papers as instances. It includes the PubMed ID of the papers authored by the given author name. We extract the title, author list, publication venue, and abstract of each PubMed ID from the PubMed website.</p><p>We use LDA <ref type="bibr" target="#b2">(Blei, Ng, and Jordan 2003)</ref>, HC <ref type="bibr" target="#b5">(Chang, Pei, and Chen 2014)</ref> and <ref type="bibr">STM (Wang et al. 2015)</ref> as baselines. We do not compare with non-text feature-based models <ref type="bibr" target="#b13">(Tang et al. 2012;</ref><ref type="bibr" target="#b4">Cen et al. 2013</ref>) because our goal is to compare sense topic models on a task where the sense granularities are more varied. For STM and AutoSense, the title, publication venue and the author names are used as local contexts while the abstract is used as the global context. This decision is based on conclusions from previous works <ref type="bibr" target="#b13">(Tang et al. 2012</ref>) that the title, publication venue, and the author names are more informative than the abstract when disambiguating author names. We use the same parameters as used above, and we set S to 5, 25, 50, and 100 to com-4 https://aminer.org/disambiguation 5 https://github.com/Yonsei-TSMM/author_ name_disambiguation  pare the performances of the models as the number of senses increases.</p><p>Results For evaluation, we use the pairwise F1 measure to compare the performance of competing models, following <ref type="bibr" target="#b13">(Tang et al. 2012)</ref>. Results are shown in <ref type="figure">Figure 5</ref>. Au-toSense performs the best on almost all settings, except on the PubMed dataset and when S = 5, where it garners a comparable result with STM. However, in the case where S is set close to the maximum number of senses in the dataset (i.e. 28 in PubMed and 112 in Arnet), AutoSense performs the best among the models. LDA and HC perform badly on all settings and greatly decrease their performances when S becomes high. STM also shows decrease in performance on the PubMed dataset when S = 100. This is because the PubMed dataset has a lower maximum number of senses, and STM is sensitive in the setting of S, and thus hurts the robustness of the model to different sense granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a solution to answer the sense granularity problem, one of the major challenges of the WSI task. We introduced AutoSense, a latent variable model that not only throws away garbage senses, but also induces fine-grained senses. We showed that AutoSense greatly outperforms the current state-of-the-art models in both SemEval 2010 and 2013 WSI datasets. We also show experiments on how Au-toSense is able to overcome sense granularity problem, a well-known flaw of latent variable models on. We further applied our model to UAND task, a similar task but with more varying number of senses, and showed that AutoSense performs the best among latent variable models, proving its robustness to different sense granularities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Three senses of the noun cold and six of 17 senses of the noun play in WordNet. Sense granularity problem refers to the inflexibility of the model to the different number of senses different words may have (i.e. 3 vs. 17).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Graphical representation of AutoSense. Nodes are random variables, edges are dependencies, and plates are replications. Nodes shaded in black are observed. The node shaded in red is the observed target word. The dependency edges of θ s|t , θ t|s , and θ st are not shown for clarity: They are all generated by the Dirichlet prior α. Moreover, sense variables are dependent to θ s|t and θ st , while topic variables are dependent to θ t|s and θ st .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Unimelb) as reported in (Jurgens and Klapaftis 2013), c) Sense-Topic Model STM, d) STM with word2vec weights (STM+w2v) as reported in (Wang et al. 2015), e) Word Graph embeddings (WG), f) Distributional Inclusion Vector Embedding (DIVE) as reported in (Chang et al. 2018), and g) Multi Context Continuous model MCC as reported in (Komninos and Manandhar 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Cluster error of models with different number of senses S. The vertical dashed lines correspond to the mean and the max of the actual number of senses. The x-axes are log-scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Meanings of the notations in AutoSense pairs at once in the local context, instead of generating words one by one. A target-neighbor pair (w t , w) consists of the target word w t and a neighboring word w in the local context. For example, the target-neighbor pairs in "cold snowy weather", where w t is cold, are (cold, snowy) and (cold, weather</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance of different models on the datasets. Best scores are bold-faced. LVMs are Latent Variable Models, while NBEs are Neural-based Embeddings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the number of senses of target words/names in the datasets used in the paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Paired F1 measures of competing models with different number of senses S on UAND datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.cs.york.ac.uk/semeval2010_WSI 3 https://www.cs.york.ac.uk/semeval-2013/ task13/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Samsung Research Funding Center of Samsung Electronics under Project Number SRFCIT1701-01, and by Next-Generation Information Computing Development Program through the National Re-search Foundation of Korea (NRF), funded by the Ministry of Science, ICT (No.NRF-2017M3C4A7065887).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Msda: Wordsense discrimination using context vectors and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poesio</forename><surname>Almuhareb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almuhareb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="543" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ai-ku: Using substitute vectors and cooccurrence modeling for word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Baskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="300" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">;</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Author disambiguation by hierarchical agglomerative clustering with adaptive stopping criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03257</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient graph-based word sense induction by distributional inclusion vector embeddings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling general and specific aspects of documents with a probabilistic topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ;</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chemudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dependency networks for inference, collaborative filtering, and data visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rounthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National academy of Sciences</title>
		<meeting>the National academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2000-10" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="49" to="75" />
		</imprint>
	</monogr>
	<note>Finding scientific topics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured generative models of continuous features for word sense induction. COLING 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komninos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
	<note>EACL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">unimelb: Topic modelling-based word sense induction for web snippet clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cook</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="217" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Manandhar</surname></persName>
		</author>
		<idno>Manning et al. 2014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Proceedings of the 5th international workshop on semantic evaluation</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making sense of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Pelevina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word embeddings, sense embeddings and their application to word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Councill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Comp. Linguistics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="249" to="265" />
		</imprint>
		<respStmt>
			<orgName>The University of Rochester</orgName>
		</respStmt>
	</monogr>
	<note>ICDE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A sense-topic model for word sense induction with unsupervised data enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="59" to="71" />
		</imprint>
	</monogr>
	<note>Sharing clusters among related groups: Hierarchical dirichlet processes</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expectations of word sense in parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van Durme ;</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing</title>
		<meeting>TextGraphs-6: Graph-based Methods for Natural Language Processing<address><addrLine>Yao, Van Durme, and Callison-Burch</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="621" to="625" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
