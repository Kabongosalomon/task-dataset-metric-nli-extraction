<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Estimation of Age and Gender from Unconstrained Face Images using Lightweight Multi-task CNN for Mobile Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Yen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Estimation of Age and Gender from Unconstrained Face Images using Lightweight Multi-task CNN for Mobile Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic age and gender classification based on unconstrained images has become essential techniques on mobile devices. With limited computing power, how to develop a robust system becomes a challenging task. In this paper, we present an efficient convolutional neural network (CNN) called lightweight multi-task CNN for simultaneous age and gender classification. Lightweight multi-task CNN uses depthwise separable convolution to reduce the model size and save the inference time. On the public challenging Adience dataset, the accuracy of age and gender classification is better than baseline multi-task CNN methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding age and gender from the human face plays an essential role in social interaction. To make communication proper and efficient, people subconsciously judge others' age or gender. Thus, age and gender estimation is important in several applications, including reidentification in surveillance videos, intelligent advertising and human-computer interaction. Nevertheless, accurately and efficiently estimating age and gender from unconstrained face images is difficult.</p><p>Prior to deep neural network era, several approaches estimate age and gender from face images using designed image features and machine learning. In <ref type="bibr" target="#b3">[4]</ref>, Eidinger et al. combine four-patch local binary patterns (FPLBP) <ref type="bibr" target="#b13">[14]</ref> and support vector machine (SVM) <ref type="bibr" target="#b2">[3]</ref> to achieve the jointestimation of age and gender. Han et al. <ref type="bibr" target="#b4">[5]</ref> use biologically inspired features (BIF) and their designed hierarchical estimator for this task. Since deep CNNs get a great success in object classification, Rothe et al. <ref type="bibr" target="#b10">[11]</ref> develop the DEX method which consists of the face detector in <ref type="bibr" target="#b9">[10]</ref> and a deep CNN architecture VGG-16 <ref type="bibr" target="#b12">[13]</ref> for age estimation. Levi Hassner <ref type="bibr" target="#b7">[8]</ref> introduce a five-layer CNN architecture that achieves the most favorable performance on the unconstrained public Adience dataset <ref type="bibr" target="#b3">[4]</ref>. Although the Levi Hassner CNN <ref type="bibr" target="#b7">[8]</ref> can achieve high accuracy, it needs two independent models for predicting the age and gender, respectively. To reduce the memory cost, we develop a method simplifying the weights with a single light weight model through multi-task learning and the technique of depthwise separable convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are two types of structures commonly used in multi-task learning with deep neural networks <ref type="bibr" target="#b11">[12]</ref>, hard parameter sharing and sof t parameter sharing of hidden layers. Soft parameter sharing means that each task has its own deep neural network with same structure, and then a similarity function is utilized to regularize these models <ref type="bibr" target="#b14">[15]</ref>. Thus the space required at run time is proportional to the number of tasks. To reduce the space complexity, the structure of hard parameter sharing is most commonly used in deep multi-task learning. It only employs one shared deep neural network and keeps several task-specific output layers <ref type="bibr" target="#b17">[18]</ref>. The hard parameter sharing can not only reduce the space complexity, but can also decrease the risk of over-fitting <ref type="bibr" target="#b1">[2]</ref>.</p><p>The getting popular mobile device motivates researchers to develop deep neural networks for mobile applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref>. MobileNet <ref type="bibr" target="#b5">[6]</ref> is one of the most interesting approaches about speedup of a deep neural network. The Mo-bileNet is depended on a streamlined architecture that uses depthwise separable convolutions to factorize a general convolution into a depthwise convolution and a pointwise convolution. By combining the output values of the depth-wise convolution with point wise convolution, a lightweight deep neural network thus can be constructed. To further parameterize the tradeoff between latency and accuracy, they use two global hyper-parameters, width multiplier and resolution multiplier to adjust the computational cost of the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lightweight Multi-Task CNN</head><p>We refine the state-of-the-art approach <ref type="bibr" target="#b7">[8]</ref> on two aspects: simultaneous inference and model redution, which are critical for mobile applications. Unlike Levi Hassner CNN <ref type="bibr" target="#b7">[8]</ref> that uses two independent models to recognize age and gender, only one single CNN for feature extraction is used for multiple tasks in our system. Thus, the memory requirement for deep neural networks is reduced. We employ a hard parameter sharing paradigm to learn the single CNN for both tasks. To further decrease the computation cost, we decompose the general CNN in <ref type="bibr" target="#b7">[8]</ref> into depthwise and pointwise convolution networks. The pointwise convolution is a convolution with 1 × 1 kernel's size, and it combines the output values of the depthwise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Depthwise Separable Convolution</head><p>Before detailing our network architecture, we give a brief review of the depthwise seperable convolution in this section. First, we consider the computational complexity of a general convolution. Let us denote the size of a general convolutional layer by  </p><formula xml:id="formula_0">D K × D K × C I × C O , where D K is</formula><formula xml:id="formula_1">W I × H I × C I × D K × D K × C O .</formula><p>In depthwise separable convolution, we split the general convolution layer into two layers. One is the depthwise convolution layer with size D K × D K × 1 of a 2D kernel filter per each input channel C I , and the other is the pointwise convolution layer with 1 × 1 convolution used to generate a linear combination of the output of the depthwise layer, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b). The computational cost of the depthwise separable convolution layer can be derived by the following equation:</p><formula xml:id="formula_2">W I × H I × C I × (D K × D K + C O ).</formula><p>Dividing the computational cost of general convolution by depthwise separable convolution, we can obtain the computational cost ratio is</p><formula xml:id="formula_3">D 2 K ×C O D 2 K +C O .</formula><p>The greater the number of channels, the greater the speedup the depthwise separable convolution can be achieved. In <ref type="bibr" target="#b5">[6]</ref>, Howard et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lightweight Multi-Task Network</head><p>The Lightweight Multi-Task CNN (LMTCNN) is composed of one general convolution layer, two depthwise separable convolution layers and two fully connected layers. Thus, it can accomplish multiple tasks while reducing the memory cost. The system overview is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>To handle the age and gender classification on the Adience dataset <ref type="bibr" target="#b3">[4]</ref>, our proposed method consists of three steps.</p><p>First, input color face image is scaled to the 256 × 256 and then cropped into the 227 × 227 in size using oversampling. The over-sampling here means that the system extracts five cropped regions from the scaled color face image, four cropped regions from the corners and one from the center of the scaled color face image. The LMTCNN processes five cropped regions with their horizontal reflections and estimates the final result by the average score of these regions.</p><p>Second, the size 3 × 7 × 7 pixel values of 96 kernel filters are applied to the input in the first general convolution layer, followed by a rectified linear unit (ReLU), a max pooling layer with window size equals to 3 × 3 and strides equal to two pixels and a local response normalization layer. The output feature map (size 28 × 28 × 96)of the first general convolution layer is processed by the two subsequent depthwise separable convolution layers defined in <ref type="table" target="#tab_0">Table 1</ref>. The output feature map of the last depthwise separable convolution layer is fed to the kernel size 3 × 3 of a max pooling layer that partitions the input feature map into a set of nonoverlapping regions.</p><p>Finally, the output feature map of the max pooling layer is fed to the two fully connected layers which contain 512 neurons, followed by a ReLU and a dropout layer. To achieve both the age classification for eight age classes and the gender classification for two gender classes, two separate softmax layers are followed by the output feature map of the average pooling layer. The first softmax layer assigns a probability for each class of the age and the other assigns a probability for each class of the gender. <ref type="figure" target="#fig_0">Figure 1</ref> shows </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment and Result</head><p>Our proposed network is implemented using the Tensorflow framework <ref type="bibr" target="#b0">[1]</ref>. Training and Testing are executed on the desktop with Intel Xeon E5 3.5 GHz CPU, 64G RAM and GeForce GTX TITAN X GPU. Training our proposed network takes approximately six hours. When testing on the desktop, predicting age and gender on a single image requires approximately 7.6 milliseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Result of Adience Dataset</head><p>The Adience dataset <ref type="bibr" target="#b3">[4]</ref> is composed of pictures taken by camera from smartphone or tablets. The images of Adience dataset capture extreme variations, including extreme blur (low-resolution), occlusions, out-of-plane pose variations, expressions. The entire Adience dataset includes 26,580 unconstrained images of 2,284 subjects. Its age labels contain eight groups, including (0 − 2), (4 − 6), <ref type="bibr">(8 − 13)</ref>, <ref type="bibr">(15 − 20)</ref>, (25 − 32), (38 − 43), (48 − 53), (60+).</p><p>Unlike other datasets (such as Morph II) where the face images are taken in a controlled envoriment, the Adience dataset is a in-the-wild benchmark for joint age and gender estimation, and is thus more demanding. Because our purpose is to develop a mobile system that can estimate age and gender in real environments, testing this benchmark can reflect the performance more appropriately.</p><p>For age and gender classification, we measure and compare the accuracy using a five-fold cross validation. The number of images in each fold for training, validation and testing are shown in <ref type="table" target="#tab_1">Table 2</ref>. The in-plane aligned version of the faces defined in <ref type="bibr" target="#b3">[4]</ref> is used.</p><p>We compare our proposed method with baseline Levi Hassner CNN <ref type="bibr" target="#b7">[8]</ref> by using five-fold cross validation with the number of images shown in <ref type="table" target="#tab_1">Table 2</ref> to train by the training set and test by the testing set in each fold. Our proposed method is LMTCNN with the width multiplier of each depthwise separable convolution equals to 1 or 2. In   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mobile Applications</head><p>To run a deep neural network model on mobile devices with Android operation system, we convert deep neural network model into the computational graph of Tensorflow library <ref type="bibr" target="#b0">[1]</ref> and we compare the model size of each method, as shown in <ref type="table" target="#tab_4">Table 4</ref>. The model size of LMTCNN with width multiplier = 1 of the first depthwise separable convolution and width multiplier = 1 of the second depthwise separable convolution is approximately nine times smaller than that of Levi Hassner CNN <ref type="bibr" target="#b7">[8]</ref>, and the model size of LMTCNN with width multiplier = 2 of the first depthwise separable convolution and width multiplier = 1 of the second depthwise separable convolution is approximately half smaller.</p><p>For mobile application, we port the system with face detection, age recognition, and gender recognition on mobile devices, such as smartphone, tablet and smart robot. The face detection is implemented using the method of the MTCNN <ref type="bibr" target="#b15">[16]</ref>. Then the region the facial regions are  <ref type="bibr" target="#b7">[8]</ref> to recognize the age and gender. <ref type="figure" target="#fig_4">Figure 3</ref> demonstrates our system on the ASUS Zenbo and ASUS Zenfone 3. The ASUS Zenbo is a smart robot developed by the ASUS incorporation with Intel Atom x5-Z8550 2.4GHz CPU, 4G RAM and Android 6.0.1 system and The ASUS Zenfone 3 is a smartphone developed by the ASUS incorporation with Qualcomm Snapdragon 625 2.02GHz CPU, 3G RAM and Android 7.0 system. We also calculate the processing time of each method on the ASUS Zenbo and ASUS Zenfone 3, as shown in <ref type="table">Table 5</ref>.</p><p>In summary, the above results <ref type="table" target="#tab_2">(Table 3</ref>, 4 and 5) reveal that LMTCNN can decrease the size of model and speed up the inference on mobile devices while maintaining the accuracy of age and gender classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce the new network structure, LMTCNN, which accomplishes multiple tasks while maintaining the accuracy of age and gender classification. We also show that our architecture can be realized on mobile devices with limited computational resources. In the future, we will improve the performance of LMTCNN and reduce the size of model for the datasets of unconstrained face images with face attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The architecture of the LMTCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the size of kernel K, and C I and C O are the number of input and output channels, respectively. The dimension of input map is W I × H I × C I , where W I and H I are the width and height of the input feature map, respectively. The size of output map O is W O × H O × C O , where W O and H O are the width and height of the map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>a) shows a common feature convolutional layer. The computational cost of the common convolution layer is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>(a)The general convolutional filter (b) The depthwise separable filter demonstrate that simplifying the architecture of a CNN in this manner can considerably increase the inference speed without sacrificing the classification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The demonstration of LMTCNN executed on mobile devices. (a) Asus Zenbo and (b) Asus Zenfone 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . The architecture of the two depth- wise separable convolution layers used in the lightweight multi-task CNN</head><label>1</label><figDesc></figDesc><table><row><cell>Type</cell><cell>Filter Shape</cell><cell>Input Size</cell></row><row><cell>dw Conv1</cell><cell>3 x 3 x 96</cell><cell>28 x 28 x 96</cell></row><row><cell cols="2">pw Conv1 1 x 1 x 96 x 256</cell><cell>28 x 28 x 96</cell></row><row><cell>dw Conv2</cell><cell>3 x 3 x 256</cell><cell>14 x 14 x 256</cell></row><row><cell cols="3">pw Conv2 1 x 1 x 256 x 384 14 x 14 x 256</cell></row><row><cell cols="3">the network configuration visualization.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . The number of images in each fold of the training, validation and testing sets</head><label>2</label><figDesc></figDesc><table><row><cell>Fold</cell><cell cols="3">Training Validation Testing</cell></row><row><cell>First</cell><cell>11,136</cell><cell>1,242</cell><cell>3,879</cell></row><row><cell>Second</cell><cell>11,905</cell><cell>1,348</cell><cell>3.005</cell></row><row><cell>Third</cell><cell>11,814</cell><cell>1,323</cell><cell>3,121</cell></row><row><cell>Forth</cell><cell>12,056</cell><cell>1,335</cell><cell>2,866</cell></row><row><cell>Fifth</cell><cell>11,593</cell><cell>1,277</cell><cell>3,387</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . The accuracy of the age and gen- der classification generated by five-fold cross validation in Adience dataset</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Age</cell><cell></cell><cell>Gender</cell></row><row><cell>Methods</cell><cell>Top-1</cell><cell>Top-2</cell><cell>Top-1</cell></row><row><cell></cell><cell cols="3">Acc.(%) Acc.(%) Acc.(%)</cell></row><row><cell>Levi Hassner CNN [8]</cell><cell>44.14</cell><cell>69.98</cell><cell>82.52</cell></row><row><cell>LMTCNN-1-1</cell><cell>40.84</cell><cell>66.10</cell><cell>82.04</cell></row><row><cell>LMTCNN-2-1</cell><cell>44.26</cell><cell>70.78</cell><cell>85.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 ,</head><label>3</label><figDesc>we demonstrate that the accuracy of the LMTCNN with width multiplier = 2 of the first depthwise separable convolution and width multiplier = 1 of the second depthwise separable convolution for age and gender classification. As can be seen, although our architectures are more compact, their performance are comparable to that of the Levi Hassner CNN<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 . The Comparison of the model size</head><label>4</label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="2">Model size (MB)</cell></row><row><cell cols="2">Levi Hassner CNN [8]</cell><cell>70.8</cell></row><row><cell>LMTCNN-1-1</cell><cell></cell><cell>8.7</cell></row><row><cell>LMTCNN-2-1</cell><cell></cell><cell>30.0</cell></row><row><cell cols="3">Table 5. The speed of each method executed</cell></row><row><cell cols="2">in the mobile devices</cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Asus Zenbo Asus Zenfone 3</cell></row><row><cell></cell><cell>Speed</cell><cell>Speed</cell></row><row><cell></cell><cell>(ms/frame)</cell><cell>(ms/frame)</cell></row><row><cell>Levi Hassner CNN [8]</cell><cell>≈ 4800</cell><cell>≈ 4800</cell></row><row><cell>LMTCNN-1-1</cell><cell>204.7</cell><cell>204.9</cell></row><row><cell>LMTCNN-2-1</cell><cell>297.6</cell><cell>367.2</cell></row><row><cell cols="3">cropped from each frame for LMTCNN or Levi Hassner</cell></row><row><cell>CNN</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A bayesian/information theoretic model of learning to learn via multiple task sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Support-vector networks. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Age and gender classification using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeprebirth: Accelerating deep neural network execution on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">deep neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Descriptor based methods in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on faces in&apos;real-life&apos;images: Detection, alignment, and recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trace norm regularised deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SPL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
