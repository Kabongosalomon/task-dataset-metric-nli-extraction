<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple yet efficient real-time pose-based action recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-19">19 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Ludl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gulde</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristóbal</forename><surname>Curio</surname></persName>
						</author>
						<title level="a" type="main">Simple yet efficient real-time pose-based action recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-19">19 Apr 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing human actions is a core challenge for autonomous systems as they directly share the same space with humans. Systems must be able to recognize and assess human actions in real-time. In order to train corresponding data-driven algorithms, a significant amount of annotated training data is required. We demonstrated a pipeline to detect humans, estimate their pose, track them over time and recognize their actions in real-time with standard monocular camera sensors. For action recognition, we encode the human pose into a new data format called Encoded Human Pose Image (EHPI) that can then be classified using standard methods from the computer vision community. With this simple procedure we achieve competitive state-of-the-art performance in pose-based action detection and can ensure real-time performance. In addition, we show a use case in the context of autonomous driving to demonstrate how such a system can be trained to recognize human actions using simulation data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simple yet efficient real-time pose-based action recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dennis Ludl, Thomas Gulde and Cristóbal Curio</head><p>Abstract-Recognizing human actions is a core challenge for autonomous systems as they directly share the same space with humans. Systems must be able to recognize and assess human actions in real-time. In order to train corresponding data-driven algorithms, a significant amount of annotated training data is required. We demonstrated a pipeline to detect humans, estimate their pose, track them over time and recognize their actions in real-time with standard monocular camera sensors. For action recognition, we encode the human pose into a new data format called Encoded Human Pose Image (EHPI) that can then be classified using standard methods from the computer vision community. With this simple procedure we achieve competitive state-of-the-art performance in pose-based action detection and can ensure real-time performance. In addition, we show a use case in the context of autonomous driving to demonstrate how such a system can be trained to recognize human actions using simulation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There is an increasing consensus that a human-like understanding of human behavior is a major challenge for autonomous systems, like self-driving cars in urban areas <ref type="bibr" target="#b0">[1]</ref>. In the future, autonomous systems and human beings will co-exist in shared public spaces. Reliably inferring the world state with series sensor technology is still a challenge. One area that we consider very important is the detection of human actions. This area is still an open field and there are no systems that can be used productively in a stable and reliable manner. In areas where autonomous systems have to interact with people, it is very important that they have information about what people are exactly doing in their immediate environment. This is especially true if a direct interaction with the human being is to take place. Since human actions are highly dynamic, it is not only important to predict the actions correctly but also in real-time.</p><p>In addition to the runtime requirement for an algorithm, data-driven algorithms require massive amounts of training data. Data acquisition is usually one of the main problems in the development of a data-driven algorithm, thus we consider the provision of sufficient data to an algorithm to be an important factor in the design of this algorithm. We have shown in <ref type="bibr" target="#b1">[2]</ref> that we can train pose recognition algorithms with simulated data to recognize corner cases. As a continuation of this work we see great potential in the application of simulated data for the training of action detection algorithms. Since there is a domain shift from simulated visual data to real data, we decided to design a D. <ref type="bibr">Ludl</ref> {Dennis.Ludl, Thomas.Gulde, Cristobal.Curio}@Reutlingen-University.de <ref type="figure">Fig. 1</ref>. From skeletal joints to an Encoded Human Pose Image (EHPI), exemplified by the right wrist. The x and y coordinates are normalized (for the visualization the normalization process is simplified and ranges between 0 and 255 for RGB values), afterwards the X value is used as red component and the y value as green component. This RGB value is set for each frame in an n-dimensional vector at a fixed location. In the example 15 joints are used, the right wrist is set in row 9. The full EHPI is of size 32 × 15 × 3.</p><p>pose-based action recognition algorithm that works without direct dependence on visual sensor information. With this abstraction layer we want to enable the training of such an algorithm with simulated data and overcome domain transfer issues. This would save a lot manual effort that is required when recording and annotating real sensor data.</p><p>Our current project Open Fusion Platform 1 is about an autonomous vehicle with a valet parking function. It should automatically search for a free parking space on a parking lot and automatically be able to drive back to a pick-up point. Pedestrians can be present on the parking lot, thus it is important to recognize them. In addition to the pure recognition of pedestrians, it is also important to recognize what they are doing. In our use case, they are allowed to be in front of our parked vehicle, as long as the vehicle is not moving. In order to drive off while a pedestrian is detected in front of the vehicle, the pedestrian must clearly indicate, by a waving gesture, that the vehicle is allowed to drive out. Thus, the pedestrians have to be detected and further their current actions have to be classified. For this parking lot use case we have specified that the actions idle, walk and wave must be detected.</p><p>Our contributions in this work are: 1) A recognition pipeline which operates on 2D monocular camera images in real-time. It contains functionality to detect objects, humans and their poses as well as to track and estimate humans and their actions. 2) A novel pose-based action recognition algorithm with state-of-the-art performance. 3) A demonstration on how to improve our action recognition algorithm with simulated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are various directions of research in the area of human action recognition. Some approaches are based on Convolutional Neural Networks (CNNs). They usually follow a multistream approach <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which uses an RGB image for visual feature extraction as well as a representation of the temporal flow, usually in the form of optical flow. There is also work which make use of human poses, either using pose directly <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> or apply some attention like mechanism to get visual features from important areas around the human skeleton <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Those approaches often rely on recurrent neural networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Other approaches rely on handcrafted features extracted from human pose <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Most similar to our work is the work of Choutas et al. <ref type="bibr" target="#b10">[11]</ref>. They encoded time information in human body joint proposal heatmaps with color and use this stacked, colored joint heatmaps as an input for a CNN to classify the action. To reach state-of-the-art performance they combined this approach with another multistream approach <ref type="bibr" target="#b11">[12]</ref>. Most of these approaches are relatively complex and therefore do not meet the real-time requirements of autonomous systems. Our approach is much simpler and still delivers competitive performance.</p><p>Not less important than action recognition algorithms is the generation of action recognition datasets. Khodabandeh et al. <ref type="bibr" target="#b12">[13]</ref> provide a method to automatically generate an action recognition dataset by partitioning a video into action, subject and context. Souza et al. <ref type="bibr" target="#b13">[14]</ref> proposed a database of simulated human actions. They used motion capture data containing action annotations combined with 3D human models in a simulated environment and show that it improves action recognition rates when combined with a small amount of annotated real world data. Other simulations containing animated humans exists <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, but does not have a strong focus on realistic human actions. Nevertheless, the works in simulations to train and evaluate algorithms show that there is a large demand of realistic human motion data. Most current work either contains basic representations of humans or represents bigger databases which were generated procedurally targeting a broad range of motions. Human actions, on the other hand, must be simulated very precisely in order to be significant, especially when it comes to interactions. We have shown in previous work, that it is possible to easily finetune neural networks for pose detection by simulation in such a way that they can detect corner case poses, where they did not deliver any or insufficient results before <ref type="bibr" target="#b1">[2]</ref>. Following this approach we demonstrate how motion capture driven simulation is a useful method to generate human action recognition training data.</p><p>A drawback of simulated training data is the transfer of algorithms trained on simulated data to the application on real world data. There is usually a domain shift, which should be minimized by domain adaptation algorithms. Such approaches include the common use of few real data and many simulated data <ref type="bibr" target="#b16">[17]</ref>, methods which use decorrelated features <ref type="bibr" target="#b17">[18]</ref> and more advanced domain confusion algorithms <ref type="bibr" target="#b18">[19]</ref>. As an open field of research it is important to find alternatives to avoid the domain transfer problem. We view the abstraction of input data as a promising approach to apply algorithms trained on simulated data directly to real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RECOGNITION PIPELINE</head><p>All steps in our pipeline 2 are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Each step is described in more detail in the following subsections. The action detection is described in a separate section. In general each step in this pipeline is based on the output data from the previous step. In the first step, we recognize objects in a 2D camera image, especially people (c.f. section III-A). From this detection step we get bounding boxes around the respective object. If a person is recognized in a frame, we apply a pose recognition algorithm to the image information within that person's bounding box (c.f. section III-B). The pose recognition is a single person solution and has to be done for every human in the image. Based on the human poses in successive frames we developed a posebased tracking (c.f. section III-C) of humans. Based on the tracked human poses we finally perform an action detection (c.f. section IV).</p><p>We decided to not use an end-to-end approach from sensor input to action detection, but to use a modular pipeline from object detection to actual action detection. As can be seen in the pipeline (c.f. <ref type="figure" target="#fig_0">Figure 2</ref>), only object detection is performed on the entire sensor image. All other algorithms work on image regions and further pure pose data. This allows the more complex algorithms, such as pose recognition, to be applied only specifically, e.g. if a person is close to the autonomous system, or in the case of autonomous vehicles, if a construction worker has been identified. This also gives us the opportunity to use different training data at different steps and with different levels of abstraction layers, which enables us in particular to train our action recognition algorithm with simulated data (c.f. section V-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object Detection</head><p>An object detection algorithm is used to obtain an initial estimate of human's presence in the image. In addition to the accuracy of the algorithm, the running time is the main criterion for the selection of the object detection algorithm. Possible false detections of the object detection algorithm can be compensated by the pose detection and the tracking of humans (c.f. section III-C). In this paper we use Yolo V3 <ref type="bibr" target="#b19">[20]</ref> as a compromise between runtime and accuracy, which was pre-trained on ImageNet <ref type="bibr" target="#b21">[22]</ref> and the MSCOCO <ref type="bibr" target="#b22">[23]</ref> dataset. The object detection algorithm can be replaced by alternative object detection algorithms, depending on accuracy and  <ref type="bibr" target="#b19">[20]</ref>, 3) Pose Recognition <ref type="bibr" target="#b20">[21]</ref>, 4) Pose-based human tracking and 5) Pose-based action recognition runtime requirements. The input into the object detector is an RGB camera image, which may be scaled down to allow faster processing. The algorithm then estimates possible object locations in the image in the form of bounding boxes and classifies their content. After postprocessing the resulting data is a list of classified bounding boxes. In this work, only bounding boxes that have been classified as humans, are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose Estimation</head><p>For the human pose estimation we use the approach from Xiao et al. <ref type="bibr" target="#b20">[21]</ref> with its network pre-trained on the MSCOCO <ref type="bibr" target="#b22">[23]</ref> and MPII <ref type="bibr" target="#b23">[24]</ref> datasets. The algorithm requires human bounding boxes as input and estimates a human skeleton in this cropped region. Like in most state-of-theart pose recognition algorithms, a heat map is predicted for each joint, indicating the estimated probability for each joint. During the post-processing non maximum suppression is performed and a human skeleton is reconstructed in the form of 2D joint positions and their connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Human Tracking</head><p>For our action recognition algorithm, a person's skeletal information is required across multiple frames. Since the pose recognition algorithm described above is applied to single images, the skeletons in several frames are initially independent of each other. In order to establish the reference of skeletons across several frames, we track the skeletons based on their joint positions. We use the pyramidal implementation of the Lukas Kanade Feature Tracker <ref type="bibr" target="#b24">[25]</ref> and take the joint positions of the human skeletons in the image as features to be tracked. We end up with an estimated skeleton in frame n for each skeleton in frame n−1. With these tracked skeletons from frame n − 1 and new detected skeletons from frame n we have a number of skeleton proposals which need to be merged as follows.</p><p>A merge is done by measuring the similarity of two human poses. If they are similar enough the two skeletons will be merged to one human skeleton, thus tracking the human over time. In addition to comparing detected and tracked people for a possible merge, all detected people must also be compared, since the same person could be detected several times by false detection of the object or pose recognition algorithm.</p><p>The first step to merge two humans is to find the similarity between two human skeletons. Let a and b be human skeleton hypotheses from a list of detected or tracked skeletons. We define ∆ a as the maximum distance between joint i in two skeletons to be considered being part of the same skeleton. ∆ a is calculated by using the bounding box width w a and height h a of human a (c.f. Equation 1).</p><formula xml:id="formula_0">∆ a = F ( w 2 a + h 2 a )<label>(1)</label></formula><p>Factor F denotes a hyperparameter, which corresponds to the percentage of the human's bounding box diagonal. Setting F = 0.025 has proven in practice. Next, the Euclidean distance δ abi of joint i between human skeleton a and b is calculated (c.f. Equation 2).</p><formula xml:id="formula_1">δ abi = a i − b i 2<label>(2)</label></formula><p>This distance is only considered if a and b contain joint i with a minimum probability of T J that specifies the minimum joint quality required to use joints in the tracking process. Setting T J = 0.4 worked well in practice. This constraint is included to enable tracking even when some joints are not recognized or only poorly recognized, e.g. due to occlusion. We then calculate the similarity score (S abi ) for joint i by comparing the actual joint distance with the maximum acceptable distance (c.f. Equation 3).</p><formula xml:id="formula_2">S abi = 1 − (δ abi /∆ a ), if δ abi &lt; ∆ a 0, otherwise<label>(3)</label></formula><p>The similarity (S ab ) between human skeleton a and b is calculated by combining all joint similarities S abi (c.f. section 4).</p><formula xml:id="formula_3">S ab = 1 I I i=1 (s abi )<label>(4)</label></formula><p>Factor I denotes the number of joints used for tracking. We then try to merge all detected human skeletons with other detected human skeletons to avoid repeated recognition of skeletons belonging to the same person. We define a threshold T S = 0.15 to specify when two human skeletons are similar. If the similarity score is above T S for two detected humans the detection with the lower score is removed from the detection list. After merging the detected human skeletons they are merged with all tracked humans from previous frames. In this merge process every detected human is compared to every tracked human. If the similarity score of two skeletons is above T S the identifier of the tracked human is assigned to the detected human and the tracked skeleton is removed from the tracking list. If after the merge process some tracked humans are left over, meaning that the object detector did not provide a human proposal at the location of a tracked human, we apply the pose estimation on the bounding box of the tracked human and if a human skeleton with a score higher than T J is estimated we keep this human with its identifier as a detected human. With this approach we can compensate false negatives from the object detector and we are even able to deactivate the object detection completely to improve performance (c.f. section III-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance</head><p>The runtime of our pipeline scales with the number of people to be detected. Usually only the humans in the immediate surrounding area of the autonomous system are relevant, so the entire pipeline may normally not have to be used for all humans in the image. For one human with input image resolution of 1280x720, downscaled for processing to 640x360, the entire pipeline runs on average with 29 FPS. For two people, the FPS is reduced to around 21 FPS, which still ensures real-time processing. To improve performance in special cases, object detection, which is usually performed for each frame, can be disabled once a person has been detected. The pose and action detection can then be continued for this person based on bounding box proposals from our tracking process. By switching off the object detection, on average 57 FPS can be achieved for one person. Depending on the requirements, it would be possible to perform object detection only on a limited number of frames. It is also important to note that our implementation is not completely designed for performance, as more emphasis was placed on code readability. It can be assumed that the performance can be increased with appropriate adaptations. All performance tests were carried out on a laptop with an Intel i7-8700 six core CPU and a NVIDIA GTX 1080 GPU using Ubuntu 18.04 with CUDA 10.0 and CUDNN 7.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. POSE-BASED ACTION RECOGNITION</head><p>Since a lot of progress has been made in the field of convolutional neural networks, we have decided to investigate an approach in which human skeletons are encoded over time in an image-like data structure. We expected a more stable and accurate system compared to recurrent neural networks.</p><p>The basic process is shown above in <ref type="figure">Figure 1</ref>. Once the pose of a human has been extracted from the camera image, the basic idea is to encode the x, y and z positions of the joints as red, green and blue values in an RGB image. In this paper we work with 2D pose detection on monocular camera images, so the z value is not used and thus the blue channel is set to zero. The channel could also be removed as long as only the x and y coordinates are used. In order to convert the global joint coordinates into corresponding 'color values' we normalize them. This process is described in more detail in section IV-B. Note that we normalize the values as network input in the continuous range from zero to one and not as discrete integer values from 0 − 255 thus the analogy of an image is therefore not entirely accurate. Yet for visualization purpose in this paper we normalized the joint positions to 0 − 255 for the figures. Basically any number of joints can be encoded. In the current work we use nose, neck, hip center, left shoulder, left elbow, left wrist, right shoulder, right elbow, right wrist, left hip, left knee, left ankle, right hip, right knee and right ankle in this particular order. The encoded joints are assigned in a fixed order in a 1 × n × 3 matrix, where n stands for the number of joints. After the human pose for a frame has been encoded into such a matrix, it is appended as last column to a m × n × 3 matrix and replaces the first column of this matrix if it already contains m frames. Each column represents an encoded human pose in a frame. The full matrix represents an Encoded Human Pose Image. In the current work we use m = 32 which were chosen because we want to analyze about one to two seconds of movements to give an action estimate. Additionally, it corresponds to standard image width used in machine learning applications. To stabilize our action recognition we take the recognitions of the last 20 frames and use the action class with the highest summed probability in the last 20 frames as our prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network</head><p>For the classification of the EHPIs we used a very simple network consisting of six convolutional layers, of which each has a 3 × 3 kernel as well as both a padding and stride of one. A fully connected layer is placed at the end for the final action classification (see <ref type="figure" target="#fig_1">Figure 3</ref>). Each convolutional layer is followed by batch normalization <ref type="bibr" target="#b25">[26]</ref>. As activation function we use ReLU in the convolutional layers. After the second and fourth convolution we apply a max pooling layer with a kernel size of 2×2 which reduces the spatial resolution by factor two. After the last convolutional layer we apply a global average pooling layer. We use Xavier initialization <ref type="bibr" target="#b26">[27]</ref> for all convolutional layers. By using such a deep neural network architecture the deeper the network is, the more spatio-temporal context should be encoded in the learned <ref type="figure">Fig. 4</ref>. EHPI examples of different actions. The example of the right wrist, which is explicitly shown at three times its height, clearly shows that a smooth color gradient is visible in the idle action, a color gradient from green to orange is visible during walking and a repetitive gradient from green to red is observable during waving.</p><p>features due larger receptive fields.</p><p>Since we have used considerably more data in our use case than is available in the JHMDB <ref type="bibr" target="#b27">[28]</ref> dataset (see section V), the network is no longer sufficient. Expanding the network with further convolutional layers and also increasing the size of the fully connected layer would result in the network having more parameters than some existing and efficient CNNs for classification. Therefore we employ the ShuffleNet v2 <ref type="bibr" target="#b28">[29]</ref> architecture with which we also demonstrate the application of standard computer vision algorithms to EHPIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>The normalization of the EHPI takes place on the entire m × n × 3 matrix. We normalize the encoded x and y values independently between zero and one. This type of normalization is intended to ensure the independence of the body size of different people while maintaining the relative change in scale through a different distance to the camera. We consider correspondingly the local range of motion of a person for a time window of length m. Before normalization, we also remove human body joints that are outside the image as a preprocessing step by setting their coordinates to zero. When joints are not recognized or have a probability below T J the x and y values for them are set to zero. The same applies for human poses which are not recognized at all, here we set the complete 1 × n × 3 matrix to zero. We define that an EHPI requires at least two frames with human poses to be considered. <ref type="figure">Figure 4</ref> shows the EHPIs and a camera image of the last frame (rightmost column) of the EHPI for three examples of the actions idle, walk and wave. The row that represents the joint of the right wrist is plotted in an enlarged view <ref type="figure">Fig. 5</ref>. Five frames from a sequence with camera image, EHPI (right wrist, enlarged) and the full EHPI. The EHPI for the right wrist moves during waving towards red (max. in x direction). The first picture shows a false detection of the left wrist (EHPI, row 6), which is filtered by the application of also noisy training data of the action detection. In the last image the whole EHPI is shifted more into red. This is due to the fact that there are no more extreme false detections of the left wrist that shift the maximum x value during normalization. since it is diagnostic for discriminating the actions of interest in the following example. For the action idle the color representation is relatively constant over the whole period, because there is hardly any movement of the joint. For the action walk, one can notice a smooth transition from green to orange. This is due to the fact that the joint of the right wrist moves from left to right of the image during the EHPI period. Therefore the normalized x value moves more and more towards one (in the visualization thus the red value towards 255), while the y value (in the visualization the green value) remains relatively constant. During the wave action one can notice a repetitive color gradient from green to red, because the joint of the right hand moves repeatedly in x direction during the wave movement. <ref type="figure">Figure 5</ref> shows a sequence of a wave movement in more detail. At the end (right) part of the EHPI for the joint of the right wrist, the color encoding gets more red when the joint is on the right side of the picture. Further, the effects of false detection of body joints is displayed clearly. In this example, the left ankle (row 6 in EHPI) is partially recognized incorrectly and thus is encoded much further to the right than it actually is. This becomes clear with the strongly red coded areas, which appear without a clean transition from green to red. The joint of the left ankle was recognized with a probability above T J by the pose estimation algorithm, thus it is not encoded as zero. Due to the distorted, more extreme red values of the left hand, the entire EHPI is shifted a little into the green area, which becomes clear in the last image (frame 71), where most of the joint recognition errors are no longer present and the color of the entire EHPI shifts towards red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASETS</head><p>The JHMDB <ref type="bibr" target="#b27">[28]</ref> dataset consists of 928 videos of which each has an annotation label denoting one of 21 action classes. Each video has a resolution of 320x240 pixels. The evaluation is done on three splits of which each uses about 30% test data. Results are reported as mean over all splits. From here on JHMDB refers to the full dataset, while JHMDB-1 refers to JHMDB split 1 with pose data from our pipeline and JHMDB-1-GT refers to JHMDB-1 with pose data from the JHMDB ground truth.</p><p>Our automotive parking lot use case dataset SIM consists of various camera sequences. We have recorded different videos with a Logitech C920 webcam, an iTracker GS6000 dashcam and a Yi 4k+ camera. It is a very use case specific dataset, which only contains the actions idle, walk and wave. Recordings were partly taken inside buildings, partly also in use case situations in the vehicle. In addition, our dataset contains some simulated elements, which are described in more detail in section V-A. The entire dataset consists of 216 labeled sequences and a total of 61826 EHPIs. All videos have a resolution of 1280x720 at 30 FPS. All sequences contain actions from the same person. For the evaluation 27 sequences with a total of 8351 frames are used. All sequences are cuts from one scene, which corresponds to our use case. The scene was recorded simultaneously from the dashcam and the action cam to get data from two different sensors on slightly different locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulated data</head><p>Due to our positive experiences with the use of simulation to improve pose recognition algorithms <ref type="bibr" target="#b1">[2]</ref>, we decided to use simulation data in this work to further enrich our training data. The advantage of our modular pipeline is that we can use simulation data as training data at different steps in the pipeline, while real data is used at other steps. In the case of action detection, this offers the great advantage that we have the abstraction layer of the pose data between the sensor information and the action detection. The underlying hypothesis is that sensor domain transfer problems between simulation and real data are prevented by this abstraction layer. Motion data is required to generate the simulation data. In principle, the motion capture data alone is sufficient to generate ground truth data for our pose-based action detection. By a corresponding 2D projection of the 3D joint coordinates for any number of camera positions in 3D space, 2D pose information can be obtained without generating camera sensor simulations. Since pose recognition algorithms are not perfect and artifacts like a slight jittering of the joint positions, false recognition of joints or not recognizing joints can occur, we additionally generate the simulated camera images to apply the pose recognition algorithms and use the output as ground truth with such kind of natural noise for the action recognition. In previous work we have shown evidence that pose recognition algorithms have similar problems on simulated data as on real data <ref type="bibr" target="#b1">[2]</ref>, thus we expect to see the same bias on estimated human poses on simulation data as on real data. To simulate sensor information it is also necessary to use a 3D environment and a 3D human model. We recorded the motion data of the actions idle, walk and wave in our motion capture laboratory. One person performed every action ten times for ten seconds. The motion data is used to animate a 3D human model in our Unity R based simulation. The environment is not very relevant in this case, because we only need to recognize an animated 3D human model with our pose recognition algorithm and using its output as ground truth for action recognition. Therefore we only use a flat area with a skybox for the background without any other environmental details (c.f. <ref type="figure" target="#fig_2">Figure 6</ref>). At the end, various virtual camera sensors can be placed around the person, which then generate corresponding sensor information. For each virtual camera image the ground truth, in this case the 2D pose and the corresponding action, can be generated automatically <ref type="bibr" target="#b1">[2]</ref>. <ref type="figure" target="#fig_2">Figure 6</ref> shows some examples of the simulated sensor information. We used a total of six camera positions in this work.</p><p>In the following we distinguish between SIM (gt) which contains the perfect pose data from motion capturing directly, SIM (pose) which contains the pose data from the output of our pose detection pipeline and SIM which contains the data from both sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Augmentation</head><p>To increase the variance in our training data we use data augmentation. Joints are flipped horizontally in 50% of the cases. If the image is flipped, in 50% of the flipped images the indexes of the left and right joints are also switched, thus we simulate in 25% of the cases a person looking in the other camera direction than originally. In addition, we partly remove joints to simulate occlusion. In 25% of the cases we remove the joints of both feet and in 6.25% of the cases we also remove the joints of the knees. This type of augmentation is mainly the result of our use case, in which it can happen that the feet and partly also the knees are covered by the hood of the vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. TRAINING</head><p>We used 33% of the JHMDB-1 training data for validation. We did not focus on the tuning of hyperparameters and therefore only varied the batch size, the learning rate and the number of epochs to find out how to train the network fast and stable. We trained the network with a batch size of 64, an initial learning rate of 0.05 and a standard momentum of 0.9 for 200 epochs (140 on JHMDB-1-GT) for our experiments on the JHMDB dataset. Since the JHMDB data set is quite small, we also used a weight decay (L2 regularization <ref type="bibr" target="#b29">[30]</ref>) of 5e −4 to counteract overfitting. For optimization we use the stochastic gradient descent (SGD) and the cross-entropy loss. We reduce the learning rate every 50 epochs by a factor of ten. The classes in the JHMDB data set are not evenly distributed, especially as far as the number of EHPIs per video sequence is concerned, as they vary in length. Therefore, we apply a sampling per epoch that outputs a balanced number of samples for each class by reusing samples from classes with few samples and using only a subset of samples from classes with many samples. We use the same parameters for our use case data set, but since there is considerably more data available, we adjust the batch size to 128. We train the network with five different seeds to exclude random effects during weight initialization and to ensure the reproducibility of our results. We therefore report results as the mean value with standard deviation over these five runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. JHMDB evaluation</head><p>The current state-of-the-art approach PoTion <ref type="bibr" target="#b10">[11]</ref> combines their pose-based action detection with the multi-stream approach I3D <ref type="bibr" target="#b11">[12]</ref> and achieves a total performance of 85.5% on the JHMDB data set. Since we only need the three actions idle, walk and wave for our use case, which can also be distinguished purely with pose data, and real-time is a necessary prerequisite, we use the pure pose-based EHPIs in this work. We therefore compare ourselves with parts of other work that also report results for pure pose-based algorithms. The results are summarized in <ref type="table" target="#tab_1">Table I</ref> in terms of accuracies. JHMDB results are reported as mean value over the three dataset splits. In cases where our pose recognition pipeline was unable to find a human in a video sequence we could not apply the action recognition algorithm and thus we counted that sample as recognized falsely. In 904 of 928 videos we were able to recognize a human skeleton in at least two frames and thus created an EHPI and performed the action detection. For cases where we detected more than one person in a video we used the one with the highest pose score.</p><p>On the whole JHMDB dataset we outperform PoTion <ref type="bibr" target="#b10">[11]</ref> by a margin of 3.5%. On JHMDB-1 a result is also provided by Zolfaghari et al. <ref type="bibr" target="#b4">[5]</ref>. We outperformed PoTion by a margin of 1.2% and Zolfaghari et al. <ref type="bibr" target="#b4">[5]</ref> by a margin of 14.8%. Using only the ground truth pose information provided by the JHMDB Dataset the results of PoTion outperform our results by a margin of 5.3%. This can be either caused because our pose recognition pipeline provides better pose information or because PoTion was applied to a cropped image around the actuator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Automotive parking lot use case evaluation</head><p>To evaluate our system we compared two types of results. First, we show how many of the action sequences were correctly recognized, denoted by Accuracy (Seq). Since a sequence can sometimes last several seconds and the total detection consists of the accumulated predictions of the individual EHPIs, we also consider it useful to indicate how many of the individual EHPIs are correctly detected, denoted by Accuracy (EHPIs). The results are shown in <ref type="table" target="#tab_1">Table II</ref>. With real data only we are able to correctly classify 99.26% of the test sequences. The misclassified sequence is an wave sequence that has been classified as idle. The false detection was probably caused by the fact that the waving in this sequence was executed with the left hand, for which only little training data was available. The overall great results are due to the fact that the use case is rather focused and we have enough similar training data available. With 81.48% correctly recognized sequences and 70.64% correctly recognized EHPIs when trained purely on simulated data, there seems to be no big domain shift between simulated and real training data. We also found that the performance is slightly better when we use the noisy pose data from our pose detection pipeline as ground truth rather than using the pose information directly from the motion capture system, hinting that it is beneficial to use both ground truth sources. As the standard deviation shows, the hyperparameters are not yet optimal for training, but in this paper the network tuning is not the focus. By combining real and simulated training data for action detection, we were able to increase the overall detection rate of all EHPIs by 1.32% to 97.07%. Considering how easily and quickly the simulated data can be generated, the use of the simulation approach, at least as an addition to real data, is very promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this work, we have shown how an efficient pipeline can be built that can recognize humans in real-time, estimate and track their poses, and recognize their current action. We have introduced a new encoding technique, in which human poses are encoded over a fixed period of time into an imagelike data structure that can be used for action recognition using classification CNNs. Our EHPI based action detection delivers state-of-the-art performance compared to other posebased algorithms and still runs in real-time. In future work it should be investigate how further scene properties and context can be encoded into an EHPI in order to be able to recognize actions that are not distinguishable on pose data only. In addition, we were able to realize the requirements of our automotive parking lot use case with the presented pipeline. Action recognition results could be transferred to other sensors, environments and people in first tests. We were also able to show that the use of simulation data in combination with real data is suitable for the enrichment of training of action detection algorithms. The results obtained on the purely simulated training data are also very promising. This approach must be further evaluated to determine if the portion of real data for the training can be reduced further or even be omitted. Further, we have currently used standard image classification CNN architectures for the classification of the EHPIs. These do not take into account the special spatiotemporal structure of an EHPI. With more specific network architectures exploiting these spatiotemporal relationships between joints, the process could probably be further improved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Real-time action recognition pipeline from a monocular camera mounted in the car observing the gesture of a potential user of the autonomous vehicle. From left to right: 1) Raw camera image, 2) Object detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Simple network architecture that we used to classify the EHPIs on the JHMDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Demonstration of virtual sensor information used to train our action recognition algorithm. On top a picture of a real sensor of the motion recording in the motion capture lab and below the simulated scene from two different camera positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, T. Gulde and C. Curio are with the Cognitive</figDesc><table><row><cell>Systems</cell><cell>Group,</cell><cell>Computer</cell><cell>Science</cell><cell>Department,</cell><cell>Reutlingen</cell></row><row><cell>University,</cell><cell cols="2">Germany.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON THE JHMDB DATASET COMPARED TO OTHER PURE POSE-BASED ALGORITHMS.</figDesc><table><row><cell>Method</cell><cell>JHMDB</cell><cell>JHMDB-1</cell><cell>JHMDB-1-GT</cell></row><row><cell>PoTion[11]</cell><cell>57.0</cell><cell>59.1</cell><cell>70.8</cell></row><row><cell cols="2">Zholfaghari et al.[5] N/A</cell><cell>45.5</cell><cell>56.8</cell></row><row><cell>EHPI (ours)</cell><cell cols="2">60.5 ± 0.2 60.3 ± 1.3</cell><cell>65.5 ± 2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">USE CASE RESULTS</cell></row><row><cell>Method</cell><cell cols="2">Accuracy (Seq) Accuracy (EHPIs)</cell></row><row><cell>SIM (Pose)</cell><cell>80.74 ± 2.77</cell><cell>69.72 ± 1.80</cell></row><row><cell>SIM (GT)</cell><cell>79.26 ± 3.78</cell><cell>67.78 ± 1.86</cell></row><row><cell>SIM</cell><cell>81.48 ± 3.31</cell><cell>70.64 ± 2.60</cell></row><row><cell>Real only</cell><cell>99.26 ± 1.48</cell><cell>95.75 ± 1.65</cell></row><row><cell cols="2">SIM + Real 99.26 ± 1.48</cell><cell>97.07 ± 1.80</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.ofp-projekt.de/ (Last visited on 2019-04-08)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code available at https://github.com/noboevbo/ehpi action recognition</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This project has been sponsored by BMBF project OFP (16EMO0114) and MoCap 4.0 (03FH015IN6).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Big Problem With Self-Driving Cars Is People</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum: Technology, Engineering, and Science News</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using Simulation to Improve Human Pose Estimation for Corner Cases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ludl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gulde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thalji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Int. Conf. on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3575" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human Action Recognition Based on Temporal Pose CNN and Multi-dimensional Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops, ser. Lecture Notes in Computer Science</title>
		<editor>L. Leal-Taixé and S. Roth</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="426" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-stream CNN: Learning representations based on human-related regions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="3745" to="3754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ActionXPose: A Novel 2D Multi-view Pose-based Algorithm for Real-time Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Naqvi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12126</idno>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4263" to="4270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On-Board Detection of Pedestrian Intentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">2193</biblScope>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handcrafting vs Deep Learning: An Evaluation of NTraj+ Features for Pose Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop: New Challenges in Neural Computation (NC2)</title>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PoTion: Pose MoTion Representation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
	<note>Action Recognition? A New Model and the Kinetics Dataset</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DIY Human Action Dataset Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodabandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zharkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pradeep</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">910</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Procedural Generation of Videos to Train Deep Action Recognition Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sim4CV: A Photo-Realistic Simulator for Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="902" to="919" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CARLA: An Open Urban Driving Simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Annual Conference on Robot Learning</title>
		<meeting>the 1st Annual Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual and Real World Adaptation for Pedestrian Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marín</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gerónimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="809" />
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple Baselines for Human Pose Estimation and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="472" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common Objects in Context,&quot; in Computer Vision -ECCV 2014, ser. Lecture Notes in Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<editor>Science, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2D Human Pose Estimation: New Benchmark and State of the Art Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramidal implementation of the Lucas Kanade feature tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Corporation</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>Microprocessor Research Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Int. Conf. on Machine Learning, ser. ICML&apos;15</title>
		<meeting>the 32nd Int. Conf. on Machine Learning, ser. ICML&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Int. Conf. on Artificial Intelligence and Statistics</title>
		<meeting>the 13th Int. Conf. on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards Understanding Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature selection, L1 vs. L2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st Int. Conf. on Machine Learning -ICML &apos;04</title>
		<meeting><address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
