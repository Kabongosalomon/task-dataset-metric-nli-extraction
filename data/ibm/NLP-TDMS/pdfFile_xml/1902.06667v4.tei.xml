<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenyu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
							<email>yanqiao.zhu@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) have been successfully applied in node classification tasks of network mining. However, most of these models based on neighborhood aggregation are usually shallow and lack the "graph pooling" mechanism, which prevents the model from obtaining adequate global information. In order to increase the receptive field, we propose a novel deep Hierarchical Graph Convolutional Network (H-GCN) for semisupervised node classification. H-GCN first repeatedly aggregates structurally similar nodes to hypernodes and then refines the coarsened graph to the original to restore the representation for each node. Instead of merely aggregating one-or two-hop neighborhood information, the proposed coarsening procedure enlarges the receptive field for each node, hence more global information can be captured. The proposed H-GCN model shows strong empirical performance on various public benchmark graph datasets, outperforming state-of-the-art methods and acquiring up to 5.9% performance improvement in terms of accuracy. In addition, when only a few labeled samples are provided, our model gains substantial improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs nowadays become ubiquitous owing to the ability to model complex systems such as social relationships, biological molecules, and publication citations. The problem of classifying graph-structured data is fundamental in many areas. Besides, since there is a tremendous amount of unlabeled data in nature and labeling data is often expensive and time-consuming, it is often challenging and crucial to analyze graphs in a semi-supervised manner. For instance, for semi-supervised node classification in citation networks, where nodes denote articles and edges represent citation, the task is to predict the label of every article with only a few labeled data.</p><p>As an efficient and effective approach to graph analysis, network embedding has attracted a lot of research interests. It aims to learn low-dimensional representations for nodes whilst still preserving the topological structure and node feature attributes. Many methods have been proposed for network embedding, which can be used in the node classification task, such as DeepWalk <ref type="bibr" target="#b9">[Perozzi et al., 2014]</ref> and node2vec <ref type="bibr" target="#b5">[Grover and Leskovec, 2016]</ref>. They convert the graph structure into sequences by performing random walks on the graph. Then, the proximity between the nodes can be captured based on the co-occurrence statistics in these sequences. But they are unsupervised algorithms and cannot perform node classification tasks in an end-to-end fashion. Unlike previous random-walk-based approaches, employing neural networks on graphs has been studied extensively in recent years. Using an information diffusion mechanism, the graph neural network (GNN) model updates states of the nodes and propagate them until a stable equilibrium <ref type="bibr" target="#b9">[Scarselli et al., 2009</ref>]. Both of the highly non-linear topological structure and node attributes are fed into the GNN model to obtain the graph embedding. Recently, there is an increasing research interest in applying convolutional operations on the graph. These graph convolutional networks (GCNs) <ref type="bibr" target="#b8">[Kipf and Welling, 2017;</ref><ref type="bibr">Veličković et al., 2018]</ref> are based on the neighborhood aggregation scheme which generates node embedding by combining information from neighborhoods. Comparing with conventional methods, GCNs achieve promising performance in various graph analytical tasks such as node classification and graph classification <ref type="bibr" target="#b3">[Defferrard et al., 2016]</ref> and has shown effective for many application domains, for instance, recommendation <ref type="bibr" target="#b2">Cui et al., 2019]</ref>, traffic forecasting , and action recognition <ref type="bibr">[Yan et al., 2018]</ref>.</p><p>Nevertheless, GCN-based models are usually shallow and lack the "graph pooling" mechanism, which restricts the scale of the receptive field. For example, there are only two layers in GCN <ref type="bibr" target="#b8">[Kipf and Welling, 2017]</ref>. As each graph convolutional layer acts as the approximation of aggregation on the first-order neighbors, the two-layer GCN model only aggregates information from two-hop neighborhoods for each node. Because of the restricted receptive field, the model has difficulty in obtaining adequate global information. However, it has been observed from the reported results <ref type="bibr" target="#b8">[Kipf and Welling, 2017</ref>] that simply adding more layers will de-Multi-channel GCNs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarsening operation</head><p>Refining operation Softmax classifier Shortcut connection n 1 × d n 2 × d n 3 × d n 3 × d n 2 × d n 1 × d <ref type="figure">Figure 1</ref>: The workflow of H-GCN. In this illustration, there are seven layers with three coarsening layers, three symmetric refining layers, and one output layer. Coarsening layer at level i produces graph Gi+1 of ni+1 hyper-nodes with d-dimensional latent representations, vice versa for refining layers.</p><p>grade the performance. As explained in , each GCN layer acts as a form of Laplacian smoothing in essence, which makes the features of nodes in the same connected component similar. Thereby, adding too many convolutional layers will result in the output features over-smoothed and make them indistinguishable. Meanwhile, deeper neural networks with more parameters are harder to train. Although some recent methods <ref type="bibr" target="#b2">[Chen et al., 2018;</ref><ref type="bibr" target="#b9">Xu et al., 2018;</ref><ref type="bibr" target="#b10">Ying et al., 2018]</ref> try to get the global information through deeper models, they are either unsupervised models or need many training examples. As a result, they are still not capable of solving the semi-supervised node classification task directly.</p><p>To this end, we propose a novel architecture of Hierarchical Graph Convolutional Networks, H-GCN for brevity, for node classification on graphs 1 . Inspired from the flourish of applying deep architectures and the pooling mechanism into image classification tasks, we design a deep hierarchical model with coarsening mechanisms. The H-GCN model increases the receptive field of graph convolutions and can better capture global information. As illustrated in <ref type="figure">Figure 1</ref>, H-GCN mainly consists of several coarsening layers and refining layers. For each coarsening layer, the graph convolutional operation is first conducted to learn node representations. Then, a coarsening operation is performed to aggregate structurally similar nodes into hyper-nodes. After the coarsening operation, each hyper-node represents a local structure of the original graph, which can facilitate exploiting global structures on the graph. Following coarsening layers, we apply symmetric graph refining layers to restore the original graph structure for node classification tasks. Such a hierarchical model manages to comprehensively capture nodes' information from local to global perspectives, leading to better node representations.</p><p>The main contributions of this paper are twofold. Firstly, to the best of our knowledge, it is the first work to design a deep hierarchical model for the semi-supervised node classification task. Compared to previous work, the proposed model consists of more layers with larger receptive fields, which is able to obtain more global information through the coarsening and refining procedures. Secondly, we conduct extensive experiments on a variety of public datasets and show that the proposed method constantly outperforms other state-of-theart approaches. Notably, our model gains a considerable improvement over other approaches with very few labeled samples provided for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review some previous work on graph convolutional networks for semi-supervised node classification, hierarchical representation learning on graphs, and graph reduction algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Networks</head><p>In the past few years, there has been a surge of applying convolutions on graphs. These approaches are essentially based on the neighborhood aggregation scheme and can be further divided into two branches: spectral approaches and spatial approaches.</p><p>The spectral approaches are based on the spectral graph theory to define parameterized filters. <ref type="bibr" target="#b0">Bruna et al. (2014)</ref> first define the convolutional operation in the Fourier domain. However, its heavy computational burden limits the application to large-scale graphs. In order to improve efficiency, <ref type="bibr" target="#b3">Defferrard et al. (2016)</ref> propose ChebNet to approximate the K-polynomial filters by means of a Chebyshev expansion of the graph Laplacian. <ref type="bibr" target="#b8">Kipf and Welling (2017)</ref> further simplify the ChebNet by truncating the Chebyshev polynomial to the first-order neighborhood. <ref type="bibr">DGCN [Zhuang and Ma, 2018]</ref> uses random walks to construct a positive mutual information matrix. Then, it utilizes that matrix along with the graph's adjacency matrix to encode both local consistency and global consistency.</p><p>The spatial approaches generate node embedding by combining the neighborhood information in the vertex domain. MoNet <ref type="bibr" target="#b9">[Monti et al., 2017]</ref> and SplineCNN <ref type="bibr" target="#b4">[Fey et al., 2018]</ref> integrate the local signals by designing a universe patch operator. To generalize to unseen nodes in an inductive setting, GraphSAGE <ref type="bibr" target="#b6">[Hamilton et al., 2017]</ref> samples a fixed number of neighbors and employs several aggregation functions, such as concatenation, max-pooling, and LSTM aggregator. <ref type="bibr">GAT [Veličković et al., 2018]</ref> introduces the attention mechanism to model different influences of neighbors with learnable parameters. <ref type="bibr" target="#b5">Gao et al. (2018)</ref> select a fixed number of neighborhood nodes for each feature and enables the use of regular convolutional operations on Euclidean spaces. However, the above two branches of GCNs are usually shallow and cannot obtain adequate global information as a consequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Representation Learning on Graphs</head><p>Some work has been proposed for learning hierarchical information on graphs. <ref type="bibr" target="#b2">Chen et al. (2018)</ref> and Liang et al.</p><p>(2018) use a coarsening procedure to construct a coarsened graph of smaller size and then employ unsupervised methods, such as DeepWalk <ref type="bibr" target="#b9">[Perozzi et al., 2014]</ref> and node2vec <ref type="bibr" target="#b5">[Grover and Leskovec, 2016]</ref> to learn node embedding based on that coarsened graph. Then, they conduct a refining procedure to get the original graph embedding. However, their two-stage methods are not capable of utilizing node attribute information and can neither perform node classification task in an end-to-end fashion. JK-Nets <ref type="bibr" target="#b9">[Xu et al., 2018]</ref> proposes general layer aggregation mechanisms to combine the output representation in every GCN layer. However, it can only propagate information across edges of the graph and are unable to aggregate information hierarchically. Therefore, the hierarchical structure of the graph cannot be learned by JK-Nets. To solve this problem, DiffPool <ref type="bibr" target="#b10">[Ying et al., 2018]</ref> proposes a pooling layer for graph embedding to reduce the size by a differentiable network. As DiffPool is designed for graph classification tasks, it cannot generate embedding for every node in the graph; hence it cannot be directly applied in node classification scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Reduction</head><p>Many approaches have been proposed to reduce the graph size without losing too much information, which facilitate downstream network analysis tasks such as community discovery and data summarization. There are two main classes of methods that reduce the graph size: graph sampling and graph coarsening. The first category is based on graph sampling strategy <ref type="bibr">[Papagelis et al., 2013;</ref><ref type="bibr" target="#b7">Hu and Lau, 2013;</ref><ref type="bibr" target="#b1">Chen et al., 2017]</ref>, which might lose key information during the sampling process. The second category applies graph coarsening strategies that collapse structure-similar nodes into hyper-nodes to generate a series of increasingly coarser graphs. The coarsening operation typically consists of two steps, i.e. grouping and collapsing. At first, every node is assigned to groups in a heuristic manner. Here a group refers to a set of nodes that constitute a hyper-node. Then, these groups are used to generate a coarser graph. For an unmatched node, Hendrickson and Leland (1995) randomly select one of its un-matched neighbors and merge these two nodes. Karypis and Kumar (1998) merge the two un-matched nodes by selecting those with the maximum weight edge. LaSalle and Karypis (2015) use a secondary jump during grouping.</p><p>However, these graph reduction approaches are usually used in unsupervised scenarios, such as community detection and graph partition. For semi-supervised node classification tasks, existing graph reduction methods cannot be used directly, as they are not capable of learning complex attributive and structural features of graphs. In this paper, H-GCN conducts graph reduction for non-Euclidean geometry like the pooling mechanism for Euclidean data. In this sense, our work bridges graph reduction for unsupervised tasks to the practical but more challenging semi-supervised node classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Notations and Problem Definition For the input undirected graph G 1 = (V 1 , E 1 ), where V 1 and E 1 are respectively the set of n 1 nodes and e 1 edges, let A 1 ∈ R n1×n1 be the adjacency matrix describing its edge weights and X ∈ R n1×d1 be the node feature matrix, where d 1 is the dimension of the attributive features. We use edge weights to indicate connection strengths between nodes. For the H-GCN network, the graph fed into the i th layer is represented as G i with n i nodes. The adjacency matrix and hidden representation matrix of G i are represented by A i ∈ R ni×ni and H i ∈ R ni×di respectively.</p><p>Since coarsening layers and refining layers are symmetrical, A i is identical to A l−i+1 , where l is the total number of layers in the network. For example, in the seven-layer model illustrated in <ref type="figure">Figure 1</ref>, G 3 is the input graph for the third layer and G 5 is the resulting graph from the fourth layer. After one coarsening operation and one refining operation, G 3 and G 5 share exactly the same topological structure A 3 . As nodes will be assigned as a hyper-node, we define node weight as the number of nodes contained in a hyper-node.</p><p>Given the labeled node set V L containing m n 1 nodes, where each node v i ∈ V L is associated with a label y i ∈ Y, our objective is to predict labels of V\V L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Networks</head><p>Graph convolutional networks achieve promising generalization in various tasks and our work is built upon the GCN module. At layer i, taking graph adjacency matrix A i and hidden representation matrix H i as input, each GCN module outputs a hidden representation matrix G i ∈ R ni×di+1 , which is described as:</p><formula xml:id="formula_0">G i = ReLU D − 1 2 iÃ iD − 1 2 i H i θ i ,<label>(1)</label></formula><p>where H 1 = X, ReLU(x) = max(0, x), adjacency matrix with self-loopÃ i = A i + I,D i is the degree matrix ofÃ i , and θ i ∈ R di×di+1 is a trainable weight matrix. For ease of parameter tuning, we set output dimension d i = d for all coarsening and refining layers throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Overall Architecture</head><p>For a H-GCN network of l layers, the i th graph coarsening layer first conducts a graph convolutional operation as formulated in Eq.</p><p>(1) and then aggregates structurally similar nodes into hyper-nodes, producing a coarser graph G i+1 and node embedding matrix H i+1 with fewer nodes. The corresponding adjacent matrix A i+1 and H i+1 will be fed into the (i + 1) th layer. Symmetrically, the graph refining layer also performs a graph convolution at first and then refines the coarsened graph to restore the finer graph structure. In order to boost optimization in deeper networks, we add shortcut connections [He et al., 2016] across each coarsened graph and its corresponding refined part.</p><p>Since the topological structure of the graph changes between layers, we further introduce a node weight embedding matrix S i , which transforms the number of nodes contained in each hyper-node into real-valued vectors. Both of the node weight embedding and H i will be fed into the i th layer. Besides, we add multiple channels by employing different GCNs to explore different feature subspaces.</p><p>The graph coarsening layers and refining layers altogether integrate different levels of node features and thus avoid oversmoothing during repeated neighborhood aggregation. After the refining process, we obtain a node embedding matrix H l−1 ∈ R n1×d , where each row represents a node representation vector. In order to classify each node, we apply an additional GCN module followed by a softmax classifier on H l−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Graph Coarsening Layer</head><p>Every graph coarsening layer consists of two steps, i.e. graph convolution and graph coarsening. A GCN module is firstly used to extract structural and attributive features by aggregating neighborhoods' information as described in Eq. (1). For the graph coarsening procedure, we design the following two hybrid grouping strategies to assign nodes with similar structures into a hyper-node in the coarser graph. We first conduct structural equivalence grouping, followed by structural similarity grouping. Structural equivalence grouping (SEG). If two nodes share the same set of neighbors, they are considered to be structurally equivalent. We then assign these two nodes to be a hyper-node. For example, as illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>, nodes B and D are structurally equivalent, so these two nodes are allocated as a hyper-node. We mark all these structurally equivalent nodes and leave other nodes unmarked to avoid repetitive grouping operation on nodes. Structural similarity grouping (SSG). Then, we calculate the structural similarity between the unmarked node pairs</p><formula xml:id="formula_1">(v j , v k ) as the normalized connection strength s(v j , v k ): s(v j , v k ) = A jk D(v j ) · D(v k ) ,<label>(2)</label></formula><p>where A jk is the edge weight between v j and v k , and D(·) is the node weight. We iteratively take out an unmarked node v j and calculate normalized connection strengths with all its unmarked neighbors. After that, we select its neighbor node v k which has the largest structural similarity to form a new hyper-node and mark the two nodes. Particularly, if one node is left unmarked and all of its immediate neighbors are marked, it will be marked as well and constitutes a hyper-node by itself. For example, in <ref type="figure" target="#fig_3">Figure 2</ref>, node pair (C, E) has the largest structural similarity, so they are grouped together to form a hypernode. After that, since only node A remains unmarked, it constitutes a hyper-node by itself.</p><p>Please note that if we take out unmarked nodes in a different order, the resulting hyper-graph will be different. The later we take a node out, the less its neighbors will be left unmarked. So, for a node with fewer neighbors, it has fewer probabilities to be grouped when it is taken out late. Therefore, we take out the unmarked nodes in ascending order according to the number of neighbors.</p><p>Using the above two grouping strategies, we are able to acquire all the hyper-nodes. For one hyper-node v i , its edge weight to v j is the summation over edge weights of v j 's neighbor nodes contained in v i . The updated node weights and edge weights will be used in Eq. (2) in the next coarsening layer.</p><p>In order to help restore the coarsened graph to original graph, we preserve the grouping relationship between nodes and their corresponding hyper-nodes in a matrix    R ni×ni+1 . Formally, at layer i, entry m jk in the grouping matrix M i is calculated as:</p><formula xml:id="formula_2">M i ∈ A B C D E 1 1 1 1 1 1 SEG SSG Output M1 = 0 B B B B @ 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 C C C C A A B C D E &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A q x j 7 + b o f c U s f B D L f T M z I M V 9 K 3 Y = " &gt; A A A C r X i c d Z H f b 9 M w E M e d M G C E X x 1 7 3 M t p F W g g V J I x a X u Z t B 8</formula><formula xml:id="formula_3">q i K C G Q 6 A S E 7 s T A I 0 x F a p i W r N F 7 b y 8 G T U E E b y C s J 2 U B g 2 0 Q s f t / n 8 6 B B T V r I v o E m i R z u 3 r 1 T w S m v X Y r Z Q 6 O F n C 6 R I + L O F j A 7 8 j T n v 9 c B C 2 B n c h 6 q B P O h t O e z d 0 l v M y Q 2 W 5 Z M a M o 7 C w E x f N C i 6 x D m h p s G D 8 k q U 4 d q h Y h m Z S t d W u 4 a V T Z p D k 2 k 1 l o V X / v F G x z J h F F r u T G b N z s + p r x H / 5 x q V N D i a V U E V p U f H b R E k p w e b Q t A 5 m Q i O 3 c u G A c S 3 c W 4 H P m W b c u g Y H r g j R 6 p f v w t n u I H o / 2 P 2 y 1 z 9 6 2 5 V j n W y R b b J D I r J P j s g n M i Q j w r 0 3 3 t D 7 5 n 3 3 3 / k j n / o / b o / 6 X n d n k / x l f v o L K J H B n w = = &lt; / l a t e x i t &gt; A BD CE M1 = 0 B B B B @ 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 C C C C A A B C D E &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A q x j 7 + b o f c U s f B D L f T M z I M V 9 K 3 Y = " &gt; A A A C r X i c d Z H f b 9 M w E M e d M G C E X x 1 7 3 M t p F W g g V J I x a X u Z t B 8</formula><formula xml:id="formula_4">q i K C G Q 6 A S E 7 s T A I 0 x F a p i W r N F 7 b y 8 G T U E E b y C s J 2 U B g 2 0 Q s f t / n 8 6 B B T V r I v o E m i R z u 3 r 1 T w S m v X Y r Z Q 6 O F n C 6 R I + L O F j A 7 8 j T n v 9 c B C 2 B n c h 6 q B P O h t O e z d 0 l v M y Q 2 W 5 Z M a M o 7 C w E x f N C i 6 x D m h p s G D 8 k q U 4 d q h Y h m Z S t d W u 4 a V T Z p D k 2 k 1 l o V X / v F G x z J h F F r u T G b N z s + p r x H / 5 x q V N D i a V U E V p U f H b R E k p w e b Q t A 5 m Q i O 3 c u G A c S 3 c W 4 H P m W b c u g Y H r g j R 6 p f v w t n u I H o / 2 P 2 y 1 z 9 6 2 5 V j n W y R b b J D I r J P j s g n M i Q j w r 0 3 3 t D 7 5 n 3 3 3 / k j n / o / b o / 6 X n d n k / x l f v o L K J H B n w = = &lt; / l a t e x i t &gt; M1 = 0 B B B B @ 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 C C C C A A B C D E &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A q x j 7 + b o f c U s f B D L f T M z I M V 9 K 3 Y = " &gt; A A A C r X i c d Z H f b 9 M w E M e d M G C E X x 1 7 3 M t p F W g g V J I x a X u Z t B 8</formula><formula xml:id="formula_5">q i K C G Q 6 A S E 7 s T A I 0 x F a p i W r N F 7 b y 8 G T U E E b y C s J 2 U B g 2 0 Q s f t / n 8 6 B B T V r I v o E m i R z u 3 r 1 T w S m v X Y r Z Q 6 O F n C 6 R I + L O F j A 7 8 j T n v 9 c B C 2 B n c h 6 q B P O h t O e z d 0 l v M y Q 2 W 5 Z M a M o 7 C w E x f N C i 6 x D m h p s G D 8 k q U 4 d q h Y h m Z S t d W u 4 a V T Z p D k 2 k 1 l o V X / v F G x z J h F F r u T G b N z s + p r x H / 5 x q V N D i a V U E V p U f H b R E k p w e b Q t A 5 m Q i O 3 c u G A c S 3 c W 4 H P m W b c u g Y H r g j R 6 p f v</formula><formula xml:id="formula_6">m jk = 1, if v j in G i is grouped into v k in G i+1 ; 0, otherwise.<label>(3)</label></formula><p>An example of the coarsening operation on a toy graph is given in <ref type="figure" target="#fig_3">Figure 2</ref>. Note that m 11 = 1 in this illustration, since node A constitutes its hyper-node by itself. Next, the hidden node embedding matrix is determined as:</p><formula xml:id="formula_7">H i+1 = M i · G i .<label>(4)</label></formula><p>In the end, we generate a coarser graph G i+1 , whose adjacency matrix can be calculated as:</p><formula xml:id="formula_8">A i+1 = M i · A i · M i .<label>(5)</label></formula><p>The coarser graph G i+1 along with the resulting representation matrix H i+1 will be fed into the next layer as input. The resulting node embedding to generate in each coarsening layer will then be of lower resolution. The graph coarsening procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Graph Refining Layer</head><p>To restore the original topological structure of the graph and further facilitate node classification, we stack the same numbers of graph refining layers as coarsening layers. Like the coarsening procedure, each refining layer contains two steps, namely generating node embedding vectors and restoring node representations.</p><p>To learn a hierarchical representation of nodes, a GCN is employed at first. Since we have saved the grouping relationship in the grouping matrix during the coarsening process, we utilize M l−i to restore the refined node representation matrix of layer i. We further employ residual connections between the two corresponding coarsening and refining layers. In summary, node representations are computed by:</p><formula xml:id="formula_9">H i = M l−i · G i + G l−i .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Node Weight Embedding and Multiple Channels</head><p>As depicted in <ref type="figure" target="#fig_3">Figure 2</ref>, since different hyper-nodes may have different node weights, we assume such consequent node</p><formula xml:id="formula_10">lÃ lD − 1 2 l H l−1 θ l ,<label>(8)</label></formula><p>where θ l ∈ R d×|Y| is a trainable weight matrix and H l ∈ R n1×|Y| denotes the probabilities of nodes belonging to each class y ∈ Y.</p><p>The loss function is defined as the cross-entropy of predictions over the labeled nodes:</p><formula xml:id="formula_11">L = − m i=1 |Y| y=1 I(h i = y i ) log P (h i , y i ),<label>(9)</label></formula><p>where I(·) is the indicator function, y i is the true label for v i , h i is the prediction for labeled node v i , and P (h i , y i ) is the predicted probability that v i is of class y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Complexity Analysis and Model Comparison</head><p>In this section, we analyze the model complexity and compare it with mainstream graph convolutional models, such as GCN and GAT. For GCN, preprocessing matricesD For H-GCN, the preprocessing takes O(n log n) to sort the unmarked nodes and O(mn) for SSG, where m is the average number of neighborhoods. For training, the complexity is also O(|E|CF ). Therefore, H-GCN is as asymptotically efficient as GCN and is more efficient than GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings Datasets</head><p>For a comprehensive comparison with state-of-the-art methods, we use four widely-used datasets including three citation networks and one knowledge graph. We conduct semisupervised node classification task in the transductive setting. The statistics of these datasets are summarized in <ref type="table">Table 1</ref>. We set the node weight and edge weight of the graph to one for all four datasets. The dataset configuration follows the same setting in <ref type="bibr" target="#b10">[Yang et al., 2016;</ref><ref type="bibr" target="#b8">Kipf and Welling, 2017]</ref> for a fair comparison. For citation networks, documents and citations are treated as nodes and edges, respectively. For the knowledge graph, each triplet (e 1 , r, e 2 ) will be assigned with separate relation nodes r 1 and r 2 as (e 1 , r 1 ) and (e 2 , r 2 ), where e 1 and e 2 are entities and r is the relation between them. During training, only 20 labels per class are used for each citation network and only one label per class is used for NELL during training. Besides, 500 nodes in each dataset are selected randomly as the validation set. We do not use the labels of the validation set for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>To evaluate the performance of H-GCN, we compare our method with the following representative methods: <ref type="bibr">et al., 2014]</ref> generates the node embedding via random walks in an unsupervised manner, then nodes are classified by feeding the embedding vectors into an SVM classifier.   <ref type="bibr" target="#b10">Ma, 2018]</ref> utilizes the graph adjacency matrix and the positive mutual information matrix to encode both local consistency and global consistency.</p><formula xml:id="formula_12">• DeepWalk [Perozzi</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Settings</head><p>We train our model using Adam optimizer with a learning rate of 0.03 for 250 epochs. The dropout is applied to all feature vectors with rates of 0.85. Besides, the 2 regularization factor is set to 0.0007. Considering different scales of datasets, we set the total number of layers l to 9 for citation networks and 11 for the knowledge graph, and apply four-channel GCNs in both coarsening and refining layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node Classification Results</head><p>To demonstrate the overall performance of semi-supervised node classification, we compare the proposed method with other state-of-the-art methods. The performance in terms of accuracy is shown in <ref type="table" target="#tab_1">Table 2</ref>. The best performance of each column is highlighted in boldface. The performance of our proposed method is reported based on the average of 20 measurements. Note that running GAT on the NELL dataset requires more than 64G memory; hence its performance on NELL is not reported.</p><p>The results show that the proposed method consistently outperforms other state-of-the-art methods, which verify the effectiveness of the proposed coarsening and refining mechanisms. Notably, compared with citation networks, H-GCN surpasses other baselines by larger margins on the NELL dataset. To be specific, the accuracy of H-GCN exceeds GCN  <ref type="table">Table 3</ref>: Results of node classification in terms of accuracy on Pubmed with labeled vertices varying from 20 per class to 5. and DGCN by 7.1% and 5.9% on NELL dataset respectively. We analyze the results as follows.</p><p>Regarding traditional random-walk-based algorithms such as DeepWalk and Planetoid, their performance is relatively poor. DeepWalk cannot model the attribute information, which heavily restricts its performance. Though Planetoid combines supervised information with an unsupervised loss, there is information loss of graph structure during random sampling. To avoid that problem, GCN and GAT employ the neighborhood aggregation scheme to boost performance. GAT outperforms GCN as it can model different relations to different neighbors rather than with a pre-defined order. DGCN further jointly models both local and global consistency, yet its global consistency is still obtained through random walks. As a result, the information in the graph structure might lose in DGCN as well. On the contrary, the proposed H-GCN manages to capture global information through different levels of convolutional layers and achieves the best results among all four datasets.</p><p>Besides, on the NELL dataset, there are fewer training samples per class than in citation networks. Under such circumstance, training nodes are further away from testing nodes on average. The baseline models with the restricted receptive field are unable to propagate the features and the supervised information of the training nodes to other nodes sufficiently. As a result, the proposed H-GCN with increased receptive fields and deeper layers obtains more promising improvements than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Scale of Training Data</head><p>We suppose that a larger receptive field in the convolutional model promotes the propagation of features and labels on graphs. To verify the proposed H-GCN can get a larger receptive field, we reduce the number of training samples to check if H-GCN still performs well when limited labeled data is given. As in nature, there are plenty of unlabeled data; it is also of great significance to train the model with limited labeled data. In this section, we conduct experiments with different numbers of labeled instances on the Pubmed dataset. We vary the number of labeled nodes from 20 to 5 per class, where the labeled data is randomly chosen from the original training set. All parameters are the same as previously described. The corresponding performance in terms of accuracy is reported in <ref type="table">Table 3</ref>.</p><p>From the table, it can be observed that our method outperform other baselines in all cases. With the number of labeled data decreasing, our method obtains a more considerable margin over these baseline algorithms. Especially when only five labeled nodes per class (≈ 0.08% labeling rate) are given,  <ref type="table">Table 4</ref>: Results of the ablation study the accuracy of H-GCN exceeds GCN, DGCN, and GAT by 7.5%, 6.4%, and 6.2% respectively. When the number of training data decreases, it is more likely for an unlabeled node to be further away from these labeled nodes. Only when the receptive field is large enough can information from those training nodes be captured. As the receptive field of GCN and GAT does not exceed 2-hop neighborhoods, supervised information contained in the training nodes cannot propagate sufficiently to other nodes. Therefore, these baselines downgrade considerably. However, owing to its larger receptive field, the performance of H-GCN declines slightly when labeled data decreases dramatically. Overall, it is verified that the proposed H-GCN with increased receptive fields is wellsuited when training data is extremely scarce and thereby is of significant practical values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To verify the effectiveness of the proposed coarsening and refining layers, we conduct ablation study on coarsening and refining layers and node weight embeddings respectively in this section. The results are shown in <ref type="table">Table 4</ref>.</p><p>Coarsening and refining layers. We remove all coarsening and refining operations of H-GCN and compare its performance with the original H-GCN. Different from simply adding too many GCN layers, we preserve the short-cut connection between the symmetric layers in the ablation study. From the results, it is evident that the proposed H-GCN has better performance compared to H-GCN without coarsening mechanisms on all datasets. It can be verified that the coarsening and refining mechanisms contribute to the performance improvements since they can obtain global information with larger receptive fields.</p><p>Node weight embeddings. To study the impact of node weight embeddings, we compare H-GCN with no node weight embeddings used. It can be seen from results that the model with node weight embeddings performs better, which verifies the necessity to add this embedding vector in the node embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sensitivity Analysis</head><p>Last, we analyze hyper-parameter sensitivity. Specifically, we investigate how different numbers of coarsening layers and different numbers of channels will affect the results respectively. The performance is reported in terms of accuracy on all four datasets. While one parameter studied in the sensitivity analysis is changed, other hyper-parameters remain the same.  Effects of coarsening layers. Since the coarsening layers in our model control the granularity of the receptive field enlargement, we experiment with one to eight coarsening and symmetric refining layers, where the results are shown in <ref type="figure" target="#fig_5">Figure 3(a)</ref>. It is seen that the performance of H-GCN achieves the best when there are four coarsening layers on three citation networks and five on the knowledge graph. It is suspected that, since less labeled nodes are supplied on NELL than others, deeper layers and larger receptive fields are needed. However, when adding too many coarsening layers, the performance drops due to overfitting.</p><p>Effects of channel numbers. Next, we investigate the impact of different amounts of channels on the performance. Multiple channels benefit the graph convolutional network model, since they explore different feature subspaces, as shown in <ref type="figure" target="#fig_5">Figure 3(b)</ref>. From the figure, it can be found that the performance improves with the number of channels increasing until four channels, which demonstrates that more channels help capture accurate node features. Nevertheless, too many channels will inevitably introduce redundant parameters to the model, leading to overfitting as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel hierarchical graph convolutional networks for the semi-supervised node classification task. The H-GCN model consists of coarsening layers and symmetric refining layers. By grouping structurally similar nodes to hyper-nodes, our model can get a larger receptive field and enable sufficient information propagation. Compared with other previous work, our proposed H-GCN is deeper and can fully utilize both local and global information. Comprehensive experiments have confirmed that the proposed method consistently outperformed other state-ofthe-art methods. In particular, our method has achieved substantial gains over them in the case that labeled data is extremely scarce.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>g 8 Y J U J L o N 6 l I 5 7 i W 1 5 j i R 7 U y q o v x 3 + w v 2 x n + D k 6 U C O j j L 1 s f f s + / s u 7 i Q w t g w / O n 5 9 9 b u P 3 i 4 / i h 4 / O T p s + e 9 j R d n J i 8 1 x x H P Z a 4 v Y m Z Q C o U j K 6 z E i 0 I j y 2 K J 5 / H l a e M / v 0 J t R K 6 + 2 k W B k 4 y l S i S C M + u k a e / 6 8 x Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>g 8 Y J U J L o N 6 l I 5 7 i W 1 5 j i R 7 U y q o v x 3 + w v 2 x n + D k 6 U C O j j L 1 s f f s + / s u 7 i Q w t g w / O n 5 9 9 b u P 3 i 4 / i h 4 / O T p s + e 9 j R d n J i 8 1 x x H P Z a 4 v Y m Z Q C o U j K 6 z E i 0 I j y 2 K J 5 / H l a e M / v 0 J t R K 6 + 2 k W B k 4 y l S i S C M + u k a e / 6 8 x Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>g 8 Y J U J L o N 6 l I 5 7 i W 1 5 j i R 7 U y q o v x 3 + w v 2 x n + D k 6 U C O j j L 1 s f f s + / s u 7 i Q w t g w / O n 5 9 9 b u P 3 i 4 / i h 4 / O T p s + e 9 j R d n J i 8 1 x x H P Z a 4 v Y m Z Q C o U j K 6 z E i 0 I j y 2 K J 5 / H l a e M / v 0 J t R K 6 + 2 k W B k 4 y l S i S C M + u k a e / 6 8 x Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>w t n u I H o / 2 P 2 y 1 z 9 6 2 5 V j n W y R b b J D I r J P j s g n M i Q j w r 0 3 3 t D 7 5 n 3 3 3 / k j n / o / b o / 6 X n d n k / x l f v o L K J H B n w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " A q x j 7 + b o f c U s f B D L f T M z I M V 9 K 3 Y = " &gt; A A A C r X i c d Z H f b 9 M w E M e d M G C E X x 1 7 3 M t p F W g g V J I x a X u Z t B 8 g 8 Y J U J L o N 6 l I 5 7 i W 1 5 j i R 7 U y q o v x 3 + w v 2 x n + D k 6 U C O j j L 1 s f f s + / s u 7 i Q w t g w / O n 5 9 9 b u P 3 i 4 / i h 4 / O T p s + e 9 j R d n J i 8 1 x x H P Z a 4 v Y m Z Q C o U j K 6 z E i 0 I j y 2 K J 5 / H l a e M / v 0 J t R K 6 + 2 k W B k 4 y l S i S C M + u k a e / 6 8 x Q q i K C G Q 6 A S E 7 s T A I 0 x F a p i W r N F 7 b y 8 G T U E E b y C s J 2 U B g 2 0 Q s f t / n 8 6 B B T V r I v o E m i R z u 3 r 1 T w S m v X Y r Z Q 6 O F n C 6 R I + L O F j A 7 8 j T n v 9 c B C 2 B n c h 6 q B P O h t O e z d 0 l v M y Q 2 W 5 Z M a M o 7 C w E x f N C i 6 x D m h p s G D 8 k q U 4 d q h Y h m Z S t d W u 4 a V T Z p D k 2 k 1 l o V X / v F G x z J h F F r u T G b N z s + p r x H / 5 x q V N D i a V U E V p U f H b R E k p w e b Q t A 5 m Q i O 3 c u G A c S 3 c W 4 H P m W b c u g Y H r g j R 6 p f v w t n u I H o / 2 P 2 y 1 z 9 6 2 5 V j n W y R b b J D I r J P j s g n M i Q j w r 0 3 3 t D 7 5 n 3 3 3 / k j n / o / b o / 6 X n d n k / x l f v o L K J H B n w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " I c E h F 2 I T a e / r m / f + X Y G G d B G m e s U = " &gt; A A A C B H i c b V B N S 8 N A E N 3 4 W e t X 1 G M v i 0 X x V J K 2 o M e C F 4 8 V 7 A c 0 p W y 2 m 3 b p Z h N 3 J 0 I J O X j x r 3 j x o I h X f 4 Q 3 / 4 3 b N g d t f T D w e G + G m X l + L L g G x / m 2 1 t Y 3 N r e 2 C z v F 3 b 3 9 g 0 P 7 6 L i t o 0 R R 1 q K R i F T X J 5 o J L l k L O A j W j R U j o S 9 Y x 5 9 c z / z O A 1 O a R / I O p j H r h 2 Q k e c A p A S M N 7 J I X K E J T N 0 s 9 f a 8 g r W E P e M g 0 r m f Z w C 4 7 F W c O v E r c n J R R j u b A / v K G E U 1 C J o E K o n X P d W L o p 0 Q B p 4 J l R S / R L C Z 0 Q k a s Z 6 g k Z l E / n T + R 4 T O j D H E Q K V M S 8 F z 9 P Z G S U O t p 6 J v O k M B Y L 3 s z 8 T + v l 0 B w 1 U + 5 j B N g k i 4 W B Y n A E O F Z I n j I F a M g p o Y Q q r i 5 F d M x M a m A y a 1 o Q n C X X 1 4 l 7 W r F r V W q t / V y 4 z y P o 4 B K 6 B R d I B d d o g a 6 Q U 3 U Q h Q 9 o m f 0 i t 6 s J + v F e r c + F q 1 r V j 5 z g v 7 A + v w B o l S X / Q = = &lt; / l a t e x i t &gt; 1 p 4 ⇥ 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p P j y n o J 9 8 8 c 5 5 0 6 g X A v W Z n E 1 9 s A = " &gt; A A A C B H i c b V B N S 8 N A E N 3 U r 1 q / o h 5 7 W S y K p 5 L U g h 4 L X j x W s B / Q l L L Z b t q l m 0 3 c n Q g l 5 O D F v + L F g y J e / R H e / D d u 2 x y 0 9 c H A 4 7 0 Z Z u b 5 s e A a H O f b K q y t b 2 x u F b d L O 7 t 7 + w f 2 4 V F b R 4 m i r E U j E a m u T z Q T X L I W c B C s G y t G Q l + w j j + 5 n v m d B 6 Y 0 j + Q d T G P W D 8 l I 8 o B T A k Y a 2 G U v U I S m b p Z 6 + l 5 B W s c e 8 J B p 7 G b Z w K 4 4 V W c O v E r c n F R Q j u b A / v K G E U 1 C J o E K o n X P d W L o p 0 Q B p 4 J l J S / R L C Z 0 Q k a s Z 6 g k Z l E / n T + R 4 V O j D H E Q K V M S 8 F z 9 P Z G S U O t p 6 J v O k M B Y L 3 s z 8 T + v l 0 B w 1 U + 5 j B N g k i 4 W B Y n A E O F Z I n j I F a M g p o Y Q q r i 5 F d M x M a m A y a 1 k Q n C X X 1 4 l 7 V r V v a j W b u u V x l k e R x G V 0 Q k 6 R y 6 6 R A 1 0 g 5 q o h S h 6 R M / o F b 1 Z T 9 a L 9 W 5 9 L F o L V j 5 z j P 7 A + v w B n 1 G X + w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " z t H 4 0 0 M C h F X 5 J 7 E j S z t 2 A 4 S 2 2 4 0 = " &gt; A A A C l n i c b Z F R a x N B E M f 3 z q r 1 W j W 1 L 4 I v g 6 G l g o S 7 K l Q f L N U i 9 j G C a Q v Z E P Y 2 c 8 n S v b 1 j d 0 4 I x 3 2 k f h n f / D Z u k q u 2 q b M M / P n N z O 7 s T F p q 5 S i O f w f h g 4 2 H j x 5 v P o m 2 t p 8 + e 9 7 Z e X H u i sp K H M h C F / Y y F Q 6 1 M j g g R R o v S 4 s i T z V e p F e n i / j F T 7 R O F e Y H z U s c 5 W J q V K a k I I / G n e v P M I Y a E m j g E 3 C N G R 1 E w F O c K l M L a 8 W 8 8 V H 5 9 z Q Q x b D v 0 / 9 5 D J x H K 3 E D v H M O U X I L r P Q N v Z O 6 p P E 6 j T i a S d u C 7 8 i q 6 Y z e j D v d u B c v D e 6 L p B V d 1 l p / 3 P n F J 4 W s c j Q k t X B u m M Q l j f y 1 p K T G J u K V w 1 L I K z H F o Z d G 5 O h G 9 X Ks D e x 5 M o G s s N 4 N w Z L e r q h F 7 t w 8 T 3 1 m L m j m 1 m M L + L / Y s K L s w 6 h W p q w I j V w 9 l F U a q I D F j m C i L E r S c y + E t M r 3 C n I m r J D k N x n 5 I S T r X 7 4 v z g 9 7 y b v e 4 f f 3 3 Z O 3 7 T g 2 2 S v 2 m h 2 w h B 2 x E 3 b G + m z A Z L A b f A y + B K f h y / A 4 / B p + W 6 W G Q V u z y + 5 Y 2 P 8 D b 7 m w 6 Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " A q x j 7 + b o f c U s f B D L f T M z I M V 9 K 3 Y = " &gt; A A A C r X i c d Z H f b 9 M w E M e d M G C E X x 1 7 3 M t p F W g g V J I x a X u Z t B 8 g 8 Y J U J L o N 6 l I 5 7 i W 1 5 j i R 7 U y q o v x 3 + w v 2 x n + D k 6 U C O j j L 1 s f f s + / s u 7 i Q w t g w / O n 5 9 9 b u P 3 i 4 / i h 4 / O T p s + e 9 j R d n J i 8 1 x x H P Z a 4 v Y m Z Q C o U j K 6 z E i 0 I j y 2 K J 5 / H l a e M / v 0 J t R K 6 + 2 k W B k 4 y l S i S C M + u k a e / 6 8 x Qq i K C G Q 6 A S E 7 s T A I 0 x F a p i W r N F 7 b y 8 G T U E E b y C s J 2 U B g 2 0 Q s f t / n 8 6 B B T V r I v o E m i R z u 3 r 1 T w S m v X Y r Z Q 6 O F n C 6 R I + L O F j A 7 8 j T n v 9 c B C 2 B n c h 6 q B P O h t O e z d 0 l v M y Q 2 W 5 Z M a M o 7 C w E x f N C i 6 x D m h p s G D 8 k q U 4 d q h Y h m Z S t d W u 4 a V T Z p D k 2 k 1 l o V X / v F G x z J h F F r u T G b N z s + p r x H / 5 x q V N D i a V U E V p U f H b R E k p w e b Q t A 5 m Q i O 3 c u G A c S 3 c W 4 H P m W b c u g Y H r g j R 6 p f vw t n u I H o / 2 P 2 y 1 z 9 6 2 5 V j n W y R b b J D I r J P j s g n M i Q j w r 0 3 3 t D 7 5 n 3 3 3 / k j n / o / b o / 6 X n d n k / x l f v o L K J H B n w = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " z t H 4 0 0 M C h F X 5 J 7 E j S z t 2 A 4 S 2 2 4 0 = " &gt; A A A C l n i c b Z F R a x N B E M f 3 z q r 1 W j W 1 L 4 I v g 6 G l g o S 7 K l Q f L N U i 9 j G C a Q v Z E P Y 2 c 8 n S v b 1 j d 0 4 I x 3 2 k f h n f / D Z u k q u 2 q b M M / P n N z O 7 s T F p q 5 S i O f w f h g 4 2 H j x 5 v P o m 2 t p 8 + e 9 7 Z e X H u i s p K H M h C F / Y y F Q 6 1 M j g g R R o v S 4 s i T z V e p F e n i / j F T 7 R O F e Y H z U s c 5 W J q V K a k I I / G n e v P M I Y a E m j g E 3 C N G R 1 E w F O c K l M L a 8 W 8 8 V H 5 9 z Q Q x b D v 0 / 9 5 D J x H K 3 E D v H M O U X I L r P Q N v Z O 6 p P E 6 j T i a S d u C 7 8 i q 6 Y z e j D v d u B c v D e 6 L p B V d 1 l p / 3 P n F J 4 W s c j Q k t X B u m M Q l j f y 1 p K T G J u K V w 1 L I K z H F o Z d G 5 O h G 9 X K s D e x 5 M o G s s N 4 N w Z L e r q h F 7 t w 8 T 3 1 m L m j m 1 m M L + L / Y s K L s w 6 h W p q w I j V w 9 l F U a q I D F j m C i L E r S c y + E t M r 3 C n I m r J D k N x n 5 I S T r X 7 4 v z g 9 7 y b v e 4 f f 3 3 Z O 3 7 T g 2 2 S v 2 m h 2 w h B 2 x E 3 b G + m z A Z L A b f A y + B K f h y / A 4 / B p + W 6 W G Q V u z y + 5 Y 2 P 8 D b 7 m w 6 Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " z t H 4 0 0 M C h F X 5 J 7 E j S z t 2 A 4 S 2 2 4 0 = " &gt; A A A C l n i c b Z F R a x N B E M f 3 z q r 1 W j W 1 L 4 I v g 6 G l g o S 7 K l Q f L N U i 9 j G C a Q v Z E P Y 2 c 8 n S v b 1 j d 0 4 I x 3 2 k f h n f / D Z u k q u 2 q b M M / P n N z O 7 s T F p q 5 S i O f w f h g 4 2 H j x 5 v P o m 2 t p 8 + e 9 7 Z e X H u i s p K H M h C F / Y y F Q 6 1 M j g g R R o v S 4 s i T z V e p F e n i / j F T 7 R O F e Y H z U s c 5 W J q V K a k I I / G n e v P M I Y a E m j g E 3 C N G R 1 E w F O c K l M L a 8 W 8 8 V H 5 9 z Q Q x b D v 0 / 9 5 D J x H K 3 E D v H M O U X I L r P Q N v Z O 6 p P E 6 j T i a S d u C 7 8 i q 6 Y z e j D v d u B c v D e 6 L p B V d 1 l p / 3 P n F J 4 W s c j Q k t X B u m M Q l j f y 1 p K T G J u K V w 1 L I K z H F o Z d G 5 O h G 9 X K s D e x 5 M o G s s N 4 N w Z L e r q h F 7 t w 8 T 3 1 m L m j m 1 m M L + L / Y s K L s w 6 h W p q w I j V w 9 l F U a q I D F j m C i L E r S c y + E t M r 3 C n I m r J D k N x n 5 I S T r X 7 4 v z g 9 7 y b v e 4 f f 3 3 Z O 3 7 T g 2 2 S v 2 m h 2 w h B 2 x E 3 b G + m z A Z L A b f A y + B K f h y / A 4 / B p + W 6 W G Q V u z y + 5 Y 2 P 8 D b 7 m w 6 Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " Q C d c 7 D W / B v 2 T 1 K z f R e w D W A u W c 0 8 = " &gt; A A A C n H i c b V F d a 9 s w F J W 9 r 9 b 7 y t q n M R i X h Z U O R r C z w f p S 2 r I 9 d I x C B 0 t a i L M g K 9 e J q C w b 6 X o Q j H 9 V / 0 n f 9 m + q O O 6 W t b t C 3 M M 5 5 1 5 J V 0 m h p K U w / O 3 5 9 + 4 / e P h o Y z N 4 / O T p s + e d F 1 t D m 5 d G 4 E D k K j f n C b e o p M Y B S V J 4 X h j k W a L w L L n 4 v N T P f q G x M t c / a F H g O O M z L V M p O D l q 0 r k 8 g g l U 0 I c a 9 u G k w Z H D P 1 2 O K S 8 c P P p D / p X 3 I V a Y 0 i 4 E c Y I z q S t u D F / U T h X N q i G o I H R p p + 2 9 0 x b G c b D G r D t W S r R i b g x 9 x 6 O e 3 r S P j Z z N 6 d 2 k 0 w 1 7 Y R N w F 0 Q t 6 L I 2 T i e d q 3 i a i z J D T U J x a 0 d R W N D Y d S U p F N Z B X F o s u L j g M x w 5 q H m G d l w 1 w 6 3 h r W O m k O b G b U 3 Q s O s V F c + s X W S J c 2 a c 5 v a 2 t i T / p 4 1 K S v f G l d R F S a j F 6 q C 0 V E A 5 L H 8 K p t K g I L V w g A s j 3 V 1 B z L n h g t x / B m 4 I 0 e 0 n 3 w X D f i / 6 0 O t / / 9 g 9 f N + O Y 4 O 9 Y m / Y L o v Y J 3 b I j t k p G z D h v f Q O v G P v q / / a / + J / 8 0 9 W V t 9 r a 7 b Z P + E P r w G / V 7 v B &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " Q C d c 7 D W / B v 2 T 1 K z f R e w D W A u W c 0 8 = " &gt; A A A C n H i c b V F d a 9 s w F J W 9 r 9 b 7 y t q n M R i X h Z U O R r C z w f p S 2 r I 9 d I x C B 0 t a i L M g K 9 e J q C w b 6 X o Q j H 9 V / 0 n f 9 m + q O O 6 W t b t C 3 M M 5 5 1 5 J V 0 m h p K U w / O 3 5 9 + 4 / e P h o Y z N 4 / O T p s + e d F 1 t D m 5 d G 4 E D k K j f n C b e o p M Y B S V J 4 X h j k W a L w L L n 4 v N T P f q G x M t c / a F H g O O M z L V M p O D l q 0 r k 8 g g l U 0 I c a 9 u G k w Z H D P 1 2 O K S 8 c P P p D / p X 3 I V a Y 0 i 4 E c Y I z q S t u D F / U T h X N q i G o I H R p p + 2 9 0 x b G c b D G r D t W S r R i b g x 9 x 6 O e 3 r S P j Z z N 6 d 2 k 0 w 1 7 Y R N w F 0 Q t 6 L I 2 T i e d q 3 i a i z J D T U J x a 0 d R W N D Y d S U p F N Z B X F o s u L j g M x w 5 q H m G d l w 1 w 6 3 h r W O m k O b G b U 3 Q s O s V F c + s X W S J c 2 a c 5 v a 2 t i T / p 4 1 K S v f G l d R F S a j F 6 q C 0 V E A 5 L H 8 K p t K g I L V w g A s j 3 V 1 B z L n h g t x / B m 4 I 0 e 0 n 3 w X D f i / 6 0 O t / / 9 g 9 f N + O Y 4 O 9 Y m / Y L o v Y J 3 b I j t k p G z D h v f Q O v G P v q / / a / + J / 8 0 9 W V t 9 r a 7 b Z P + E P r w G / V 7 v B &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " f 5 k n x d / I e e M A Y F 4 s w W 1 m N L 9 1 c Z o = " &gt; A A A C x X i c b V F b a x N B F J 5 d b 3 W 9 R X 3 0 5 W C w V J C w G 4 X 6 U m h a R V + E C q Y t Z G K Y n Z x N h s 7 O b m f O F s K y + B 9 9 E / w x T p K t p q 1 n G M 7 H 9 5 3 L z D l p q Z W j O P 4 V h L d u 3 7 l 7 b + t + 9 O D h o 8 d P O k + f H b u i s h K H s t C F P U 2 F Q 6 0 M D k m R x t P S o s h T j S f p 2 e F S P 7 l A 6 1 R h v t G i x H E u Z k Z l S g r y 1 K T z e w A T q K E P D e z B l x V O P P 7 u P a e i 9 H D w l / w n 7 w H X m N E O R D z F m T K 1 s F Y s G q / K 1 W k g q i H 2 b r u t v d 0 m c h 5 t M J s R a y V Z M 5 c B f c + j m V 6 W 5 1 b N 5 v T 6 a t N a N t G A 8 4 M P n B 9 + 9 F U 2 E i a d b t y L V w Y 3 Q d K C L m v t a N L 5 y a e F r H I 0 J L V w b p T E J Y 1 9 N V J S Y x P x y m E p 5 J m Y 4 c h D I 3 J 0 4 3 q 1 h Q Z e e W Y K W W H 9 N Q Q r d j O j F r l z i z z 1 k b m g u b u u L c n / a a O K s v f j W p m y I j R y 3 S i r N F A B y 5 X C V F m U p B c e C G m V f y v I u b B C k l 9 8 5 I e Q X P / y T X D c 7 y V v e / 2 v 7 7 r 7 b 9 p x b L E X 7 C X b Y Q n b Z f v s M z t i Q y a D g 2 A e n A c 2 / B T m I Y U X 6 9 A w a H O e s y s W / v g D z P P M b Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " f 5 k n x d / I e e M A Y F 4 s w W 1 m N L 9 1 c Z o = " &gt; A A A C x X i c b V F b a x N B F J 5 d b 3 W 9 R X 3 0 5 W C w V J C w G 4 X 6 U m h a R V + E C q Y t Z G K Y n Z x N h s 7 O b m f O F s K y + B 9 9 E / w x T p K t p q 1 n G M 7 H 9 5 3 L z D l p q Z W j O P 4 V h L d u 3 7 l 7 b + t + 9 O D h o 8 d P O k + f H b u i s h K H s t C F P U 2 F Q 6 0 M D k m R x t P S o s h T j S f p 2 e F S P 7 l A 6 1 R h v t G i x H E u Z k Z l S g r y 1 K T z e w A T q K E P D e z B l x V O P P 7 u P a e i 9 H D w l / w n 7 w H X m N E O R D z F m T K 1 s F Y s G q / K 1 W k g q i H 2 b r u t v d 0 m c h 5 t M J s R a y V Z M 5 c B f c + j m V 6 W 5 1 b N 5 v T 6 a t N a N t G A 8 4 M P n B 9 + 9 F U 2 E i a d b t y L V w Y 3 Q d K C L m v t a N L 5 y a e F r H I 0 J L V w b p T E J Y 1 9 N V J S Y x P x y m E p 5 J m Y 4 c h D I 3 J 0 4 3 q 1 h Q Z e e W Y K W W H 9 N Q Q r d j O j F r l z i z z 1 k b m g u b u u L c n / a a O K s v f j W p m y I j R y 3 S i r N F A B y 5 X C V F m U p B c e C G m V f y v I u b B C k l 9 8 5 I e Q X P / y T X D c 7 y V v e / 2 v 7 7 r 7 b 9 p x b L E X 7 C X b Y Q n b Z f v s M z t i Q y a D g 2 A e n A c 2 / B T m I Y U X 6 9 A w a H O e s y s W / v g D z P P M b Q = = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " Q C d c 7 D W / B v 2 T 1 K z f R e w D W A u W c 0 8 = " &gt; A A A C n H i c b V F d a 9 s w F J W 9 r 9 b 7 y t q n M R i X h Z U O R r C z w f p S 2 r I 9 d I x C B 0 t a i L M g K 9 e J q C w b 6 X o Q j H 9 V / 0 n f 9 m + q O O 6 W t b t C 3 M M 5 5 1 5 J V 0 m h p K U w / O 3 5 9 + 4 / e P h o Y z N 4 / O T p s + e d F 1 t D m 5 d G 4 E D k K j f n C b e o p M Y B S V J 4 X h j k W a L w L L n 4 v N T P f q G x M t c / a F H g O O M z L V M p O D l q 0 r k 8 g g l U 0 I c a 9 u G k w Z H D P 1 2 O K S 8 c P P p D / p X 3 I V a Y 0 i 4 E c Y I z q S t u D F / U T h X N q i G o I H R p p + 2 9 0 x b G c b D G r D t W S r R i b g x 9 x 6 O e 3 r S P j Z z N 6 d 2 k 0 w 1 7 Y R N w F 0 Q t 6 L I 2 T i e d q 3 i a i z J D T U J x a 0 d R W N D Y d S U p F N Z B X F o s u L j g M x w 5 q H m G d l w 1 w 6 3 h r W O m k O b G b U 3 Q s O s V F c + s X W S J c 2 a c 5 v a 2 t i T / p 4 1 K S v f G l d R F S a j F 6 q C 0 V E A 5 L H 8 K p t K g I L V w g A s j 3 V 1 B z L n h g t x / B m 4 I 0 e 0 n 3 w X D f i / 6 0 O t / / 9 g 9 f N + O Y 4 O 9 Y m / Y L o v Y J 3 b I j t k p G z D h v f Q O v G P v q / / a / + J / 8 0 9 W V t 9 r a 7 b Z P + E P r w G / V 7 v B &lt; / l a t e x i t &gt; The graph coarsening operation of a toy graph. Numbers indicate edge weights and nodes in shadow are hyper-nodes.In SEG, node B and D share the same neighbors, so they are grouped into a hyper-node. In SSG, node C and E are grouped because they have the largest normalized connection weight. Node A constitutes a hyper-node by itself since it remains unmarked.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3 ), and the training process for each layer takes O(|E|CF ), where E is the edge set and C, F are embedding dimensions. For GAT, the masked attention over all nodes requires O(n 2 ) in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Results of H-GCN with varying layers and channels in terms of accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Results of node classification in terms of accuracy</cell></row><row><cell>• Planetoid [Yang et al., 2016] not only learns node em-</cell></row><row><cell>bedding but also predicts the context in graph. It also</cell></row><row><cell>leverages label information to build both transductive</cell></row><row><cell>and inductive formulations.</cell></row><row><cell>• GCN [Kipf and Welling, 2017] produces node embed-</cell></row><row><cell>ding vectors by truncating the Chebyshev polynomial to</cell></row><row><cell>the first-order neighborhoods.</cell></row><row><cell>• GAT [Veličković et al., 2018] generates node embed-</cell></row><row><cell>ding vectors by modeling the differences between the</cell></row><row><cell>node and its one-hop neighbors.</cell></row><row><cell>• DGCN [Zhuang and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>GCN 79.8% 79.3% 78.6% 76.5%</figDesc><table><row><cell>Method</cell><cell>20</cell><cell>15</cell><cell>10</cell><cell>5</cell></row><row><cell>GCN</cell><cell>79.0%</cell><cell cols="3">76.9% 72.2% 69.0%</cell></row><row><cell>GAT</cell><cell cols="2">79.0% 77.3%</cell><cell cols="2">75.4% 70.3%</cell></row><row><cell>DGCN</cell><cell>79.3%</cell><cell cols="3">77.4% 76.7% 70.1%</cell></row><row><cell>H-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To make our results reproducible, all relevant source codes are publicly available at https://github.com/CRIPAC-DIG/H-GCN.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is jointly supported by National Natural Science Foundation of China (61772528) and National Key Research and Development Program (2016YFB1001000, 2018YFB1402600).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>weights could reflect the hierarchical characteristics of coarsened graphs. In order to better capture the hierarchical information, we use the node weight embedding to supplement information in H i . Here we transform the node weight into real-valued vectors by looking up one randomly initialized node weight embedding matrix V ∈ R |T |×p , where T is the set of node weights and p is the dimension of the embedding. We apply node weight embedding in every coarsening and refining layer. For graph G i , we obtain its node weight embedding S i ∈ R ni×p by looking up V according to the node weight. For example, if one hyper-node contains three nodes, the third row of V will be selected as its node weight embedding. We then concatenate H i and S i and the resulting (d + p)-dimensional matrix will be fed into the next GCN layer subsequently. Multi-channel mechanisms help explore features in different subspaces and H-GCN employs multiple channels on GCN to obtain rich information jointly at each layer. After obtained c channels G 1 i , G 2 i , . . . , G c i , we perform weighted average on these feature maps:</p><p>where w j is a trainable weight of channel j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The Output Layer</head><p>Finally, in the output layer l, we use a GCN with a softmax classifier on H l−1 to output probabilities of nodes:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual analysis of large heterogeneous network through interactive centrality based sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNSC</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="378" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dressing as a whole: Outfit compatibility learning based on node-wise graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Defferrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eric Lenssen, Frank Weichert, and Heinrich Müller. SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Fey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>Grover and Leskovec</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
	<note>KDD</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing</title>
		<imprint>
			<publisher>Hendrickson and Leland</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lau ; Pili</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wing Cheong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">5865</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>A survey and taxonomy of graph sampling. CoRR, abs/1308</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Welling ; Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Lasalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The Graph Neural Network Model. TNN</title>
		<editor>Wu et al., 2019] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
	</analytic>
	<monogr>
		<title level="m">Haoteng Yin, and Zhanxing Zhu</title>
		<meeting><address><addrLine>Bing Yu</addrLine></address></meeting>
		<imprint>
			<publisher>Zhuang and Ma</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
	<note>WWW</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
