<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Selective Kernel Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">PCALab</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Sknet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">are with PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">Tsinghua National Laboratory for Information Science and Technology (TNList)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Selective Kernel Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Xiang Li is also a visiting scholar at Momenta. † Wenhai Wang is with National Key Lab for Novel Software Technol-ogy, Nanjing University. He was an research intern at Momenta. ‡ Xiaolin Hu is with the § Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The local receptive fields (RFs) of neurons in the primary visual cortex (V1) of cats <ref type="bibr" target="#b13">[14]</ref> have inspired the construction of Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b25">[26]</ref> in the last century, and it continues to inspire mordern CNN structure construction. For instance, it is well-known that in the visual cortex, the RF sizes of neurons in the same area (e.g., V1 region) are different, which enables the neurons to collect multi-scale spatial information in the same processing stage. This mechanism has been widely adopted in recent Convolutional Neural Networks (CNNs). A typical example is InceptionNets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41]</ref>, in which a simple concatenation is designed to aggregate multi-scale information from, e.g., 3×3, 5×5, 7×7 convolutional kernels inside the "inception" building block.</p><p>However, some other RF properties of cortical neurons have not been emphasized in designing CNNs, and one such property is the adaptive changing of RF size. Numerous experimental evidences have suggested that the RF sizes of neurons in the visual cortex are not fixed, but modulated by the stimulus. The Classical RFs (CRFs) of neurons in the V1 region was discovered by Hubel and Wiesel <ref type="bibr" target="#b13">[14]</ref>, as determined by single oriented bars. Later, many studies (e.g., <ref type="bibr" target="#b29">[30]</ref>) found that the stimuli outside the CRF will also affect the responses of neurons. The neurons are said to have non-classical RFs (nCRFs). In addition, the size of nCRF is related to the contrast of the stimulus: the smaller the contrast, the larger the effective nCRF size <ref type="bibr" target="#b36">[37]</ref>. Surprisingly, by stimulating nCRF for a period of time, the CRF of the neuron is also enlarged after removing these stimuli <ref type="bibr" target="#b32">[33]</ref>. All of these experiments suggest that the RF sizes of neurons are not fixed but modulated by stimulus <ref type="bibr" target="#b37">[38]</ref>. Unfortunately, this property does not receive much attention in constructing deep learning models. Those models with multi-scale information in the same layer such as In-ceptionNets have an inherent mechanism to adjust the RF size of neurons in the next convolutional layer according to the contents of the input, because the next convolutional layer linearly aggregates multi-scale information from different branches. But that linear aggregation approach may be insufficient to provide neurons powerful adaptation ability.</p><p>In the paper, we present a nonlinear approach to aggregate information from multiple kernels to realize the adaptive RF sizes of neurons. We introduce a "Selective Kernel" (SK) convolution, which consists of a triplet of operators: Split, Fuse and Select. The Split operator generates multiple paths with various kernel sizes which correspond to different RF sizes of neurons. The Fuse operator combines and aggregates the information from multiple paths to obtain a global and comprehensive representation for selection weights. The Select operator aggregates the feature maps of differently sized kernels according to the selection weights.</p><p>The SK convolutions can be computationally lightweight and impose only a slight increase in parameter and computational cost. We show that on the ImageNet 2012 dataset <ref type="bibr" target="#b34">[35]</ref> SKNets are superior to the previous state-of-the-art models with similar model complexity. Based on SKNet-50, we find the best settings for SK convolution and show the contribution of each component. To demonstrate their general applicability, we also provide compelling results on smaller datasets, CIFAR-10 and 100 <ref type="bibr" target="#b21">[22]</ref>, and successfully embed SK into small models (e.g., ShuffleNetV2 <ref type="bibr" target="#b26">[27]</ref>).</p><p>To verify the proposed model does have the ability to adjust neurons' RF sizes, we simulate the stimulus by enlarging the target object in natural images and shrinking the background to keep the image size unchanged. It is found that most neurons collect information more and more from the larger kernel path when the target object becomes larger and larger. These results suggest that the neurons in the proposed SKNet have adaptive RF sizes, which may underlie the model's superior performance in object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multi-branch convolutional networks. Highway networks <ref type="bibr" target="#b38">[39]</ref> introduces the bypassing paths along with gating units. The two-branch architecture eases the difficulty to training networks with hundreds of layers. The idea is also used in ResNet <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, but the bypassing path is the pure identity mapping. Besides the identity mapping, the shake-shake networks <ref type="bibr" target="#b6">[7]</ref> and multi-residual networks <ref type="bibr" target="#b0">[1]</ref> extend the major transformation with more identical paths. The deep neural decision forests <ref type="bibr" target="#b20">[21]</ref> form the tree-structural multi-branch principle with learned splitting functions. FractalNets <ref type="bibr" target="#b24">[25]</ref> and Multilevel ResNets <ref type="bibr" target="#b51">[52]</ref> are designed in such a way that the multiple paths can be expanded fractally and recursively. The InceptionNets <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41]</ref> carefully configure each branch with customized kernel filters, in order to aggregate more informative and multifarious features. Please note that the proposed SKNets follow the idea of InceptionNets with various filters for multiple branches, but differ in at least two important aspects: 1) the schemes of SKNets are much simpler without heavy customized design and 2) an adaptive selection mechanism for these multiple branches is utilized to realize adaptive RF sizes of neurons. Grouped/depthwise/dilated convolutions. Grouped convolutions are becoming popular due to their low computational cost. Denote the group size by G, then both the number of parameters and the computational cost will be divided by G, compared to the ordinary convolution. They are first adopted in AlexNet <ref type="bibr" target="#b22">[23]</ref> with a purpose of distributing the model over more GPU resources. Surprisingly, using grouped convolutions, ResNeXts <ref type="bibr" target="#b46">[47]</ref> can also improve accuracy. This G is called "cardinality", which characterize the model together with depth and width.</p><p>Many compact models such as IGCV1 <ref type="bibr" target="#b52">[53]</ref>, IGCV2 <ref type="bibr" target="#b45">[46]</ref> and IGCV3 <ref type="bibr" target="#b39">[40]</ref> are developed, based on the interleaved grouped convolutions. A special case of grouped convolutions is depthwise convolution, where the number of groups is equal to the number of channels. Xception <ref type="bibr" target="#b2">[3]</ref> and Mo-bileNetV1 <ref type="bibr" target="#b10">[11]</ref> introduce the depthwise separable convolution which decomposes ordinary convolutions into depthwise convolution and pointwise convolution. The effectiveness of depthwise convolutions is validated in the subsequent works such as MobileNetV2 <ref type="bibr" target="#b35">[36]</ref> and ShuffleNet <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b26">27]</ref>. Beyond grouped/depthwise convolutions, dilated convolutions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> support exponential expansion of the RF without loss of coverage. For example, a 3×3 convolution with dilation 2 can approximately cover the RF of a 5×5 filter, whilst consuming less than half of the computation and memory. In SK convolutions, the kernels of larger sizes (e.g., &gt;1) are designed to be integrated with the grouped/depthwise/dilated convolutions, in order to avoid the heavy overheads. Attention mechanisms. Recently, the benefits of attention mechanism have been shown across a range of tasks, from neural machine translation <ref type="bibr" target="#b1">[2]</ref> in natural language processing to image captioning <ref type="bibr" target="#b48">[49]</ref> in image understanding. It biases the allocation of the most informative feature expressions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref> and simultaneously suppresses the less useful ones. Attention has been widely used in recent applications such as person re-ID <ref type="bibr" target="#b3">[4]</ref>, image recovery <ref type="bibr" target="#b54">[55]</ref>, text abstraction <ref type="bibr" target="#b33">[34]</ref> and lip reading <ref type="bibr" target="#b47">[48]</ref>. To boost the performance of image classification, Wang et al. <ref type="bibr" target="#b43">[44]</ref> propose a trunk-and-mask attention between intermediate stages of a CNN. An hourglass module is introduced to achieve the global emphasis across both spatial and channel dimension. Furthermore, SENet <ref type="bibr" target="#b11">[12]</ref> brings an effective, lightweight gating mechanism to self-recalibrate the feature map via channel-wise importances. Beyond channel, BAM <ref type="bibr" target="#b31">[32]</ref> and CBAM <ref type="bibr" target="#b44">[45]</ref> introduce spatial attention in a similar way. In contrast, our proposed SKNets are the first to explicitly focus on the adaptive RF size of neurons by introducing the attention mechanisms. Dynamic convolutions. Spatial Transform Networks <ref type="bibr" target="#b17">[18]</ref> learns a parametric transformation to warp the feature map, which is considered difficult to be trained. Dynamic Filter <ref type="bibr" target="#b19">[20]</ref> can only adaptively modify the parameters of filters, without the adjustment of kernel size. Active Convolution <ref type="bibr" target="#b18">[19]</ref> augments the sampling locations in the convolution with offsets. These offsets are learned end-to-end but become static after training, while in SKNet the RF sizes of neurons can adaptively change during inference. Deformable Convolutional Networks <ref type="bibr" target="#b5">[6]</ref> further make the location offsets dynamic, but it does not aggregate multi-scale information in the same way as SKNet does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Selective Kernel Convolution</head><p>To enable the neurons to adaptively adjust their RF sizes, we propose an automatic selection operation, "Selective Kernel" (SK) convolution, among multiple kernels with different kernel sizes. Specifically, we implement the SK convolution via three operators -Split, Fuse and Select, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, where a two-branch case is shown. Therefore in this example, there are only two kernels with different kernel sizes, but it is easy to extend to multiple branches case.</p><p>Split: For any given feature map X ∈ R H ×W ×C , by default we first conduct two transformations F : X → U ∈ R H×W ×C and F : X → U ∈ R H×W ×C with kernel sizes 3 and 5, respectively. Note that both F and F are composed of efficient grouped/depthwise convolutions, Batch Normalization <ref type="bibr" target="#b14">[15]</ref> and ReLU <ref type="bibr" target="#b28">[29]</ref> function in sequence. For further efficiency, the conventional convolution with a 5×5 kernel is replaced with the dilated convolution with a 3×3 kernel and dilation size 2.</p><p>Fuse: As stated in Introduction, our goal is to enable neurons to adaptively adjust their RF sizes according to the stimulus content. The basic idea is to use gates to control the information flows from multiple branches carrying different scales of information into neurons in the next layer.</p><p>To achieve this goal, the gates need to integrate information from all branches. We first fuse results from multiple (two in <ref type="figure" target="#fig_0">Fig. 1</ref>) branches via an element-wise summation:</p><formula xml:id="formula_0">U = U + U,<label>(1)</label></formula><p>then we embed the global information by simply using global average pooling to generate channel-wise statistics as s ∈ R C . Specifically, the c-th element of s is calculated by shrinking U through spatial dimensions H × W :</p><formula xml:id="formula_1">s c = F gp (U c ) = 1 H × W H i=1 W j=1 U c (i, j).<label>(2)</label></formula><p>Further, a compact feature z ∈ R d×1 is created to enable the guidance for the precise and adaptive selections. This is achieved by a simple fully connected (fc) layer, with the reduction of dimensionality for better efficiency:</p><formula xml:id="formula_2">z = F f c (s) = δ(B(Ws)),<label>(3)</label></formula><p>where δ is the ReLU function <ref type="bibr" target="#b28">[29]</ref>, B denotes the Batch Normalization <ref type="bibr" target="#b14">[15]</ref>, W ∈ R d×C . To study the impact of d on the efficiency of the model, we use a reduction ratio r to control its value:</p><formula xml:id="formula_3">d = max(C/r, L),<label>(4)</label></formula><p>where L denotes the minimal value of d (L = 32 is a typical setting in our experiments). Select: A soft attention across channels is used to adaptively select different spatial scales of information, which is guided by the compact feature descriptor z. Specifically, a softmax operator is applied on the channel-wise digits:</p><formula xml:id="formula_4">a c = e Acz e Acz + e Bcz , b c = e Bcz e Acz + e Bcz ,<label>(5)</label></formula><p>where A, B ∈ R C×d and a, b denote the soft attention vector for U and U, respectively. Note that A c ∈ R 1×d is the c-th row of A and a c is the c-th element of a, likewise   <ref type="table">Table 1</ref>. The three columns refer to ResNeXt-50 with a 32×4d template, SENet-50 based on the ResNeXt-50 backbone and the corresponding SKNet-50, respectively. Inside the brackets are the general shape of a residual block, including filter sizes and feature dimensionalities.</p><formula xml:id="formula_5">  1 × 1, 128 3 × 3, 128, G = 32 1 × 1, 256   × 3     1 × 1, 128 3 × 3, 128, G = 32 1 × 1, 256 f c, [16, 256]     × 3   1 × 1, 128 SK[M = 2, G = 32, r = 16], 128 1 × 1, 256   × 3 28 × 28   1 × 1, 256 3 × 3, 256, G = 32 1 × 1, 512   × 4     1 × 1, 256 3 × 3, 256, G = 32 1 × 1, 512 f c, [32, 512]     × 4   1 × 1, 256 SK[M = 2, G = 32, r = 16], 256 1 × 1, 512   × 4 14 × 14   1 × 1, 512 3 × 3, 512, G = 32 1 × 1, 1024   × 6     1 × 1, 512 3 × 3, 512, G = 32 1 × 1, 1024 f c, [64, 1024]     × 6   1 × 1, 512 SK[M = 2, G = 32, r = 16], 512 1 × 1, 1024   × 6 7 × 7   1 × 1, 1024 3 × 3, 1024, G = 32 1 × 1, 2048   × 3     1 × 1, 1024 3 × 3, 1024, G = 32 1 × 1, 2048 f c, [128, 2048]     × 3   1 × 1, 1024 SK[M = 2, G = 32, r = 16], 1024 1 × 1, 2048   × 3 1 × 1 7 × 7 global average pool, 1000-d f c,</formula><p>The number of stacked blocks on each stage is presented outside the brackets. "G = 32" suggests the grouped convolution. The inner brackets following by f c indicates the output dimension of the two fully connected layers in an SE module. #P denotes the number of parameter and the definition of FLOPs follow <ref type="bibr" target="#b53">[54]</ref>, i.e., the number of multiply-adds.</p><p>B c and b c . In the case of two branches, the matrix B is redundant because a c + b c = 1. The final feature map V is obtained through the attention weights on various kernels:</p><formula xml:id="formula_6">V c = a c · U c + b c · U c , a c + b c = 1,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">V = [V 1 , V 2 , ..., V C ], V c ∈ R H×W .</formula><p>Note that here we provide a formula for the two-branch case and one can easily deduce situations with more branches by extending Eqs. (1) (5) (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Using the SK convolutions, the overall SKNet architecture is listed in <ref type="table">Table 1</ref>. We start from ResNeXt <ref type="bibr" target="#b46">[47]</ref> for two reasons: 1) it has low computational cost with extensive use of grouped convolution, and 2) it is one of the state-ofthe-art network architectures with high performance on object recognition. Similar to the ResNeXt <ref type="bibr" target="#b46">[47]</ref>, the proposed SKNet is mainly composed of a stack of repeated bottleneck blocks, which are termed "SK units". Each SK unit consists of a sequence of 1×1 convolution, SK convolution and 1×1 convolution. In general, all the large kernel convolutions in the original bottleneck blocks in ResNeXt are replaced by the proposed SK convolutions, enabling the network to choose appropriate RF sizes in an adaptive manner. As the SK convolutions are very efficient in our design, SKNet-50 only leads to 10% increase in the number of parameters and 5% increase in computational cost, compared with ResNeXt-50.</p><p>In SK units, there are three important hyper-parameters which determine the final settings of SK convolutions: the number of paths M that determines the number of choices of different kernels to be aggregated, the group number G that controls the cardinality of each path, and the reduction ratio r that controls the number of parameters in the fuse operator (see Eq. <ref type="formula" target="#formula_3">(4)</ref>). In <ref type="table">Table 1</ref>, we denote one typical setting of SK convolutions SK[M, G, r] to be SK <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref>. The choices and effects of these parameters are discussed in Sec. 4.3. <ref type="table">Table 1</ref> shows the structure of a 50-layer SKNet which has four stages with {3,4,6,3} SK units, respectively. By varying the number of SK units in each stage, one can obtain different architectures. In this study, we have experimented with other two architectures, SKNet-26, which has {2,2,2,2} SK units, and SKNet-101, which has {3,4,23,3} SK units, in their respective four stages.</p><p>Note that the proposed SK convolutions can be applied to other lightweight networks, e.g., MobileNet <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>, Shuf-fleNet <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b26">27]</ref>, in which 3×3 depthwise convolutions are extensively used. By replacing these convolutions with the SK convolutions, we can also achieve very appealing results in the compact architectures (see Sec. 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>The ImageNet 2012 dataset <ref type="bibr" target="#b34">[35]</ref> comprises 1.28 million training images and 50K validation images from 1,000 classes. We train networks on the training set and report the top-1 errors on the validation set. For data augmentation, we follow the standard practice and perform the random- size cropping to 224 ×224 and random horizontal flipping <ref type="bibr" target="#b41">[42]</ref>. The practical mean channel subtraction is adpoted to normalize the input images for both training and testing. Label-smoothing regularization <ref type="bibr" target="#b42">[43]</ref> is used during training. For training large models, we use synchronous SGD with momentum 0.9, a mini-batch size of 256 and a weight decay of 1e-4. The initial learning rate is set to 0.1 and decreased by a factor of 10 every 30 epochs. All models are trained for 100 epochs from scratch on 8 GPUs, using the weight initialization strategy in <ref type="bibr" target="#b7">[8]</ref>. For training lightweight models, we set the weight decay to 4e-5 instead of 1e-4, and we also use slightly less aggressive scale augmentation for data preprocessing. Similar modifications can as well be referenced in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">54]</ref> since such small networks usually suffer from underfitting rather than overfitting. To benchmark, we apply a centre crop on the validation set, where 224×224 or 320×320 pixels are cropped for evaluating the classification accuracy. The results reported on ImageNet are the averages of 3 runs by default.</p><p>Comparisons with state-of-the-art models. We first compare SKNet-50 and SKNet-101 to the public competitive models with similar model complexity. demonstrates the superiority of adaptive aggregation for multiple kernels. We also note that using slightly less parameters, SKNets can obtain 0.3∼0.4% gains to SENet counterparts in both 224×224 and 320×320 evaluations.</p><p>Selective Kernel vs. Depth/Width/Cardinality. Compared with ResNeXt (using the setting of 32×4d), SKNets inevitably introduce a slightly increase in parameter and computation due to the additional paths of differnet kernels and the selection process. For fair comparison, we increase the complexity of ResNeXt by changing its depth, width and cardinality, to match the complexity of SKNets. <ref type="table" target="#tab_3">Table  3</ref> shows that increased complexity does lead to better prediction accuracy. However, the improvement is marginal when going deeper (0.19% from ResNeXt-50 to ResNeXt-53) or wider (0.1% from ResNeXt-50 to ResNeXt-50 wider), or with slightly more cardinality (0.23% from ResNeXt-50 (32×4d) to ResNeXt-50 (36×4d)).</p><p>In contrast, SKNet-50 obtains 1.44% absolute improvement over the baseline ResNeXt-50, which indicates that SK convolution is very efficient.</p><p>Performance with respect to the number of parameters. We plot the top-1 error rate of the proposed SKNet with respect to the number of parameters in it <ref type="figure" target="#fig_1">(Fig. 2)</ref>. Three architectures, SK-26, SKNet-50 and SKNet-101 (see Section 3.2 for details), are shown in the figure. For comparison, we plot the results of some state-of-the-art models including ResNets <ref type="bibr" target="#b8">[9]</ref>, ResNeXts <ref type="bibr" target="#b46">[47]</ref>, DenseNets <ref type="bibr" target="#b12">[13]</ref>, DPNs <ref type="bibr" target="#b4">[5]</ref> and SENets <ref type="bibr" target="#b11">[12]</ref> in the figure. Each model has multiple variants. The details of the compared architectures are provided in the Supplementary Materials. All Top-1 errors are reported in the references. It is seen that SKNets utilizes parameters more efficiently than these models. For instance, achieving ∼20.2 top-1 error, SKNet-101 needs 22% fewer parameters than DPN-98.</p><p>Lightweight models. Finally, we choose the representative compact architecture -ShuffleNetV2 <ref type="bibr" target="#b26">[27]</ref>, which is one of the strongest light models, to evaluate the generalization ability of SK convolutions. By exploring different scales of models in <ref type="table" target="#tab_4">Table 4</ref>, we can observe that SK convolutions not only boost the accuracy of baselines significantly but also  perform better than SE <ref type="bibr" target="#b11">[12]</ref> (achieving around absolute 1% gain). This indicates the great potential of the SK convolutions in applications on low-end devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CIFAR Classification</head><p>To evaluate the performance of SKNets on smaller datasets, we conduct more experiments on CIFAR-10 and 100 <ref type="bibr" target="#b21">[22]</ref>. The two CIFAR datasets <ref type="bibr" target="#b21">[22]</ref> consist of colored natural scence images, with 32×32 pixel each. The train and test sets contain 50k images and 10k images respectively. CIFAR-10 has 10 classes and CIFAR-100 has 100. We take the architectures as in <ref type="bibr" target="#b46">[47]</ref> for reference: our networks have a single 3×3 convolutional layer, followed by 3 stages each having 3 residual blocks with SK convolution. We also apply SE blocks on the same backbone (ResNeXt-29, 16×32d) for better comparisons. More architectural and training details are provided in the supplemantary materials. Notably, SKNet-29 achieves better or comparable performance than ResNeXt-29, 16×64d with 60% fewer parameters and it consistently outperforms SENet-29 on both CIFAR-10 and 100 with 22% fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we report ablation studies on the Ima-geNet dataset to investigate the effectiveness of SKNet. The dilation D and group number G. The dilation D and group number G are two crucial elements to control the RF size. To study their effects, we start from the two-branch case and fix the setting 3×3 filter with dilation D = 1 and group G = 32 in the first kernel branch of SKNet-50. Under the constraint of similar overall complexity, there are two ways to enlarge the RF of the second kernel branch: 1) increase the dilation D whilst fixing the group number G, and 2) simultaneously increase the filter size and the group number G. <ref type="table">Table 6</ref> shows that the optimal settings for the other branch are those with kernel size 5×5 (the last column), which is larger than the first fixed kernel with size 3×3. It is proved beneficial to use different kernel sizes, and we attribute the reason to the aggregation of multi-scale information.</p><p>There are two optimal configurations: kernel size 5×5 with D = 1 and kernel size 3×3 with D = 2, where the latter has slightly lower model complexity. In general, we empirically find that the series of 3×3 kernels with various dilations is moderately superior to the corresponding counterparts with the same RF (large kernels without dilations) in both performance and complexity.</p><p>Combination of different kernels. Next we investigate the effect of combination of different kernels. Some kernels may have size larger than 3×3, and there may be more than two kernels. To limit the search space, we only use three different kernels, called "K3" (standard 3×3 convolutional kernel), "K5" (3×3 convolution with dilation 2 to approximate 5×5 kernel size), and "K7" (3×3 with dilation 3 to approximate 7×7 kernel size). Note that we only consider the dilated versions of large kernels (5×5 and 7×7) as <ref type="table">Table 6</ref> has suggested. G is fixed to 32. If "SK" in <ref type="table" target="#tab_5">Table 7</ref> is ticked, it means that we use the SK attention across the corresponding kernels ticked in the same row (the output of each SK unit is V in <ref type="figure" target="#fig_0">Fig. 1</ref>), otherwise we simply sum up the results with these kernels (then the output of each SK unit is U in <ref type="figure" target="#fig_0">Fig. 1)</ref> as a naive baseline model. The results in <ref type="table" target="#tab_5">Table 7</ref> indicate that excellent performance of SKNets can be attributed to the use of multiple kernels and the adaptive selection mechanism among them. From <ref type="table" target="#tab_5">Table 7</ref>, we have the following observations: (1) When the number of paths M increases, in general the recognition error decreases. The top-1 errors in the first block of the table (M = 1) are generally higher than those in the second block (M = 2), and the errors in the second block are generally higher than the third block (M = 3). (2) No matter M = 2 or 3, SK attention-based aggregation of multiple paths always achieves lower top-1 error than the simple aggregation method (naive baseline model). (3) Using SK attention, the performance gain of the model from M = 2 to M = 3 is marginal (the top-1 error decreases from 20.79% to 20.76%). For better trade-off between performance and efficiency, M = 2 is preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis and Interpretation</head><p>To understand how adaptive kernel selection works, we analyze the attention weights by inputting same target object but in different scales. We take all the image instances from the ImageNet validation set, and progressively enlarge the central object from 1.0× to 2.0× via a central cropping and subsequent resizing (see top left in <ref type="figure" target="#fig_2">Fig. 3a,b)</ref>.</p><p>First, we calculate the attention values for the large kernel (5×5) in each channel in each SK unit. <ref type="figure" target="#fig_2">Fig. 3a,b</ref> (bottom left) show the attention values in all channels for two randomly samples in SK <ref type="bibr">3 4, and</ref>  <ref type="figure" target="#fig_2">Fig. 3c (bottom left)</ref> shows the averaged attention values in all channels across all validation images. It is seen that in most channels, when the target object enlarges, the attention weight for the large kernel (5×5) increases, which suggests that the RF sizes of  the neurons are adaptively getting larger, which agrees with our expectation. We then calculate the difference between the the mean attention weights associated with the two kernels (larger minus smaller) over all channels in each SK unit. <ref type="figure" target="#fig_2">Fig. 3a,b</ref> (right) show the results for two random samples at different SK units, and <ref type="figure" target="#fig_2">Fig. 3c (right)</ref> show the results averaged over all validation images. We find one surprising pattern about the role of adaptive selection across depth: The larger the target object is, the more attention will be assigned to larger  kernels by the Selective Kernel mechanism in low and middle level stages (e.g., SK 2 3, SK 3 4). However, at much higher layers (e.g., SK 5 3), all scale information is getting lost and such a pattern disappears.</p><p>Further, we look deep into the selection distributions from the perspective of classes. For each category, we draw the average mean attention differences on the representative SK units for 1.0× and 1.5× objects over all the 50 images which belong to that category. We present the statistics of 1,000 classes in <ref type="figure" target="#fig_3">Fig. 4</ref>. We observe the previous pattern holds true for all 1,000 categories, as illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, where the importance of kernel 5×5 consistently and simultaneously increases when the scale of targets grows. This suggests that in the early parts of networks, the appropriate kernel sizes can be selected according to the semantic awareness of objects' sizes, thus it efficiently adjusts the RF sizes of these neurons. However, such pattern is not existed in the very high layers like SK 5 3, since for the high-level representation, "scale" is partially encoded in the feature vector, and the kernel size matters less compared to the situation in lower layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Inspired by the adaptive receptive field (RF) sizes of neurons in visual cortex, we propose Selective Kernel Networks (SKNets) with a novel Selective Kernel (SK) convolution, to improve the efficiency and effectiveness of object recognition by adaptive kernel selection in a soft-attention manner. SKNets demonstrate state-of-the-art performances on various benchmarks, and from large models to tiny models. In addition, we also discover several meaningful behaviors of kernel selection across channel, depth and category, and empirically validate the effective adaption of RF sizes for SKNets, which leads to a better understanding of its mechanism. We hope it may inspire the study of architectural design and search in the future. <ref type="table" target="#tab_4">Table 4</ref> For fair comparisons, we re-implement the Shuf-fleNetV2 <ref type="bibr" target="#b26">[27]</ref> with 0.5× and 1.0× settings (see <ref type="bibr" target="#b26">[27]</ref> for details). Our implementation changes the numbers of blocks in the three stages from {4,8,4} to {4,6,6}, therefore the performances and computational costs are slightly different from those reported in the original paper <ref type="bibr" target="#b26">[27]</ref> (see <ref type="table" target="#tab_4">Table 4</ref> in the main body of the paper for detailed results). In <ref type="table" target="#tab_4">Table  4</ref>, "+ SE" means that the SE module <ref type="bibr" target="#b11">[12]</ref> is integrated after each shuffle layer in ShuffleNetV2, "+ SK" means that each 3×3 depthwise convolution is replaced by a SK unit with M = 2 (K3 and K5 kernels are used in the two paths, respectively), r = 4 and G is the same as the number of channels in the corresponding stage due to the depthwise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the Models in</head><p>Note that the 3×3 depthwise convolution in the original ShuffleNet is not followed by a ReLU activation function. We verify that the best practice for integrating SK units into ShuffleNet is also without ReLU activation functions in both paths in each SK unit <ref type="table" target="#tab_8">(Table S8)</ref> C. Details of the Compared Models in <ref type="figure" target="#fig_1">Figure 2</ref> We have plotted the results of some state-of-the-art models including ResNet, ResNeXt, DenseNet, DPN and SENet in <ref type="figure" target="#fig_1">Figure 2</ref> in the main body of the paper. Each dot represents a variant of certain model. <ref type="table">Table S9</ref> shows the settings of these variants, the numbers of parameters, and the evaluation results on the ImageNet validation set. Note that SENets are based on the corresponding ResNeXts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details on CIFAR Datasets (Section 4.2)</head><p>On CIFAR-10 and CIFAR-100 datasets, all networks are trained on 2 GPUs with a mini-batch size 128 for 300 epochs. The initial learning rate is 0.1 for CIFAR-10 and 0.05 for CIFAR-100, and is divided by 10 at 50% and 75% of the total number of training epochs. Following <ref type="bibr" target="#b8">[9]</ref>, we use a weight decay of 5e-4 and a momentum of 0.9. We adopt the weight initialization method introduced in <ref type="bibr" target="#b7">[8]</ref>. The ResNeXt-29 backbone is described in <ref type="bibr" target="#b46">[47]</ref>. Based on it, SENet-29 applies SE unit before each residual connection, and SKNet-29 modifies the grouped 3×3 convolution to SK convolution with setting SK <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>. In order to prevent overfitting on these smaller datasets, we replace the 5×5 kernel in the second path in the SK unit to 1×1, while the setting for the first path remains the same.   <ref type="figure" target="#fig_4">Figure S5</ref> shows attention results for more images with three differently sized targets. Same as in <ref type="figure" target="#fig_2">Figure 3</ref> in the main body of the paper, we see a trend in low and middle level stages: the larger the target object is, the more atten-tion is assigned to larger kernels by the dynamic selection mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Examples of Dynamic Selection</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Selective Kernel Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Relationship between the performance of SKNet and the number of parameters in it, compared with the state-of-the-arts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) and (b): Attention results for two randomly sampled images with three differently sized targets (1.0x, 1.5x and 2.0x). Top left: sample images. Bottom left: the attention values for the 5×5 kernel across channels in SK 3 4. The plotted results are the averages of 16 successive channels for the ease of view. Right: the attention value of the kernel 5×5 minus that of the kernel 3×3 in different SK units. (c): Average results over all image instances in the ImageNet validation set. Standard deviation is also plotted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Average mean attention difference (mean attention value of kernel 5×5 minus that of kernel 3×3) on SK units of SKNet-50, for each of 1,000 categories using all validation samples on ImageNet. On low or middle level SK units (e.g., SK 2 3, SK 3 4), 5×5 kernels are clearly imposed with more emphasis if the target object becomes larger (1.0x → 1.5x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S5 .</head><label>S5</label><figDesc>Attention results for two randomly sampled images with three differently sized targets (1.0x, 1.5x and 2.0x). The notations are the same as inFigure 3a,b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">top-1 err (%) 224× 320×</cell><cell>#P</cell><cell>GFLOPs</cell></row><row><cell>ResNeXt-50</cell><cell cols="3">22.23 21.05 25.0M</cell><cell>4.24</cell></row><row><cell>AttentionNeXt-56 [44]</cell><cell>21.76</cell><cell>-</cell><cell>31.9M</cell><cell>6.32</cell></row><row><cell>InceptionV3 [43]</cell><cell>-</cell><cell cols="2">21.20 27.1M</cell><cell>5.73</cell></row><row><cell>ResNeXt-50 + BAM [32]</cell><cell cols="3">21.70 20.15 25.4M</cell><cell>4.31</cell></row><row><cell>ResNeXt-50 + CBAM [45]</cell><cell cols="3">21.40 20.38 27.7M</cell><cell>4.25</cell></row><row><cell>SENet-50 [12]</cell><cell cols="3">21.12 19.71 27.7M</cell><cell>4.25</cell></row><row><cell>SKNet-50 (ours)</cell><cell cols="3">20.79 19.32 27.5M</cell><cell>4.47</cell></row><row><cell>ResNeXt-101</cell><cell cols="3">21.11 19.86 44.3M</cell><cell>7.99</cell></row><row><cell>Attention-92 [44]</cell><cell>-</cell><cell cols="2">19.50 51.3M</cell><cell>10.43</cell></row><row><cell>DPN-92 [5]</cell><cell cols="3">20.70 19.30 37.7M</cell><cell>6.50</cell></row><row><cell>DPN-98 [5]</cell><cell cols="3">20.20 18.90 61.6M</cell><cell>11.70</cell></row><row><cell>InceptionV4 [41]</cell><cell>-</cell><cell cols="2">20.00 42.0M</cell><cell>12.31</cell></row><row><cell>Inception-ResNetV2 [41]</cell><cell>-</cell><cell cols="2">19.90 55.0M</cell><cell>13.22</cell></row><row><cell>ResNeXt-101 + BAM [32]</cell><cell cols="3">20.67 19.15 44.6M</cell><cell>8.05</cell></row><row><cell cols="4">ResNeXt-101 + CBAM [45] 20.60 19.42 49.2M</cell><cell>8.00</cell></row><row><cell>SENet-101 [12]</cell><cell cols="3">20.58 18.61 49.2M</cell><cell>8.00</cell></row><row><cell>SKNet-101 (ours)</cell><cell cols="3">20.19 18.40 48.9M</cell><cell>8.46</cell></row></table><note>. Comparisons to the state-of-the-arts under roughly identi- cal complexity. 224× denotes the single 224×224 crop for evalu- ation, and likewise 320×. Note that SENets/SKNets are all based on the corresponding ResNeXt backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on ImageNet validation set when the computational cost of model with more depth/width/cardinality is increased to match that of SKNet. The numbers in brackets denote the gains of performance.</figDesc><table><row><cell></cell><cell>top-1 err. (%)</cell><cell>#P</cell><cell>GFLOPs</cell></row><row><cell cols="2">ResNeXt-50 (32×4d) 22.23</cell><cell>25.0M</cell><cell>4.24</cell></row><row><cell>SKNet-50 (ours)</cell><cell>20.79 (1.44)</cell><cell>27.5M</cell><cell>4.47</cell></row><row><cell>ResNeXt-50, wider</cell><cell>22.13 (0.10)</cell><cell>28.1M</cell><cell>4.74</cell></row><row><cell>ResNeXt-56, deeper</cell><cell>22.04 (0.19)</cell><cell>27.3M</cell><cell>4.67</cell></row><row><cell cols="2">ResNeXt-50 (36×4d) 22.00 (0.23)</cell><cell>27.6M</cell><cell>4.70</cell></row></table><note>The results show that SKNets consistently improve performance over the state-of-the-art attention-based CNNs under similar bud- gets. Remarkably, SKNet-50 outperforms ResNeXt-101 by above absolute 0.32%, although ResNeXt-101 is 60% larger in parameter and 80% larger in computation. With comparable or less complexity than InceptionNets, SKNets achieve above absolute 1.5% gain of performance, which</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>ShuffleNetV2</cell><cell cols="2">top-1 err.(%) MFLOPs</cell><cell>#P</cell></row><row><cell>0.5× [27]</cell><cell>39.70</cell><cell>41</cell><cell>1.4M</cell></row><row><cell>0.5× (our impl.)</cell><cell>38.41</cell><cell>40.39</cell><cell>1.40M</cell></row><row><cell>0.5× + SE [12]</cell><cell>36.34</cell><cell>40.85</cell><cell>1.56M</cell></row><row><cell>0.5× + SK</cell><cell>35.35</cell><cell>42.58</cell><cell>1.48M</cell></row><row><cell>1.0× [27]</cell><cell>30.60</cell><cell>146</cell><cell>2.3M</cell></row><row><cell>1.0× (our impl.)</cell><cell>30.57</cell><cell>140.35</cell><cell>2.45M</cell></row><row><cell>1.0× + SE [12]</cell><cell>29.47</cell><cell>141.73</cell><cell>2.66M</cell></row><row><cell>1.0× + SK</cell><cell>28.36</cell><cell>145.66</cell><cell>2.63M</cell></row></table><note>. Single 224×224 crop top-1 error rates (%) by variants of lightweight models on ImageNet validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Results of SKNet-50 with different combinations of multiple kernels. Single 224×224 crop is utilized for evaluation.</figDesc><table><row><cell>Models</cell><cell></cell><cell></cell><cell cols="2">#P</cell><cell cols="3">CIFAR-10 CIFAR-100</cell></row><row><cell cols="5">ResNeXt-29, 16×32d 25.2M</cell><cell cols="2">3.87</cell><cell>18.56</cell></row><row><cell cols="3">ResNeXt-29, 8×64d</cell><cell cols="2">34.4M</cell><cell cols="2">3.65</cell><cell>17.77</cell></row><row><cell cols="5">ResNeXt-29, 16×64d 68.1M</cell><cell cols="2">3.58</cell><cell>17.31</cell></row><row><cell cols="2">SENet-29 [12]</cell><cell></cell><cell cols="2">35.0M</cell><cell cols="2">3.68</cell><cell>17.78</cell></row><row><cell cols="3">SKNet-29 (ours)</cell><cell cols="2">27.7M</cell><cell cols="2">3.47</cell><cell>17.33</cell></row><row><cell cols="8">Table 5. Top-1 errors (%, average of 10 runs) on CIFAR. SENet-29</cell></row><row><cell cols="8">and SKNet-29 are all based on ResNeXt-29, 16×32d.</cell></row><row><cell cols="2">Settings Kernel D</cell><cell>G</cell><cell cols="2">top-1 err. (%)</cell><cell>#P</cell><cell cols="2">GFLOPs</cell><cell>Resulted Kernel</cell></row><row><cell>3×3</cell><cell>3</cell><cell>32</cell><cell>20.97</cell><cell></cell><cell>27.5M</cell><cell cols="2">4.47</cell><cell>7×7</cell></row><row><cell>3×3</cell><cell>2</cell><cell>32</cell><cell>20.79</cell><cell></cell><cell>27.5M</cell><cell cols="2">4.47</cell><cell>5×5</cell></row><row><cell>3×3</cell><cell>1</cell><cell>32</cell><cell>20.91</cell><cell></cell><cell>27.5M</cell><cell cols="2">4.47</cell><cell>3×3</cell></row><row><cell>5×5</cell><cell>1</cell><cell>64</cell><cell>20.80</cell><cell></cell><cell>28.1M</cell><cell cols="2">4.56</cell><cell>5×5</cell></row><row><cell>7×7</cell><cell>1</cell><cell cols="2">128 21.18</cell><cell></cell><cell>28.1M</cell><cell cols="2">4.55</cell><cell>7×7</cell></row><row><cell cols="8">Table 6. Results of SKNet-50 with different settings in the second</cell></row><row><cell cols="8">branch, while the setting of the first kernel is fixed. "Resulted</cell></row><row><cell cols="8">kernel" in the last column means the approximate kernel size with</cell></row><row><cell cols="3">dilated convolution.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">K3 K5 K7</cell><cell>SK</cell><cell cols="2">top-1 err. (%)</cell><cell>#P</cell><cell>GFLOPs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">22.23</cell><cell>25.0M</cell><cell>4.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.14</cell><cell>25.0M</cell><cell>4.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">25.51</cell><cell>25.0M</cell><cell>4.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">21.76</cell><cell>26.5M</cell><cell>4.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">20.79</cell><cell>27.5M</cell><cell>4.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">21.82</cell><cell>26.5M</cell><cell>4.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">20.97</cell><cell>27.5M</cell><cell>4.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">23.64</cell><cell>26.5M</cell><cell>4.46</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">23.09</cell><cell>27.5M</cell><cell>4.47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">21.47</cell><cell>28.0M</cell><cell>4.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">20.76</cell><cell>29.3M</cell><cell>4.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S8 .</head><label>S8</label><figDesc>Influence of activation functions in two paths of SK units based on ShuffleNetV2 1.0×. Single 224×224 crop is used for evaluation on the ImageNet validation set.</figDesc><table><row><cell></cell><cell></cell><cell>.</cell></row><row><cell>K3 + ReLU ?</cell><cell>K5 + ReLU ?</cell><cell>Top-1 error (%)</cell></row><row><cell></cell><cell></cell><cell>28.65</cell></row><row><cell></cell><cell></cell><cell>28.40</cell></row><row><cell></cell><cell></cell><cell>28.36</cell></row><row><cell></cell><cell></cell><cell>28.49</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors would like to thank the editor and the anonymous reviewers for their critical and constructive comments and suggestions. This work was supported by the National Science Fund of China under Grant No. U1713208, Program for Changjiang Scholars and National Natural Science Foundation of China, Grant no. 61836014.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05672</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: A technique for the experimental evaluation of dependability in modern computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Madeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08107</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06211</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Deformable convolutional networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational modelling of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep neural decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiterau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11164</idno>
		<title level="m">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Orientation-selective inhibition from beyond the classic visual receptive field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06514</idno>
		<title level="m">Bam: Bottleneck attention module</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic changes in receptive-field size in cat primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Pettet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast&apos;s effect on spatial summation by macaque v1 neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Sceniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ringach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hawken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond the classical receptive field: the effect of contextual stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dresp-Langley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00178</idno>
		<title level="m">Igcv3: Interleaved lowrank group convolutions for efficient deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06904</idno>
		<title level="m">Residual attention network for image classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06521</idno>
		<title level="m">Cbam: Convolutional block attention module</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Igcv 2: Interleaved structured sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lcanet: Endto-end lipreading with cascaded attention-ctc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cassimatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Residual networks of residual networks: Multilevel residual networks. Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interleaved group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02758</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">We provide more details for the variants of ResNeXt-50 in Table 3 in the main body of the paper. Compared with this baseline</title>
		<imprint/>
	</monogr>
	<note>ResNeXt-50, wider&quot; has 1 16 more channels in all bottleneck blocks. ResNeXt-56, deeper&quot; has extra 2 blocks in the end of the fourth stage of ResNeXt-50</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">has a cardinality of 36 instead of 32. These three structures match the overall complexity of SKNet-50, which makes the comparisons fair</title>
		<idno>ResNeXt-50 (36×4d</idno>
		<imprint/>
	</monogr>
	<note>Method #P Top-1 error (%)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Table S9. The top-1 error rates (%) on the ImageNet validation set with single 224×224 crop testing</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
