<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Sengupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ignas Budvytis</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>SENGUPTA ET AL.: SYNTHETIC TRAINING FOR 3D HUMAN POSE AND SHAPE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of monocular 3D human shape and pose estimation from an RGB image. Despite great progress in this field in terms of pose prediction accuracy, state-of-the-art methods often predict inaccurate body shapes. We suggest that this is primarily due to the scarcity of in-the-wild training data with diverse and accurate body shape labels. Thus, we propose STRAPS (Synthetic Training for Real Accurate Pose and Shape), a system that utilises proxy representations, such as silhouettes and 2D joints, as inputs to a shape and pose regression neural network, which is trained with synthetic training data (generated on-the-fly during training using the SMPL statistical body model) to overcome data scarcity. We bridge the gap between synthetic training inputs and noisy real inputs, which are predicted by keypoint detection and segmentation CNNs at test-time, by using data augmentation and corruption during training. In order to evaluate our approach, we curate and provide a challenging evaluation dataset for monocular human shape estimation, Sports Shape and Pose 3D (SSP-3D). It consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters, obtained via multi-frame optimisation. We show that STRAPS outperforms other state-of-the-art methods on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets and metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human shape and pose estimation from a single RGB image is a challenging computer vision problem, with widespread applications in computer animation and augmented reality. Recently, several deep-learning-based methods have been proposed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. Such methods provide impressive 3D pose reconstructions given single RGB images as inputs, by leveraging datasets of images of humans in a diverse range of labelled 3D poses <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. However, these approaches often predict inaccurate body shapes, as shown in <ref type="figure">Figure 1</ref>. We suggest that this is due to a lack of body shape diversity within the prevalent training datasets. Most learning-based models will struggle to generalise to unseen test data if the distribution of the test data is significantly different from the training c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. arXiv:2009.10013v2 [cs.CV] 22 Sep 2020 <ref type="figure">Figure 1</ref>: STRAPS predicts body shapes with greater accuracy than other approaches to monocular human 3D shape and pose estimation, such as SPIN <ref type="bibr" target="#b14">[15]</ref>, CMR <ref type="bibr" target="#b15">[16]</ref> and HMR <ref type="bibr" target="#b7">[8]</ref>, without requiring training images annotated with 3D labels. The images shown in this figure are part of the dataset we provide, SSP-3D. data distribution. Thus, increasingly inaccurate body shapes are predicted as the shape of the test subject is further removed from the training datasets' mean shape.</p><p>In this paper, we present Synthetic Training for Real Accurate Pose and Shape (STRAPS), a deep-learning-based framework that uses synthetic training data to overcome the lack of shape diversity in current datasets. Given an input image, inference occurs in two stages (see <ref type="figure" target="#fig_0">Figure 2a</ref>). First, we predict a proxy representation, which encodes the subject's silhouette and 2D joint locations, using off-the-shelf segmentation and 2D keypoint detection CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref>. Then, we use a neural network regressor to predict the parameters of a statistical body model (SMPL <ref type="bibr" target="#b19">[20]</ref>) from the proxy representation. The regressor is trained using synthetic input-target pairs generated on-the-fly during model training. This is done by sampling target SMPL shape and pose parameters from a training distribution and rendering the corresponding silhouettes and 2D joints, which act as inputs. Since we can choose the form of the training distribution, we have control over the diversity of human body shapes seen by the regressor during training. We utilise simple data augmentation and corruption techniques (see <ref type="figure" target="#fig_0">Figure 2a</ref>) to make our regressor robust to noisy inputs encountered at test-time.</p><p>Moreover, we curate and provide Sports Shape and Pose 3D (SSP-3D), a dataset which contains images of tightly-clothed sports-persons with a diverse range of body shapes in varied environments, obtained from the Sports-1M video dataset <ref type="bibr" target="#b8">[9]</ref>. We use multi-frame optimisation, with forced shape consistency between frames, to obtain pseudo-ground-truth SMPL shape and pose parameters for the sports-person in each image. We evaluate our neural network regressor, along with several recent learning-based approaches, on SSP-3D and report shape prediction accuracy in terms of per-vertex Euclidean error in a neutral pose. Examples from SSP-3D are shown in <ref type="figure" target="#fig_1">Figure 3</ref>, along with statistics illustrating the greater body shape diversity in SSP-3D compared to widely-used 3D human datasets.</p><p>In summary, we have two main contributions: (i) a deep-learning framework which uses synthetic training data and simple data augmentation techniques to overcome the lack of body shape diversity within prevalent datasets and (ii) the SSP-3D evaluation dataset, which we use to show that our neural network regressor results in better shape prediction accuracy than other competing approaches. Our code and dataset are available for research purposes at https://github.com/akashsengupta1997/STRAPS-3DHumanShapePose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss recent approaches to 3D human pose and shape estimation, as well as the training datasets typically used for this task.</p><p>Monocular 3D human pose and shape estimation approaches can be classified into two paradigms: optimisation-based and learning-based. Optimisation-based approaches attempt to fit a parametric body model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> to 2D observations, such as 2D joints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>, body surface landmarks <ref type="bibr" target="#b16">[17]</ref>, silhouettes <ref type="bibr" target="#b16">[17]</ref> or body part segmentations <ref type="bibr" target="#b32">[33]</ref>. These approaches produce reliable results without requiring 3D-labelled datasets, which are expensive to obtain. However, they are slow at test-time, sensitive to initialisation and can get stuck in bad local minima, which motivates learning-based approaches.</p><p>Learning-based approaches can be further divided into two types: non-parametric 3D regression and body model parameter regression. Non-parametric 3D regression involves predicting a 3D human body representation from an image, such as a voxel occupancy grid <ref type="bibr" target="#b28">[29]</ref> or vertex mesh <ref type="bibr" target="#b15">[16]</ref>. However, each representation has associated drawbacks for body shape prediction: e.g. voxels are limited by the resolution of the voxel grid and direct mesh predictions can result in surface artifacts such as wrinkles and sharp protrusions. Body model parameter regression involves predicting the parameters of a statistical body model, such as SMPL <ref type="bibr" target="#b19">[20]</ref>, which provides a useful prior over body shape. Several approaches first predict a proxy representation from the input RGB image, such as surface keypoints <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, silhouettes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>, body part segmentations <ref type="bibr" target="#b21">[22]</ref> or IUV maps <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, and use this representation as the input to a regressor. Other approaches directly predict body model parameters from the input image <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>. Fundamentally, learning-based approaches are dependent on the label accuracy and sample diversity of the training datasets used. This results in a significant drawback when the training data distribution is not sufficiently diverse in terms of body shape, pose and image (e.g. background) conditions, as discussed below. 3D human pose and shape datasets. Learning-based approaches are trained using datasets of images paired with labels in the form of 3D joints or body model parameters. 3D labels may be obtained using motion capture (as for Human3.6M <ref type="bibr" target="#b6">[7]</ref> and BML MoVi <ref type="bibr" target="#b2">[3]</ref>), using inertial motion units (as for 3DPW <ref type="bibr" target="#b29">[30]</ref>), or by optimisation (as for UP3D <ref type="bibr" target="#b16">[17]</ref>). While current datasets contain varied and accurate 3D poses, they all suffer from limited body shape diversity, which greatly hampers the shape prediction accuracy of learning-based approaches. Additional drawbacks include: baggy clothing obscuring body shape and data captured in indoor MoCap environments being unrepresentative of in-the-wild images. We overcome these limitations of current training datasets by using synthetic training data. Furthermore, we create our own in-the-wild dataset, SSP-3D, to evaluate monocular 3D body shape predictions, which contains subjects with a greater variety of body shapes than current datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we describe STRAPS, our framework which utilises synthetic training data to overcome the lack of body shape diversity in real datasets. We also detail the multi-frame optimisation procedure used to create SSP-3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">STRAPS</head><p>The proposed synthetic training process has two parts: synthetic data generation and neural encoder and regressor training, both of which use a parametric 3D body model. Parametric 3D body model. The SMPL <ref type="bibr" target="#b19">[20]</ref> body model provides a fully-differentiable function M(θ θ θ , β β β ) that takes shape-space coefficients β β β and 3D joint rotations θ θ θ as inputs and outputs a human vertex mesh v ∈ R N×3 . 3D joint locations are obtained as a linear Synthetic training data is generated by sampling SMPL <ref type="bibr" target="#b19">[20]</ref> pose and shape parameters and decoding them into 3D vertices and joints, which are projected, rendered and corrupted to form an input proxy representation. The proxy representation is passed through an encoder and iterative regressor (both with trainable weights) that predicts pose, shape and camera parameters. Supervision signals are applied to SMPL parameters, 3D joints and vertices, and 2D joints. At test-time, off-the-shelf detection and segmentation CNNs are used to create the input proxy representation. Optimisation for SSP-3D involves fitting SMPL to multi-frame 2D joints and silhouettes of a subject, which yields optimised pose and camera parameters for each frame and shape parameters for the subject.</p><p>combination of the vertices, j 3D = Jv, where J ∈ R L×N is a regression matrix for L joints of interest. Synthetic data generation. SMPL is used to generate training data on-the-fly (top of <ref type="figure" target="#fig_0">Figure  2a</ref>). In each iteration of the training loop, β β β and θ θ θ are sampled from any training dataset with SMPL parameters -paired images are not required. A camera translation vector t is sampled randomly, while camera intrinsics and rotation matrices, K and R, are fixed. To combat the insufficient body shape diversity in prevalent training datasets, we perform body shape augmentation by replacing β β β with a new random vector β β β , generated by sampling each shape parameter β n ∼ N (µ, σ 2 n ), where σ n is chosen (empirically) to provide greater body shape variance than current datasets. Then, the 3D vertices v and 3D joints j 3D corresponding to θ θ θ and β β β are perspective-projected and rendered <ref type="bibr" target="#b9">[10]</ref> into a silhouette S ∈ [0, 1] H×W and 2D joint locations j 2D ∈ R L×2 . j 2D is transformed into 2D Gaussian joint heatmaps, G ∈ R H×W ×L , where each channel corresponds to a separate joint location. We obtain our clean synthetic proxy representation (PR), X ∈ R H×W ×(L+1) by concatenating S and G along the channel dimension. Note that we opt for simple silhouettes and 2D joints as our PR, instead of more complex part segmentations or IUV maps <ref type="bibr" target="#b3">[4]</ref>, because the synthetic-to-real domain gap is smaller for a simple representation, and can be more easily bridged with proxy representation augmentation during training. This involves modelling the failure modes of the off-the-shelf detection and segmentation CNNs used at test-time. In particular, noisy keypoint and silhouette predictions are modelled by adding uniform random noise to the 2D joint centres and silhouette edges in X. Occlusion is modelled by randomly removing body parts from and adding occluding boxes to the silhouette in X. The augmented PR X serves as the training input to our neural encoder. The training labels consist of θ θ θ , β β β , v, j 3D and j 2D . Neural encoder and regressor. STRAPS is architecture-agnostic. For this paper, we use the same network architecture as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>, which consists of a convolutional encoder for feature extraction and an iterative regressor that outputs predicted SMPL pose, shape and camera parameters (θ θ θ ,β β β andp) given these features. Note that <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref> implement additional modules, namely an adversarial prior <ref type="bibr" target="#b7">[8]</ref> or âȂIJin-the-loopâȂİ optimisation <ref type="bibr" target="#b14">[15]</ref>, necessitated by their use of training images with only 2D joint labels. However, 2D joints crucially fail to supervise 3D shape, unlike our strong 3D supervision, which also does not require such additional modules. Weak-perspective camera parameters are predicted during regression, represented byp = [ŝ,t], where s ∈ R represents scale andt ∈ R 2 represents x-y camera translation. We use a continuous 6-dimensional rotation representation forθ θ θ , as proposed by <ref type="bibr" target="#b34">[35]</ref>, instead of the discontinuous Euler rotation vectors used by SMPL as default. Fromθ θ θ andβ β β , SMPL is used to obtain predicted 3D vertices and joints,v andĵ 3D . Finally, projected 2D joint predictions are obtained byĵ 2D = sΠ(ĵ 3D +t), where Π represents an orthographic projection.</p><p>We train our network using a combination of 5 loss functions in a highly-multi-task framework. We use homoscedastic uncertainty <ref type="bibr" target="#b10">[11]</ref> to adaptively learn the loss weights during training, which results in an objective function of the form</p><formula xml:id="formula_0">L = 1 σ 2 β L β + 1 σ 2 θ L θ + 1 σ 2 v L v + 1 σ 2 j 3D L j 3D + 1 σ 2 j 2D L j 2D + log(σ β σ θ σ v σ j 3D σ j 2D ),<label>(1)</label></formula><p>where the σ 2 terms represent task uncertainties and the L terms represent mean squared error losses. Empirically, we found that redundancy in the multi-task objective -e.g. applying losses on SMPL parameters as well as on 3D vertices, despite v being fully determined by (θ θ θ , β β β ) -improved both network convergence and final performance. We hypothesise that this is because each supervision signal has a different granularity. For instance, a loss on vertices provides a finer-scale supervision signal than a loss on 3D joints. Thus, we apply mean squared error losses on 3D joints (L j 3D ), 3D vertices (L v ), SMPL pose parameters in the 6D rotation representation of <ref type="bibr" target="#b34">[35]</ref> (L θ ), and SMPL shape coefficients (L β ). We employ an additional loss on projected 2D joints (L j 2D ) to enforce image-model alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SSP-3D</head><p>SSP-3D contains 311 in-the-wild images of 62 tightly-clothed sports-persons (selected from the Sports-1M video dataset <ref type="bibr" target="#b8">[9]</ref>) with a diverse range of body shapes, along with corresponding pseudo-ground-truth SMPL shape and pose labels. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the greater body shape diversity in SSP-3D compared to Human3.6M <ref type="bibr" target="#b6">[7]</ref>, 3DPW <ref type="bibr" target="#b29">[30]</ref> and MoVi <ref type="bibr" target="#b2">[3]</ref>. Note that Human3.6M and MoVi are not in-the-wild and have homogeneous backgrounds. SMPL shape and pose labels were acquired via optimisation, using an extended version of SMPLify <ref type="bibr" target="#b1">[2]</ref> in a similar manner to the UP-3D dataset <ref type="bibr" target="#b16">[17]</ref>. Unlike <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b16">[17]</ref>, we used multiple frames of the same subject in parallel, obtaining different optimised poses θ θ θ * n and camera translations t * n for each frame n, but forcing the optimised shape β β β * to be the same across all frames to exploit multi-view information. We used 2D joints, acquired using Keypoint-RCNN <ref type="bibr" target="#b5">[6]</ref>, and pixel-accurate silhouettes, acquired using PointRend <ref type="bibr">[</ref>  Note that β 2 is strongly correlated with variation in body fat content. β 1 is not used because it is strongly correlated with overall size, which is ambiguous in monocular predictions.</p><p>FPN <ref type="bibr" target="#b17">[18]</ref>, as the target 2D observations into which we fit the SMPL model. The use of multiframe silhouettes ensures that SSP-3D has significantly more accurate body shape labels than UP-3D. To prevent getting stuck in bad local minima, we obtained an initialisation for perframe pose and camera parameters, θ θ θ init n and t init n , and shape coefficients, β β β init , by using VIBE <ref type="bibr" target="#b13">[14]</ref>, a method for SMPL prediction from video (see <ref type="figure" target="#fig_1">Figure 3a</ref>). Suitable frames for optimisation, with good SMPL initialisations and accurate target silhouettes and 2D joints, were hand-picked by human annotators. Our objective function is the sum of 6 error terms:</p><formula xml:id="formula_1">E(β β β , {θ θ θ n , t n } N n=1 ) = λ j E j + λ S E S + λ a E a + λ θ E θ + λ β E β + λ θ init E θ init ,<label>(2)</label></formula><p>where N is the number of frames and the λ terms represent weights. The silhouette error term, E S , penalises the L 1 difference between target and SMPL silhouettes. It is defined as</p><formula xml:id="formula_2">E S (β β β , {θ θ θ n , t n } N n=1 ; {S n } N n=1 ) = 1 N N ∑ n=1 Ŝ (Π K (v n (β β β , θ θ θ n ) + t n )) − S n 1 W H ,<label>(3)</label></formula><p>where W, H are the width and height of the target silhouettes S n andv n (β β β , θ θ θ n ) represents the SMPL vertices for the n-th frame. Π K () is a perspective-projection with intrinsic camera parameters K. Neural Mesh Renderer <ref type="bibr" target="#b9">[10]</ref> is used to differentiably render SMPL silhouetteŝ S n from projected vertices.</p><p>E θ init is a pose regularisation term, which penalises the L 2 distance between the current and initial estimates (from VIBE <ref type="bibr" target="#b13">[14]</ref>) of the SMPL pose parameters in rotation matrix form. We observed that the optimiser would use perspective effects to fit the SMPL model to target silhouettes of large persons, instead of updating the shape parameters. For example, the global rotation parameters would be updated to make the SMPL body lean towards the camera, enlarging the rendered silhouette. E θ init was incorporated to prevent such effects. It is defined as</p><formula xml:id="formula_3">E θ init ({θ θ θ n } N n=1 ; {θ θ θ init n } N n=1 ) = 1 N N ∑ n=1 r(θ θ θ n ) − r(θ θ θ init n ) 2 2 |r(θ θ θ n )| ,<label>(4)</label></formula><p>where r(θ θ θ n ) ∈ R 216 represents the vector of flattened and concatenated rotation matrices (for each of the 24 SMPL joints) corresponding to θ θ θ n . E j , E a , E θ and E β are derived from SMPLify and full definitions can be found in <ref type="bibr" target="#b1">[2]</ref>. In short, E j is a weighted 2D joint reprojection error, E a is an angle prior term which penalises unnatural bending of the elbow and knees, E θ is the negative log-likelihood of a Gaussian mixture model pose prior and E β is a L 2 regularisation penalty upon shape parameters.</p><p>We optimise our objective function using the Adam <ref type="bibr" target="#b11">[12]</ref> optimiser with a learning rate of 0.01. After convergence, a human annotator selects good SMPL fits. Details on the human annotation, as well as all hyperparameter values, are available in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>Training datasets. To generate synthetic training data, we sample SMPL pose parameters from the training sets of UP-3D <ref type="bibr" target="#b16">[17]</ref> and 3DPW <ref type="bibr" target="#b29">[30]</ref>, and from Human3.6M <ref type="bibr" target="#b6">[7]</ref> subjects S1, S5, S6, S7 and S8 (after applying MoSh <ref type="bibr" target="#b18">[19]</ref> to obtain SMPL poses from 3D joint labels). For our baseline experiments without shape augmentation, we also use the SMPL shape parameters from these datasets. Synthetic silhouettes are cropped and resized to 256 × 256. Evaluation datasets. We report metrics on Human3.6M (Protocol 2 <ref type="bibr" target="#b7">[8]</ref> subjects S9, S11), 3DPW (test set), BML-MoVi <ref type="bibr" target="#b2">[3]</ref> (F-PG1 videos) and SSP-3D. For Human3.6M and 3DPW, we report mean per joint position error after rigid alignment with Procrustes analysis (MPJPE-PA <ref type="bibr" target="#b20">[21]</ref>). For BML-MoVi and SSP-3D, we report scale-corrected per-vertex Euclidean error in a neutral pose (or T-pose), i.e. PVE-T-SC. A description of the scale-correction technique used to combat scale ambiguity is given in the supplementary material. We also report silhouette mean intersection-over-union on SSP-3D. Architecture. We use a ResNet-18 <ref type="bibr" target="#b4">[5]</ref> encoder, the output of which is average pooled, producing a feature vector φ φ φ ∈ R 512 . The iterative regression network consists of two fully connected layers with 512 neurons each, followed by an output layer with 157 neurons. We use the Adam <ref type="bibr" target="#b11">[12]</ref> optimiser to train our encoder and regressor, with a learning rate of 0.0001 and a batch size of 140. We train for 240 epochs, which takes 5 days on a single 2080Ti GPU. During inference, 2D joint predictions are obtained using Keypoint-RCNN <ref type="bibr" target="#b5">[6]</ref> and silhouette predictions are obtained using DensePose <ref type="bibr" target="#b3">[4]</ref>. All implementations are in PyTorch <ref type="bibr" target="#b22">[23]</ref>. Inference runs at ∼4fps, 90% of which is silhouette and joint prediction.   <ref type="figure">Figure 4</ref>: Ablation study. (a) illustrates that applying shape and proxy representation (PR) augmentation improves predictions of non-typical body shapes and develops robustness against noisy inputs. (b) reports pose (MPJPE-PA in mm) and shape (PVE-T-SC in mm) metrics when using synthetic proxy representations (rendered from ground-truth SMPL labels) versus predicted proxy representations (from DensePose and Keypoint-RCNN) as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>In this section, we present results from our ablative study, which investigates the effects of shape and proxy representation augmentation during synthetic training. We also compare our method to other approaches in terms of shape and pose accuracy. ---75.9 -HMR (unpaired) <ref type="bibr" target="#b7">[8]</ref> 20  Ablation studies. Our ablative study investigates the effects of shape and proxy representation (PR) augmentation applied during synthetic training. We compare four networks, trained with: (i) no augmentation (baseline), (ii) only shape augmentation, (iii) only PR augmentation and (iv) shape + PR augmentation. Evaluations are carried out with two types of input proxy representations: synthetic silhouettes and 2D joints generated from GT SMPL labels and "real" silhouettes and 2D joints predicted from test RGB images using DensePose Results from SPIN <ref type="bibr" target="#b14">[15]</ref>, CMR <ref type="bibr" target="#b15">[16]</ref> and HMR <ref type="bibr" target="#b7">[8]</ref> are shown for comparison. Our method is able to accurately predict a diverse range of body shapes, whereas other approaches are biased towards an average body shape prediction.</p><p>[4] and Keypoint-RCNN <ref type="bibr" target="#b5">[6]</ref> respectively. SSP-3D evaluates 3D shape prediction across a diverse range of body shapes, while Human 3.6M and 3DPW evaluate 3D pose prediction. The quantitative performance of the baseline network on GT synthetic inputs (see <ref type="figure">Figure</ref> 4b first row) motivates the use of synthetic training data, since, in this ideal case, it achieves greater than SOTA accuracy (compare with <ref type="figure" target="#fig_4">Figure 5b</ref>). However, in the practicallyapplicable situation using "real" inputs, the baseline network has two key failure modes: firstly, the predicted body shape is inaccurate, particularly for non-typical subjects (see <ref type="figure">Figure</ref> 4a, top row) and secondly, the network becomes reliant on the perfectly-rendered synthetic inputs and is unable to deal with noisy real inputs. Incorporating shape augmentation alleviates the first problem, since the network sees a greater variety of shapes during training. However, the second problem is greatly exacerbated, particularly in cases with occluded silhouettes (see <ref type="figure">Figure 4a</ref>, bottom row). Hence, the network trained with shape augmentation results in better shape (and pose) metrics on synthetic inputs compared to the baseline, as shown in <ref type="figure">Figure 4b</ref>, while the metrics on real inputs are poor. Incorporating PR augmentation shrinks the performance deterioration when using real versus synthetic inputs by explicitly modelling input noise and occlusion during the synthetic training process. By combining PR and shape augmentation, we are able to predict a diverse range of body shapes, improve our pose accuracy significantly over the baseline and produce semantically-plausible outputs on all datasets, even when the input is heavily corrupted (see <ref type="figure">Figure 4a</ref>). Comparison with the state-of-the-art. Our method, with shape and PR augmentation, surpasses the state-of-the-art in terms of PVE-T-SC and mIOU on SSP-3D and MoVi. The dis-  <ref type="figure" target="#fig_4">Figure 5a</ref>, suggests that our method is able to maintain shape prediction accuracy for challenging evaluation samples while the performance of competing approaches degrades for samples featuring non-average body shapes, qualitative examples of which are given in <ref type="figure" target="#fig_5">Figure 6</ref>. Our method may give erroneous reconstructions for outlier body shapes, in which case DensePose fails to predict an accurate silhouette, or due to poses with substantial self-occlusion, as shown in <ref type="figure" target="#fig_5">Figure 6</ref> bottom row.</p><p>Although we focus on shape prediction, our method is competitive with the SOTA on H3.6M and 3DPW in terms of MPJPE-PA and outperforms other methods that do not require training data comprised of images paired with expensive-to-obtain 3D labels. Qualitative examples are given in <ref type="figure" target="#fig_6">Figure 7</ref>. We observe that we perform relatively better on 3DPW than H3.6M, as compared to other methods, particularly up to the median error ( <ref type="figure" target="#fig_4">Figure  5a</ref>). This is because methods trained on images captured in an indoor MoCap environment (like H3.6M) do not maintain the same pose prediction accuracy for test images with unconstrained background and lighting conditions (like in 3DPW). We create our input proxy representation using 2D segmentation and detection CNNs, which are more easily trained to be invariant to such variables. Thus, we match or surpass the SOTA on 3DPW for samples up to the median MPJPE-PA. However, 3DPW contains samples with severe occlusion (beyond what is modelled by PR augmentation) and overlapping persons, which cause DensePose to predict erroneous silhouettes and results in worse MPJPE-PA in the 75-100% quartile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we addressed the problem of monocular 3D human shape and pose estimation. In particular, we observed that current approaches often predict inaccurate body shapes, particularly for non-typical subjects, due to a lack of body shape diversity in prevalent 3D human datasets. Thus, we proposed STRAPS, a learning framework that overcomes the lack of diversity by generating synthetic training data with diverse body shapes on-the-fly, such that the regressor sees a new body shape at every training iteration. To evaluate our approach, we created a challenging evaluation dataset for monocular human shape estimation, SSP-3D, which consists of RGB images of tightly-clothed sports-persons with a variety of body shapes and corresponding pseudo-ground-truth SMPL shape and pose parameters. We showed that STRAPS outperforms other approaches on SSP-3D in terms of shape prediction accuracy, while remaining competitive with the state-of-the-art on pose-centric datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) STRAPS Training and Inference (b) SSP-3D Optimisation (a) Overview of the training and inference pipelines for STRAPS and (b) optimisation pipeline for SSP-3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>SSP-3D samples and statistics. (a) shows RGB images and corresponding optimised SMPL body shape and pose labels provided in the SSP-3D dataset. It also illustrates the improvement in shape and pose parameter estimates after optimisation, compared to initial estimates obtained with VIBE<ref type="bibr" target="#b13">[14]</ref>. (b) and (c) demonstrate the greater body shape diversity in SSP-3D compared to widely-used datasets, by considering the distribution of the 2nd and 3rd shape coefficient labels β 2 and β 3 . (b) plots β 2 and β 3 for each sample in each dataset. (c) gives the number of subjects and the variance of β 2 and β 3 labels in each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Quantitative comparison with the SOTA on SSP-3D, MoVi, Human3.6M (Protocol 2) and 3DPW. We report PVE-T-SC (mm) and mIOU on shape-centric datasets SSP-3D and MoVi and MPJPE-PA (mm) on pose-centric datasets 3DPW and Human3.6M. (a) plots the (sorted) distributions of metrics per evaluation sample. (b) lists mean metrics over all samples. Methods in the top part of (b) do not require training data comprised of images paired with 3D ground truth, while methods in the bottom part do. Numbers marked with * were evaluated for this paper, all other numbers are reported by the respective papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison on SSP-3D. Each row shows examples from different PVE-T-SC quartiles (for our method), top to bottom: 0-25%, 25-50%, 50-75%, 75-100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on Human3.6M (left), 3DPW (middle) and MoVi (right). Each row shows examples from different error metric quantiles (MPJPE-PA for H3.6M and 3DPW, PVE-T-SC for MoVi). Top to bottom: 0-33%, 33-66%, 66-100%. Input silhouettes and input 2D keypoints are visualised over each image. tribution of errors per SSP-3D sample, shown in</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SCAPE: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MoVi: A Large Multipurpose Motion and Video Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimia</forename><surname>Mahdaviani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">James</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Blohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<idno type="DOI">10.5683/SP2/JRHDRN</idno>
		<ptr target="https://doi.org/10.5683/SP2/JRHDRN" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Riza Alp Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-toend recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural 3D mesh renderer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">VIBE: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the loop between 3D and 2D human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model-based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga ; Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Alban Desmaison</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">A A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3D human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3D human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Vince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">De-tectron2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes -the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Danet: Decompose-and-aggregate network for 3D human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jingwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jimei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
