<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation from a Single Image via Distance Matrix Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Robòtica</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Informàtica Industrial (CSIC-UPC)</orgName>
								<address>
									<postCode>08028</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation from a Single Image via Distance Matrix Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D position of the N body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2N -to-3N regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by representing both the 2D and 3D human poses using N × N distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted matrices. The approach has also the advantage to naturally handle missing observations and allowing to hypothesize the position of non-observed joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D human pose from a single RGB image is known to be a severally ill-posed problem, because many different body configurations can virtually have the same projection. A typical solution consists in using discriminative strategies to directly learn mappings from image evidence (e.g. HOG, SIFT) to 3D poses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref>. This has been recently extended to end-to-end mappings using CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. In order to be effective, though, these approaches require large amounts of training images annotated with the ground truth 3D pose. While obtaining this kind of data is straightforward for 2D poses, even for images 'in the wild' (e.g. FLIC <ref type="bibr" target="#b33">[34]</ref> or LSP <ref type="bibr" target="#b17">[18]</ref> datasets), it requires using sophisticated motion capture systems for the 3D case. Additionally, the datasets acquired this way (e.g. Humaneva <ref type="bibr" target="#b35">[36]</ref>, Human3.6M <ref type="bibr" target="#b16">[17]</ref>) are mostly indoors and their images are not representative of the type of image ap-pearances outside the laboratory.</p><p>It seems therefore natural to split the problem in two stages: Initially use robust image-driven 2D joint detectors, and then infer the 3D pose from these image observations using priors learned from mocap data. This pipeline has already been used in a number of works <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref> and is the strategy we consider in this paper. In particular, we first estimate 2D joints using a recent CNN detector <ref type="bibr" target="#b49">[50]</ref>. For the second stage, however, most previous methods perform the 2D-to-3D inference in Cartesian space, between 2N -and 3N -vector representations of the N body joints. In contrast, we propose representing 2D and 3D poses using N × N matrices of Euclidean distances between every pair of joints, and formulate the 3D pose estimation problem as one of 2D-to-3D distance matrix regression <ref type="bibr" target="#b0">1</ref>  <ref type="figure">. Fig. 1</ref>, illustrates our pipeline.</p><p>Despite being extremely simple to compute, Euclidean Distance Matrices (EDMs) have several interesting advantages over vector representations that are particularly suited for our problem. Concretely, EDMs: 1) naturally encode structural information of the pose. Inference on vector representations needs to explicitly formulate such constraints; 2) are invariant to in-plane image rotations and translations, and normalization operations bring invariance to scaling; 3) capture pairwise correlations and dependencies between all body joints.</p><p>In order to learn a regression function that maps 2D-to-3D EDMs we consider Fully Connected (FConn) and Fully Convolutional (FConv) Network architectures. Since the dimension of our data is small (N × N square matrices, with N = 14 joints in our model), input-to-output mapping can be achieved through shallow architectures, with only 2 hidden layers for the FConn and 4 convolutional layers for the FConv. And most importantly, since the distance matrices used to train the networks are built from solely point configurations, we can easily synthesize artifacts and train the network under 2D detector noise and body part occlusion.</p><p>We achieve state-of-the-art results on standard benchmarks including Humaneva-I and Human3.6M datasets, and we show our approach to be robust to large 2D detector er- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Approaches to estimate 3D human pose from single images can be roughly split into two main categories: methods that rely on generative models to constrain the space of possible shapes and discriminative approaches that directly predict 3D pose from image evidence.</p><p>The most straightforward generative model consists in representing human pose as linear combinations of modes learned from training data <ref type="bibr" target="#b4">[5]</ref>. More sophisticated models allowing to represent larger deformations include spectral embedding <ref type="bibr" target="#b41">[42]</ref>, Gaussian Mixtures on Euclidean or Riemannian manifolds <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref> and Gaussian processes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>. However, exploring the solution space defined by these generative models requires iterative strategies and good enough initializations, making these methods more appropriate for tracking purposes.</p><p>Early discriminative approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref> focused on directly predicting 3D pose from image descriptors such as SIFT of HOG filters, and more recently, from rich features encoding body part information <ref type="bibr" target="#b15">[16]</ref> and from the entire image in Deep architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. Since the mapping between feature and pose space is complex to learn, the success of this family of techniques depends on the existence of large amounts of training images annotated with ground truth 3D poses. Humaneva <ref type="bibr" target="#b35">[36]</ref> and Hu-man3.6M <ref type="bibr" target="#b16">[17]</ref>, are two popular MoCap datasets used for this purpose. However, these datasets are acquired in laboratory conditions, preventing the methods that uniquely use their data, to generalize well to unconstrained and realistic images. <ref type="bibr" target="#b32">[33]</ref> addressed this limitation by augmenting the training data for a CNN with automatically synthesized images made of realistic textures.</p><p>Lying in between the two previous categories, there are a series of methods that first use discriminative formulations to estimate the 2D joint position, and then infer 3D pose using e.g. regression forests, Expectation Maximization or evolutionary algorithms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>. The two steps can be iteratively refined <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b48">49]</ref> or formulated independently <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39]</ref>. By doing this, it is then possible to exploit the full power current CNN-based 2D detectors like DeepCut <ref type="bibr" target="#b27">[28]</ref> or the Convolutional Pose Machines (CPMs) <ref type="bibr" target="#b49">[50]</ref>, which have been trained with large scale datasets of images 'in-the-wild'.</p><p>Regarding the 3D body pose parameterization, most approaches use a skeleton with a number N of joints ranging between 14 and 20, and represented by 3N vectors in a Cartesian space. Very recently, <ref type="bibr" target="#b9">[10]</ref> used a generative volumetric model of the full body. In order to enforce joint dependency during the 2D-to-3D inference, <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b44">[45]</ref> considered latent joint representations, obtained through Kernel Dependency Estimation and autoencoders. In this paper, we propose using N × N Euclidean Distance Matrices for capturing such joint dependencies.</p><p>EDMs have already been used in similar domains, e.g. in modal analysis to estimate shape basis <ref type="bibr" target="#b1">[2]</ref>, to represent protein structures <ref type="bibr" target="#b19">[20]</ref>, for sensor network localization <ref type="bibr" target="#b6">[7]</ref> and for the resolution of kinematic constraints <ref type="bibr" target="#b28">[29]</ref>. It is worth to point that for 3D shape recognition tasks, Geodesic Distance Matrices (GDMs) are preferred to EDMs, as they are invariant to isometric deformations <ref type="bibr" target="#b40">[41]</ref>. Yet, for the same reason, GDMs are not suitable for our problem, because multiple shape deformations yield the same GDM. In contrast, the shape that produces a specific EDM is unique (up to translation, rotation and reflection), and it can be estimated via Multidimensional Scaling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Finally, representing 2D and 3D joint positions by distance matrices, makes it possible to perform inference with simple Neural Networks. In contrast to recent CNN based methods for 3D human pose estimation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref> we do not need to explicitly modify our networks to model the underlying joint dependencies. This is directly encoded by the distance matrices.  <ref type="figure">Fig. 1</ref> illustrates the main building blocks of our approach to estimate 3D human pose from a single RGB image. Given that image, we first detect body joints using a state-of-the-art detector. Then, 2D joints are normalized and represented by a EDM, which is fed into a Neural Network to regress a EDM for the 3D body coordinates. Finally, the position of the 3D joints is estimated via a 'reflexion-aware' Multidimensional Scaling approach <ref type="bibr" target="#b6">[7]</ref>. We next describe in detail each of these steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We represent the 3D pose as a skeleton with N =14 joints and parameterized by a 3N vector y = [p 1 , . . . , p N ] , where p i is the 3D location of the i-th joint. Similarly, 2D poses are represented by 2N vectors x = [u 1 , . . . , u N ] , where u i are pixel coordinates. Given a full-body person image, our goal is to estimate the 3D pose vector y. For this purpose, we follow a regression based discriminative approach. The most general formulation of this problem would involve using a set of training images to learn a function that maps input images, or its features, to 3D poses. However, as discussed above, such a procedure would require a vast amount of data to obtain good generalization.</p><p>Alternatively, we will first compute the 2D joint position using the Convolutional Pose Machine detector <ref type="bibr" target="#b49">[50]</ref>. We denote byx the output of the CPM, which is a noisy version of the ground truth 2D pose x. We also contemplate the possibility that some entries ofx are not observed due to joint occlusions or mis-detections. In order to not to change the dimension ofx, the entries corresponding to these nonobserved joints will be set to zero.</p><p>We can then formally write our problem as that of learning a mapping function f : R 2N → R 3N from potentially corrupted 2D joint observationsx to 3D poses y, given an annotated and clean training dataset</p><formula xml:id="formula_0">{x i , y i } D i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Representing Human Pose with EDMs</head><p>In oder to gain depth-scale invariance we first normalize the vertical coordinates of the projected 2D poses x i to be within the range [−1, 1]. 3D joint positions y i are expressed in meters with no further pre-processing. We then represent both 2D and 3D poses by means of Euclidean Distance Matrices. For the 3D pose y we define edm(y) to be the N ×N matrix where its (m, n) entry is computed as:</p><formula xml:id="formula_1">edm(y) m,n = p m − p n 2 .<label>(1)</label></formula><p>Similarly, edm(x) is the N × N matrix built from the pairwise distances between normalized 2D joint coordinates. Despite being simple to define, EDMs have several advantages over Cartesian representations: EDMs are coordinate-free, invariant to rotation, translation and reflection. Previous regression-based approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b32">33]</ref> need to compensate for this invariance by pre-aligning the training 3D poses y i w.r.t. a global coordinate frame, usually defined by specific body joints. Additionally, EDMs do not only encode the underlying structure of plain 3D vector representations, but they also capture richer information about pairwise correlations between all body joints. A direct consequence of both these advantages, is that EDMbased representations allow reducing the inherent ambiguities of the 2D-to-3D human pose estimation problem.</p><p>To empirically support this claim we randomly picked pairs of samples from the Humaneva-I dataset and plotted the distribution of relative distances between their 3D and 2D poses, using either Cartesian or EDM representations (see <ref type="figure" target="#fig_0">Fig. 2</ref>). For the Cartesian case (left-most plot), an entry to the graph corresponds to</p><formula xml:id="formula_2">[dist(y i , y j ), dist(x i , x j )],</formula><p>where dist(·) is a normalized distance and i, j are two random indices. Similarly, for the EDMs (second plot), an entry to the graph corresponds to [dist(edm(y i ), edm(y j )), dist(edm(x i ), edm(x j ))]. Observe that 3D and 2D pairwise differences are much more correlated in this case. The interpretation of this pattern is that distance matrices yield larger 3D pose differences for most dissimilar 2D poses. The red circles in the graphs, correspond to the most ambiguous shapes, i.e., pairs of disimilar poses {y i , y j } with very similar image projections {u i , u j }. Note that when using EDMs, these critical samples depict larger differences along the vertical axis, i.e., on the 2D representation. This kind of behavior makes it easier the subsequent task of learning the 2D-to-3D mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">2D-to-3D Distance Matrix Regression</head><p>The problem formulated in Sec. 3.1 can now be rewritten in terms of finding the mapping f : R N ×N → R N ×N , from potentially corrupted distance matrices edm(x) to matrices edm(y) encoding the 3D pose, given a training set</p><formula xml:id="formula_3">{edm(x i ), edm(y i )} D i=1</formula><p>. The expressiveness and low dimensionality of the input and output data (14 × 14 matrices) will make it possible to learn this mapping with relatively tiny Neural Network architectures, which we next describe.</p><p>Fully Connected Network. Since distance matrices are symmetric, we first consider a simple FConn architecture with 40K free parameters that regresses the N (N − 1)/2 = 91 elements above the diagonal. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>-left, the network consists of three Fully Connected (FC) layers with 128-128-91 neurons. Each FC layer is followed by a rectified linear unit (ReLU). To reduce overfitting, we use dropout after the first two layers, with a dropout ratio of 0.5 (50% of probability to set a neuron's output value to zero).</p><p>The 91-dimensional vector at the output is used to build the 14 × 14 output EDM, which by construction, is guaranteed to be symmetric. Additionally, the last ReLU layer enforces positiveness of all elements of the matrix, another necessary (but not sufficient) condition for an EDM.</p><p>Fully Convolutional Network. Motivated by the recent success of Fully Convolutional Networks in tasks like semantic segmentation <ref type="bibr" target="#b23">[24]</ref>, flow estimation <ref type="bibr" target="#b12">[13]</ref> and change dectection <ref type="bibr" target="#b3">[4]</ref>, we also consider the architecture shown in <ref type="figure" target="#fig_1">Fig. 3</ref>-right to regress entire 14 × 14 distance matrices.</p><p>FConv Networks were originally conceived to map images or bi-dimensional arrays with some sort of spatial continuity. The EDMs however, do not convey this continuity and, in addition, they are defined up to a random permutation of the skeleton joints. In any event, for the case of human motion, the distance matrices turn to be highly structured, particularly when handling occluded joints, which results in patterns of zero columns and rows within the input EDM. In the experimental section we will show that FConv networks are also very effective for this kind of situations.</p><p>Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>, we explored an architecture with a contractive and an expansive part. The contractive part consists of two convolutional blocks, with 7×7 kernels and 64 features, each. Convolutional layers are followed by a Batch Normalization (BN) layer with learned parameters, relieving from the task of having to compute such statistics from data during test. BN output is forwarded to a non-linear ReLU; a 2 × 2 max-pooling layer with stride 2 that performs the actual contraction; and finally, to a dropout layer with 0.5 ratio.</p><p>The expansive part also has two main blocks which start with a deconvolution layer, that internally performs a ×2 upsampling and, again, a convolution with 7 × 7 kernels and 64 features. The deconvolution is followed by a ReLU and a dropout layer with ratio 0.5. For the second block, dropout is replaced by a convolutional layer that contracts the 64, 14 × 14 features into a single 14 × 14 channel.</p><p>Note that there are no guarantees that the output of the expansive part will be a symmetric and positive matrix, as expected for a EDM. Therefore, before computing the actual loss we designed a layer called 'Matrix Symmetrization' (MS) which enforces symmetry. If we denote by Z the output of the expansive part, MS will simply compute (Z+Z )/2, which is symmetric. A final ReLU layer, guarantees that all values will be also positive.</p><p>This Fully Convolutional Network has 606K parameters. Training. In the experimental section we will report results on multiple training setups. In all of them, the two networks were trained from scratch, and randomly initial-Walking (Action 1, Camera 1) Jogging (Action 2, Camera 1)</p><p>Boxing (Action 5, Camera 1) Method S1 S2 S3 Average S1 S2 S3 Average S1 S2 S3 Average  <ref type="table">Table 1</ref>. Results on the Humaneva-I dataset. Average error (in mm) between the ground truth and the predicted joint positions. '-' indicates that the results for that specific 'action' and 'subject' are not reported. The results of all approaches are obtained from the original papers, except for (*), which were obtained from <ref type="bibr" target="#b9">[10]</ref>.</p><p>ized using the strategy proposed in <ref type="bibr" target="#b13">[14]</ref>. We use a standard L2 loss function in the two cases. Optimization is carried out using Adam <ref type="bibr" target="#b18">[19]</ref>, with a batch size of 7 EDMs for Humaneva-I and 200 EDMs for Human3.6M. FConn generally requires about 500 epochs to converge and FConv about 1500 epochs. We use default Adam parameters, except for the step size α, which is initialized to 0.001 and reduced to 0.0001 after 250 (FConn) and 750 (FConv) epochs. The model definition and training is run under MatconvNet <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">From Distance Matrices to 3D Pose</head><p>Retrieving the 3D joint positions y = [p 1 , . . . , p N ] from a potentially noisy distance matrix edm(y) estimated by the neural network, can be formulated as the following error minimization problem:</p><formula xml:id="formula_4">arg min p1,...,p N m,n | p m − p n 2 2 − edm(y) 2 m,n | .<label>(2)</label></formula><p>We solve this minimization using <ref type="bibr" target="#b6">[7]</ref>, a MDS algorithm which poses a semidefinite programming relaxation of the non-convex Eq. 2, refined by a gradient descent method. Yet, note that the shape y we retrieve from edm(y) is up to a reflection transformation, i.e., y and its reflected version y * yield the same distance matrix. In order to disambiguate this situation, we keep either y or y * based on their degree of anthropomorphism, measured as the number of joints with angles within the limits defined by the physically-motivated prior provided by <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We extensively evaluate the proposed approach on two publicly available datasets, namely Humaneva-I <ref type="bibr" target="#b35">[36]</ref> and Human3.6M <ref type="bibr" target="#b16">[17]</ref>. Besides quantitative comparisons w.r.t. state-of-the-art we also assess the robustness of our approach to noisy 2D observations and joint occlusion. We furhter provide qualitative results on the LSP dataset <ref type="bibr" target="#b17">[18]</ref> Unless specifically said, we assume the 2D joint positions in our approach are obtained with the CPM detector <ref type="bibr" target="#b49">[50]</ref>, fed with a bounding box of the full-body person image. As common practice in literature <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b50">51]</ref>, the reconstruction error we report refers to the average 3D Euclidean joint error (in mm), computed after rigidly aligning the estimated shape with the ground truth (if available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on Humaneva-I</head><p>For the experiments with Humaneva-I we train our EDM regressors on the training sequences for the Subjects 1, 2 and 3, and evaluate on the 'validation' sequences. This is the same evaluation protocol used by the baselines we compare against <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10]</ref>. We report the performance on the 'Walking', 'Jogging' and 'Boxing' sequences. Regarding our own approach we consider several configurations depending on the type of regressor: Fully Connected or Fully Convolutional Network; and depending on the type of 2D source used for training: Ground Truth (GT), CPM or GT+CPM. The network is trained with pairs of occluded joints and is able to predict one occluded limb (2 neighboring joints) at a time. Note how the generated tracks highly resemble the ground truth ones.</p><p>Walking (Action 1, Camera 1)</p><p>Jogging (Action 2, Camera 1) Boxing (Action 5, Camera 1) NN Arch. Occl. Type Error S1 S2 S3 Average S1 S2 S3 Average S1 S2 S3 Average  <ref type="table">Table 2</ref>. Results on the Humaneva-I under Occlusions. Average overall joint error and average error of the occluded and hypothesized joints (in mm) using the proposed Fully Connected and Fully Convolutional regressors. We train the two architectures with 2D GT+CPM and with random pairs of occluded joints. Test is carried out using the CPM detections with specific occlusion configurations.</p><p>The 2D source used for evaluation is always CPM. <ref type="table">Table 1</ref> summarizes the results, and shows that all configurations of our approach significantly outperform stateof-the-art. The improvement is particularly relevant when modeling the potential deviations of the CPM by directly using its 2D detections to train the regressors. Interestingly, note that the results obtained by FConn and FConv are very similar, being the former a much simpler architecture. However, as we will next show, FConv achieves remarkably better performance when dealing with occlusions.</p><p>Robustness to Occlusions. The 3D pose we estimate obviously depends on the quality of the 2D CPM detections. Despite the CPM observations we have used do already contain certain errors, we have explicitly evaluated the robustness of our approach under artificial noise and occlusion artifacts. We next assess the impact of occlusions. We leave the study of the 2D noise for the following section.</p><p>We consider two-joint occlusions, synthetically produced by removing two nodes of the projected skeleton. In order to make our networks robust to such occlusions and also able to hypothesize the 3D position of the non-observed joints, we re-train them using pairs</p><formula xml:id="formula_5">{edm(x occ i ), edm(y i )} D i=1</formula><p>, where x occ i is the same as x i , but with two random entries set to zero. Note that this will make random rows and columns of edm(x occ i ) to be zero as well. At test, we consider the cases with random joint occlusions or with structured occlusions, in which we completely remove the observation of one body limb (full leg or full arm). <ref type="table">Table 2</ref> reports the reconstruction error for FConn and FConv. Overall results show a clear and consistent advantage of the FConv network, yielding error values which are even comparable to state-of-the-art approaches when observing all joints (see <ref type="table">Table 1</ref>). Furthermore, note that the error of the hypothesized joints is also within very reasonable bounds, exploiting only for the right arm position in the 'boxing' activity. This is in the line of previous works which have shown that the combination of convolutional and deconvolutional layers is very effective for image reconstruction and segmentation tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52]</ref>. A specific example of joint hallucination is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on Human3.6M</head><p>The Human3.6M dataset consists of 3.6 Million 3D poses of 11 subjects performing 15 different actions under 4 viewpoints. We found in the literature 3 different evaluation protocols. For Protocol #1, 5 subjects (S1, S5, S6, S7 and S8) are used for training and 2 (S9 and S11) for testing. Training and testing is done independently per each action and using all camera views. Testing is carried out in all images. This protocol is used in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55]</ref>. Protocol #2 only differs from Protocol #1 in that only the frontal view is considered for test. It has been recently used in <ref type="bibr" target="#b9">[10]</ref>, which also evaluates <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b53">54]</ref>  <ref type="table">Table 3</ref>. Results on the Human3.6M dataset. Average joint error (in mm) considering the 3 evaluation prototocols described in the text.</p><p>The results of all approaches are obtained from the original papers, except for (*), which are from <ref type="bibr" target="#b9">[10]</ref>.  <ref type="table">Table 5</ref>. Results on the Human3.6M dataset under 2D Noise. Average 3D joint error for increasing levels of 2D noise. The network is trained with 2D Ground Truth (GT) data and evaluated with GT+N (0.σ). where σ is the standard deviation (in pixels) of the noise.</p><p>#3, training data comprises all actions and viewpoints. Six subjects (S1, S5, S6, S7, S8 and S9) are used for training and every 64 th frame of the frontal view of S11 is used for testing. This is the protocol considered in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b32">33]</ref>. We will evaluate our approach on the three protocols. However, since detecting the 2D joints with CPM takes several seconds per frame, for Protocols #1 and #2 we will test on every 8 th frame. For the same reason CPM detections will no longer be used during training, and we will directly use the ground truth 2D positions. For Protocol #3 we choose the training set by randomly picking 400K samples among all poses and camera views, a similar number as in <ref type="bibr" target="#b50">[51]</ref>. In contrast to these works, no preprocessing is applied on the training set to maximize the pose variability. For the rest of experiments we will only consider the FConv regressor, which showed overall better performance than FConn in the Humaneva dataset.</p><p>The results on Human3.6M are summarized in <ref type="table">Table 3</ref>. For Protocols #1 and #3 our approach improves state-ofthe-art by a considerable margin, and for Protocol #2 is very similar to <ref type="bibr" target="#b9">[10]</ref>, a recent approach that relies on a highquality volumetric prior of the body shape.</p><p>Robustness to Occlusions. We perform the same occlu-sion analysis as we did for Humaneva-I and re-train the network under randomly occluded joints and test for random and structured occlusions. The results (under Protocol #3) are reported in <ref type="table" target="#tab_4">Table 4</ref>. Again, note that the average body error remains within reasonable bounds. There are, however, some specific actions (e.g. 'Sit', 'Photo') for which the occluded leg or arm are not very well hypothesized. We believe this is because in these actions, the limbs are in configurations with only a few samples on the training set. Indeed, state of the art methods also report poor performance on these actions, even when observing all joints.</p><p>Robustness to 2D Noise. We further analyze the robustness of our approach (trained on clean data) to 2D noise. For this purpose, instead of using CPM detections for test, we used the 2D ground truth test poses with increasing amounts of Gaussian noise. The results of this analysis are given in <ref type="table">Table 5</ref>. Note that the 3D error gradually increases with the 2D noise, but does not seem to break the system. Noise levels of up to 20 pixels std are still reasonably supported. As a reference, the mean 2D error of the CPM detections considered in <ref type="table" target="#tab_4">Tables 4 and 3</ref> is of 10.91 pixels. Note also that there is still room for improvement, as more precise 2D detections can considerably boost the 3D pose accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on Leeds Sports Pose Dataset</head><p>We finally explore the generalization capabilities of our approach on the LSP dataset. For each input image, we locate the 2D joints using the CPM detector, perform the 2D-to-3D EDM regression using the Fully Convolutional network learned on Human3.6M (Protocol #3) and compute the 3D pose through MDS. Additionally, once the 3D pose is estimated, we retrieve the rigid rotation and translation that aligns it with the input image using a PnP algorithm <ref type="bibr" target="#b24">[25]</ref>. Since the internal parameters of the camera are unknown, we sweep the focal length configuration space and keep the solution that minimizes the reprojection error.</p><p>The lack of 3D annotation makes it not possible to perform a quantitative evaluation of the 3D shapes accuracy. Instead, in <ref type="table">Table 6</ref>, we report three types of 2D reprojection errors per body part, averaged over the 2000 images of the dataset: 1) Error of the CPM detections; 2) Error of the reprojected shapes when estimated using CPM 2D detections; and 3) Error of the reprojected shapes when estimated using 2D GT annotations. While these results do not guarantee good accuracy of the estimated shapes, they are indicative that the method is working properly. A visual inspection of the 3D estimated poses, reveals very promising results, even for poses which do not appear on the Human3.6M dataset used for training (see <ref type="figure" target="#fig_3">Fig. 5</ref>). There still remain failure cases (shown on the right-most column), due to e.g. detector mis-  <ref type="table">Table 6</ref>. Reprojection Error (in pixels) on the LSP dataset.</p><p>detections, extremal body poses or camera viewpoints that largely differ from those of Human3.6M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we have formulated the 3D human pose estimation problem as a regression between matrices encoding 2D and 3D joint distances. We have shown that such matrices help to reduce the inherent ambiguity of the problem by naturally incorporating structural information of the human body and capturing joints correlations. The distance matrix regression is carried out by simple Neural Network architectures, that bring robustness to noise and occlusions of the 2D detected joints. In the latter case, a Fully Convolutional network has allowed to hypothesize unobserved body parts. Quantitative evaluation on standard benchmarks shows remarkable improvement compared to state of the art. Additionally, qualitative results on images 'in the wild' show the approach to generalize well to untrained data. Since distance matrices just depend on joint positions, new training data from novel viewpoints and shape configurations can be readily synthesized. In the future, we plan to explore online training strategies exploiting this.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>EDMs vs Cartesian representations. Left: Distribution of relative 3D and 2D distances between random pairs of poses, represented as Cartesian vectors (first plot) and EDM matrices (second plot). Cartesian representations show a more decorrelated pattern (Pearson correlation coefficient of 0.09 against 0.60 for the EDM), and in particular suffer from larger ambiguities, i.e. poses with similar 2D projections and dissimilar 3D shape. Red circles indicate the most ambiguous such poses, and green circles the most desirable configurations (large 2D and 3D differences). Note that red circles are more uniformly distributed along the vertical axis when using EDM representations, favoring larger differences and better discriminability. Right: Pairs of dissimilar 3D poses with similar (top) and dissimilar (bottom) projections. They correspond to the dark red and dark green 'asterisks' in the left-most plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Neural Network Architectures used to perform 2D-to-3D regression of symmetric Euclidean Distance Matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Hypothesysing Occluded Body Parts. Ground truth and hypothesized body parts obtained using the Fully Convolutional distance matrix regressor (Subject 3, action 'Jogging' from Humaneva-I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Results on the LSP dataset. The first six columns show correctly estimated poses. The right-most column shows failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 1. Overview. We formulate the 3D human pose estimation problem as a regression between two Euclidean Distance Matrices (EDMs), encoding pairwise distances of 2D and 3D body joints, respectively. The regression is carried out by a Neural Network, and the 3D joint estimates are obtained from the predicted 3D EDM via Multidimensional Scaling.</figDesc><table><row><cell>Input Image and 2D Detections x</cell><cell>Input 2D EDM edm(x)</cell><cell>2D-to-3D EDM Regression using a Neural Network</cell><cell>Estimated 3D EDM edm(y)</cell><cell>Multidimensional Scaling</cell><cell>3D Shape y</cell></row></table><note>rors, while (for the case of the FConv) also allowing to hy- pothesize reasonably well occluded body limbs. Addition- ally, experiments in the Leeds Sports Pose dataset, using a network learned on Human3.6M, demonstrate good gener- alization capability on images 'in the wild'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. Finally, in Protocol 133.37 164.39 162.12 205.94 150.61 171.31 151.57 243.03 162.14 170.69 177.13 125.28 118.02 112.38 129.17 138.89 224.90 118.42 182.73 138.75 55.07 103.16 116.18 143.32 106.88 99.78 124.52 199.23 107.42 118.09 114.23</figDesc><table><row><cell>Method</cell><cell cols="2">Direct. Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD</cell><cell cols="2">Smoke Photo</cell><cell>Wait</cell><cell cols="3">Walk WalkD WalkT</cell><cell>Avg</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ionescu PAMI'14 [17]</cell><cell>132.71</cell><cell cols="13">183.55 96.60</cell><cell cols="2">127.88 162,20</cell></row><row><cell>Li ICCV'15 [23]</cell><cell>-</cell><cell>136.88</cell><cell cols="2">96.94 124.74</cell><cell>-</cell><cell>168.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>132.17</cell><cell>69.97</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin BMVC'16 [45]</cell><cell>-</cell><cell>129.06</cell><cell cols="2">91.43 121.68</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>162.17</cell><cell>-</cell><cell>65.75</cell><cell>130.53</cell><cell>-</cell><cell>-</cell></row><row><cell>Tekin CVPR'16 [46]</cell><cell>102.41</cell><cell>147.72</cell><cell cols="12">88.83 126.29</cell><cell>65.76</cell><cell>124.97</cell></row><row><cell>Zhou CVPR'16 [55]</cell><cell>87.36</cell><cell>109.31</cell><cell cols="12">87.05 79.39</cell><cell>97.70</cell><cell>112.91</cell></row><row><cell>Ours, FConv, Test 2D: CPM</cell><cell>69.54</cell><cell>80.15</cell><cell>78.20</cell><cell cols="3">87.01 100.75 76.01</cell><cell cols="3">69.65 104.71 113.91</cell><cell>89.68</cell><cell cols="2">102.71 98.49</cell><cell>79.18</cell><cell>82.40</cell><cell>77.17</cell><cell>87.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Protocol #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ramakrishna ECCV'12 [31](*) 137.40</cell><cell cols="15">149.30 141.60 154.30 157.70 141.80 158.10 168.60 175.60 160.40 158.90 161.70 174.80 150.00 150.20 156.03</cell></row><row><cell>Akhter CVPR'15 [3](*)</cell><cell cols="16">1199.20 177.60 161.80 197.80 176.20 195.40 167.30 160.70 173.70 177.80 186.50 181.90 198.60 176.20 192.70 181.56</cell></row><row><cell>Zhou PAMI'16 [54](*)</cell><cell>99.70</cell><cell>95.80</cell><cell cols="4">87.90 116.80 108.30 93.50</cell><cell cols="10">95.30 109.10 137.50 106.00 107.30 102.20 110.40 106.50 115.20 106.10</cell></row><row><cell>Bogo ECCV'16 [10]</cell><cell>62.00</cell><cell>60.20</cell><cell>67.80</cell><cell>76.50</cell><cell>92.10</cell><cell>73.00</cell><cell cols="3">75.30 100.30 137.30</cell><cell>83.40</cell><cell>77.00</cell><cell>77.30</cell><cell>86.80</cell><cell>79.70</cell><cell>81.70</cell><cell>82.03</cell></row><row><cell>Ours. FConv, Test 2D: CPM</cell><cell>66.07</cell><cell>77.94</cell><cell>72.58</cell><cell>84.66</cell><cell>99.71</cell><cell>74.78</cell><cell>65.29</cell><cell cols="2">93.40 103.14</cell><cell>85.03</cell><cell>98.52</cell><cell>98.78</cell><cell>78.12</cell><cell>80.05</cell><cell>74.77</cell><cell>83.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Protocol #3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yasin CVPR'16 [51]</cell><cell>88.40</cell><cell>72.50</cell><cell cols="3">108.50 110.20 97.10</cell><cell cols="7">81.60 107.20 119.00 170.80 108.20 142.50 86.90</cell><cell>92.10</cell><cell cols="3">165.70 102.00 110.18</cell></row><row><cell>Rogez NIPS'16 [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.10</cell></row><row><cell>Ours, FConv, Test 2D: CPM</cell><cell>67.44</cell><cell>63.76</cell><cell>87.15</cell><cell>73.91</cell><cell>71.48</cell><cell>69.88</cell><cell>65.08</cell><cell>71.69</cell><cell>98.63</cell><cell>81.33</cell><cell>93.25</cell><cell>74.62</cell><cell>76.51</cell><cell>77.72</cell><cell>74.63</cell><cell>76.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Occl. Joints 177.44 177.68 152.06 220.28 145.93 180.42 143.24 192.42 154.62 184.24 253.88 213.6 176.11 160.44 188.38 181.38 Results on Human3.6M under Occlusions. Average overall joint error and average error of the hypothesized occluded joints (in mm). The network is trained and evaluated according to the 'Protocol #3' described in the text. 109.84 110.21 117.13 115.16 107.08 116.92 107.14 101.82 131.43 114.76 115.07 112.54 125.50 118.93 129.73 115.55</figDesc><table><row><cell>Occ. Type</cell><cell>Error</cell><cell cols="2">Direct. Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD</cell><cell cols="2">Smoke Photo</cell><cell>Wait</cell><cell cols="3">Walk WalkD WalkT</cell><cell>Avg</cell></row><row><cell cols="2">2 Rnd.Joints Avg. Error</cell><cell>88.53</cell><cell>97.83</cell><cell cols="10">139.99 99.57 106.13 102.78 92.97 113.35 126.62 111.73 122.74 109.85</cell><cell>95.1</cell><cell>96.76</cell><cell>97.97</cell><cell>106.79</cell></row><row><cell></cell><cell>Err.Occl. Joints</cell><cell>94.77</cell><cell cols="12">104.37 155.66 110.48 119.62 103.83 91.04 141.31 135.35 137.76 146.68 131.41 116.16</cell><cell>96.11</cell><cell>99.73</cell><cell>118.95</cell></row><row><cell>Left Arm</cell><cell>Avg. Error</cell><cell cols="13">197.86 101.88 123.91 109.72 93.00 106.15 100.55 113.19 129.50 111.15 135.72 118.07 99.21</cell><cell cols="3">100.73 100.94 109.44</cell></row><row><cell cols="2">Err.Right Leg Avg. Error</cell><cell>79.94</cell><cell>82.23</cell><cell cols="4">132.64 92.05 100.77 97.32</cell><cell cols="6">76.37 126.95 125.51 106.66 109.82 95.92</cell><cell>94.88</cell><cell>89.82</cell><cell>91.60</cell><cell>100.17</cell></row><row><cell></cell><cell>Err.Occl. Joints</cell><cell>81.23</cell><cell>92.57</cell><cell cols="11">177.80 103.69 148.45 120.74 92.63 200.56 183.03 146.10 145.29 107.36 133.11</cell><cell>105.9</cell><cell cols="2">120.12 130.57</cell></row><row><cell>2D Input</cell><cell></cell><cell cols="2">Direct. Discuss</cell><cell>Eat</cell><cell cols="2">Greet Phone</cell><cell>Pose</cell><cell>Purch.</cell><cell>Sit</cell><cell>SitD</cell><cell cols="2">Smoke Photo</cell><cell>Wait</cell><cell cols="3">Walk WalkD WalkT</cell><cell>Avg</cell></row><row><cell>GT</cell><cell></cell><cell>53.51</cell><cell>50.52</cell><cell>65.76</cell><cell>62.47</cell><cell>56.9</cell><cell>60.63</cell><cell>50.83</cell><cell>55.95</cell><cell>79.62</cell><cell>63.68</cell><cell>80.83</cell><cell>61.80</cell><cell>59.42</cell><cell>68.53</cell><cell>62.11</cell><cell>62.17</cell></row><row><cell>GT+N (0, 5)</cell><cell></cell><cell>57.05</cell><cell>56.05</cell><cell>70.33</cell><cell>65.46</cell><cell>60.39</cell><cell>64.49</cell><cell>59.06</cell><cell>58.62</cell><cell>82.80</cell><cell>67.85</cell><cell>83.97</cell><cell>70.13</cell><cell>66.76</cell><cell>75.04</cell><cell>68.62</cell><cell>67.11</cell></row><row><cell>GT+N (0, 10)</cell><cell></cell><cell>76.46</cell><cell>70.74</cell><cell>77.18</cell><cell>77.25</cell><cell>73.42</cell><cell>81.94</cell><cell>64.65</cell><cell>71.05</cell><cell>97.08</cell><cell>76.91</cell><cell>93.45</cell><cell>77.12</cell><cell>85.14</cell><cell>80.96</cell><cell>83.47</cell><cell>79.12</cell></row><row><cell>GT+N (0, 15)</cell><cell></cell><cell>90.72</cell><cell>91.99</cell><cell>96.54</cell><cell>94.99</cell><cell cols="3">87.43 101.81 89.39</cell><cell cols="2">84.46 107.26</cell><cell>93.31</cell><cell cols="3">106.01 95.96 100.38</cell><cell>96.59</cell><cell>104.41</cell><cell>96.08</cell></row><row><cell>GT+N (0, 20)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Once the 3D distance matrix is predicted, the position of the 3D body joints can be readily estimated using Multidimensional Scaling (MDS).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>This work is partly funded by the Spanish MINECO project RobInstruct TIN2014-58178-R and by the ERA-Net Chistera project I-DRESS PCIN-2015-147. The author thanks Nvidia for the hardware donation under the GPU grant program, and Germán Ros for fruitful discussions that initiated this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Human Pose from Silhouettes by Relevance Vector Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="882" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mode-Shape Interpretation: Re-Thinking Modal Space for Recovering Deformable Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-conditioned Joint Angle Limits for 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Street-View Change Detection with Deconvolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detailed Human Shape and Pose from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D Pictorial Structures for Multiple Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semidefinite Programming Approaches for Sensor Network Localization With Noisy Distance Measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twin Gaussian Processes for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="28" to="52" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Large Scale Conditional 3D Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Modern Multidimensional Scaling: Theory and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient Convnet-Based Marker-Less Motion Capture in General Scenes with a Low Number of Cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FlowNet: Learning Optical Flow with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian Reconstruction of 3D Human Motion from Single-Camera Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="820" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterated Second-Order Label Sensitive Pooling for 3D Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered Pose and Nonlinear Appearance Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distance Matrix-based Approach to Protein Structure Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kloczkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Jernigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pokarowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Structural and Functional Genomics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Depth Sweep Regression Forests for Estimating 3D Human Pose from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical Gaussian Process Latent Variable Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast and Globally Convergent Pose Estimation from Video Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mjolsness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="610" to="622" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Recovering 3D Human Body Configurations Using Shape Contexts. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1052" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear embeddings in non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Branch-and-Prune Solver for Distance Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="187" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monocular Image 3D Human Pose Estimation Under Self-Occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstructing 3D Human Pose from 2D Image Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Randomized Trees for Human Pose Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modec: Multimodal Decomposable Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast pose estimation with parameter-sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrellr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>-2)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Loose-limbed People: Estimating 3D Human Pose and Motion Using Non-parametric Belief Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="48" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Joint Model for 2D and 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Single Image 3D Human Pose Estimation from Noisy Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D Human Pose Tracking Priors using Geodesic Mixture Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Isometric Deformation Invariant 3D Shape Recognition. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smeets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suetens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2817" to="2831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative Modeling for Continuous Non-Linearly Embedded Visual Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generative Modeling for Continuous Non-Linearly Embedded Visual Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanaujia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamical Binary Latent Variable Models for 3D Human Pose Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting People&apos;s 3D Poses from Short Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3D People Tracking with Gaussian Process Dynamical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust estimation of 3d human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Dual-Source Approach for 3D Pose Estimation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human Motion Tracking by Temporal-Spatial Local Gaussian Process Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1141" to="1151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
