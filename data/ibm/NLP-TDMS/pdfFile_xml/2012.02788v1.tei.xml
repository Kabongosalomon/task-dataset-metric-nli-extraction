<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Dynamic Policies for End-to-End Sensorimotor Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Bahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Mukadam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FAIR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CMU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">CMU</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Dynamic Policies for End-to-End Sensorimotor Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decisions individually at each timestep in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed the structure of a dynamical system into deep neural network-based policies by reparameterizing action spaces via second-order differential equations. We propose Neural Dynamic Policies (NDPs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where actions represent the raw control space. The embedded structure allows end-to-end policy learning for both reinforcement and imitation learning setups. We show that NDPs outperform the prior state-of-the-art in terms of either efficiency or performance across several robotic control tasks for both imitation and reinforcement learning setups. Project video and code are available at: https://shikharbahl.github.io/neural-dynamic-policies/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is to draw the planar digit 4 from the start position. The dynamical structure in NDP induces a smooth vector field in trajectory space. In contrast, a vanilla policy has to reason individually in different parts.</p><p>Consider an embodied agent tasked with throwing a ball into a bin. Not only does the agent need to decide where and when to release the ball, but also needs to reason about the whole trajectory that it should take such that the ball is imparted with the correct momentum to reach the bin. This form of reasoning is necessary to perform many such everyday tasks. Common methods in deep learning for robotics tackle this problem either via imitation or reinforcement learning. However, in most cases, the agent's policy is trained in raw action spaces like torques, joint angles, or end-effector positions, which forces the agent to make decisions at each time step of the trajectory instead of making decisions in the trajectory space itself (see <ref type="figure" target="#fig_1">Figure 1</ref>). But then how do we reason about trajectories as actions?</p><p>A good trajectory parameterization is one that is able to capture a large set of an agent's behaviors or motions while being physically plausible. In fact, a similar question is also faced by scientists Forward Integrator while modeling physical phenomena in nature. Several systems in science, ranging from motion of planets to pendulums, are described by differential equations of the formÿ = m −1 f (y,ẏ), where y is the generalized coordinate,ẏ andÿ are time derivatives, m is mass, and f is force. Can a similar parameterization be used to describe the behavior of a robotic agent? Indeed, classical robotics has leveraged this connection to represent task specific robot behaviors for many years. In particular, dynamic movement primitives (DMP) <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b40">41]</ref> have been one of the more prominent approaches in this area. Despite their successes, DMPs have not been explored much beyond behavior cloning paradigms. This is partly because these methods tend to be sensitive to parameter tuning and aren't as flexible or generalizable as current end-to-end deep network based approaches.</p><formula xml:id="formula_0">! ! ! ! f θ Φ!t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Dynamic Policy</head><p>In this work, we propose to bridge this gap by embedding the structure of dynamical systems 1 into deep neural network-based policies such that the agent can directly learn in the space of physically plausible trajectory distributions (see <ref type="figure" target="#fig_1">Figure 1(b)</ref>). Our key insight is to reparameterize the action space in a deep policy network with nonlinear differential equations corresponding to a dynamical system and train it end-to-end over time in either reinforcement learning or imitation learning setups. However, this is quite challenging to accomplish, since naively predicting a full arbitrary dynamical system directly from the input, trades one hard problem for another. Instead, we want to prescribe some structure such that the dynamical system itself manifests as a layer in the deep policy that is both, amenable to take arbitrary outputs of previous layers as inputs, and is also fully differentiable to allow for gradients to backpropagate.</p><p>We address these challenges through our approach, Neural Dynamic Policies (NDPs). Specifically, NDPs allow embedding desired dynamical structure as a layer in deep networks. The parameters of the dynamical system are then predicted as outputs of the preceding layers in the architecture conditioned on the input. The 'deep' part of the policy then only needs to reason in the lowerdimensional space of building a dynamical system that then lets the overall policy easily reason in the space of trajectories. In this paper, we employ the aforementioned DMPs as the structure for the dynamical system and show its differentiability, although they only serve as a design choice and can possibly be swapped for a different differentiable dynamical structure, such as RMPs <ref type="bibr" target="#b38">[39]</ref>.</p><p>We evaluate NDPs in imitation as well as reinforcement learning setups. NDPs can utilize highdimensional inputs via demonstrations and learn from weak supervisory signals as well as rewards. In both settings, NDPs exhibit better or comparable performance to state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Trajectories with Dynamical Systems</head><p>Consider a robotic arm exhibiting a certain behavior to accomplish some task. Given a choice of coordinate system, such as either joint-angles or end-effector position, let the state of the robot be y, velocityẏ and accelerationÿ. In mechanics, Euler-Lagrange equations are used to derive the equations of motion as a general second order dynamical system that perfectly captures this behavior <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">Chapter 6]</ref>. It is common in classical robotics to represent movement behaviors with such a dynamical system. Specifically, we follow the second order differential equation structure imposed by Dynamic Movement Primitives <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>. Given a desired goal state g, the behavior is represented as:</p><formula xml:id="formula_1">ÿ = α(β(g − y) −ẏ) + f (x),<label>(1)</label></formula><p>where α, β are global parameters that allow critical damping of the system and smooth convergence to the goal state. f is a non-linear forcing function which captures the shape of trajectory and operates over x which serves to replace time dependency across trajectories, giving us the ability to model time invariant tasks, e.g., rhythmic motions. x evolves through the first-order linear system:</p><formula xml:id="formula_2">x = −a x x<label>(2)</label></formula><p>The specifics of f are usually design choices. We use a sum of weighted Gaussian radial basis functions <ref type="bibr" target="#b21">[22]</ref> shown below:</p><formula xml:id="formula_3">f (x, g) = ψ i w i ψ i x(g − y 0 ), ψ i = e (−hi(x−ci) 2 )<label>(3)</label></formula><p>where i indexes over n which is the number of basis functions. Coefficients c i = e −iαx n are the horizontal shifts of each basis function, and h i = n ci are the width of each of each basis function. The weights on each of the basis functions w i parameterize the forcing function f . This set of nonlinear differential equations induces a smooth trajectory distribution that acts as an attractor towards a desired goal (see <ref type="bibr">Figure 1,</ref><ref type="bibr">right)</ref>. We now discuss how to combine this dynamical structure with deep neural network based policies in an end-to-end differentiable manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Dynamic Policies (NDPs)</head><p>We condense actions into a space of trajectories, parameterized by a dynamical system, while keeping all the advantages of a deep learning based setup. We present a type of policy network, called Neural Dynamic Policies (NDPs) that given an input, image or state, can produce parameters for an embedded dynamical structure, which reasons in trajectory space but outputs raw actions to be executed. Let the unstructured input to robot be s, (an image or any other sensory input), and the action executed by the robot be a. We describe how we can incorporate a dynamical system as a differentiable layer in the policy network, and how NDPs can be utilized to learn complex agent behaviors in both imitation and reinforcement learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Network Layer Parameterized by a Dynamical System</head><p>Throughout this paper, we employ the dynamical system described by the second order DMP equation <ref type="bibr" target="#b0">(1)</ref>. There are two key parameters that define what behavior will be described by the dynamical system presented in Section 2: basis function weights w = {w 1 , . . . , w i , . . . , w n } and goal g. NDPs employ a neural network Φ which takes an unstructured input s 2 and predicts the parameters w, g of the dynamical system. These predicted w, g are then used to solve the second order differential equation (1) to obtain system states {y,ẏ,ÿ}. Depending on the difference between the choice of robot's coordinate system for y and desired action a, we may need an inverse controller Ω(.) to convert y to a, i.e., a = Ω(y,ẏ,ÿ). For instance, if y is in joint angle space and a is a torque, then Ω(.) is the robot's inverse dynamics controller, and if y and a both are in joint angle space then Ω(.) is the identity function.</p><p>As summarized in <ref type="figure" target="#fig_2">Figure 2</ref>, neural dynamic policies are defined as π(a|s; θ) Ω DE Φ(s; θ) where DE(w, g) → {y,ẏ,ÿ} denotes solution of the differential equation <ref type="bibr" target="#b0">(1)</ref>. The forward pass of π(a|s) involves solving the dynamical system and backpropagation requires it to be differentiable. We now show how we differentiate through the dynamical system to train the parameters θ of NDPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training NDPs by Differentiating through the Dynamical System</head><p>To train NDPs, estimated policy gradients must flow from a, through the parameters of the dynamical system w and g, to the network Φ(s; θ). At any time t, given the previous state of robot y t−1 and velocityẏ t−1 the output of the DMP in Equation (1) is given by the acceleration</p><formula xml:id="formula_4">y t = α(β(g − y t−1 ) −ẏ t−1 + f (x t , g)<label>(4)</label></formula><p>Through Euler integration, we can find the next velocity and position after a small time interval dṫ y t =ẏ t−1 +ÿ t−1 dt, y t = y t−1 +ẏ t−1 dt (5)</p><p>In practice, this integration is implemented in m discrete steps. To perform a forward pass, we unroll the integrator for m iterations starting from initialẏ 0 ,ÿ 0 . We can either apply all the m intermediate robot states y as actions on the robot using inverse controller Ω(.), or equally sub-sample them into k ∈ {1, m} actions in between, where k is the NDP rollout length. This frequency of sampling could allow robot operation at a much higher frequency (.5-5KHz) than the environment (usually 100Hz). The sampling frequency need not be same at training and inference as discussed further in Section 3.5.</p><p>Now we can compute gradients of the trajectory from the DMP with respect to w and g using Equations (3)-(5) as follows:</p><formula xml:id="formula_5">∂f (x t , g) ∂w i = ψ i j ψ j (g − y 0 )x t , ∂f (x t , g) ∂g = ψ j w j j ψ j x t<label>(6)</label></formula><p>Using this, a recursive relationship follows between, (similarly to the one derived by Pahic et al. <ref type="bibr" target="#b28">[29]</ref>) ∂yt ∂wi , ∂yt ∂g and the preceding derivatives of w i , g with respect to y t−1 , y t−2 ,ẏ t−1 andẏ t−2 . Complete derivation of equation <ref type="formula" target="#formula_5">(6)</ref> is given in appendix.</p><p>We now discuss how NDPs can be leveraged to train policies for imitation learning and reinforcement learning setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training NDPs for Imitation (Supervised) Learning</head><p>Training NDPs in imitation learning setup is rather straightforward. Given a sequence of input {s, s , . . . }, NDP's π(s; θ) outputs a sequence of actions a, a . . .. In our experiments, s is a high dimensional image input. Let the demonstrated action sequence be τ target , we just take a loss between the predicted sequence as follows:</p><formula xml:id="formula_6">L imitation = s ||π(s) − τ target (s)|| 2<label>(7)</label></formula><p>The gradients of this loss are backpropagated as described in Section 3.2 to train the parameters θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training NDPs for Reinforcement Learning Algorithm 1 Training NDPs for RL</head><p>Require: Policy π, k NDP rollout length, Ω lowlevel inverse controller for 1, 2, ... episodes do for t = 0, k, . . . , until end of episode do w, g = Φ(st) Robot yt,ẏt from st (pos, vel) for m = 1, ..., M (integration steps) do Estimateẋm via <ref type="formula" target="#formula_2">(2)</ref>  We now show how an NDP can be used as a policy, π in the RL setting. As discussed in Section 3.2, NDP samples k actions for the agent to execute in the environment given input observation s. One could use any underlying RL algorithm to optimize the expected future returns. In this paper, we use Proximal Policy Optimization (PPO) <ref type="bibr" target="#b41">[42]</ref> and treat a independently when computing the policy gradient for each step of the NDP rollout and backprop via a reinforce objective.</p><p>There are two choices for value function critic V π (s): either predict a single common value function for all the actions in the k-step rollout or predict different critic values for each step in the NDP rollout sequence. We found that the latter works better in practice. We call this a multiaction critic architecture and predict k different estimates of value using k-heads on top of the critic network. Later, in the experiments we perform ablations over the choice of k. To further create a strong baseline comparison, as we discuss in Section 4, we also design and compare against a variant of PPO that predicts multiple actions using our multi-action critic architecture.  <ref type="figure">Figure 4</ref>: Imitation (supervised) learning results on held-out test images of the digit writing task. Given an input image (left), the output action is the end-effector position of a planar robot. All methods have the same neural network architecture for fair comparison. We find that the trajectories predicted by NDPs (ours) are dynamically smooth as well as more accurate than both baselines (a vanilla CNN and the architecture from Pahic et al. <ref type="bibr" target="#b28">[29]</ref>.</p><p>Algorithm 1 provides a summary of our method for training NDPs with policy gradients. We only show results of using NDPs with on-policy RL (PPO), however, NDPs can similarly be adapted to off-policy methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference in NDPs</head><p>In the case of inference, our method uses the NDP policy π once every k environment steps, hence requires k-times fewer forward passes as actions applied to the robot. While reducing the inference time in simulated tasks may not show much difference, in real world settings, where large perception systems are usually involved, reducing inference time can help decrease overall time costs. Additionally, deployed real world systems may not have the same computational power as many systems used to train state-of-the-art RL methods on simulators, so inference costs end up accumulating, thus a method that does inference efficiently can be beneficial. Furthermore, as discussed in Section 3.2, the rollout length of NDP can be more densely sampled at test-time than at training allowing the robot to produce smooth and dynamically stable motions. Compared to about 100Hz frequency of the simulation, our method can make decisions an order of magnitude faster (at about 0.5-5KHz) at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Environments To test our method on dynamic environments, we took existing torque control based environments for Picking and Throwing <ref type="bibr" target="#b16">[17]</ref> and modified them to enable joint angle control. The robot is a 6-DoF Kinova Jaco Arm. In Throwing, the robot tosses a cube into a bin, and in Picking, the robot picks up a cube and lifts it as high as possible. To test on quasi-static tasks, we use Pushing, Soccer, Faucet-Opening from the Meta-World <ref type="bibr" target="#b49">[50]</ref> task suite, as well as a setup that requires learning all 50 tasks (MT50) jointly (see <ref type="figure" target="#fig_3">Figure 3</ref>). These Meta-World environments are all in end-effector position control settings and based on a Sawyer Robot simulation in Mujoco <ref type="bibr" target="#b46">[47]</ref>. In order to make the tasks more realistic, all environments have some degree of randomization. Picking and Throwing have random starting positions, while the rest have randomized goals.</p><p>Baselines We use PPO <ref type="bibr" target="#b41">[42]</ref> as the underlying optimization algorithm for NDPs and all the other baselines compared in the reinforcement learning setup. The first baseline is the PPO algorithm itself without the embedded dynamical structure. Further, as mentioned in the Section 3.2, NDP is able to operate the robot at a much higher frequency than the world. Precisely, its frequency is k-times higher where k is the NDP rollout length (described in Section 3.2). Even though the robot moves at a higher frequency, the environment/world state is only observed at normal rate, i.e., once every k robot steps and the reward computation at the intermediate k steps only uses the stale environment/world state from the first one of the k-steps. Hence, to create a stronger baseline that can also operate at higher frequency, we create a "PPO-multi" baseline that predicts multiple actions and also uses our multi-action critic architecture as described in Section 3.4. All methods are compared in terms of performance measured against the environment sample states observed by the agent. In addition, we also compare to Variable Impedance Control in End-Effector Space (VICES) <ref type="bibr" target="#b25">[26]</ref> and Dynamics-Aware Embeddings (Dyn-E) <ref type="bibr" target="#b48">[49]</ref> . VICES learns to output parameters of a PD controller or an Impedance controller directly. Dyn-E, on the other hand, using forward prediction based on environment dynamics, learns a lower dimensional action embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results: NDPs for Imitation and Reinforcement Learning</head><p>We validate our approach on Imitation Learning and RL tasks in order to ascertain how NDP compares to state-of-the-art methods. (success rates between 0 and 1) on Mujoco <ref type="bibr" target="#b46">[47]</ref> environments. We see that NDP outperforms the neural network baseline in many tasks.</p><p>To evaluate NDPs in imitation learning settings we train an agent to perform various control tasks. We evaluate NDPs on the Mujoco <ref type="bibr" target="#b46">[47]</ref> environments discussed in Section 4 (Throwing, Picking, Pushing, Soccer and Faucet-Opening). Experts are trained using PPO <ref type="bibr" target="#b41">[42]</ref> and are subsequently used to collect trajectories. We train an NDP via the behaviour cloning procedure described in Section 3.3, on the collected expert data. We compare against a neural network policy (using roughly the same model capacity for both). Success rates in <ref type="table">Table 1</ref> indicate that NDPs show superior performance on a wide variety of control tasks.</p><p>In order to evaluate the ability of NDPs to handle complex visual data, we perform a digit-writing task using a 2D end-effector. The goal is to train a planar robot to trace the digit, given its image as input. The output action is the robot's end-effector position, and supervision is obtained by treating ground truth trajectories as demonstrations. We compare NDPs to a regular behavior cloning policy parameterized by a CNN and the prior approach which maps image to DMP parameters <ref type="bibr" target="#b28">[29]</ref> (dubbed, CNN-DMP). CNN-DMP <ref type="bibr" target="#b28">[29]</ref> trains a single DMP for the whole trajectory and requires supervised demonstrations. In contrast, to NDPs can generate multiple DMPs across time and can be used in an RL setup as well. However, for a fair comparison, we compare both methods apples-to-apples with single DMP for whole trajectory, i.e., k = 300.  We report the mean loss across 10 digit classes. The input is the image of the digit to be written and action output is the end-effector position of robot.</p><p>Our method significantly outperforms the baseline.</p><p>Qualitative examples are in <ref type="figure">Figure 4</ref>, and quantitative results in <ref type="table" target="#tab_3">Table 2</ref> report the mean loss between output trajectory and ground truth. NDP outperforms both CNN and CNN-DMP <ref type="bibr" target="#b28">[29]</ref> drastically. Our method also produces much higher quality and smoother reconstructions as shown in <ref type="figure">Figure 4</ref>. Results show that our method can efficiently capture dynamic motions in a supervised setting, while learning from visual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reinforcement Learning</head><p>In contrast to imitation learning where the rollout length of NDP is high (k = 300), we set k = 5 in RL because the reward becomes too sparse if k is very large. We compare the success rate of our method with that of the baseline methods PPO, PPO-multi with k = 5, VICES and DYN-E.</p><p>As shown in In <ref type="figure">Figure 5</ref>, our method NDP sees gains in both efficiency and performance in most tasks. In Soccer, PPO reaches a higher final performance, but our method shows twice the efficiency at a small loss in performance. The final task of training jointly across 50 Meta-World tasks is too hard for all methods. Nevertheless, our NDP attains a slightly higher absolute performance than baseline but doesn't show efficiency gains over baselines.</p><p>PPO-multi, a multi-action algorithm based on our proposed multi-action critic setup tends to perform well in some case (Faucet Opening, Pushing etc) but is inconsistent in its performance across all tasks and fails completely at times, (Picking etc.). Our method also outperforms prior state-of-the-art methods that re-paremeterize action spaces, namely, VICES <ref type="bibr" target="#b25">[26]</ref> and Dyn-E <ref type="bibr" target="#b48">[49]</ref>. VICES is only slightly successful in tasks like throwing, since a PD controller can efficiently solve the task, but suffer in more complex settings due to a large action space dimensionality (as it predicts multiple quantities per degree of freedom). Dyn-E, on the other hand, performs well on tasks such as Pushing, or Soccer, which have simpler dynamics and contacts, but fails to scale to more complex environments.</p><p>Through these experiments, we show the diversity and versatility of NDP, as it has a strong performance across different types of control tasks. NDP outperforms baselines in both dynamic (throwing) and static tasks (pushing) while being able to learn in a more data efficient manner. It is able to reason in a space of physically meaningful trajectories, but it does not lose the advantages and flexibility that other policy setups have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Ablations for NDPs in Reinforcement Learning Setup</head><p>We aim to understand how design choices affect the RL performance of NDP. We run comparisons on 1+( x) 2 . Additionally, we investigate the effect of different NDP components on its performance. To this end, we ablate a setting where only g (the goal) is learnt while the radial basis function weights (the forcing function) are 0 (we call this setting 'only-g'). We also ablate a version of NDP that learns the global constant α (from Equation 4), in addition to the other parameters (g and w). <ref type="figure">Figure 6</ref> shows results from ablating different NDP parameters. Varying N (number of basis functions) controls the shape of the trajectory taken by the agent. A small N may not have the power to represent the nuances of the motion required, while a big N may make the parameter space too large to learn efficiently. We see that number of integration steps do not have a large effect on performance, similarly to the type of radial basis function. Most radial basis functions generally have similar interpolation and representation abilities. We see that k = 3 (the length of each individual rollout within NDP) has a much lower performance due to the fact that 3 steps cannot capture the smoothness or intricacies of a trajectory. Overall, we mostly find that NDP is robust to design choices. <ref type="figure">Figure 7</ref> shows that the current formulation of NDP outperforms the one where α is learnt. We also observe that setting the forcing term to 0 (only learning the goal, g) is significantly less sample efficient than NDPs while converging to a slightly lower asymptotic performance.  <ref type="figure">Figure 7</ref>: Ablations for different NDP design choices. The first entails NDP also learning the parameter α (shown as az). In the second one, g is learnt but not wi, i.e. the forcing function is 0 ('only-g'). Results indicate that NDP outperforms both these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Dynamic Movement Primitives Previous works have proposed and used Dynamic Movement Primitives (DMP) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref> for robot control <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. Work has been done in representing dynamical systems, both as extensions of DMPs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref>, and beyond DMPs by learning kernels <ref type="bibr" target="#b18">[19]</ref> and designing Riemannian metrics <ref type="bibr" target="#b38">[39]</ref>. Learning methods have been used to incorporate environment sensory information into the forcing function of DMPs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. DMPs have also been prime candidates to represent primitives when learning hierarchical policies, given the range of motions DMPs can be used for in robotics <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. Parametrization of DMPs using gaussian processes has also been proposed to facilitate generalization <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>. Recently, deep learning has also been used to map images to DMPs <ref type="bibr" target="#b28">[29]</ref> and to learn DMPs in latent feature space <ref type="bibr" target="#b7">[8]</ref>. However most of these works require pre-trained DMPs via expert demonstrations or are only evaluated in the supervised setting. Furthermore, either a single DMP is used to represent the whole trajectory or the demonstration is manually segmented to learn a different DMP for each segment. In contrast, NDPs outputs a new dynamical system for each timestep to fit diverse trajectory behaviours across time.</p><p>Since we embed dynamical structure into the deep network, NDPs can flexibly be incorporated not just in visual imitation but also in deep reinforcement learning setups, in an end-to-end manner.</p><p>Reparameterized Policy and Action Spaces A broader area of work that makes use of action reparameterization is the study of Hierarchical Reinforcement Learning (HRL). Works in the options framework <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">46]</ref> attempt to learn an overarching policy that controls usage of lower-level policies or primitives. Lower-level policies are usually pre-trained therefore require supervision and knowledge of the task beforehand, limiting the generalizability of such methods. For example, Daniel et al. <ref type="bibr" target="#b12">[13]</ref>, Parisi et al. <ref type="bibr" target="#b29">[30]</ref> incorporate DMPs into option-based RL policies, using a pre-trained DMPs as the high level actions. This setup requires re-learning DMPs for different types of tasks and does not allow the same policy to generalize, since it needs to have access to an extremely large number of DMPs. Action spaces can also be reparameterized in terms of pre-determined PD controller <ref type="bibr" target="#b50">[51]</ref> or learned impedance controller parameters <ref type="bibr" target="#b25">[26]</ref>. While this helps for policies to adapt to contact rich behaviors, it does not change the trajectories taken by the robot. This often leads to high dimensionality, and thus a decrease sample efficiency. In addition, Whitney et al. <ref type="bibr" target="#b48">[49]</ref> learn an action embedding based on passive data, however, this does not take environment dynamics or explicit control structure into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure in Policy Learning</head><p>Various methods in the field of control and robotics have employed physical knowledge, dynamical systems, optimization, and more general task/environment dynamics to create structured learning. Works such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref> propose networks constrained through physical properties such as Hamiltonian co-ordinates or Lagrangian Dynamics. However, the scope of these works is limited to toy examples such as a point mass, and are often used for supervised learning.</p><p>Similarly, other works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> all employ dynamical systems to model demonstrations, and do not tackle generalization or learning beyond imitation. Fully differentiable optimization problems have also been incorporated as layers inside a deep learning setup <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>. Whil they share the underlying idea of embedding structure in deep networks such that some aspects of this structure can be learned end-to-end, they have not been explored in tackling complex robotic control tasks. Furthermore, it is common in RL setups to incorporate planning based on a system model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. However, this is usually learned from experience or from attempts to predict the effects of actions on the environment (forward and inverse models), and often tends to fail for complex dynamic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Our method attempts to bridge the gap between classical robotics, control and recent approaches in deep learning and deep RL. We propose a novel re-parameterization of action spaces via Neural Dynamic Policies, a set of policies which impose the structure of a dynamical system on action spaces. We show how this set of policies can be useful for continuous control with RL, as well as in supervised learning settings. Our method obtains superior results due to its natural imposition of structure and yet it is still generalizable to almost any continuous control environment.</p><p>The use of DMPs in this work was a particular design choice within our architecture which allows for any form of dynamical structure that is differentiable. As alluded to in the introduction, other similar representations can be employed in their place. In fact, DMPs are a special case of a general second order dynamical system <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> where the inertia term is identity, and potential and damping functions are defined in a particular manner via first order differential equations with a separate forcing function which captures the complexities of the desired behavior. Given this, one can setup a dynamical structure such that it explicitly models and learns the metric, potential, and damping explicitly. While this brings advantages in better representation, it also brings challenges in learning. We leave these directions for future work to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Videos</head><p>Videos can also be found at: https://shikharbahl.github.io/neural-dynamic-policies/. We found that NDP results look dynamically more stable and smooth in comparison to the baselines. PPO-multi generates shaky trajectories, while corresponding NDP (ours) trajectories are much smoother .This is perhaps due to the embedded dynamical structure in NDPs as all other aspects in PPO-multi and NDP (ours) are compared apples-to-apples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Differentiability Proof of Dynamical Structure in NDPs</head><p>In Section 3.2, we provide an intuition for how NDP is incorporates a second order dynamical system (based on the DMP system, described in Section 2) in a differentiable manner. Let us start by observing that, when implementing our algorithm, y 0 ,ẏ 0 are known andÿ 0 = 0, as well as x 0 = 1.</p><p>Assuming that the output states of NDP are y 0 , y 1 , ..., y t , ... and assuming that there exists a loss function L which takes in y t , we want partial derivatives with respect to DMP weights w i and goal g:</p><formula xml:id="formula_7">∂L(y t ) ∂w i , ∂L(y t ) ∂w i (8) ∂L(y t ) ∂y t ∂y t ∂w i<label>(9)</label></formula><p>Starting with w i , using the Chain Rule we get that</p><formula xml:id="formula_8">∂L(y t ) ∂w i = ∂L(y t ) ∂y t ∂y t ∂w i<label>(10)</label></formula><p>Hence, we want to be able to calculated ∂yt ∂wi . For simplicity let:</p><formula xml:id="formula_9">W t = ∂y t ∂w i (11) W t = ∂ẏ t ∂w i (12) W t = ∂ÿ t ∂w i<label>(13)</label></formula><p>From section 3.2 we know that:</p><formula xml:id="formula_10">y t = α(β(g − y t−1 ) −ẏ t−1 + f (x t , g)<label>(14)</label></formula><p>and, the discretization over a small time interval dt gives us:</p><formula xml:id="formula_11">y t =ẏ t−1 +ÿ t−1 dt, y t = y t−1 +ẏ t−1 dt<label>(15)</label></formula><p>From these and the fact that y 0 ,ẏ 0 are known andÿ 0 = 0, as well as x 0 = 1, we get that y 1 = y 0 +ẏ 0 dt andẏ 1 =ẏ 0 + 0dt =ẏ 0 , as well asÿ <ref type="bibr" target="#b0">1</ref> </p><formula xml:id="formula_12">= α(β(g − y 0 ) −ẏ 0 + f (x 1 , g).</formula><p>Using Equations <ref type="formula" target="#formula_1">(14)</ref> and <ref type="bibr" target="#b14">(15)</ref> we get that:</p><formula xml:id="formula_13">W t = ∂ ∂w i (y t−1 +ẏ t−1 dt) (16) W t = W t−1 +Ẇ t−1 dt (17) andẆ t−1 =Ẇ t−2 +Ẅ t−1 dt<label>(18)</label></formula><p>In turn,Ẅ</p><formula xml:id="formula_14">t−1 = ∂ ∂w i (α(β(g − y t−2 ) −ẏ t−2 ) + f (x t−1 , g))<label>(19)</label></formula><p>From section 3.2, we know that ∂f (x t−1 , g)</p><formula xml:id="formula_15">∂w i = ψ i j ψ j (g − y 0 )x t−1<label>(20)</label></formula><p>Hence:Ẅ</p><formula xml:id="formula_16">t−1 = α(β(−W t−2 ) −Ẇ t−2 ) + ψ i j ψ j (g − y 0 )x t−1<label>(21)</label></formula><p>Plugging equations the value ofẄ t−1 into Equation <ref type="formula" target="#formula_1">(18)</ref>:</p><formula xml:id="formula_17">W t−1 =Ẇ t−2 + (α(β(−W t−2 ) −Ẇ t−2 ) + ψ i j ψ j (g − y 0 )x t−1 )dt<label>(22)</label></formula><p>Now plugging the value ofẆ t−1 in Equation <ref type="formula" target="#formula_1">(17)</ref>:</p><formula xml:id="formula_18">W t = W t−1 + (Ẇ t−2 + (α(β(−W t−2 ) −Ẇ t−2 ) + ψ i j ψ j (g − y 0 )x t−1 )dt)dt<label>(23)</label></formula><p>We see that the value of W t is dependent on W t−1 ,Ẇ t−2 , W t−2 . We can now show that we can acquire a numerical value for W t by recursively following the gradients, given that W t−1 ,Ẇ t−2 , W t−2 are known. Since we showed that y 0 ,ẏ 0 , y 1 ,ẏ 1 do not require w i in their computation, W 1 ,Ẇ 0 , W 0 = 0. Hence by recursively following the relationship defined in Equation <ref type="formula" target="#formula_2">(23)</ref>, we achieve a solution for W t .</p><p>Similarly, let:</p><formula xml:id="formula_19">G t = ∂ ∂g (y t−1 +ẏ t−1 dt) (24) G t = G t−1 +Ġ t−1 dt (25) andĠ t−1 =Ġ t−2 +G t−1 dt<label>(26)</label></formula><p>Using section 3.2, we get that</p><formula xml:id="formula_20">∂f (x t−1 , g) ∂g = ψ j w j j ψ j x t−1<label>(27)</label></formula><p>Hence:G</p><formula xml:id="formula_21">t−1 = α(β(1 − G t−2 ) −Ġ t−2 ) + ψ j w j j ψ j x t−1<label>(28)</label></formula><p>and we get a similar relationship as Equation <ref type="formula" target="#formula_2">(23)</ref>:</p><formula xml:id="formula_22">G t = G t−1 + (Ġ t−2 + (α(β(1 − G t−2 ) −Ġ t−2 ) + ψ j w j j ψ j x t−1 )dt)dt<label>(29)</label></formula><p>Hence, G t , similarly is dependent on G t−1 ,Ġ t−2 , G t−2 . We can use a similar argument as with w i to show that G t is also numerically achievable. We have now shown that y t , the output of the dynamical system defined by a DMP, is differentiable with respect to w i and g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Ablations</head><p>We present ablations similar to the ones in our paper, using the Throwing task. The results are showin in <ref type="figure">Figure 8</ref>. We see that NDPs show similar robustness across all variations. Secondly, we ran ablations with the forcing term set to 0 and found it variant to be significantly less sample efficient than NDPs while converging to a slightly lower asymptotic performance. Finally, we ran ablation where α is also learned by the policy while setting β = α 4 for critical damping. We see in <ref type="figure">Figure 9</ref> that NDPs outperforms both settings where α is learnt and where the the forcing term f is set to 0.</p><p>Additionally, we ran multiple ablations for the VICES baseline. We present a version of VICES for throwing and picking tasks that acts in end-effector space instead of joint-space (we call this 'vicespos')., as well e ran another version of VICES where the higher level policy runs at similar frequency as NDP which we call 'vices-low-freq'. The results are presented in <ref type="figure">Figure 9a</ref> and <ref type="figure">Figure 9b</ref>. We found it to be less sample efficient and have a lower performance than NDP. A.4 Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters and Design Choices</head><p>We use default parameters and stick as closely as possible to the default code. In multi-action cases (for PPO-multi and NDP), we kept rollout length k fixed for training and at inference time, one can sample arbitrarily high value for k as demanded by the task setup. For reinforcement learning, we kept k = 5 because the reward becomes too sparse if k is very large. For imitation learning, this is not an issue, and hence, k = 300 for learning from demonstration.</p><p>In reinforcement learning setup for NDP, we tried number of basis functions in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10]</ref> for each RL task. We fixed the number of integration steps per NDP rollout to 35. We also tried α (as described in Section 2) values in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. NDP (ours), PPO <ref type="bibr" target="#b41">[42]</ref>, PPO-Multi, VICES <ref type="bibr" target="#b25">[26]</ref> all use a similar 3-layer fully-connected network of hidden layer sizes of [100, 100] with tanh non-linearities. All these use PPO <ref type="bibr" target="#b41">[42]</ref> as the underlying RL optimizer. For Dyn-E <ref type="bibr" target="#b48">[49]</ref>, we used off-the-shelf architecture because it is based on off-policy RL and doesn't use PPO.</p><p>Hyper-parameters for underlying PPO optimization use off-the-shelf without any further tuning from PPO <ref type="bibr" target="#b41">[42]</ref> implementation in Kostrikov <ref type="bibr" target="#b24">[25]</ref> as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Environments</head><p>For Picking and Throwing, we adapted tasks from Ghosh et al. <ref type="bibr" target="#b16">[17]</ref> (https://github.com/ dibyaghosh/dnc. We modified these tasks to enable joint-angle position control. For other RL tasks, we used the Meta-World environment package <ref type="bibr" target="#b49">[50]</ref> (https://github.com/rlworkgroup/ metaworld). Since VICES <ref type="bibr" target="#b25">[26]</ref> operates using torque control, we modified Meta-World environments to support torque-based Impedance control. Further, as mentioned in the Section 3.4 and 4, NDP and PPO-multi are able to operate the robot at a higher frequency than the world. Precisely, frequency is k-times higher where k = 5 is the NDP rollout length (described in Section 3.2). Even though the robot moves at higher frequency, the environment/world state is only observed at normal rate, i.e., once every k robot steps and the reward computation at the intermediate k steps only use stale environment/world state from the first one of the k-steps. For instance, if the robot is pushing a puck, the reward is function of robot as well as puck's location. The robot will knows its own position at every policy step but will have access to stale value of puck's location only from actual environment step (sampled at a lower frequency than policy steps, specifically 5x less). We implemented this for all 50 Meta-World environments as well as Throwing and Picking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Codebases: NDPs (ours) and Baselines</head><p>Our code can be found at: https://shikharbahl.github.io/neural-dynamic-policies/.</p><p>Our algorithm is based on top of Proximal Policy Optimization (PPO) <ref type="bibr" target="#b41">[42]</ref> from https://github. com/ikostrikov/pytorch-a2c-ppo-acktr-gail <ref type="bibr" target="#b24">[25]</ref>. Additionally, we use code from Whitney et al. <ref type="bibr" target="#b48">[49]</ref> (DYN-E): https://github.com/willwhitney/dynamics-aware-embeddings. For our implementation of VICES <ref type="bibr" target="#b25">[26]</ref>, we use the controllers provided them in https://github.com/ pairlab/robosuite/tree/vices_iros19/robosuite and overlay those on our environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Vector field induced by NDPs. The goal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Given an observation from the environment, st, a Neural Dynamic Policy generates parameters w (weights of basis functions) and g (goal for the robot) for a forcing function f θ . An open loop controller then uses this function to output a set of actions for the robot to execute in the environment, collecting future states and rewards to train the policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Environment snapshot for different tasks considered in experiments. (a,b) Throwing and Picking tasks are adapted from [17] on the Kinova Jaco arm. (c-f) Remaining tasks are adapted from Yu et al. [50].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Evaluation of reinforcement learning setup for continuous control tasks. Y axis is success rate and X axis is number of environment samples. We compare to PPO<ref type="bibr" target="#b41">[42]</ref>, a multi-action version of PPO, VICES<ref type="bibr" target="#b25">[26]</ref> and DYN-E<ref type="bibr" target="#b48">[49]</ref>. The dynamic rollout for NDP &amp; PPO-multi is k = 5. Ablation of NDPs with respect to different hyperparameters in the RL setup (pushing). We ablate different choices of radial basis functions in (a). We ablate across number of basis functions, integration steps, and length of the NDP rollout in (b,c,d). Plots indicate that NDPs are fairly stable across a wide range of choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 1+( x) 2 ,</head><label>12</label><figDesc>the pushing task, varying the number of basis functions N (in the set {2, 6, 10, 15, 20}), DMP rollout lengths (in set {3, 5, 7, 10, 15}), number of integration steps (in set {15, 25, 35, 45}), as well as different basis functions: Gaussian RBF (standard), ψ defined in Equation (3), a liner map ψ(x) = x, a multiquadric map: ψ(x) = 1 + ( x) 2 , a inverse quadric map ψ(x) = and an inverse multiquadric map: ψ(x) = 1 √</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Ablation of NDPs with respect to different hyperparameters in the RL setup (throw). We ablate different choices of radial basis functions in (a). We ablate across number of basis functions, integration steps, and length of the NDP rollout in (b,c,d). Plots indicate that NDPs are fairly stable across a wide range of choices. Ablations for learning α (az) as well, only learning g (only-g), VICES at a lower control frequency (vices-lower-freq)and VICES for Throw in end-effector space (vices-pos)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Imitation learning on digit writing task.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Dynamical systems here should not be confused with dynamics model of the agent. We incorporate dynamical differential equations to represent robot's behavioral trajectory and not physical transition dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">robot's state y is not to be confused with environment observation s which contains world as well as robot state (and often velocity). s could be given by either an image or true state of the environment.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Giovanni Sutanto, Stas Tiomkin and Adithya Murali for fruitful discussions. We also thank Franziska Meier, Akshara Rai, David Held, Mengtian Li, George Cazenavette, and Wen-Hsuan Chu for comments on early drafts of this paper. This work was supported in part by DARPA Machine Common Sense grant and Google Faculty Award to DP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optnet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differentiable mpc for end-to-end planning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D J</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of direct and model-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Santamaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The option-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Geometric Control of Mechanical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on task-parameterized movement learning and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Service Robotics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning-based control strategy for safe humanrobot interaction exploiting task and robot redundancies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic movement primitives in latent space of time-dependent variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning in a handful of trials using probabilistic dynamics models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12114</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active learning of probabilistic movement primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Spergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04630</idno>
		<title level="m">Lagrangian neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical relative entropy policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pilco: A model-based and data-efficient approach to policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A survey on policy search for robotics. Found. Trends Robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaussian processes for data-efficient learning in robotics and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Divide-and-conquer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09874</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hamiltonian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greydanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernelized movement primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silvério</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Movement imitation with nonlinear dynamical systems in humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<editor>ICRA. IEEE</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning attractor landscapes for learning motor primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamical movement primitives: Learning attractor models for motor behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning motor primitives for robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robot motor skill coordination with em-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kormushev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pytorch implementations of reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<ptr target="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail" />
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variable impedance control in end-effector space: An action space for reinforcement learning in contact-rich tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to select and generalize striking movements in robot table tennis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mülling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kroemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning robot motions with stable dynamical systems under diffeomorphic transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep encoder-decoder networks for mapping raw images to dynamic movement primitives. ICRA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pahic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforcement learning vs human programming in tetherball robot games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdulsamad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paraschos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning and generalization of motor skills by learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skill learning and task outcome prediction for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast diffeomorphic matching to learn globally asymptotically stable nonlinear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schlehuber-Caissier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning task-parameterized dynamic movement primitives using mixture of gmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pervez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Service Robotics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reinforcement learning for humanoid robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Humanoid Robots</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic movement primitives for humanrobot interaction: Comparison with human behavioral observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Remazeilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Endo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning feedback terms for reactive planning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sutanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13143</idno>
		<title level="m">Euclideanizing flows: Diffeomorphic reduction for learning stable dynamical systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Issac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02854</idno>
		<title level="m">Riemannian motion policies</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning partially contracting dynamical systems from demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ravichandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dynamic movement primitives-a framework for motor control in humans and humanoid robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive motion of animals and machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Robot modeling and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Spong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vidyasagar</surname></persName>
		</author>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reinforcement learning with sequences of motion primitives for robust manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning sensor feedback models from demonstrations via phase-modulated neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sutanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Task-specific generalization of discrete and periodic dynamic movement primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09357</idno>
		<title level="m">Dynamics-aware embeddings</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10897</idno>
		<title level="m">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
