<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Deformation Meta-Networks for One-Shot Learning (a) (b) (c) (d) (e)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Schools of Computer Science, and Data Science</orgName>
								<orgName type="laboratory">Jilian Technology Group (Video++)</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<email>yanweifu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Schools of Computer Science, and Data Science</orgName>
								<orgName type="laboratory">Jilian Technology Group (Video++)</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
							<email>yuxiongw@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
							<email>hebert@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Deformation Meta-Networks for One-Shot Learning (a) (b) (c) (d) (e)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Illustration of a variety of image deformations: ghosted (a, b), stitched (c), montaged (d), and partially occluded (e) images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Humans can robustly learn novel visual concepts even when images undergo various deformations and lose certain information. Mimicking the same behavior and synthesizing deformed instances of new concepts may help visual recognition systems perform better one-shot learning, i.e., learning concepts from one or few examples. Our key insight is that, while the deformed images may not be visually realistic, they still maintain critical semantic information and contribute significantly to formulating classifier decision boundaries. Inspired by the recent progress of meta-learning, we combine a meta-learner with an image deformation sub-network that produces additional training examples, and optimize both models in an end-to-end manner. The deformation sub-network learns to deform images by fusing a pair of images -a probe image that keeps the visual content and a gallery image that diversifies the deformations. We demonstrate results on the widely used one-shot learning benchmarks (miniImageNet and Im-ageNet 1K Challenge datasets), which significantly outperform state-of-the-art approaches. Code is available at https://github.com/tankche1/IDeMe-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Humans can robustly learn novel visual concepts even when images undergo various deformations and lose certain information. Mimicking the same behavior and synthesizing deformed instances of new concepts may help visual recognition systems perform better one-shot learning, i.e., learning concepts from one or few examples. Our key insight is that, while the deformed images may not be visually realistic, they still maintain critical semantic information and contribute significantly to formulating classifier decision boundaries. Inspired by the recent progress of meta-learning, we combine a meta-learner with an image deformation sub-network that produces additional training examples, and optimize both models in an end-to-end manner. The deformation sub-network learns to deform images by fusing a pair of images -a probe image that keeps the visual content and a gallery image that diversifies the deformations. We demonstrate results on the widely used one-shot learning benchmarks (miniImageNet and Im-ageNet 1K Challenge datasets), which significantly outperform state-of-the-art approaches. Code is available at https://github.com/tankche1/IDeMe-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep architectures have made significant progress in various visual recognition tasks, such as image classification * Yanwei Fu is the corresponding author. and object detection. This success typically relies on supervised learning from large amounts of labeled examples. In real-world scenarios, however, one may not have enough resources to collect large training sets or need to deal with rare visual concepts. It is also unlike the human visual system, which can learn a novel concept with very little supervision. One-shot or low/few-shot learning <ref type="bibr" target="#b4">[4]</ref>, which aims to build a classifier for a new concept from one or very few labeled examples, has thus attracted more and more attention.</p><p>Recent efforts to address this problem have leveraged a learning-to-learn or meta-learning paradigm <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b13">13]</ref>. Meta-learning algorithms train a learner, which is a parameterized function that maps labeled training sets to classifiers. Meta-learners are trained by sampling a collection of one-shot learning tasks and the corresponding datasets from a large universe of labeled examples of known (base) categories, feeding the sampled small training set to the learner to obtain a classifier, and then computing the loss of the classifier on the sampled test set. The goal is that the learner is able to tackle the recognition of unseen (novel) categories from few training examples.</p><p>Despite their noticeable performance improvements, these generic meta-learning algorithms typically treat images as black boxes and ignore the structure of the visual world. By contrast, our biological vision system is very robust and trustable in understanding images that undergo various deformations <ref type="bibr" target="#b27">[27]</ref>. For instance, we can easily recognize the objects in <ref type="figure" target="#fig_0">Figure 1</ref>, despite ghosting ( <ref type="figure" target="#fig_0">Figure 1</ref>(a, b)), stitching (Figure 1(c)), montaging ( <ref type="figure" target="#fig_0">Figure 1(d)</ref>), and partially occluding <ref type="figure" target="#fig_0">(Figure 1</ref>(e)) the images. While these deformed images may not be visually realistic, our key insight is that they still maintain critical semantic information and presumably serve as "hard examples" that contribute significantly to formulating classifier decision boundaries. Hence, by leveraging such modes of deformations shared across categories, the synthesized deformed images could be used as additional training data to build better classifiers.</p><p>A natural question then arises: how could we produce informative deformations? We propose a simple parametrization that linearly combines a pair of images to generate the deformed image. We use a probe image to keep the visual content and overlay a gallery image on a patch level to introduce appearance variations, which could be attributed to semantic diversity, artifacts, or even random noise. <ref type="bibr">Figure 5</ref> shows some examples of our deformed images. Importantly, inspired by <ref type="bibr" target="#b30">[30]</ref>, we learn to deform images that are useful for a classification objective by end-to-end metaoptimization that includes image deformations in the model.</p><p>Our Image Deformation Meta-Network (IDeMe-Net) thus consists of two components: a deformation subnetwork and an embedding sub-network. The deformation sub-network learns to generate the deformed images by linearly fusing the patches of probe and gallery images. Specifically, we treat the given small training set as the probe images and sample additional images from the base categories to form the gallery images. We evenly divide the probe and gallery images into nine patches, and the deformation sub-network estimates the combination weight of each patch. The synthesized images are used to augment the probe images and train the embedding sub-network, which maps images to feature representations and performs oneshot classification. The entire network is trained in an endto-end meta-learning manner on base categories.</p><p>Our contributions are three-fold. (1) We propose a novel image deformation framework based on metalearning to address one-shot learning, which leverages the rich structure of shared modes of deformations in the visual world. (2) Our deformation network learns to synthesize diverse deformed images, which effectively exploits the complementarity and interaction between the probe and gallery image patches. (3) By using the deformation network, we effectively augment and diversify the one-shot training images, leading to a significant performance boost on one-shot learning tasks. Remarkably, our approach achieves state-ofthe-art performance on both the challenging ImageNet1K and miniImageNet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Meta-Learning. Typically, meta-learning <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b15">15]</ref> aims at training a parametrized mapping from a few training instances to model parameters in simulated one-shot learning scenarios. Other meta-learning strategies in one-shot learning include graph CNNs <ref type="bibr" target="#b7">[7]</ref> and memory networks <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b1">1]</ref>. Attention is also introduced in meta-learning, in ways of analyzing the relation between visual and semantic representations <ref type="bibr" target="#b29">[29]</ref> and learning the combination of temporal convolutions and soft attention <ref type="bibr" target="#b14">[14]</ref>. Different from prior work, we focus on exploiting the complementarity and interaction between visual patches through the meta-learning mechanism. Metric Learning. This is another important line of work in one-shot learning. The goal is to learn a metric space which can be optimized for one-shot learning. Recent work includes Siamese networks <ref type="bibr" target="#b11">[11]</ref>, matching networks <ref type="bibr" target="#b28">[28]</ref>, prototypical networks <ref type="bibr" target="#b22">[22]</ref>, relation networks <ref type="bibr" target="#b23">[23]</ref>, and dynamic few-shot learning without forgetting <ref type="bibr" target="#b8">[8]</ref>. Data Augmentation. The key limitation of one-shot learning is the lack of sufficient training images. As a common practice, data augmentation has been widely used to help train supervised classifiers <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b34">34]</ref>. The standard techniques include adding Gaussian noise, flipping, rotating, rescaling, transforming, and randomly cropping training images. However, the generated images in this way are particularly subject to visual similarity with the original images. In addition to adding noise or jittering, previous work seeks to augment training images by using semi-supervised techniques <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b16">16]</ref> and utilizing relation between visual and semantic representations <ref type="bibr" target="#b3">[3]</ref> , or directly synthesizing new instances in the feature domain <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b6">6]</ref> to transfer knowledge of data distribution from base classes to novel classes. By contrast, we also use samples from base classes to help synthesize deformed images but directly aim at maximizing the one-shot recognition accuracy.</p><p>The most relevant to our approach is the work of <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b35">35]</ref>. Wang et al. <ref type="bibr" target="#b30">[30]</ref> introduce a GAN-like generator to hallucinate novel instances in the feature domain by adding noise, whereas we focus on learning to deform two real images in the image domain without introducing noise. Zhang et al. <ref type="bibr" target="#b35">[35]</ref> randomly sample image pairs and linearly combine them to generate additional training images. In this mixup augmentation, the combination is performed with weights randomly sampled from a prior distribution and is thus constrained to be convex. The label of the generated image is similarly the linear combination of the labels (as onehot label vectors) of the image pairs. However, they ignore structural dependencies between images as well as image patches. By contrast, we learn classifiers to select images that are similar to the probe images from the unsupervised gallery image set. Our combination weights are learned through a deformation sub-network on the image patch level and the combination is not necessarily convex.</p><p>In addition, our generated image preserves the label of its probe image. Comparing with these methods, our approach learns to dynamically fuse patches of two real images in an end-to-end manner. The produced images maintain the important patches of original images while being visually different from them, thus facilitating training one-shot classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">One-Shot Learning Setup</head><p>Following recent work <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b30">30]</ref>, we establish one-shot learning in a meta-learning framework: we have a base category set C base and a novel category set C novel , in which C base ∩ C novel = ∅; correspondingly, we have a base dataset D base = {(I i , y i ) , y i ∈ C base } and a novel dataset</p><formula xml:id="formula_0">D novel = {(I i , y i ) , y i ∈ C novel }.</formula><p>We aim to learn a classification algorithm on D base that can generalize to unseen classes C novel with one or few training examples per class.</p><p>To mimic the one-shot learning scenario, meta-learning algorithms learn from a collection of N -way-m-shot classification tasks/datasets sampled from D base and are evaluated in a similar way on D novel . Each of these sampled datasets is termed as an episode, and we thus have different meta-sets for meta-training and meta-testing. Specifically, we randomly sample N classes L ∼ C k for a meta-training (i.e., k = base) or meta-testing episode (i.e., k = novel). We then randomly sample m and q labeled images per class in L to construct the support set S and query set Q, respectively, i.e., |S| = N × m and |Q| = N × q. During meta-training, we sample S and Q to train our model. During meta-testing, we evaluate by averaging the classification accuracy on query sets Q of many meta-testing episodes.</p><p>We view the support set as supervised probe images and different from the previous work, we introduce an additional gallery image set G that serves as an unsupervised image pool to help generate deformed images. To construct G, we randomly sample some images per base class from the base dataset, i.e., G ∼ D base . The same G is used in both the meta-training and meta-testing episodes. Note that since it is purely sampled from D base , the newly introduced G does not break the standard one-shot setup as in <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b17">17]</ref>. We do not introduce any additional images from the novel categories C novel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image Deformation Meta-Networks</head><p>We now explain our image deformation meta-network (IDeMe-Net) for one-shot learning. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of IDeMe-Net f θ (·) parametrized by θ. IDeMe-Net is composed of two modules -a deformation subnetwork and an embedding sub-network. The deformation sub-network adaptively fuses the probe and gallery images to synthesize the deformed images. The embedding subnetwork maps the images to feature representations and then constructs the one-shot classifier. The entire metanetwork is trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deformation Sub-network</head><p>This sub-network f θ def (·) learns to explore the interaction and complementarity between the probe images I probe ((I probe , y probe ) ∈ S) and the gallery images I gallery ∈ G, and fuses them to generate the synthesized deformed images I syn , i.e., I syn = f θ def (I probe , I gallery ). Our goal is to synthesize meaningful deformed images such that y syn = y probe . This is achieved by using two strategies: (1) y syn = y probe is explicitly enforced as a constraint during the end-to-end optimization; <ref type="bibr" target="#b2">(2)</ref> we propose an approach to sample I gallery that are visually or semantically similar to the images of y probe . Specifically, for each class y probe , we directly use the feature extractor and one-shot classifier learned in the embedding sub-network to select the top % images from G which have the highest class probability of y probe . From this initial pool of images, we randomly sample I gallery for each probe image (I probe , y probe ). Note that during meta-training, both I probe and I gallery are randomly sampled from base classes, so they might belong to the same class. We find that further constraining them to belong to different base classes has little impact on the performance. During meta-testing, I probe and I gallery belong to different classes, with I probe sampled from novel classes and I gallery still from base classes.</p><p>Two branches, ANET and BNET, are used to parse I probe and I gallery , respectively. Each of them is a residual network <ref type="bibr" target="#b10">[10]</ref> without fully-connected layers. The outputs of ANET and BNET are then concatenated to be fed into a fully-connected layer, which produces a 9-D weight vector w. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we evenly divide the images into 3×3 patches. The deformed image is thus simply generated as a linearly weighted combination of I probe and I gallery on the patch level. That is, for the qth patch, we have</p><formula xml:id="formula_1">I syn,q = w q I probe,q + (1 − w q ) I gallery,q .<label>(1)</label></formula><p>We assign the class label y probe to the synthesized deformed image I syn . For any probe image I i probe , we sample n aug gallery images from the corresponding pool and produce n aug synthesized deformed images. We thus obtain an augmented support set</p><formula xml:id="formula_2">S = I i probe , y i probe , I i,j syn , y i,j probe naug j=1 N ×m i=1 . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Embedding Sub-network</head><p>The embedding sub-network f θ emb (·) consists of a deep convolutional network for feature extraction and a nonparametric one-shot classifier. Given an input image I, we use a residual network <ref type="bibr" target="#b10">[10]</ref> to produce its feature representation f θ emb (I). To facilitate the training process, we introduce an additional softmax classifier, i.e., a fully-connected layer on top of the embedding sub-network with a crossentropy loss (CELoss), that outputs |C base | scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">One-Shot Classifier</head><p>Due to its superior performance, we use the nonparametric prototype classifier <ref type="bibr" target="#b22">[22]</ref> as the one-shot classifier. During each episode, given the sampled S, Q, and G, the deformation sub-network produces the augmented support setS. Following <ref type="bibr" target="#b22">[22]</ref>, we calculate the prototype vector p c θ for each class c inS as</p><formula xml:id="formula_3">p c θ = 1 Z (Ii,yi)∈S f θ emb (I i ) · y i = c ,<label>(3)</label></formula><p>where Z = Σ (Ii,yi)∈S y i = c is the normalization factor. · is the Iverson's bracket notation: x = 1 if x is true, and 0 otherwise. Given any query image I i ∈ Q, its probability of belonging to class c is computed as</p><formula xml:id="formula_4">P θ (y i = c|I i ) = exp − f θ emb (I i ) − p c θ 2 N j=1 exp − f θ emb (I i ) − p j θ 2 ,<label>(4)</label></formula><p>where · indicates the Euclidean distance. The one-shot classifier P thus predicts the class label of I i as the highest probability over N classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Strategy of IDeMe-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training Loss</head><p>Training the entire IDeMe-Net includes two subtasks: (1) training the deformation sub-network which maximally improves the one-shot classification accuracy; (2) building the robust embedding sub-network which effectively deals with various synthesized deformed images. Note that our one-shot classifier has no parameters, which does not need to be trained. We use the prototype loss and the crossentropy loss to train these two sub-networks, respectively. Update the deformation sub-network. We optimize the following prototype loss function to endow the deformation sub-network with the desired one-shot classification ability: //sample the query set <ref type="bibr">8:</ref> train the prototype classifier P from f θ emb (S) 9:S ← S initialize the augment support set <ref type="bibr">10:</ref> for c in L do enumerate the chosen classes <ref type="bibr">11:</ref> pool ←use P to select % images in G that have the highest class probability of c <ref type="bibr">12:</ref> for (I probe , c) in S c do <ref type="bibr">13:</ref> for j = 1 to n aug do <ref type="bibr">14:</ref> I gallery ← randomly sample instances from pool <ref type="bibr">15:</ref> I syn ← f θ def (I probe , I gallery ) <ref type="bibr" target="#b16">16</ref> train the prototype classifierP from f θ emb (S) <ref type="bibr">21:</ref> useP to classify f θ emb (Q) and obtain the prototype loss <ref type="bibr">22:</ref> use the softmax classifier to classify f θ emb (S) and obtain the CELoss <ref type="bibr">23:</ref> update θ emb with the CELoss <ref type="bibr">24:</ref> update θ def with the prototype loss 25: end procedure where P θ (y i | I i ) is the one-shot classifier in Eq. (4). Using the prototype loss encourages the deformation sub-network to generate diverse instances to augment the support set. Update the embedding sub-network. We use the crossentropy loss to train the embedding sub-network to directly classify the augmented support setS. Note that with the augmented support setS, we have relatively more training instances to train this sub-network and the cross-entropy loss is the standard loss function in training a supervised classification network. Empirically, we find that using the cross-entropy loss speeds up the convergence and improves the recognition performance than using the prototype loss only.</p><formula xml:id="formula_5">min θ E G,L∼D base E S,Q∼L   (Ii,yi)∈Q −logP θ (y i | I i )   ,<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Strategy</head><p>We summarize the entire training procedure of our IDeMe-Net on the base dataset D base in Algorithm 1. We have access to the same, predefined gallery G from D base for both meta-training and meta-testing. During metatraining, we sample the N -way-m-shot training episode to produce S and Q from D base . The embedding sub-network learns an initial one-shot classifier on S using Eq. (4). Given a probe image I probe , we then sample the gallery images I gallery ∼ G and train the deformation sub-network to generate the augmented support setS using Eq. (1).S is further used to update the embedding sub-network and learn a better one-shot classifier. We then conduct the final oneshot classification on the query set Q and back-propagate the prediction error to update the entire network. During meta-testing, we sample the N -way-m-shot testing episode to produce S and Q from the novel dataset D novel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Our IDeMe-Net is evaluated on two standard benchmarks: miniImageNet <ref type="bibr" target="#b28">[28]</ref> and ImageNet 1K Challenge <ref type="bibr" target="#b9">[9]</ref> datasets. miniImageNet is a widely used benchmark in oneshot learning, which includes 600 images per class and has 100 classes in total. Following the data split in <ref type="bibr" target="#b17">[17]</ref>, we use 64, 16, 20 classes as the base, validation, and novel category set, respectively. The hyper-parameters are cross-validated on the validation set. Consistent with <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b17">17]</ref>, we evaluate our model in 5-way-5-shot and 5-way-1-shot settings.</p><p>For the large-scale ImageNet 1K dataset, we divide the original 1K categories into 389 base (D base ) and 611 novel (D novel ) classes following the data split in <ref type="bibr" target="#b9">[9]</ref>. The base classes are further divided into two disjoint subsets: base validation set D cv base (193 classes) and evaluation set D f in base (196 classes) and the novel classes are divided into two subsets as well: novel validation set D cv novel (300 classes) and evaluation set D f in novel (311 classes). We use the base/novel validation set D cv for cross-validating hyper-parameters and use the base/novel evaluation set D f in to conduct the final experiments. The same experimental setup is used in <ref type="bibr" target="#b9">[9]</ref> and the reported results are averaged over 5 trials. Here we focus on synthesizing novel instances and we thus evaluate the performance primarily on novel classes, i.e., 311-waym-shot settings, which is also consistent with most of the contemporary work <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b17">17</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on ImageNet 1K Challenge</head><p>Setup. We use ResNet-10 architectures for ANET and BNET (i.e., the deformation sub-network). For a fair comparison with <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b30">30]</ref>, we evaluate the performance of using ResNet-10 <ref type="table" target="#tab_1">(Table 1)</ref> and ResNet-50 <ref type="table">(Table 2)</ref> for the embedding sub-network. Stochastic gradient descent (SGD) is used to train IDeMe-Net in an end-to-end manner. It gets converged over 100 epochs. The initial learning rates of ANET, BNET, and the embedding sub-network are set as 3 × 10 −3 , 3 × 10 −3 , and 10 −1 , respectively, and decreased by 1/10 every 30 epochs. The batch size is set as 32. We randomly sample 10 images per base category to construct the gallery G and we set as 2. Note that G is fixed during the entire experiments. ANET, BNET, and the embedding sub-network are trained from scratch on D base . Our model is evaluated on D novel . n aug is cross-validated as 8, which balances between the computational cost and the augmented training data scale. In practice, we perform stage-wise training to overcome potential negative influence caused by misleading training images synthesized by the initial deformation sub-network. Specifically, to make the training more robust, we first fix the deformation subnetwork and train the embedding sub-network with real and deformed images. Here the deformed images are synthesized by linearly combining two images on a patch level with a randomly sampled weight vector w. Note that these two images are sampled from the same category. Then we fix the embedding sub-network and learn the deformation sub-network to reduce the discrepancy between synthesized and real images. Finally, we train the embedding and deformation sub-networks jointly (i.e., the entire IDeMe-Net) to allow them to cooperate with each other. Baselines and Competitors. We compare against several baselines and competitors as follows. <ref type="formula" target="#formula_1">(1)</ref> We directly train a ResNet-10 feature extractor on D base and use it to compute features on D novel . We then train standard supervised classifiers on D novel , including neural network, support vector machine (SVM), logistic regression (LR), and prototype classifiers. The neural network classifier consists of a fullyconnected layer and a softmax layer. (2) We compare with state-of-the-art approaches to one-shot learning, such as matching networks <ref type="bibr" target="#b28">[28]</ref>, generation SGM <ref type="bibr" target="#b9">[9]</ref>, prototypical networks <ref type="bibr" target="#b22">[22]</ref>, Cosine Classifier &amp; Att. Weight Gen (Cos &amp; Att.) <ref type="bibr" target="#b8">[8]</ref>, CP-ANN <ref type="bibr" target="#b6">[6]</ref>, PMN, and PMN w/ H <ref type="bibr" target="#b30">[30]</ref>. <ref type="bibr" target="#b3">(3)</ref> The data augmentation methods are also compared -flipping: the input image is flipped from left to right; Gaussian noise: cross-validated Gaussian noise N (0, 10) is added to each pixel of the input image; Gaussian noise (feature level): cross-validated Gaussian noise N (0, 0.3) is added to each dimension of the ResNet feature for each image; Mixup: using mixup <ref type="bibr" target="#b35">[35]</ref> to combine probe and gallery images. For fair comparisons, all these augmentation methods use the prototype classifier as the one-shot classifier. Results. <ref type="table" target="#tab_1">Tables 1 and 2</ref> summarize the results of using ResNet-10 and ResNet-50 as the embedding sub-network, respectively. For example, using ResNet-10, the top-5 accuracy of IDeMe-Net in <ref type="table" target="#tab_1">Table 1</ref> is superior to the prototypical network by 7% when m = 1, 2, 5, showing the sample efficiency of IDeMe-Net for one-shot learning. With more data (e.g., m = 10, 20), while the plain prototype classifier baseline performs worse than other baselines (e.g., PMN), our deformed images coupled with the prototype classifier still have significant effect (e.g., 3.5 point boost when m = 10). The top-1 accuracy demonstrates the similar trend. Using ResNet-50 as the embedding sub-network, the performance of all the approaches improves and our IDeMe-Net consistently achieves the best performance, as shown in <ref type="table">Table 2</ref>. <ref type="figure" target="#fig_3">Figure 3</ref>(a) further highlights that our IDeMe-Net consis-tently outperforms all the baselines by large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study on ImageNet 1K Challenge</head><p>We conduct extensive ablation studies to evaluate the contribution of each component in our model. Variants of IDeMe-Net. We consider seven different variants of our IDeMe-Net, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b) and Table 3. (1) 'IDeMe-Net -CELoss': the IDeMe-Net is trained using only the prototype loss without the cross-entropy loss (CELoss). (2) 'IDeMe-Net -Proto Loss': the IdeMe-Net is trained using only the cross-entropy loss without the prototype loss. (3) 'IDeMe-Net -Predict': the gallery images are randomly chosen in IDeMe-Net without predicting their class probability. (4) 'IDeMe-Net -Aug. Testing': the deformed images are not produced in the meta-testing phase. (5) 'IDeMe-Net -Def. Network': the combination weights in Eq. (1) are randomly generated instead of using the learned deformation sub-network. (6) 'IDeMe-Net -Gallery': the gallery images are directly sampled from the support set instead of constructing an additional Gallery. <ref type="bibr" target="#b7">(7)</ref> 'IDeMe-Net -Deform': we simply use the gallery images to serve as the deformed images. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b), our full IDeMe-Net model outperforms all these variants, showing that each component is essential and complementary to each other.</p><p>We note that (1) Using CELoss and prototype loss to update the embedding and deformation sub-networks, respectively, achieves the best result. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>(b), the accuracy of 'IDeMe-Net -CELoss' is marginally lower than IDeMe-Net but still higher than the prototype classifier baseline, while 'IDeMe-Net -Proto Loss' underperforms the baseline. (2) Our strategy for selecting the gallery images is the key to diversify the deformed images. Randomly choosing the gallery images ('IDeMe-Net -Predict') or sampling the gallery images from the support set ('IDeMe-Net -Gallery') obtains no performance improvement. One potential explanation is that they only introduce noise or redundancy and do not bring in useful information. (3) Our improved performance mainly comes from the diversified deformed images, rather than the embedding sub-network. Without producing the deformed images in the meta-testing phase ('IDeMe-Net -Aug. Testing'), the performance is close to the baseline, suggesting that training on the deformed images does not obviously benefit from the embedding sub-network. (4) Our meta-learned deformation subnetwork effectively exploits the complementarity and interaction between the probe and gallery image patches, producing the key information in the deformed images.</p><p>To show this point, we investigate two deformation strategies: randomly generating the weight vector w ('IDeMe-Net -Def. Network') and setting all the weights to be 0 ('IDeMe-Net -Deform'); in the latter case, it is equivalent to purely using the gallery images to serve as the deformed images. Both strategies perform worse than the prototype classifier baseline, indicating the importance of meta-learning a deformation strategy. Different division schemes. In the deformation subnetwork and Eq. (1), we evenly split the image into 3 × 3 patches. Some alternative division schemes are compared in <ref type="table" target="#tab_2">Table 3</ref> and <ref type="figure" target="#fig_3">Figure 3</ref>(c). Specifically, we consider the 1 × 1, 5 × 5, 7 × 7, and pixel-level division schemes and report the results as IDeMe-Net (1 × 1), IDeMe-Net (5 × 5), IDeMe-Net (7 × 7), and IDeMe-Net (pixel level), respectively. The experimental results suggest the patch-level fusion, rather  the patch-level division (3 × 3, 5 × 5, and 7 × 7) considers image patches as the basic unit to maintain some local information. In addition, the results show that using a finegrained patch size (e.g., 5 × 5 division and 7 × 7 division) may achieve slightly better results than our 3×3 division. In brief, our patch-level division not only maintains the critical region information but also increases diversity. Number of synthesized deformed images. We also show how the top-5 accuracy changes with respect to the number of synthesized deformed images in <ref type="figure" target="#fig_3">Figure 3(d)</ref>. Specifically, we change the number of synthesized deformed images n aug in the deformation sub-network, and plot the 5shot top-5 accuracy on the Imagenet 1K Challenge dataset. It shows that when n aug is changed from 0 to 8, the performance of our IDeMe-Net is gradually improved. The performance saturates when enough deformed images are generated (n aug &gt; 8).</p><p>Visualization of deformed images in feature space. <ref type="figure" target="#fig_4">Figure 4</ref> shows the t-SNE <ref type="bibr" target="#b26">[26]</ref> visualization of 5 novel classes from our IDeMe-Net, the Gaussian noise baseline, and the <ref type="table">Table 4</ref>. Top-1 accuracy (%) on novel classes of the miniImageNet dataset. "±" indicates 95% confidence intervals over tasks.</p><p>'IDeMe-Net -Deform' variant. For the Gaussian noise baseline, the synthesized images are heavily clustered and close to the probe images. By contrast, the synthesized deformed images of our IDeMe-Net scatter widely in the class manifold and tend to locate more around the class boundaries. For 'IDeMe-Net -Deform', the synthesized images are the same as the gallery images and occasionally fall into manifolds of other classes. Interesting, comparing <ref type="figure" target="#fig_4">Figure 4</ref>(b) and <ref type="figure" target="#fig_4">Figure 4</ref>(c), our IDeMe-Net effectively deforms those misleading gallery images back to the correct class manifold. Visualization of deformed images in image space. Here we show some examples of our deformed images on novel classes in <ref type="figure" target="#fig_5">Figure 5</ref>. We can observe that the deformed images (in the third row) are visually different from the probe images (in the first row) and the gallery images (in the second row). For novel classes (e.g., vase and golden retriever), our method learns to find visual samples that are similar in shape and geometry (e.g., jelly fish, garbage bin, and soup pot) or similar in appearance (e.g., poodle and walker hound). By doing so, the deformed images preserve important visual content from the probe images and introduce new visual contents from the gallery images, thus diversifying and augmenting the training images in a way that maximizes the one-shot classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results on miniImageNet</head><p>Setup and Competitors. We use a ResNet-18 architecture as the embedding sub-network. We randomly sample 30 images per base category to construct the gallery G. Other settings are the same as those on the ImageNet 1k Challenge dataset. As summarized in <ref type="table">Table 4</ref>, we mainly focus on three groups of competitors: (1) meta-learning algorithms, such as MAML <ref type="bibr" target="#b5">[5]</ref> and Meta-SGD <ref type="bibr" target="#b13">[13]</ref>; (2) metric learning algorithms, including matching networks <ref type="bibr" target="#b28">[28]</ref>, prototypical networks <ref type="bibr" target="#b22">[22]</ref>, relation networks <ref type="bibr" target="#b23">[23]</ref>, SNAIL <ref type="bibr" target="#b14">[14]</ref>, delta-encoder <ref type="bibr" target="#b21">[21]</ref>, and Cosine Classifier &amp; Att. Weight Gen (Cos &amp; Att.) <ref type="bibr" target="#b8">[8]</ref>.</p><p>Results. We report the results in <ref type="table">Table 4</ref>. Impressively, our IDeMe-Net consistently outperforms all these state-ofthe-art competitors. This further validates the general effectiveness of our proposed approach in addressing one-shot learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a conceptually simple yet powerful approach to address one-shot learning that uses a trained image deformation network to generate additional examples. Our deformation network leverages unsupervised gallery images to synthesize deformed images, which is trained end-to-end by meta-learning. The extensive experiments demonstrate that our approach achieves state-of-the-art performance on multiple one-shot learning benchmarks, surpassing the competing methods by large margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of a variety of image deformations: ghosted (a, b), stitched (c), montaged (d), and partially occluded (e) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of our image deformation meta-network (IDeMe-Net).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 ) 1</head><label>51</label><figDesc>Algorithm Meta-training procedure of our IDeMe-Net f θ . G is the fixed gallery constructed from C base . 1: procedure META-TRAIN EPISODE 2: The procedure of one meta-training episode 3: L ← randomly sample N classes from C base 4: S ← randomly sample instances belonging to L 5: //sample the support set 6:Q ← randomly sample instances belonging to L 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Ablation study on ImageNet 1K Challenge dataset: (a) highlights the comparison with several competitors; (b) shows the impact of different components on our IDeMe-Net; (c) analyzes the impact of different division schemes; (d) shows how the performance changes with respect to the number of synthesized deformed images. Best viewed in color with zoom.than image-level or pixel-level fusion in our IDeMe-Net. The image-level division (1 × 1) ignores the local image structures and deforms through a global combination, thus decreasing the diversity. The pixel-level division is particularly subject to the disarray of the local information, while (a) Gaussian Baseline (b) IDeMe-Net -Deform (c) IDeMe-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization of 5 novel classes. Dots, stars, and triangles represent the real examples, the probe images, and the synthesized deformed images, respectively. (a) Synthesis by adding Gaussian noise. (b) Synthesis by directly using the gallery images. (c) Synthesis by our IDeMe-Net. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Examples of the deformed images during meta-testing. 1st row: probe images of novel classes. 2nd: gallery images of base classes. 3rd: synthesized images. The probegallery image pairs from left to right: vase-jellyfish, vaseoboe, vase-garbage bin, vase-soup pot, golden retriever-poodle, golden retriever-walker hound, golden retriever-walker hound, and golden retriever-poodle. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:S ←S ∪ (I syn , c)</figDesc><table><row><cell>17:</cell><cell>end for</cell></row><row><cell>18:</cell><cell>end for</cell></row><row><cell>19:</cell><cell>end for</cell></row><row><cell>20:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>m = 1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell>Softmax</cell><cell></cell><cell></cell><cell>-/ 16.3</cell><cell>-/ 35.9</cell><cell>-/ 57.4</cell><cell>-/ 67.3</cell><cell>-/ 72.1</cell></row><row><cell cols="2">Baselines</cell><cell>LR SVM</cell><cell></cell><cell></cell><cell>18.3/42.8 15.9/36.6</cell><cell>26.0/54.7 22.7/48.4</cell><cell>35.8/66.1 31.5/61.2</cell><cell>41.1/71.3 37.9/69.2</cell><cell>44.9/74.8 43.9/74.6</cell></row><row><cell></cell><cell></cell><cell cols="2">Prototype Classifier</cell><cell></cell><cell>17.1/39.2</cell><cell>24.3/51.1</cell><cell>33.8/63.9</cell><cell>38.4/69.9</cell><cell>44.1/74.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Matching Network [28]</cell><cell></cell><cell>-/ 43.0</cell><cell>-/ 54.1</cell><cell>-/ 64.4</cell><cell>-/ 68.5</cell><cell>-/ 72.8</cell></row><row><cell></cell><cell></cell><cell cols="3">Prototypical Network [22]</cell><cell>16.9/41.7</cell><cell>24.0/53.6</cell><cell>33.5/63.7</cell><cell>37.7/68.2</cell><cell>42.7/72.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Generation SGM [9]</cell><cell></cell><cell>-/ 34.3</cell><cell>-/ 48.9</cell><cell>-/ 64.1</cell><cell>-/ 70.5</cell><cell>-/ 74.6</cell></row><row><cell cols="2">Competitors</cell><cell>PMN [30]</cell><cell></cell><cell></cell><cell>-/ 43.3</cell><cell>-/ 55.7</cell><cell>-/ 68.4</cell><cell>-/ 74.0</cell><cell>-/ 77.0</cell></row><row><cell></cell><cell></cell><cell cols="2">PMN w/ H [30]</cell><cell></cell><cell>-/ 45.8</cell><cell>-/ 57.8</cell><cell>-/ 69.0</cell><cell>-/ 74.3</cell><cell>-/ 77.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Cos &amp; Att. [8]</cell><cell></cell><cell>-/ 46.0</cell><cell>-/ 57.5</cell><cell>-/ 69.1</cell><cell>-/ 74.8</cell><cell>-/ 78.1</cell></row><row><cell></cell><cell></cell><cell>CP-AAN [6]</cell><cell></cell><cell></cell><cell>-/ 48.4</cell><cell>-/ 59.3</cell><cell>-/ 70.2</cell><cell>-/ 76.5</cell><cell>-/ 79.3</cell></row><row><cell></cell><cell></cell><cell>Flipping</cell><cell></cell><cell></cell><cell>17.4/39.6</cell><cell>24.7/51.2</cell><cell>33.7/64.1</cell><cell>38.7/70.2</cell><cell>44.2/74.5</cell></row><row><cell cols="2">Augmentation</cell><cell cols="3">Gaussian Noise Gaussian Noise (feature level)</cell><cell>16.8/39.0 16.7/39.1</cell><cell>24.0/51.2 24.2/51.4</cell><cell>33.9/63.7 33.4/63.3</cell><cell>38.0/69.7 38.2/69.5</cell><cell>43.8/74.5 44.0/74.2</cell></row><row><cell></cell><cell></cell><cell>Mixup [35]</cell><cell></cell><cell></cell><cell>15.8/38.7</cell><cell>24.6/51.4</cell><cell>32.0/61.1</cell><cell>38.5/69.2</cell><cell>42.1/72.9</cell></row><row><cell>Ours</cell><cell></cell><cell>IDeMe-Net</cell><cell></cell><cell></cell><cell>23.1/51.0</cell><cell>30.1/60.9</cell><cell>39.3/70.4</cell><cell>42.7/73.4</cell><cell>45.0/75.1</cell></row><row><cell>Method</cell><cell>m = 1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell></cell><cell></cell></row><row><cell>Softmax</cell><cell>-/ 28.2</cell><cell>-/ 51.0</cell><cell>-/ 71.0</cell><cell>-/ 78.4</cell><cell></cell><cell></cell></row><row><cell>SVM</cell><cell>20.1/41.6</cell><cell>29.4/57.7</cell><cell>42.6/72.8</cell><cell>49.9/79.1</cell><cell></cell><cell></cell></row><row><cell>LR</cell><cell>22.9/47.9</cell><cell>32.3/61.3</cell><cell>44.3/73.6</cell><cell>50.9/78.8</cell><cell></cell><cell></cell></row><row><cell>Proto-Clsf</cell><cell>20.8/43.1</cell><cell>29.9/58.1</cell><cell>42.4/72.3</cell><cell>49.5/79.0</cell><cell></cell><cell></cell></row><row><cell>G-SGM [9]</cell><cell>-/ 47.3</cell><cell>-/ 60.9</cell><cell>-/ 73.7</cell><cell>-/ 79.5</cell><cell></cell><cell></cell></row><row><cell>PMN [30]</cell><cell>-/ 53.3</cell><cell>-/ 65.2</cell><cell>-/ 75.9</cell><cell>-/ 80.1</cell><cell></cell><cell></cell></row><row><cell>PMN w/ H [30]</cell><cell>-/ 54.7</cell><cell>-/ 66.8</cell><cell>-/ 77.4</cell><cell>-/ 81.4</cell><cell></cell><cell></cell></row><row><cell cols="2">IDeMe-Net (Ours) 30.3/60.1</cell><cell>39.7/69.6</cell><cell>47.5/77.4</cell><cell>51.3/80.2</cell><cell></cell><cell></cell></row><row><cell cols="6">Table 2. Top-1 / Top-5 accuracy (%) on novel classes of the Im-</cell><cell></cell></row><row><cell cols="6">agenet 1K Challenge dataset. We use ResNet-50 as the embed-</cell><cell></cell></row><row><cell cols="6">ding sub-network. m indicates the number of training examples</cell><cell></cell></row><row><cell cols="6">per class. Proto-Clsf and G-SGM denote the prototype classifier</cell><cell></cell></row><row><cell cols="3">and generation SGM [9], respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Top-1 / Top-5 accuracy (%) on novel classes of the ImageNet 1K Challenge dataset. We use ResNet-10 as the embedding sub-network. m indicates the number of training examples per class. Our IDeMe-Net consistently achieves the best performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Top-1 / Top-5 accuracy (%) of the ablation study on novel classes of the ImageNet 1K Challenge dataset. We use ResNet-10 as the embedding sub-network. m indicates the number of training examples per class. Our full model achieves the best performance.</figDesc><table><row><cell></cell><cell>Method</cell><cell>m = 1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Baselines</cell><cell>LR Prototype Classifier</cell><cell>18.3/42.8 17.1/39.2</cell><cell>26.0/54.7 24.3/51.1</cell><cell>35.8/66.1 33.8/63.9</cell><cell>41.1/71.3 38.4/69.9</cell><cell>44.9/74.8 44.1/74.7</cell></row><row><cell>Variants</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work is supported in part by the grants from NSFC (#61702108), STCSM (#16JC1420400), Eastern Scholar (TP2017006), and The Thousand Talents Plan of China (for young professionals, D1410009).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ideme-Net</surname></persName>
		</author>
		<idno>Ours) 59.14±0.86 74.63±0.74</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory matching networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-level semantic feature augmentation for one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Low-shot learning via covariance-preserving adversarial augmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>4.1, 4.2</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML -Deep Learning Workshok</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Meta-SGD: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>arxiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Meta networks. In ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Institut f. Informatik, Tech. Univ. Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delta-encoder: An effective sample synthesis method for few-shot object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>2, 6.3, 6.3</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemeln</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2017</title>
		<imprint/>
	</monogr>
	<note>. 1, 2, 3, 4.3, 6, 6.1, 6, 6.3, 6.3</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>. 2, 6.3, 6.3</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to learn: Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Lifelong learning algorithms. Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online sensor registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermaak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Briers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Aerospace Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2016. 1, 2, 3, 6, 6.1, 6, 6.3, 6.3</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-attention network for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hariharan. Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning from small sample sets by combining unsupervised meta-training with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep metalearning: Learning to learn in the concept space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>arxiv:1802.03596</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
