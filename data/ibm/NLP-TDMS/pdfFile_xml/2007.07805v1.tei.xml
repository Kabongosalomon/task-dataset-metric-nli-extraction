<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongjo</forename><surname>Kim</surname></persName>
							<email>byeongjo.kim@zuminternet.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Zuminternet, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanran</forename><surname>Kim</surname></persName>
							<email>chanranhari@zuminternet.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Zuminternet, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zuminternet, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jein</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Zuminternet, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyoungsoo</forename><surname>Park</surname></persName>
							<email>gspark@zuminternet.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Zuminternet, Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data-Efficient Deep Learning Method for Image Classification Using Data Augmentation, Focal Cosine Loss, and Ensemble</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data-Efficient</term>
					<term>Deep-Learning</term>
					<term>Image Classification</term>
					<term>Data Augmentation</term>
					<term>Cosine Loss</term>
					<term>Focal Loss</term>
					<term>Ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In general, sufficient data is essential for the better performance and generalization of deep-learning models. However, lots of limitations(cost, resources, etc.) of data collection leads to lack of enough data in most of the areas. In addition, various domains of each data sources and licenses also lead to difficulties in collection of sufficient data. This situation makes us hard to utilize not only the pre-trained model, but also the external knowledge. Therefore, it is important to leverage small dataset effectively for achieving the better performance. We applied some techniques in three aspects: data, loss function, and prediction to enable training from scratch with less data. With these methods, we obtain high accuracy by leveraging ImageNet data which consist of only 50 images per class. Furthermore, our model is ranked 4th in Visual Inductive Printers for Data-Effective Computer Vision Challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are limits to collecting a lot of data used to train deep-learning models. A transfer learning and a pre-training technique trained with large image dataset are used to achieve good performance even if there is small dataset. However, most large image dataset consist of images collected from web, which may be licensed or commercially prohibited <ref type="bibr" target="#b0">[1]</ref>. Therefore, in some cases, pre-trained weights trained with ImageNet <ref type="bibr" target="#b2">[4]</ref> may not be applied for commercial applications.</p><p>Nowadays it is known that self-training can replace pre-trained model and even perform better <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b4">6]</ref>. However, self-training requires a lot of data that is not labeled. So data problems eventually arise again. Therefore, even if given dataset is small, we should use that data very effectively.</p><p>Visual Inductive Printers for Data-Effective Computer Vision Challenge <ref type="bibr" target="#b3">[5]</ref> is a challenge improving performance using small dataset productively. There are only 50 images per class, and as with the ImageNet dataset, there are total 1000 arXiv:2007.07805v1 [cs.CV] 15 Jul 2020 classes. This challenge also restricts the use of external data such as pre-trained weights, ontology knowledge. Due to the small dataset, it is difficult to perform well with general training.</p><p>We solve this problem by experimenting with various techniques. First, we propose LSB Swap data augmentation technique not only to make various data available but also to improve the robustness of the model. Then, we apply Focal Cosine Loss function to enable model training well with even a small amount of data. Last, we apply voting ensemble to compensate for the wrong answers. Through the model applied with these techniques, the Visual Inductive Printers for Data-Effective Computer Vision Challenge recorded 0.67 top-1 accuracy and ranked 4th. We apply two data augmentation techniques in the two aspects. First, Ran-dAugment <ref type="bibr" target="#b1">[3]</ref> is used to transform the data and to create the effect of training with a lot of data. Secondly, LSB(Least Significant Bit) Swap is proposed and applied to focus on structural features for classifying objects. The process of LSB Swap can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Because changing a few last bits do not make a difference to be judged by humans, this technique allows data augmentation by swapping non-critical parts of the image classification task. In addition, unlike other data augmentations which use random noise, the LSB Swap helps to use natural noise values present in the images. In environments with less training data, LSB Swap keeps these values from training as much as possible, and helps the model focus on structural features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Focal Cosine Loss</head><p>We used cosine loss, which is known as performing well when training without pre-trained weights and with small dataset <ref type="bibr" target="#b0">[1]</ref>. Let x be image, and y be the class label of x. ϕ(y) is a one-hot vector mapped each class label. f θ (x) denotes embedding(i.e., CNN) with learned parameters θ from x to feature space. ψ(·) denotes embedding(i.e., fully-connected layer) from features into prediction space. The cosine loss is defined as</p><formula xml:id="formula_0">L cos (x, y) = 1− &lt; ϕ(y), ψ(f θ (x)) &gt;<label>(1)</label></formula><p>where &lt; ·, · &gt; denotes the dot product. For classification accuracy, Barz et al. <ref type="bibr" target="#b0">[1]</ref> used cosine loss with a categorical cross-entropy loss. In addition, we add a focal loss <ref type="bibr" target="#b5">[7]</ref> to focus more on classes which are difficult to classify. The Focal Cosine Loss is defined as</p><formula xml:id="formula_1">F L(p,p) = − h p (h) (1 −p (h) ) γ log(p (h) ) L cos+f ocal (x, y) = 1− &lt; ϕ(y), ψ(f θ (x)) &gt; −λ · F L(ϕ(y), g θ (ψ(f θ (x))))<label>(2)</label></formula><p>where p (h) andp (h) , respectively, refer to the probability of label h in correct answers and the predictions. Moreover, g θ denotes an additional fully-connected layer with softmax activation function. Furthermore, we use two hyper-parameters γ for focusing and λ for combination, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Plurality Voting Ensemble</head><p>We use plurality voting technique of hard voting ensemble to compensate for the difficult problems through various models and to produce good performance. The plurality voting ensemble algorithm is shown in Algorithm 1. If the predicted confidence score(s j ) for a particular problem in the base model(B) is less than a certain threshold, it is replaced with voting result(M ode(V j )) of predictions for various models(K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Other Techniques</head><p>Various techniques are also applied. First, EMA(Exponential Moving Average) model is used to adjust parameters when the model is training, and those parameters are used for testing. Second, drop out and drop connection are used for preventing overfitting. Last, the TTA(Test Time Augmentation) technique is used, which also have a single ensemble effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Plurality Voting Ensemble</head><p>1: procedure PluralityVoting(B, K, T ) 2:</p><p>B: Base classification model 3:</p><p>K: m Classification models B also belongs to K 4:</p><p>T : n Test data 5:</p><p>Vj: Votes from m classifiers using j th test data 6:</p><p>O: results of plurality voting 7:</p><p>for j ← 1 to n do 8:</p><p>sj ← Prediction result of Tj from B 9:</p><p>if confidence score of sj &lt; threshold then 10:</p><p>for i ← 1 to m do 11:</p><p>vi ← Prediction result of Tj from Ki 12:</p><p>append vi to Vj 13: end for 14:</p><p>Oj ← M ode(Vj) 15: else 16:</p><p>Oj ← sj 17:</p><p>end if 18: end for 19:</p><p>return O 20: end procedure 3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We use open source named timm [8] for training. The code is developed by PyTorch and is capable of applying the recent image classification models and many techniques. All hyper-parameters such as optimizer, learning rate, etc. are set as recommended by timm. For LSB Swap, Focal Cosine Loss, and plurality voting ensemble described above, we developed using Python, NumPy, and PyTorch. The hyper-parameters in Focal Cosine Loss, λ and γ are set to 0.1 and 2.0 respectively. In addition, threshold for plurality voting ensemble is set to 0.7. When we apply TTA technique, we use the same RandAugment used in training and soft voting ensemble with confidence scores of 1000 classes. Last, k in LSB Swap is set to 2. We trained model using only the data given, not the pre-trained weights and external data. The given training data and validation data are combined to be used in training. <ref type="table">Table 1</ref> shows that 10 models which is trained with given data and techniques. The RandAugment, cosine loss, and other techniques are applied to all models, and LSB Swap is applied only to EfficientNet-B3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Because there is no answer for the test data, accuracy could not be calculated. However, in the case of EfficientNet-B3, the accuracy could be obtained by <ref type="table">Table 1</ref>. 10 models trained using proposed techniques and given data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>EfficientNet-B3 <ref type="bibr" target="#b6">[9]</ref> EfficientNet-B3 with TTA EfficientNet-B3 with LSB Swap EfficientNet-B3 with LSB Swap + TTA EfficientNet-B4 EfficientNet-B4 with TTA EfficientNet-B5 EfficientNet-B5 with TTA EfficientNet-B7 EfficientNet-B7 with TTA submitting it to the competition(EfficientNet-B3 in <ref type="table" target="#tab_0">Table 2</ref>). Because the size of EfficientNet-B3 model is still small, we use other larger models as shown in <ref type="table">Table 1</ref>. Then, we applied plurality voting ensemble as shown in <ref type="figure" target="#fig_1">Figure 2</ref> with 10 trained models using EfficientNet-B7 as the base model(Ensemble model in <ref type="table" target="#tab_0">Table 2</ref>). It has improved 0.09 accuracy more than the EfficientNet-B3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We apply techniques such as LSB Swap and Focal Cosine Loss to train with only a small amount of data. In addition, even if it is difficult to predict with the best single model, the final result can be supplemented through the proposed ensemble method. Because the deadline is set for the competition, it is hard to test all the techniques. For example, LSB Swap data augmentation technique apply only to EfficientNet-B3. However, considering that the training loss of EfficientNet-B3 applied with LSB Swap is lower than the one not applied, we think it will be effective in other models as well. All source code used for training and testing is provided [2], and we informs that pre-trained weights and external data are not used. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Exchange k LSB from all pixels in the image with different images. k is set to 2 in the example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An example of applying plurality voting ensemble to 10 models. Base model B is set to EfficientNet-B7(orange box) and threshold is set to 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The accuracy we got from submitting to the competition.</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell cols="2">EfficientNet-B3 0.5823</cell></row><row><cell cols="2">Ensemble model 0.6730</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">(2020) 2. byeongjokim: VIPriors Image Classification Challenge Source Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<ptr target="https://github.com/byeongjokim/VIPriors-Image-Classification-Challenge" />
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020-07-14" />
			<biblScope unit="page" from="1371" to="1380" />
		</imprint>
	</monogr>
	<note>Deep learning on small datasets without pre-training using cosine loss</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://vipriors.github.io/" />
		<title level="m">vipriors ewi: 1st Visual Inductive Priors for Data-Efficient Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2020-07-14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<title level="m">Rethinking pre-training and self-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
