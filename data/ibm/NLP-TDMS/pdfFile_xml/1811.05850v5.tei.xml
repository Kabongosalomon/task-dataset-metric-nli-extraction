<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Drop-Activation: Implicit Parameter Reduction and Harmonious Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-31">March 31, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senwei</forename><surname>Liang</surname></persName>
							<email>liang339@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehaw</forename><surname>Khoo</surname></persName>
							<email>ykhoo@galton.uchicago.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics and the College</orgName>
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhao</forename><surname>Yang</surname></persName>
							<email>haizhao@purdue.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematics Purdue University</orgName>
								<address>
									<postCode>47907</postCode>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Drop-Activation: Implicit Parameter Reduction and Harmonious Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-31">March 31, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Image Classification</term>
					<term>Overfitting</term>
					<term>Regularization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Overfitting frequently occurs in deep learning. In this paper, we propose a novel regularization method called Drop-Activation to reduce overfitting and improve generalization. The key idea is to drop nonlinear activation functions by setting them to be identity functions randomly during training time. During testing, we use a deterministic network with a new activation function to encode the average effect of dropping activations randomly. Our theoretical analyses support the regularization effect of Drop-Activation as implicit parameter reduction and verify its capability to be used together with Batch Normalization <ref type="bibr" target="#b10">[11]</ref>. The experimental results on CIFAR-10, CIFAR-100, SVHN, EMNIST, and ImageNet show that Drop-Activation generally improves the performance of popular neural network architectures for the image classification task. Furthermore, as a regularizer Drop-Activation can be used in harmony with standard training and regularization techniques such as Batch Normalization and Auto Augment <ref type="bibr" target="#b2">[3]</ref>. The code is available at https://github.com/LeungSamWai/Drop-Activation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolution neural network (CNN) is a powerful tool for computer vision tasks. With the help of gradually increasing depth and width, CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> gain a significant improvement in image classification problems by capturing multiscale features <ref type="bibr" target="#b30">[31]</ref>. However, when the number of trainable parameters is far more than that of training data, deep networks may suffer from overfitting. This leads to the routine usage of regularization methods such as data augmentation <ref type="bibr" target="#b2">[3]</ref>, weight decay <ref type="bibr" target="#b11">[12]</ref>, Dropout <ref type="bibr" target="#b20">[21]</ref> and Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref> to prevent overfitting and improve generalization.  Although regularization has been an essential part of deep learning, deciding which regularization methods to use remains an art. Even if each of the regularization methods works well on its own, combining them does not always give improved performance. For instance, the network trained with both Dropout and BN may not produce a better result <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. Dropout may change the statistical variance of layers output when we switch from training to testing, while BN requires the variance to be the same during both training and testing <ref type="bibr" target="#b15">[16]</ref>.</p><p>Our contributions: To deal with the aforementioned challenges, we propose a novel regularization method, Drop-Activation, inspired by the works in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref>, where some structures of networks are dropped to achieve better generalization. The advantages are as follows:</p><p>• Drop-Activation provides an easy-to-implement yet effective method for regularization via implicit parameter reduction.</p><p>• Drop-Activation can be used in synergy with the most popular architectures and regularization methods, leading to improved performance in various datasets for image classification.</p><p>The basic idea of Drop-Activation is that the nonlinearities in the network will be randomly activated or deactivated during training. More precisely, the nonlinear activations are turned into identity mappings with a certain probability, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. At testing time, we propose using a deterministic neural network with a new activation function which is a combination of identity mapping and the dropped nonlinearity, to represent the ensemble average of the random networks generated by Drop-Activation. Rectified linear unit (ReLU) has the advantage of reducing the saturation of gradient and accelerating the training compared with sigmoid or tanh activation function <ref type="bibr" target="#b12">[13]</ref>. It is frequently adopted in modern deep neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref>. In this paper, we focus on studying the random replacement of the ReLU activation function with the identity function. The starting point of Drop-Activation is to randomly draw an ensemble of neural networks with either an identity mapping or a ReLU activation function. The training process of Drop-Activation is to identify a set of parameters such that various neural networks in this ensemble work well when being assigned with these parameters. By "fitting" to many neural-networks instead of a fixed one, overfitting can potentially be prevented. Indeed, our theoretical analysis shows that Drop-Activation implicitly adds a penalty term to the loss function, aiming at network parameters such that the corresponding deep neural network can be approximated by a linear network, i.e, implicit parameter reduction.</p><p>Organizations: The remainder of this paper is structured as follows. In Section 2, we review some of the regularization methods and discuss their relations with our work. In Section 3, we formally introduce Drop-Activation. In Section 4, the theoretical analysis demonstrates the regularization of Drop-Activation and its synergy with BN. In Section 5, these advantages of Drop-Activation are further supported by our numerical experiments carried on different datasets and networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various regularization methods have been proposed to reduce the risk of overfitting. Data augmentation achieves regularization by directly enlarging the original training dataset via randomly transforming the input images <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> or output labels <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b24">25]</ref>. Another class of methods regularize the network by adding randomness into various neural network structures such as nodes <ref type="bibr" target="#b20">[21]</ref>, connections <ref type="bibr" target="#b23">[24]</ref>, pooling layers <ref type="bibr" target="#b29">[30]</ref>, activations <ref type="bibr" target="#b26">[27]</ref> and residual blocks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28]</ref>. In particular <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14]</ref> add randomness by dropping some structures of neural networks at random in training.</p><p>Dropout <ref type="bibr" target="#b20">[21]</ref> drops nodes along with its connection with some fixed probability during training. Drop-Connect <ref type="bibr" target="#b23">[24]</ref> has a similar idea but masks out some weights randomly. <ref type="bibr" target="#b9">[10]</ref> improves the performance of ResNet <ref type="bibr" target="#b5">[6]</ref> by dropping the entire residual block at random during training and passing through skip connections (identity mapping). This idea is also used in <ref type="bibr" target="#b27">[28]</ref> when training ResNeXt <ref type="bibr" target="#b25">[26]</ref> type 2-residual-branch network. The idea of dropping also arises in data augmentation. Cutout <ref type="bibr" target="#b3">[4]</ref> randomly cuts out a square region of training images to prevent the neural network from putting too much emphasis on the specific region of features.</p><p>A related idea of dropping "functions" in neural networks were proposed in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>, where subnetwork structures are discarded randomly, instead of replacing an activation function with the identify function. <ref type="bibr" target="#b19">[20]</ref> proposes a framework of Swapout Θ 1 ⊗ X + Θ 2 ⊗ F(X), where X is the input feature map, F is a sub-network, Θ 1 and Θ 2 are i.i.d Bernoulli distribution, and ⊗ is the element-wise product. <ref type="bibr" target="#b19">[20]</ref> verified the effectiveness of the proposed Swapout framework on a large structure, i.e., F is a residual in ResNet <ref type="bibr" target="#b5">[6]</ref> which consists of layers of BN, ReLU, Convolution. Similarly, Zoneout <ref type="bibr" target="#b13">[14]</ref> discussed the case when F is a set of layers. Dropping a subnetwork structure can lead to instability of training and it requires more careful hyperparameter tuning. On the contrary, Drop-Activation focuses on the nonlinear activation functions, a smaller and more basic structure of networks.We will show the effectiveness of Drop-Activation on a wider range of networks and datasets than those in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>In the next section, inspired by the above methods, we propose the Drop-Activation method for regularization. We want to emphasize that the improvement by Drop-Activation is universal to most neural-network architectures, and it can be readily used in conjunction with many regularizers without conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formulation of Drop-Activation</head><p>This section describes the Drop-Activation method. Suppose x 0 is an input vector of an L-layer feed forward network. Let x l be the output of l-th layer. f (·) is the element-wise nonlinear activation operator that maps an input vector to an output vector by applying a nonlinearity on each of the entries of the input. Without the loss of generality, we assume f :</p><formula xml:id="formula_0">R d → R d , e.g, f (x) = σ (x[1]) , · · · , σ (x[d]) T ∈ R d , x = x[1], · · · , x[d] T ∈ R d ,<label>(1)</label></formula><p>where σ could be a ReLU, a sigmoid or a tanh function but we only consider that σ is a ReLU function in our paper. For a standard fully connected or convolution network, the d-dimensional output can be written as</p><formula xml:id="formula_1">x l+1 = f (W l x l ),<label>(2)</label></formula><p>where W l ∈ R d×d is the weight matrix of the l-th layer. Biases are neglected for the convenience of presentation.</p><p>In what follows, we modify the way of applying the nonlinear activation operator f to achieve regularization. In the training phase, we remove the pointwise nonlinearities in f randomly. In the testing phase, the function f is replaced with a new deterministic nonlinearity.</p><p>Training Phase: During training, the d nonlinearities σ in the operator f are kept with probability p (or dropping them with probability 1 − p). The output of the (l + 1)-th layer is thus</p><formula xml:id="formula_2">x l+1 = (I − P)W l x l + P f (W l x l ) = (I − P + P f )(W l x l ),<label>(3)</label></formula><p>where P = diag(P 1 , P 2 , · · · , P d ), P 1 , · · · , P d are independent and identical random variables following a Bernoulli distribution B(p) that takes value 1 with probability p and 0 with probability 1 − p. We use I to denote the identity matrix. Intuitively, when P = I, then x l+1 = f (W l x l ), meaing all the nonlinearities in this layer are kept. When P = 0 , then x l+1 = W l x l , meaning all the nonlinearities are dropped. The general case lies somewhere between these two limits where the nonlinearities are kept or dropped partially. At each iteration, a different realization of P is sampled from the Bernoulli distribution again. When the nonlinear activation function in Eqn. <ref type="formula" target="#formula_2">(3)</ref> is ReLU, the j-th component of (I − P + P f )(x) can be written as</p><formula xml:id="formula_3">(I − P + P f )(x)[ j] =        x[ j], x[ j] ≥ 0, (1 − P j )x[ j], x[ j] &lt; 0.<label>(4)</label></formula><p>Testing Phase: During testing, we use a deterministic nonlinear function resulting from averaging the realizations of P. More precisely, we take the expectation of the Eqn. (3) with respect to the random variable P:</p><formula xml:id="formula_4">x l+1 = E P i ∼ B(p) (I − P + P f )(W l x l ) = ((1 − p)I + p f )(W l x l ),<label>(5)</label></formula><p>and the new activation function (1 − p)I + p f is the convex combination of an identity operator I and an activation operator f . Eqn. (4) is the deteministic nonlinearity used to generate a deterministic neural network for testing. In particular, when ReLU is used, then the new activation (1 − p)I + p f is the leaky ReLU with slope 1 − p in its negative part <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In Section 4.1, we show that in a ReLU neural-network with one-hidden-layer, Drop-Activation provides a regularization via penalizing the difference between nonlinear activation network and linear network, which can be understood as implicit parameter reduction, i.e, the intrinsic dimension of the parameter space is reduced. In Section 4.2, we further show that the use of Drop-Activation does not impact some other techniques such as BN, which ensures the practicality of using Drop-Activation in deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Drop-Activation as a regularizer</head><p>We use similar ideas in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b22">[23]</ref> to show that having Drop-Activation in a standard one-hidden layer fully connected neural network with ReLU activation gives rise to an explicit regularizer. Let x be the input vector, y be the output. The output of the one-hidden layer neural ReLU network iŝ y = W 2 r(W 1 x), where W 1 , W 2 are weights of the network, r : R d → R d is the function for applying ReLU elementwise to the input vector. Let r p (·) denotes the leaky ReLU with slope 1 − p in the negative part. As in Eqn. (3) and (5), applying Drop-Activation to this network giveŝ</p><formula xml:id="formula_5">y = W 2 ((I − P + Pr)W 1 x)<label>(6)</label></formula><p>during training, andŷ</p><formula xml:id="formula_6">= W 2 ((1 − p)I + pr)W 1 x = W 2 r p (W 1 x)<label>(7)</label></formula><p>during testing. Suppose we have n training samples</p><formula xml:id="formula_7">{(x i , y i )} n i=1 .</formula><p>To reveal the effect of Drop-Activation, we average the training loss function over P:</p><formula xml:id="formula_8">min W 1 ,W 2 n i=1 E W 2 [(I − P + Pr)W 1 x i ] − y i 2 2 ,<label>(8)</label></formula><p>where the expectation is taken with respect to the feature noise P 1 , · · · , P d . The use of Drop-Activation can be seen as applying a stochastic minimization to such an average loss. The result after averaging the loss function over P is summarized as follows.</p><formula xml:id="formula_9">Property 1. The optimization problem (8) is equivalent to min W 1 ,W 2 n i=1 W 2 r p (W 1 x i ) − y i 2 2 + p −1 (1 − p) W 2 W 1 x i − W 2 r p (W 1 x i ) 2 2 . (9) Proof. Suppose that x is the input vector. Let D W 1 ,x = diag{(W 1 x &gt; 0)}, where (W 1 x &gt; 0) is a 0-1 vector, and the j-th component of (W 1 x &gt; 0) is equal to 1 if the j-th component of W 1</formula><p>x is positive or is equal to 0 else. Then, the ReLU mapping of W 1 x can be written as r(</p><formula xml:id="formula_10">W 1 x) = D W 1 ,x W 1 x. For simplification, we denote S := I − P + PD W 1 ,x , S p := I − pI + pD W 1 ,x , v := W 1 x. On one hand, W 2 r p (W 1 x) − y 2 2 = W 2 S p v − y 2 2 .</formula><p>We expand it and obtain</p><formula xml:id="formula_11">W 2 S p W 1 x − y 2 2 = tr(W 2 S p vv T S p W T 2 ) − 2tr(W 2 S p vy T ) + tr(yy T ),<label>(10)</label></formula><p>where function tr(·) is the trace operator computing the sum of matrix diagonal. We denote vec(·) as a function converting the diagonal matrix into a column vector. Rewrite the first term of Eqn. (10) and get</p><formula xml:id="formula_12">tr(W 2 S p vv T S p W T 2 ) =tr(S p vv T S p W T 2 W 2 ) =tr(diag(v)vec(S p )vec(S p ) T diag(v)W T 2 W 2 ) =tr(vec(S p )vec(S p ) T diag(v)W T 2 W 2 diag(v)).<label>(11)</label></formula><p>On the other hand, we have</p><formula xml:id="formula_13">E W 2 [(I − P + Pr)W 1 x] − y 2 2 =E[ W 2 S v − y 2 2 ] =E[tr(W 2 S vv T S W T 2 )] − 2tr(W 2 S p vy T ) + tr(yy T ),<label>(12)</label></formula><p>where the expectation is taken with respect to the feature noise P = {P 1 , · · · , P d }. Similar to Eqn. (11), we combine the matrices containing random variables and obtain</p><formula xml:id="formula_14">tr(W 2 S vv T S W T 2 ) = tr(vec(S )vec(S ) T diag(v)W T 2 W 2 diag(v)).<label>(13)</label></formula><p>Since tr(·) has property of linearity, taking the expectation of Eqn. (13) with respect to P obtains</p><formula xml:id="formula_15">Etr(W 2 S vv T S W T 2 ) = tr(E(vec(S )vec(S ) T )diag(v)W T 2 W 2 diag(v)).<label>(14)</label></formula><p>Denote D W 1 ,x = diag(d 1 , · · · , d k ), and then</p><formula xml:id="formula_16">E[vec(S )vec(S ) T ] − vec(S p )vec(S p ) T =diag({E((1 − P i + P i d i ) 2 ) − (1 − p + pd i ) 2 } k i=1 ) =p(1 − p)(I − D W 1 ,x ) 2 .<label>(15)</label></formula><p>Using Eqn. <ref type="bibr" target="#b14">(15)</ref>, Eqn. <ref type="bibr" target="#b10">(11)</ref> and Eqn. <ref type="formula" target="#formula_0">(13)</ref>, we can get the difference between Eqn. <ref type="bibr" target="#b9">(10)</ref> and Eqn. <ref type="formula" target="#formula_0">(12)</ref>,</p><formula xml:id="formula_17">E[tr(W 2 S vv T S W T 2 )] − tr(W 2 S p vv T S p W T 2 ) =tr{(E(vec(S )vec(S ) T ) − vec(S p )vec(S p ) T )diag(v)W T 2 W 2 diag(v)} =p(1 − p)tr{(I − D W 1 ,x ) 2 diag(v)W T 2 W 2 diag(v)} =p(1 − p)tr{W 2 diag(v)(I − D W 1 ,x ) 2 diag(v)W T 2 } =p(1 − p) W 2 (I − D W 2 ,x )W 1 x 2 2 . Note that D W 1 ,x − I = 1 p (S p − I), so we have p(1 − p) W 2 (I − D A,x )W 1 x 2 2 = 1 − p p W 2 (I − S p )W 1 x 2 2 = 1 − p p W 2 W 1 x − W 2 r p (W 1 x) 2 2 .</formula><p>Finally, we attain the difference between Eqn. (10) and Eqn. <ref type="formula" target="#formula_0">(12)</ref>,</p><formula xml:id="formula_18">1 − p p W 2 W 1 x − W 2 r p (W 1 x) 2 2 .</formula><p>We refer to the objective function of the optimization (9). The first term is nothing but the l 2 loss during prediction time i ŷ i −y i 2 2 , whereŷ i 's are defined via <ref type="bibr" target="#b6">(7)</ref>. Therefore, Property 1 shows that Drop-Activation incurs a penalty</p><formula xml:id="formula_19">p −1 (1 − p) W 2 W 1 x i − W 2 r p (W 1 x i ) 2 2<label>(16)</label></formula><p>on top of the prediction loss. In Eqn. <ref type="bibr" target="#b15">(16)</ref>, the coefficient 1−p p influences the magnitude of the penalty. In our experiments, p is selected to be a large number close to 1 (typically 0.95). The magnitude of the penalty will not be large in our numerical experiments.</p><p>The penalty <ref type="bibr" target="#b15">(16)</ref> consists of the terms W 2 W 1 x and W 2 r p (W 1 x). W 2 W 1 x has no nonlinearity, so it is a linear network. In contrast, since W 2 r p (W 1 x) has the nonlinearity r p , it can be considered as a deep network. The two networks share the same parameters W 1 and W 2 . Therefore the penalty (16) encourages weights W 1 , W 2 such that the prediction of the relatively deep network W 2 r p (W 1 x) should be somewhat close to that of a linear network. In this way, the penalty incurs by Drop-Activation may help in reducing overfitting by implicit parameter reduction.</p><p>To illustrate this point, we perform a simple regression task for two functions. To generate the training dataset, we sample 20 (x i , y i ) pairs from the ground truth function and add gaussian noise on the outputs. Then we train a fully connected network with three hidden layers of width 1000, 800, 200, respectively. <ref type="figure" target="#fig_3">Figure 2a</ref> and 2b show that the network with ReLU has a low prediction error on training data points, but is generally erroneous in other regions. Although the network with Drop-Activation does not fit as well to the training data (comparing with using normal ReLU), overall it achieves a lower prediction error. With the effect of incurred penalty <ref type="bibr" target="#b15">(16)</ref>, the network with Drop-Activation reduces the influence of data noise and yields a smooth curve. <ref type="figure">Figure 6</ref> shows the training of ResNet164 on CIFAR100, the training error with Drop-Activation is slightly larger than that without Drop-Activation. However, in terms of the generalization error, Drop-Activation gives improved performance. This verifies that the original network has been over-parametired and Drop-Activation can regularize the network by implicit parameter reduction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compatibility of Drop-Activation with BN</head><p>In this section, we show theoretically that Drop-Activation essentially keeps the statistical property of the output of each network layer when going from training to testing phase and hence it can be used together with BN. <ref type="bibr" target="#b15">[16]</ref> argues that BN assumes the output of each layer has the same variance during training and testing. However, Dropout <ref type="bibr" target="#b20">[21]</ref> will shift the variance of the output during the testing time leading to disharmony when used in conjunction with BN. Using a similar analysis as <ref type="bibr" target="#b15">[16]</ref>, we show that unlike Dropout, Drop-Activation can be used together with BN since it maintains the output variance.  To this end, we analyze the mappings in ResNet <ref type="bibr" target="#b5">[6]</ref>. <ref type="figure" target="#fig_4">Figure 3</ref> (Left) shows a basic block of ResNet while <ref type="figure" target="#fig_4">Figure 3</ref> (Right) shows a basic block with Drop-Activation. We focus on the rectangular box with dashed line. Suppose the output from the BN 1 shown in <ref type="figure" target="#fig_4">Figure 3</ref> </p><formula xml:id="formula_20">is x = (x[1], · · · , x[d]</formula><p>). <ref type="bibr" target="#b14">[15]</ref> shows the hidden features converge in distribution to the Gaussian when d is large, so for simplification, we assume that x[i] ∼ N(0, 1), i = 1, . . . , d are i.i.d. random variables. When x is passed to the Drop-Activation layer followed by a linear transformation weight 2 with weights w = (w 1 , · · · , w d ) ∈ R 1×d , we obtain X train :</p><formula xml:id="formula_21">= d i=1 w i ((1 − P i )x[i] + P i r(x[i])),</formula><p>where P = diag(P 1 , · · · , P d ) and P i ∼ B(p). Similarly, during testing, taking the expectation over P i 's gives X test :</p><formula xml:id="formula_22">= d i=1 w i ((1 − p)x[i] + pr(x[i])</formula><p>). The output of the rectangular box X train (and X test during testing) is then used as the input to BN 2 in <ref type="figure" target="#fig_4">Figure 3</ref>. Since for BN we only need to understand the entry-wise statistics of its input, without loss of generality, we assume the linear transformation w maps a vector from R d to R, X train and X test are scalars.</p><p>We want to show X train and X test have similar statistics. By design, E P,x X train = E P,x X test . Notice that the expectation here is taken with respect to both the random variables P and the input x of the box in <ref type="figure" target="#fig_4">Figure 3</ref>. Thus the main question is whether the variances of X train and X test are the same. To this end, we introduce the Shift Ratio <ref type="bibr" target="#b15">[16]</ref>: Shift Ratio = Var(X test )/Var(X train ) as a metric for evaluating the variance shift. The shift ratio is expected to be close to 1, since the BN layer BN 2 requires its input having similar variance in both training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property 2. The shift ratio of X train and X test is</head><formula xml:id="formula_23">Var(X test )/Var(X train ) = [(π − 1)p 2 − 2πp + 2π]/[2π − πp − p 2 ].<label>(17)</label></formula><p>Proof. Since</p><formula xml:id="formula_24">x[i] ∼ N(0, 1), it is easy to get E(x[i]) = 0, E(r(x[i])) = 1 √ 2π , E(x[i] 2 ) = 1, and E(r(x[i]) 2 ) = 1 2 ,</formula><p>where the expectation is taken with respect to random variable x[i]. We have</p><formula xml:id="formula_25">E(X train ) = d i=1 w i E((1 − P i + P i r)x[i]) = p d i=1 w i √ 2π , E(X test ) = d i=1 w i E((1 − p + pr)x[i]) = p d i=1 w i √ 2π ,</formula><p>where expectation is taken with respect to feature noise P = {P 1 , · · · , P d } and inputs (x <ref type="bibr" target="#b0">[1]</ref>, · · · , x[d]). In what follows, we compute Var(X train ) and Var(X test ). Expand the square of X train to get</p><formula xml:id="formula_26">X 2 train = d i=1 w 2 i ((1 − P i )x[i] + P i r(x[i])) 2 + 2 i&lt; j w i w j ((1 − P i )x[i] + P i r(x[i]))((1 − P j )x[ j] + P j r(x[ j])).</formula><p>Then we obtain its expectation,</p><formula xml:id="formula_27">E(X 2 train ) = d i=1 w 2 i E((1 − P i ) 2 x[i] 2 + 2(1 − P i )P i x[i]r(x[i]) + P 2 i r(x[i]) 2 ) + 2 i&lt; j w i w j E(P i P j r(x[i])r(x[ j])) = d i=1 w 2 i (1 − p + 1 2 p) + p 2 π i&lt; j w i w j .</formula><p>Using the fact that Var(X train ) = E(X 2 train ) − (EX train ) 2 , we get</p><formula xml:id="formula_28">Var(X train ) = d i=1 w 2 i (1 − p + 1 2 p) + p 2 π i&lt; j w i w j − ( 1 √ 2π p d i=1 w i ) 2 = d i=1 w 2 i (1 − 1 2 p − 1 2π p 2 ).<label>(18)</label></formula><p>So far, we have finished Var(X train ). Now we are going to compute Var(X test ). Expand X 2 test to get</p><formula xml:id="formula_29">X 2 test = d i=1 w 2 i ((1 − p)x[i] + pr(x[i])) 2 + 2 i&lt; j w i w j ((1 − p)x[i] + pr(x[i]))((1 − p)x[ j] + pr(x[ j])).</formula><p>We take expectation with respect to the input x,</p><formula xml:id="formula_30">E(X 2 test ) = d i=1 w 2 i E((1 − p) 2 x[i] 2 + 2(1 − p)px[i]r(x[i]) + p 2 r(x[i]) 2 ) + 2 i&lt; j w i w j E(p 2 r(x[i])r(x[ j])) = d i=1 w 2 i ( 1 2 p 2 − p + 1) + p 2 π i&lt; j w i w j .</formula><p>Using the fact that Var(X test ) = E(X 2 test ) − (E(X test )) 2 , we can obtain that</p><formula xml:id="formula_31">Var(X test ) = d i=1 w 2 i (( 1 2 − 1 2π )p 2 − p + 1).<label>(19)</label></formula><p>With Eqn. 18 and Eqn. 19, we have</p><formula xml:id="formula_32">Var(X test ) Var(X train ) = ( 1 2 − 1 2π )p 2 − p + 1 1 − 1 2 p − 1 2π p 2 .<label>(20)</label></formula><p>In Eqn. <ref type="bibr" target="#b16">(17)</ref>, the range of the shift ratio lies on the interval [0.8, 1]. In particular, when p = 0.95, Var(X test )/Var(X train ) ≈ 0.9377, therefore Var(X test ) is close to Var(X train ). This shows that in Drop-Activation, the difference in the variance of inputs to a BN layer between the training and testing phase is rather minor.</p><p>We further demonstrate numerically that Drop-Activation does not generate an enormous shift in the variance of the internal covariates when going from the training time to the testing time. We train ResNet164 with CIFAR100. ResNet164 consists of a stack of three stages. Each stage contains 54 convolution layers with the same spatial size. We observe the statistics of the output of the second stage by evaluating its shift ratio. We compute the variances of the output for each channel and then average the channels' variance. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, the shift ratio stabilizes close to 1 at the end of the training, which is consistent with our analysis.</p><p>In summary, by maintaining the statistical property of the internal output of hidden layers in testing time, Drop-Activation can be combined with BN to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we empirically evaluate the performance of Drop-Activation and demonstrate its effectiveness. We apply Drop-Activation to modern deep neural architectures on various datasets. This section is organized as followed. Section 5.1 contains basic experiment settings. In Section 5.2, we introduce the datasets and implementation details. In section 5.3, we present the numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment design</head><p>Our experiments are to demonstrate the following points: (1) Comparison with RReLU: Due to the similarity between the activation function used in our proposed method when having f as ReLU in Eqn. <ref type="bibr" target="#b4">(5)</ref> and the randomized leaky rectified linear units (RReLU), one may speculate that the use of RReLU gives similar performance. We show that this is indeed not the case by comparing Drop-Activation with the use of RReLU. (2) Improvement upon modern neural network architectures: We show the improvement that Drop-Activation brings is rather universal by applying it to different modern network architectures on a variety of datasets. (3) Compatibility with other approaches: We show that Drop-Activation is compatible with other popular regularization methods by combining them in different network architectures.</p><p>Comparison with RReLU: RReLU is proposed in <ref type="bibr" target="#b26">[27]</ref> with the following training scheme for an input vector x,</p><formula xml:id="formula_33">RReLU(x)[ j] =        x[ j], x[ j] ≥ 0, U j x[ j], x[ j] &lt; 0,<label>(21)</label></formula><p>where U j is a random variable with a uniform distribution U(a, b) with 0 &lt; a &lt; b &lt; 1. In the case of ReLU in Drop-Activation, a comparison between Eqn. (4) with Eqn. <ref type="bibr" target="#b20">(21)</ref> shows that the main difference between our approach and RReLU is the random variable used on the negative axis. It can be seen from Eqn. <ref type="bibr" target="#b20">(21)</ref> that RReLU passes the negative data with a random shrinking rate, while Drop-Activation randomly lets the complete information pass. The parameters a and b in RReLU are set at 1/8 and 1/3 respectively, as suggested in <ref type="bibr" target="#b26">[27]</ref>. Improvement upon modern neural network architectures: The residual-type neural network structures greatly facilitate the optimization for deep neural network <ref type="bibr" target="#b5">[6]</ref> and are employed by ResNet <ref type="bibr" target="#b5">[6]</ref>, Pre-ResNet <ref type="bibr" target="#b6">[7]</ref>, DenseNet <ref type="bibr" target="#b8">[9]</ref>, ResNeXt <ref type="bibr" target="#b25">[26]</ref>, WideResNet (WRN) <ref type="bibr" target="#b28">[29]</ref> and SENet <ref type="bibr" target="#b7">[8]</ref>. We demonstrate that Drop-Activation works well with these modern architectures. Moreover, since these networks use BN to accelerate training and may contain Dropout to improve generalization. e.g., WRN, these experiments also show the ability of Drop-Activation to work in synergy with the prevalent training techniques.</p><p>Compatibility with other regularization approaches: To further show that Drop-Activation can cooperate well with other training techniques, we combine Drop-Activation with two other popular data augmentation approaches: Cutout <ref type="bibr" target="#b3">[4]</ref> and AutoAugment <ref type="bibr" target="#b2">[3]</ref>. Cutout randomly masks a square region of training data and AutoAugment uses reinforcement learning to obtain an improved data augmentation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets and implementation details</head><p>Choosing the probability of retaining activation: In our method, the only parameter that needs to be tuned is the probability p of retaining activation. To get a rough estimate of what p is, we train a simple network on CIFAR10 without data augmentation and perform a grid search for p on the interval [0.6, 1.0], with a step size equal to 0.05. The simple network consists of three convolution layers and two fully connected layers, and details are in the Appendix. We split the train set of CIFAR10 into two parts, 10% for validation and 90% for training. The <ref type="figure">Figure 5</ref> shows the validation error on CIFAR10 versus p, which is minimal at p = 0.95. Each data point is averaged over the outcomes of 20 trained neural-networks. Based on this observation, we choose p = 0.95 for all experiments. Datasets and implementation: We train the models with Drop-Activation on CIFAR10, CIFAR100 <ref type="bibr" target="#b11">[12]</ref>, SVHN <ref type="bibr" target="#b16">[17]</ref>, EMNIST ("Balanced") <ref type="bibr" target="#b1">[2]</ref> and ImageNet 2012 <ref type="bibr" target="#b17">[18]</ref> (random cropping size 224×224). When applying Drop-Activation to these models, we directly substitute all the original ReLU function with Drop-Activation except for the case of ImageNet. In particular, due to the relatively underfitting of training on ImageNet, only ReLUs in the last two stages of networks are modified by Drop-Activation. All the models are optimized using SGD with a momentum of 0.9 <ref type="bibr" target="#b21">[22]</ref>. The other implementation details are given in the Appendix.     Compatibility with other regularization approaches: We apply Drop-Activation to network models that use Cutout or AutoAugment. As shown in  <ref type="table" target="#tab_4">Table 4</ref>: Test error (%) for CIFAR100 or CIFAR10 with combination of Drop-Activation (DA) and Cutout (CO) or AutoAugement (AA). The results of Cutout are quoted from <ref type="bibr" target="#b3">[4]</ref>. The WideResNet result of Au-toAug is quoted from <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment results</head><p>Training time: The increment of the computational cost of the Drop-Activation network compared with the ReLU network comes from the different realizations of Bernoulli random variables for each activation function. This results in an unavoidable increment of training time. <ref type="table" target="#tab_5">Table 5</ref> shows the training time of each batch for different models. In particular, RReLU that we use is Pytorch official function. We train ResNet164 and WideResNet28 with batch size 128 on CIFAR10 using the workstation with CPU AMD Ryzen Threadripper 1920X and 2 GPUs 2080Ti. From  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose Drop-Activation, a regularization method that introduces randomness on the activation function. Drop-Activation works by randomly dropping the nonlinear activations in the network during training and uses a deterministic network with modified nonlinearities for prediction.</p><p>The advantage of the proposed method is two-fold. Firstly, Drop-Activation provides a simple yet effective method for regularization, as demonstrated by the numerical experiments. Furthermore, this is supported by our analysis in the case of one hidden-layer. We show that Drop-Activation gives rise to a regularizer that penalizes the difference between nonlinear and linear networks. Future direction includes the analysis of Drop-Activation with more than one hidden layer. Secondly, experiments verify that Drop-Activation improves the generalization in most modern neural networks and cooperates well with some other popular training techniques. Moreover, we show theoretically and numerically that Drop-Activation maintains the variance during both training and testing time, and thus Drop-Activation can work well with Batch Normalization. These two properties should allow the wide applications of Drop-Activation in many network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conflict of Interest</head><p>On behalf of all authors, the corresponding author states that there is no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">The simple model for finding the best parameter p</head><p>To find the best parameter for Drop-Activation, we perform a grid search on a simple model. The simple network consists of the following layers: We first stack three blocks, and each block contains convolution with 3 × 3 filter, BN, ReLU, and average pooling, as shown in <ref type="figure" target="#fig_7">Figure 7</ref>. The number of 3 × 3 filters for Block 1 , Block 2 , Block 3 is 32, 64, 128 respectively. The widths for fully connected layers are 1000 and 10 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Introdcution of datasets</head><p>We use the following datasets in our numerical experiments, CIFAR: Both CIFAR10 and CIFAR100 contain 60k color nature images of size 32 by 32. There are 50k images for training and 10k images for testing. CIFAR-10 has ten classes of objects and 6k for each class. CIFAR100 is similar to CIFAR10, except that it includes 100 classes and 600 images for each class. Normalization and standard data augmentation (random cropping and horizontal flipping) are applied to the training data as <ref type="bibr" target="#b5">[6]</ref>.</p><p>SVHN: The dataset of Street View House Numbers (SVHN) contains ten classes of color digit images of size 32 by 32. There are about 73k training images, 26k testing images, and additional 531k images. The training and additional images are used together for training, so there are totally over 600k images for training. An image in SVHN may contain more than one digit, and the recognition task is to identify the digit in the center of the image. We preprocess the images following <ref type="bibr" target="#b28">[29]</ref>. The pixel values of the images are rescaled to [0, 1], and no data augmentation is applied.</p><p>EMNIST: EMNIST is a set of 28 × 28 grayscale images containing handwritten English characters and digits. There are six different splits in this dataset and we use the split "Balanced". In the "Balanced" split, there are 131,600 images in total, including 112,800 for training and 18,800 for testing.</p><p>ImageNet 2012: The ImageNet 2012 dataset consists of 1.28 million training images and 50K validation images from 1,000 classes. The models are evaluated on the validation set. We train the models for 120 epochs with an initial learning rate 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Implementation detail</head><p>The hyper-parameters for different networks are shown in <ref type="table" target="#tab_8">Table 6</ref>, 7 and 8, and we offer the explanation of hyper-parameter names in <ref type="table" target="#tab_11">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet</head><p>PreResNet WRN-28 ResNext29-8*64 VGG19(BN) DenseNet190 DenseNet100</p><p>Batch <ref type="table" target="#tab_0">size  128  128  128  128  128  32  64  Epoch  164  164  200  300  200  300  300  Optimizer</ref> SGD(0.9) SGD(0.9) SGD(0.9) SGD(0.9) SGD(0.9) SGD(0.9) SGD(0.9) Depth --28 <ref type="bibr" target="#b28">29</ref>      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Standard neural network with nonlinearity.(b) After applying Drop-Activation during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of Drop-Activation. Left: A standard 2-hidden-layer network with nonlinear activation (Blue). Right: A new network generated by applying Drop-Activation to the network on the left. Nonlinear activation functions are randomly selected and replaced with identity maps (Red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The ground true function: x sin x. The ground true function: A piecewise constant function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between the networks equipped with Drop-Activation and normal ReLU. (a) Regression of x sin x. (b) Regression of a piecewise constant function. Blue: Ground truth functions. Orange: Regression results using ReLU. Green: Regression results using Drop-Activation. " * ": Training data perturbed by Gaussian noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Left: A basic block in ResNet. Right: A basic block of a network with Drop-Activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>The shift ratio of the output of the second stage for ResNet-164. Var(X train ) and Var(X test ) denote the average of the variance for the output of the second stage during training and testing respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Validation error on CIFAR10 with 95% confidence intervals with respect to the probability p of retaining activation (average of 20 runs). Training curves on CIFAR100 with ResNet164.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The model for finding the best parameter for Drop-Activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 ,</head><label>1</label><figDesc>2 and 3 show the testing error on different datasets. The baseline results are from original networks without Drop-Activation.Table 5shows the training time of different models. In what follows, we discuss how our results support the points raised in Section 5.1 and analyse the training time of applying Drop-Activation.Comparison with RReLU: As shown inTable 1, RReLU may have worse performance than the baseline method. However, Drop-Activation consistently results in superior performance over RReLU and almost all baseline networks. Although Drop-Activation can not reduce the testing error of ResNeXt-8×64d on CIFAR10, Drop-Activation with DenseNet190-40 has the best testing error smaller than that of the original ResNeXt29-8×64d. 56±0.13 6.60±0.22 6.38±0.07 28.67±0.<ref type="bibr" target="#b29">30</ref> 28.62±0.15 28.55±0.28 ResNet110 6.77±0.27 7.37±0.22 6.25±0.06 28.24±0.13 29.64±0.06 27.91±0.18 ResNet164 5.94±0.27 6.08±0.03 5.62±0.08 25.86±0.42 24.78±0.43 24.18±0.22 PreResNet164 5.01±0.03 5.17±0.12 4.87±0.16 23.49±0.17 23.21±0.05 22.79±0.16 WideResNet28-10 3.85±0.13 4.32±0.05 3.74±0.05 18.84±0.26 19.53±0.13 18.14±0.22 DenseNet100-12 4.73±0.10 5.06±0.03 4.38±0.09 22.66±0.25 22.59±0.20 21.80±0.21 DenseNet190-40 3.91±0.15 3.84±0.08 3.51±0.06 17.28±0.45 18.58±0.12 16.80±0.12 ResNeXt29-8×64 3.95±0.05 4.56±0.16 3.95±0.18 18.56±0.38 18.65±0.08 17.65±0.16</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell>CIFAR100</cell></row><row><cell></cell><cell>Baseline</cell><cell>RReLU</cell><cell>Drop-Act</cell><cell>Baseline</cell><cell>RReLU</cell><cell>Drop-Act</cell></row><row><cell>VGG19(BN)</cell><cell>6.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test error (%) on CIFAR10 an CIFAR100. The test accuracy is averaged over three repeated experiments. We use Baseline to indicate the usage of the original architecture without modifications.Application to modern models: As shown inTable 1, Drop-Activation in almost all cases improves the testing accuracy consistently comparing to Baseline for CIFAR10 and CIFAR100. To further demonstrate this, we apply Drop-Activation to various neural-network architectures and demonstrate the successes on the datasets SVHN, EMNIST, and ImageNet. Again, inTable 2and 3 we see a consistent improvement when Drop-Activation is used.Therefore, Drop-Activation can work with most modern networks for different datasets. Besides, our results implicitly show that Drop-Activation is compatible with regularization techniques such as BN or Dropout used in training these networks.</figDesc><table><row><cell>Models</cell><cell></cell><cell>SVHN</cell><cell cols="2">EMNIST</cell></row><row><cell></cell><cell cols="4">Base Drop-Act Base Drop-Act</cell></row><row><cell>ResNet164</cell><cell>-</cell><cell>-</cell><cell>8.85</cell><cell>8.82</cell></row><row><cell>PreResNet164</cell><cell>-</cell><cell>-</cell><cell>8.88</cell><cell>8.72</cell></row><row><cell>WRN16-8</cell><cell>1.54</cell><cell>1.46</cell><cell>-</cell><cell>-</cell></row><row><cell>WRN28-10</cell><cell>-</cell><cell>-</cell><cell>8.97</cell><cell>8.72</cell></row><row><cell cols="2">DenseNet100-12 1.76</cell><cell>1.71</cell><cell>8.81</cell><cell>8.90</cell></row><row><cell cols="2">ResNeXt29,8*64 1.79</cell><cell>1.69</cell><cell>9.07</cell><cell>8.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">ImageNet 2012</cell></row><row><cell>Models</cell><cell cols="2">Baseline Drop-Act</cell></row><row><cell>ResNet34</cell><cell>26.07</cell><cell>25.85</cell></row><row><cell>SENet50</cell><cell>23.39</cell><cell>23.18</cell></row><row><cell>: Test error (%) on SVHN, EMNIST (Balanced).</cell><cell></cell><cell></cell></row><row><cell>The Baseline results of WRN and DenseNet for SVHN are</cell><cell></cell><cell></cell></row><row><cell>obtained from the original papers.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Validation error (%) on ImageNet.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 ,</head><label>4</label><figDesc>Drop-Activation can further improve with Cutout or AutoAugment by decreasing the test error on CIFAR100 and CIFAR10.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">with Cutout (CO) with AutoAug (AA)</cell></row><row><cell>Model</cell><cell>Dataset</cell><cell>Baseline</cell><cell>DA</cell><cell>CO</cell><cell>CO+DA</cell><cell>AA</cell><cell>AA+DA</cell></row><row><cell>ResNet18</cell><cell>CIFAR100</cell><cell>22.46</cell><cell cols="2">21.61 21.96</cell><cell>20.99</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet164</cell><cell>CIFAR100</cell><cell>25.86</cell><cell>24.18</cell><cell>-</cell><cell>-</cell><cell>21.12</cell><cell>20.39</cell></row><row><cell cols="2">WideResNet28-10 CIFAR100</cell><cell>18.84</cell><cell cols="2">18.14 18.41</cell><cell>17.86</cell><cell>17.09</cell><cell>16.20</cell></row><row><cell>DenseNet190-40</cell><cell>CIFAR10</cell><cell>3.91</cell><cell>3.51</cell><cell>3.15</cell><cell>2.79</cell><cell>2.54</cell><cell>2.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 ,</head><label>5</label><figDesc>we can see that both Drop-Activation and officially implemented RReLU suffer from the training time increment.</figDesc><table><row><cell>Model</cell><cell cols="3">Baseline RReLU Drop-Activation</cell></row><row><cell>ResNet164</cell><cell>0.151</cell><cell>0.175</cell><cell>0.223</cell></row><row><cell>WideResNet28-10</cell><cell>0.128</cell><cell>0.212</cell><cell>0.179</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The training time (sec) for each batch of different models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameter setting for training models on CIFAR10/100 and EMNIST.</figDesc><table><row><cell></cell><cell>ResNet34</cell><cell>SENet50</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>256</cell></row><row><cell>Epoch</cell><cell>120</cell><cell>120</cell></row><row><cell>Optimizer</cell><cell>SGD(0.9)</cell><cell>SGD(0.9)</cell></row><row><cell>Depth</cell><cell>34</cell><cell>50</cell></row><row><cell>Schedule</cell><cell>30/60/90</cell><cell>30/60/90</cell></row><row><cell>Weight-decay</cell><cell>1.00E-04</cell><cell>1.00E-04</cell></row><row><cell>Gamma</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>LR</cell><cell>0.1</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameter setting for training models on ImageNet.</figDesc><table><row><cell></cell><cell>WRN-16</cell><cell>ResNext29-8*64</cell><cell>DenseNet100</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>128</cell><cell>64</cell></row><row><cell>Epoch</cell><cell>160</cell><cell>100</cell><cell>40</cell></row><row><cell>Optimizer</cell><cell>SGD(0.9)</cell><cell>SGD(0.9)</cell><cell>SGD(0.9)</cell></row><row><cell>Depth</cell><cell>16</cell><cell>29</cell><cell>100</cell></row><row><cell>Schedule</cell><cell>80/120</cell><cell>40/70</cell><cell>20/30</cell></row><row><cell>Weight-decay</cell><cell>5.00E-04</cell><cell>5.00E-04</cell><cell>1.00E-04</cell></row><row><cell>Gamma</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Grow-rate</cell><cell>-</cell><cell>-</cell><cell>12</cell></row><row><cell>Widen-factor</cell><cell>8</cell><cell>4</cell><cell>-</cell></row><row><cell>Cardinality</cell><cell>-</cell><cell>8</cell><cell>-</cell></row><row><cell>LR</cell><cell>0.01</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Dropout</cell><cell>0.4</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter setting for training models on SVHN.</figDesc><table><row><cell>Batch size</cell><cell>Number of samples for training at each iteration</cell></row><row><cell>Epoch</cell><cell>Number of total epochs to train</cell></row><row><cell>Depth</cell><cell>The depth of network</cell></row><row><cell>Schedule</cell><cell>Decrease learning rate at these epochs</cell></row><row><cell cols="2">Weight-decay The coefficient of l2 loss</cell></row><row><cell>Gamma</cell><cell>Learning rate is multiplied by Gamma on schedule</cell></row><row><cell cols="2">Widen-factor Widen factor</cell></row><row><cell>Cardinality</cell><cell>Model cardinality (group)</cell></row><row><cell>LR</cell><cell>initial learning rate</cell></row><row><cell>Dropout</cell><cell>Dropout ratio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The explanation of hyper-parameter names.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The computational work for this article was partially performed on resources of the national supercomputing centre, singapore</title>
		<ptr target="https://www.nscc.sg" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Emnist: an extension of mnist to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05373</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
		<title level="m">Zoneout: Regularizing rnns by randomly preserving hidden activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8570" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the disharmony between dropout and batch normalization by variance shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2682" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4753" to="4762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02375</idno>
		<title level="m">Shakedrop regularization for deep residual learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
