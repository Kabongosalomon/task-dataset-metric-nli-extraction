<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta R-CNN : Towards General Solver for Instance-level Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
							<email>wangxx35@mail2.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta R-CNN : Towards General Solver for Instance-level Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Resembling the rapid learning capability of human, fewshot learning empowers vision systems to understand new concepts by training with few samples. Leading approaches derived from meta-learning on images with a single visual object. Obfuscated by a complex background and multiple objects in one image, they are hard to promote the research of few-shot object detection/segmentation. In this work, we present a flexible and general methodology to achieve these tasks. Our work extends Faster /Mask R-CNN by proposing meta-learning over RoI (Region-of-Interest) features instead of a full image feature. This simple spirit disentangles multi-object information merged with the background, without bells and whistles, enabling Faster /Mask R-CNN turn into a meta-learner to achieve the tasks. Specifically, we introduce a Predictor-head Remodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN. PRN receives images containing few-shot objects with their bounding boxes or masks to infer their class attentive vectors. The vectors take channel-wise soft-attention on RoI features, remodeling those R-CNN predictor heads to detect or segment the objects that are consistent with the classes these vectors represent. In our experiments, Meta R-CNN yields the state of the art in few-shot object detection and improves few-shot object segmentation by Mask R-CNN. Code: https://yanxp.github.io/metarcnn.html. * indicate equal contribution (Xiaopeng Yan and Ziliang Chen). † indicates corresponding author: Liang Lin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning frameworks dominate the vision community to date, due to their human-level achievements in supervised training regimes with a large amount of data. But distinguished with human that excel in rapidly understanding visual characteristics with few demonstrations, deep neural networks significantly suffer performance drop when training data are scarce in a class. The exposed bottleneck triggers many researches that rethink the generalization of deep learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b10">11]</ref>, among which few(low)-shot learn- <ref type="figure">Figure 1</ref>. The illustration of labeled training images in few-shot setups for visual object recognition and class-aware object structure (bounding-boxs or masks) prediction. Compared with recognition, novel-class few objects in few-shot object detection/ segmentation blend with other objects in diverse backgrounds, yet requiring a few-shot learner to predict their classes and structure labels.</p><p>ing <ref type="bibr" target="#b25">[26]</ref> is a popular and very promising direction. Provided with very few labeled data (1∼10 shots) in novel classes, few-shot learners are trained to recognize the data-starveclass objects by the aid of base classes with sufficient labeled data (See <ref type="figure">Fig 1.a)</ref>. Its industrial potential increasingly drives the emergence of solution, falling under the umbrellas of Bayesian approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>, similarity learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref> and meta-learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>However, recognizing a single object in an image is solely a tip of the iceberg in real-world visual understanding. In terms of instance-level learning tasks, e.g., object detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref>/ segmentation <ref type="bibr" target="#b1">[2]</ref>, prior works in few-shot learning contexts remain rarely explored (See <ref type="figure">Fig 1.b)</ref>. Since learning the instance-level tasks requires bounding-box or masks (structure labels) consuming more labors than image-level annotations, it would be practically impactful if the novel classes, object bounding boxes and segmentation masks can be synchronously predicted by a few-shot learner. Unfortunately, these tasks in object-starve conditions become much tougher, as a learner needs to locate or segment the novelclass number-rare objects beside of classifying them. Moreover, due to multiple objects in one image, novel-class ob-jects might blend with the objects in other classes, further obfuscating the information to predict their structure labels. Given this, researchers might expect a complicated solution, as what were done to solve few-shot recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Beyond their expectation, we present a intuitive and general methodology to achieve few-shot object detection and segmentation : we propose a novel meta-learning paradigm based on the RoI (Region-of-Interest) features produced by Faster/Mask R-CNN <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref>. Faster /Mask R-CNN should be trained with considerable labeled objects and unsuited in few-shot object detection. Existing meta-learning techniques are powerful in few-shot recognition, whereas their successes are mostly based on recognizing a single object. Given an image with multi-object information merged in background, they almost fail as the meta-optimization could not disentangle this complex information. But interestingly, we found that the blended undiscovered objects could be "pre-processed" via the RoI features produced by the firststage inference in Faster /Mask R-CNNs. Each RoI feature refers to a single object or background, so Faster /Mask R-CNN may disentangle the complex information that most meta-learners suffer from.</p><p>Our observation motivates the marriage between Faster /Mask R-CNN and meta-learning. Concretely, we extend Faster /Mask R-CNN by introducing a Predictor-head Remodeling Network (PRN). PRN is fully-convoluted and shares the main backbone's parameters with Faster /Mask R-CNN. Distinct from the R-CNN counterpart, PRN receives few-shot objects drawn from base and novel classes with their bboxes or masks, inferring class-attentive vectors, corresponding to the classes that few-shot input objects belong to. Each vector takes channel-wise attention to all RoI features, inducing the detection or segmentation prediction for the classes. To this end, a Faster /Mask R-CNN predictor head has been remodeled to detect or segment the objects that refer to the PRN's inputs, including the category, position, and structure information of few-shot objects. Our framework exactly boils down to a typical meta-learning paradigm, encouraging the name Meta R-CNN.</p><p>Meta R-CNN is general (available in diverse backbones in Faster/Mask R-CNN), simple (a lightweight PRN) yet effective (a huge performance gain in few-shot object detection/ segmentation) and remains fast inference (classattentive vectors could be pre-processed before testing). We conduct the experiments across 3 benchmarks, 3 backbones for few-shot object detection/ segmentation. Meta R-CNN has achieved the new state of the art in few-shot novel-class object detection/ segmentation, and more importantly, kept competitive performance to detect base-class objects. It verifies Meta R-CNN significantly improve the generalization capability of Faster/ Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-shot object recognition aims to recognize novel visual objects given very few corresponding labeled training examples. Recent studies in vision are mainly classed into three streams based on Bayesian approaches, metric learning and meta-learning, respectively. Bayesian approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> presume a mutual organization rule behind the objects, and design probabilistic model to discover the information among latent variables. Similarity learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> tend to consider the same-category examples's features should be more similar than those between different classes. Distinct from them, meta-learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11]</ref> designs to learn a meta-learner to parametrize the optimization algorithm or predict the parameters of a classifier, so-called "learning-to-learn". Recent theories <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> show that meta-learner achieves a generalization guarantee, attracting tremendous studies to solve few-shot problems by meta-learning techniques. However, most existing methods focus on single-object recognition.</p><p>Object detection based on neural network is mainly resolved by two solver branches: one-stage / two-stage detectors. One-stage detectors attempt to predict bounding boxes and detection confidences of object categories directly, including YOLO <ref type="bibr" target="#b33">[34]</ref>, SSD <ref type="bibr" target="#b27">[28]</ref> and the variants. R-CNN <ref type="bibr" target="#b13">[14]</ref> series <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref> fall into the second stream. The methods apply covnets to classify and regress the location by the region proposals generated by different algorithms <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b35">36]</ref>. More recently, few-shot object detection has been extended from recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="bibr" target="#b20">[21]</ref> follows fullimage meta-learning principle to address this problem. Instead, we discuss the similarity and difference between fewshot object recognition and detection in Sec 3, to reasonably motivate our RoI meta-learning approach.</p><p>Object segmentation is expected to pixel-wise segment the objects of interest in an image. Leading methods are categorized into image-based and proposal-based. Proposalbased methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref> predict object masks based on the generated region proposals while image-based methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2]</ref> produce a pixel-level segmentation map over the image to identify object instance. The relevant researches in few-shot setup remain absent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tasks and Motivation</head><p>Before introducing Meta R-CNN, we consider few-shot object detection /segmentation tasks it aims to achieve. The tasks could be derived from few-shot object recognition in terms of meta-learning methods that motivate our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary: few-shot visual object recognition by meta-learning</head><p>In few-shot object recognition, a learner h(; θ) receives training data from base classes C base and novel classes C novel . So the data can be divided into two groups: D base = <ref type="figure">Figure 2</ref>. Our Meta R-CNN consists of 1) Faster/ Mask R-CNN; 2) Predictor-head Remodeling Network (PRN). Faster/ Mask R-CNN (module) receives an image to produce RoI features, by taking RoIAlign on the image region proposals extracted by RPN. In parallel, our PRN receives K-shot-m-class resized images with their structure labels (bounding boxes/segmentaion masks) to infer m class-attentive vectors. Given a class attentive vector representing class c, it takes a channel-wise soft-attention on each RoI feature, encouraging the Faster/ Mask R-CNN predictor heads to detect or segment class-c objects based on the RoI features in the image. As the class c is dynamically determined by the inputs of PRN, Meta R-CNN is a metalearner.</p><formula xml:id="formula_0">{(x base i , y base i )} n1 i=1 ∼ P base contains sufficient samples in each base class; D novel = {(x novel i , y novel i )} n2 i=1</formula><p>∼ P novel contains very few samples in each novel class. h(; θ) aims to classify test samples drawn from P novel . Notably, training h(; θ) with small dataset D novel to identify C novel suffers model overfitting, whereas training h(; θ) with D base ∪ D novel still fails, due to the extreme data quantity imbalance between D base and D novel (n 2 &lt;&lt; n 1 ).</p><p>Recent wisdoms tend to address this problem by recasting it into a meta-learning paradigm <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, thus, encouraging a fast model adaptation to novel tasks (task generalization), e.g., classifying objects in C novel . In each iter, the meta-learning paradigm draws a subset of classes C meta ∼ C base ∪ C novel and thus, use the images belonging to C meta to construct two batches: a training mini-batch D train and a small-size meta(reference)-set D meta (few-shot samples in each class). Given this, a meta-learner h(x i , D meta ; θ) simultaneously receives an image x i ∼ D train and the entire D meta and then, is trained to classify D train into C meta 1 . By replacing D meta with D novel , recent theories <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> present generalization bounds to the meta-learner, enabling h(, D novel ; θ) to correctly recognize the objects ∼ P novel .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Few-shot object detection / segmentation</head><p>From visual recognition to detection /segmentation, fewshot learning on objects becomes more complex: An image x i might contain n i objects {z i,j } ni j=1 in diverse classes, positions and shapes. Therefore the few-shot learners need to identify novel-class objects z novel i,j from other objects and background, and then, predict their classes y novel i,j and structure labels s novel i,j (bounding-boxes or masks). Most existing detection/ segmentation baselines address the problems by modeling h(x i ; θ), performing poorly in a few-shot sce-nario. However, meta-predictor h(x i , D meta ; θ) is also unsuitable, since x i contains multi-object complex information merged in diverse backgrounds.</p><p>Motivation. The real goal of meta-learning for few-shot object detection/ segmentation is to model h(z i,j , D meta ; θ) rather than h(x i , D meta ; θ). Since visual objects {z i,j } ni j=1 are blended with each other and merge with the background in x i , meta-learning with {z i,j } ni j=1 is intractable. Howbeit in two-stage detection models, e.g., Faster/ Mask R-CNNs, multi-object and their background information can be disentangled into RoI (Region-of-Interest) features {ẑ i,j }n i j=1 , which are produced by taking RoIAlign on the image region proposals extracted by the region proposal network (RPN). These RoI features are fed into the second-stage predictor head to achieve RoI-based object classification, position location and silhouette segmentation for {z i,j } ni j=1 . Given this, it is preferable to remodel the R-CNN predictor head into h(ẑ i,j , D meta ; θ) to classify, locate and segment the object z i,j behind each region of interest (RoI) featureẑ i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Meta R-CNN</head><p>Aiming at meta-learning over regions of interest (RoIs), Meta R-CNN is conceptually simple: its pipeline consists of 1). Faster/ Mask R-CNN; 2). Predictor-head Remodeling Network (PRN). Faster/ Mask R-CNN produces object proposals {ẑ i,j } ni j=1 by their region proposal networks (RPN). Then eachẑ i,j combines with class-attentive vectors inferred by our PRN, which plays the role of h(, D meta ; θ) to detect or segment the novel-class objects. The Meta R-CNN framework is illustrated in <ref type="figure">Fig 2 and</ref> we elaborate it by starting from Faster/ Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Review the R-CNN family</head><p>Faster R-CNN system is known as a two-stage pipeline. The first stage is a region proposal network (RPN), receiving an image x i to produce the candidate object bounding-boxes (so-called object region proposals) in this image. The second stage, i.e., Fast R-CNN <ref type="bibr" target="#b12">[13]</ref>, shares the RPN backbone to extract RoI (Region-of-Interest) features {ẑ i,j }n i j=1 fromn i object region proposals after RoIAlign 2 , enabling its predictor head h(ẑ i,j , θ) to classify and locate the object z i,j behind the RoI featureẑ i,j of x i . Mask R-CNN activates the segmentation ability in Faster R-CNN by adding a parallel mask branch in the predictor head h(·, θ). Due to our identical technique applied in Faster/ Mask R-CNN, we unify their predictor heads by h(·, θ).</p><p>As previously discussed, predictor head h(·, θ) in Faster/ Mask R-CNN is inappropriate to make few-shot object detection/ segmentation. To this we propose PRN that remodels h(·, θ) into a meta-predictor head h(·, D meta ; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Predictor-head Remodeling Network (PRN)</head><p>A straightforward approach to design h(·, D meta ; θ) is to learn θ to predict the optimal parameter w.r.t. an arbitrary meta-set D meta like <ref type="bibr" target="#b42">[43]</ref>. Such explicit "learning-tolearn" manner is sensitive to the architectures and h(·, θ) in Faster/ Mask R-CNN is abandoned. Instead, our work is inspired by the concise spirit of SNAIL <ref type="bibr" target="#b29">[30]</ref>, thus, incorporating class-specific soft-attention vectors to achieve channelwise feature selection on each RoI feature in {z i,j }n i j=1 <ref type="bibr" target="#b4">[5]</ref>. This soft-attention mechanism is implemented by the classattentive vectors v meta inferred from the objects in a metaset D meta via PRN. In particular, suppose that PRN denotes as v meta = f (D meta ; φ), given each RoI featureẑ i,j that belongs to image x i , it holds</p><formula xml:id="formula_1">h(ẑ i,j , D meta ; θ ) = h(ẑ i,j ⊗ v meta , θ) = h(ẑ i,j ⊗ f (D meta ; φ), θ)<label>(1)</label></formula><p>where θ, φ denote the parameters of Faster/Mask R-CNN and our PRN (most of them are shared, θ = {θ, φ}); ⊗ indicates the channel-wise multiplication operator. Eq 1 implies that PRN remodels h(·, θ) into h(·, D meta ; θ) in principles. It is intuitive, flexibly-applied and allows end-to-end joint training with its Faster/ Mask R-CNN counterpart. Suppose x i as the image Meta R-CNN aiming to detect. After RoIAlign in its R-CNN module, it turns to a set of RoI features {ẑ}n i i,j . Here we explain how PRN acts on them. Infer class-attentive vectors. As can be observed, PRN f (D meta ; φ) receives all objects in meta-set D meta as input. In the context of object detection/ segmentation, D meta denotes a series of objects distributed across images, whose classes belong to C meta and there exist K objects per class (K-shot setup). Each object in D meta presents a 4-channel input, i.e., an RGB image x with the same-spatial-size foreground structure label s that are combined to represent this object (s is a binary mask derived from the object boundingbox or segmentation mask). Hence given m as the size of C meta , PRN receives mK 4-channel object inputs in each inference process. To ease the computation burden, we standardize the spatial size of object inputs into 224×224. During inference, after passing the first convolution layer of our PRN, each object feature would be fed into the second layer of its R-CNN counterpart, undergoing the shared backbone before RoIAlign. Instead of accepting RoIAlign, the feature passes a channel-wise soft-attention layer to produce its object attentive vector v. To this end, PRN encodes mK objects in D meta into mK object attentive vectors and then, applies average pooling to obtain the class-attentive vectors v meta</p><formula xml:id="formula_2">c , i.e., v meta c = 1 K K j=1 v (c) k , (∀c ∈ C meta , v (c)</formula><p>k represents an object attentive vector inferred from a class-c object and there are K-shot objects per class).</p><p>Remodel R-CNN predictor heads. After obtaining the class-attentive vectors v meta c (∀c ∈ C meta ), PRN applies them to take channel-wise soft-attention on each RoI fea-</p><formula xml:id="formula_3">ture z i,j . Suppose thatẐ i = [ẑ i,1 ; · · · ;ẑ i,128 ] ∈ R 2048×128 denotes the RoI feature matrix generated from x i (128 de- notes the number of RoI). PRN replacesẐ i byẐ i ⊗v meta c = [ẑ i,1 ⊗ v meta c ; · · · ;ẑ i,128 ⊗ v meta c</formula><p>] to feed the primitive predictor heads in Faster /Mask R-CNNs. The refinement leads to detecting or segmenting all class-c objects in the image x i . In this spirit, each RoI featureẑ i,j produces m binary detection outcomes that refers to the classes in C meta . To this Meta R-CNN categorizesẑ i,j into the class c * with the highest confidence score and use the branchẑ i,j ⊗ v meta c * to locate or segment the object. But if the highest confidence score is lower than the objectness threshold, this RoI would be treated as background and discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation</head><p>Meta R-CNN is trained under a meta-learning paradigm. Our implementation based on Faster/ Mask R-CNN, whose hyper-parameters follow their original report.</p><p>Mini-batch construction. Simulating the meta-learning paradigm we have discussed, a training mini-batch in Meta R-CNN is comprised of m classes C meta ∼ C base ∪ C novel , a K-shot m-class meta-set D meta and m-class training set D train (classes in D meta , D train consistent with C meta ). In our implementation, D train represent the objects in the input x of Faster/ Mask R-CNNs. To keep the class consistency, we choose C meta as the object classes image x refers to, and only uses the attentive vectors inferred from the objects belonging to the classes in C meta . Therefore, if the R-CNN module receives an image input x that contains objects in m classes, a mini-batch consists of x (D train ) and mK resized images with their structure label masks.</p><p>Channel-wise soft-attention layer. This layer receives the features induced from the main backbone of the R-CNN counterpart. It performs a spatial pooling to align the object features maintaining the identical size of RoI features. Then these features undergo an element-wise sigmoid to produce attentive vectors (the size is 2048×1 in our experiment).</p><p>Meta-loss. Given an RoI featureẑ i,j , to avoid the prediction ambiguity after soft-attention, attentive vectors from different-class objects should lead to diverse feature selection effectsonẑ i,j . To achievethis we propose a simple metaloss L(φ) meta to diversify the inferred object attentive vectors in meta-learning. It is implemented by a cross-entropy loss encouraging the object attentive vectors to fall in the class each object belongs to. This auxiliary loss powerfully boosts Meta R-CNN performance (see <ref type="table">Table 6</ref> Ablation 2).</p><p>RoI meta-learning. Following the typical optimization routines in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref>, meta-learning Meta R-CNN is divided into two learning phases. In the first phase (so-called meta-train), we solely consider base-class objects to construct D meta and D train per iter. In the case that an image simultaneously includes base-class and novel-class objects, we ignore the novel-class objects in meta-train. In the second phase (so-called meta-test), objects in base and novel classes are both considered. The objective is formulated as</p><formula xml:id="formula_4">min θ, φ L(θ, φ) cls + L(θ, φ) reg + λL(θ, φ) mask Losses derived from Faster/Mask R−CNN +L(φ) meta (2) where λ = {0, 1}</formula><p>indicates the activator of mask branch. The illustration of meta-learning for Meta R-CNN is below: <ref type="figure">Figure 3</ref>. The illustrative instance of meta-optimization process in Meta R-CNN. Suppose the image Faster/ Mask R-CNN receiving contains objects in "person", "horse". Then Cmeta = ( "person", "horse" ) and Dmeta includes K-shot "person" and "horse" images with their structure labels, respectively. As the training image iteratively changes, Cmeta and Dmeta would adaptively change.</p><p>Inference. Meta R-CNN entails two inference processes based on Faster/Mask R-CNN module and PRN. In training, the object attentive vectors inferred from D meta would replace the class-attentive vectors to take soft-attention effects onẐ i and produce the object detection/ segmentation losses in Eq 2. In testing, we choose C meta = C base ∪ C novel . It is because that unknown objects in a test image may cover all possible categories. PRN receives K-shot visual objects in all classes to produce class-attentive vectors to achieve fewshot object detection/ segmentation. Note that, no matter of object or class attentive vectors, they can be pre-processed before testing, and parallelly take soft-attention on RoI feature matrices. It promises the fast inference of Faster/ Mask R-CNN will not be decelerated: In our experiment (using a single GTX TITAN XP), if shot is 3, the inference speed of Faster R-CNN is 83.0 ms/im; Meta R-CNN is 84.2ms/im; if shot is 10, the speed of Meta R-CNN is 85.4ms/im.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we propose thorough experiments to evaluate Meta R-CNN on few-shot object detection, the related ablation, and few-shot object segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Few-shot object detection</head><p>In few-shot object detection, we employ a Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> with ResNet-101 <ref type="bibr" target="#b16">[17]</ref> backbone as the R-CNN module in our Meta R-CNN framework.</p><p>Benchmarks and setups. Our few-shot object detection experiment follows the setup <ref type="bibr" target="#b20">[21]</ref>. Concretely, we evaluate all baselines on the generic object detection tracks of PAS-CAL VOC 2007 <ref type="bibr" target="#b8">[9]</ref>, 2012 <ref type="bibr" target="#b8">[9]</ref>, and MS-COCO <ref type="bibr" target="#b26">[27]</ref> benchmarks. We adopt the PASCAL Challenge protocol that a correct prediction should have more than 0.5 IoU with the ground truth and set the evaluation metric to the mean Average Precision (mAP). Among these benchmarks, VOC 2007 and 2012 consists of images covering 20 object categories for training, validation and testing sets. To create a fewshot learning setup, we consider three different novel/baseclass split settings, i.e., ("bird", "bus", "cow", "mbike", "sofa"/ rest); ("aero", "bottle","cow","horse","sofa" / rest) and ("boat", "cat", "mbike","sheep", "sofa"/ rest). During the first phase of meta-learning, only base-class objects are considered. In the second phase, there are K-shot annotated bounding boxes for objects in each novel class and 3K annotated bounding boxes for objects in each base class for training, where K is set 1, 2, 3, 5 and 10. We also evaluate our method on COCO benchmark with 80 object categories including the 20 categories in PASCAL VOC. In this experiment, we set the 20 classes included in PASCAL VOC as the novel classes, then the rest 60 classes in COCO as base classes. The union of 80k train images and a 35k subset of validation images (trainval35k) are used for training, and our evaluation is based on the remaining 5k val images (minival). Finally, we consider the cross-benchmark transfer setup of few-shot object detection from COCO to PAS-CAL <ref type="bibr" target="#b20">[21]</ref>, which leverages 60 base classes of COCO to learn knowledge representations and the evaluation is based on 20 novel classes of PASCAL VOC.</p><p>Baselines. In methodology, Meta R-CNN can be treated as the meta-learning extension of Faster R-CNN (FRCN) <ref type="bibr" target="#b35">[36]</ref> in the background of few-shot object detection. To this a question about detector generalization is probably raised:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Meta R-CNN help to improve the generalization capability of Faster R-CNN?</head><p>To answer this question, we compare our Meta R-CNN with its base FRCN. This detector is derived into three base-   lines according to the different training strategies they use. Specifically, FRCN+joint is to jointly train the FRCN detector with base-class and novel-class objects. The identical number of iteration is used for training this baseline and our Meta R-CNN. FRCN+ft takes a similar two-phase training strategy in Meta R-CNN: it only uses base-class objects (with bounding boxes) to train FRCN in the first phase, then use the combination of base-class and novelclass objects to fine-tune the network. For a fair comparison, the objects in images used to train FRCN+ft is identical to Meta R-CNN, and FRCN+ft also takes the same number of iteration (in both training phases) of Meta R-CNN. Finally, FRCN+ft-full employ the same training strategy of FRCN+ft in the first phase, yet train the detector to fully converge in the second phase. Beyond these baselines, Meta R-CNN is also compared with the state-of-the-art few-shot object detector <ref type="bibr" target="#b20">[21]</ref> modified from YOLOv2 <ref type="bibr" target="#b34">[35]</ref> (YOLO-Few-shot). Note that, YOLO-Few-shot also employs metalearning, whereas distinct from Meta R-CNN based on RoI features, it is based on a full image. Their comparison reveals whether the motivation of Meta R-CNN is reasonable.</p><p>PASCAL VOC. The experimental evaluation are shown in <ref type="table" target="#tab_0">Table 1</ref>. The K-shot object detection is performed based on K = (1, 2, 3, 5, 10) across three novel/base class splits. As can be observed, Meta R-CNN consistently outperforms the three FRCN baselines by a large margin across splits. It uncovers the generalization weakness of FRCN: without adequate number of bounding-box annotations, FRCN performs poorly to detect novel-class objects, and this weakness could not be overcome by changing the training strategies. In a comparison, by simply deploying a lightweight PRN, FRCN turns into Meta R-CNN and significantly improve the performance on novel-class object detection. It implies that our approach endows FRCN with the generalization ability in few-shot learning.</p><p>Besides, Meta R-CNN outperforms YOLO-Few-shot in the majority of the cases (except for 1/2-shot in the third split). Since the YOLO-Few-shot results are borrowed from their report, the 1/2-shot objects are probably different from what we use. Extremely-few-shot setups are sensitive to the change of the few-shot object selection and thus, hard to reveal the superiority of few-shot learning algorithms. In the more robust 5/10-shot setups, Meta R-CNN significantly exceeds YOLO-Few-shot (+11.8% in the 5-shot of the first split; +6.8 in the 10-shot of the third split.)</p><p>Let's consider detailed evaluation in <ref type="table" target="#tab_2">Table 3</ref> based on the first base/novel-class split. Note that, FRCN+joint achieved SOTA in base classes, however, at the price of the performance disaster in novel classes (72.7 in base classes yet 4.3 in novel classes given K=3). This sharp contrast caused by the extreme object quantity imbalance in the few-shot setup, further reveal the fragility of FRCN in the generalization problem. On the other hand, we find that Meta R-CNN outperforms YOLO-Few-shot both in base classes and novel   classes, which means that Meta R-CNN is the SOTA fewshot detector. Finally, Meta R-CNN outperforms all other baselines in mAP. This observation is significant: Meta R-CNN would not sacrifice the overall performance to make few-shot learning. <ref type="figure" target="#fig_0">In Fig 4,</ref> we visualize some comparison between FRCN+ft+full and Meta R-CNN on detecting novel-class objects. MS COCO. We evaluate 10-shot /30-shot setups on MS COCO <ref type="bibr" target="#b26">[27]</ref> benchmark and report the standard COCO metrics. The results on novel classes are presented in <ref type="table" target="#tab_3">Table 4</ref>. It shows that Meta R-CNN significantly outperforms other baselines and YOLO-Few-shot. Note that, the performance gain is obtained by our method compared to YOLO-Few-shot (12.4% vs. 11.1%). The improvement is lower than those on PASCAL VOC, since MS COCO is more challenging with more complex scenarios such as occlusion, ambiguities and small objects.</p><p>MS COCO to PASCAL. In this cross-dataset few-shot object detection setup, all the baselines are trained with 10shot objects in novel classes on MS COCO while they are evaluated on PASCAL VOC2007 test set. Distinct from the previous experiments that focus on evaluating crosscategory model generalization, this setup further to reveal the cross-domain generalization ability. FRCN+ft and FRCN+ft-full get the detection performances of 19.2% and 31.2% respectively. The few-shot object detector YOLO-Few-shot obtains 32.3%. Instead, Meta R-CNN achieves 37.4%, reaping a significant performance gain (approximately 5% mAP) against the second best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation</head><p>Here we conduct comprehensive ablation studies to uncover Meta R-CNN. These ablations are based on 3/10-shot object detection performances on PASCAL VOC in the first base/novel split setup.</p><p>Backbone. We ablate the backbone (i.e. ResNet-34 <ref type="bibr" target="#b16">[17]</ref> and ResNet-101 <ref type="bibr" target="#b16">[17]</ref>) of Meta R-CNN to observe the object detection performances in base and novel classes (Table 2). It's observed that our framework significantly outperforms the FRCN-ft-full on base and novel classes across <ref type="table">Table 7</ref>. Few-shot detection and instance segmentation performance on COCO minival set for novel classes under Mask R-CNN with ResNet-50. The evaluation based on 5/10/20-shot-object in novel classes (More comprehensive results see our supplementary material).  <ref type="bibr" target="#b44">45</ref>.6% with ResNet-101 on novel classes). These verify the potential of Meta R-CNN that can be flexibly-deployed across different backbones and consistently outperforms the baseline methods.</p><p>RoI meta-learning. Since Meta R-CNN is formally devised as a meta-learner, it would be important to observe whether it is truly improved by RoI meta-learning. To verify our claim, we ablate Meta R-CNN from two aspects: 1). using meta-learning or not (Ablation 1 in <ref type="table">Table 6</ref>); 2). meta-learning on full-image or RoI features ( <ref type="table">Table 5</ref>). As illustrated in <ref type="table">Table 6</ref> (Ablation 1), meta-learning significantly boosts Meta R-CNN performance by clear large margins both in novel classes (35.0% vs.9.0% in 3-shot; 51.5% vs.40.5% in 10-shot) and in base classes (38.5% vs.64.8% in 3-shot; 67.9% vs.56.9% in 10-shot). As K decreases, the improvement will be more significant. In <ref type="table">Table 5</ref>, we have observed that full-image meta-learning suffers heavy performance drop compared with RoI meta-learning and moreover, it even performs worse than the Faster R-CNN trained without meta-strategy. It shows that RoI meta-learning indeed encourages the generalization of the R-CNN family.</p><p>Meta-loss L meta (φ). Meta R-CNN takes the control of Faster R-CNN by way of class attentive vectors. Their reasonable diversity would lead to the performance improvement when detecting the objects in different classes. To verify our claim, we ablate the meta-loss L meta (φ) used to increase the diversity of class-attentive vectors. The ablation is shown in <ref type="table">Table 6</ref> Ablation 2. Obviously, the Meta R-CNN performances in base and novel classes are significantly improved by adding the meta-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Few-shot object segmentation</head><p>As we demonstrated in our methodology, Meta R-CNN is a versatile meta-learning framework to achieve few-shot object structure prediction, especially, not just limited in the object detection task. To verify our claim, we deploy PRN to change a Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> (MRCN) into its Meta R-CNN version. This Meta R-CNN using ResNet-50 <ref type="bibr" target="#b18">[19]</ref> as its backbone, would be evaluated on the instance-level object segmentation track on MS COCO benchmark. We report the standard COCO metrics based on object detection and segmentation. Noted that, AP in object segmentation is evaluated by using mask IoU. We use the trainval35k images for training and val5k for testing where the 20 classes in PASCAL VOC <ref type="bibr" target="#b8">[9]</ref> as novel classes and the remaining 60 categories in COCO <ref type="bibr" target="#b26">[27]</ref> as base classes. Base classes have abundant labeled samples with instance segmentation while novel classes only have K-shot annotated bounding boxes and instance segmentation masks. K is set to 5,10 and 20 in our object segmentation experiments.</p><p>Results. Due to the relatively competitive performances of FRCN+ft+full shown in few-shot object detection, we adopt the same-style training strategy for MRCN, leading to MRCN+ft+full on object detection and instance-level object segmentation results in <ref type="table">Table.</ref> 7. It could be observed that our proposed Meta R-CNN is consistently superior to MRCN+ft+full across 5,10,20-shot settings with significant margins in few-shot object segmentation tasks. For instance, Meta R-CNN achieves a 1.7% performance improvement (6.2% vs.4.5%) on object detection and 2.7% performance improvement (6.4% vs.3.7%) on instance segmentation. These evidences further demonstrate the superiority and universality of our Meta R-CNN presenting. Comprehensive results are found in our supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and Future Work</head><p>Few-shot object detection/ segmentation are very valuable as their successes would lead to an extensive variety of visual tasks generalizing to newly-emerged concepts without heavily consuming labor annotation. Our work takes an insightful step towards the successes by proposing a flexible and simple yet effective framework, e.g., Meta R-CNN. Standing on the shoulders of Faster/ Mask R-CNN, Meta R-CNN overcomes the shared weakness of existing metalearning algorithms that almost disable to recognize the semantic information entangled with multiple objects. Simultaneously, it endows traditional Faster/ Mask R-CNN with the generalization capability in front of few-shot objects in novel classes. It is lightweight, plug-and-play, and performs impressively in few-shot object detection/ segmentation. It is worth noting that, as Meta R-CNN solely remodels the predictor branches into a meta-learner, it potentially can be extended to a broad range of models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46]</ref> in the entire R-CNN family. To this Meta R-CNN might enable visual structure prediction in the more challenging few-shot conditions, e.g., few-shot relationship detection and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Accelerated task adaptation</head><p>Meta-learning facilitates Faster R-CNN to detect novelclass few-shot objects. Through the lens of stochastic optimization, it gives the credits to the task adaptation acceleration. More specifically, we observe the performance comparison between Faster R-CNN (trained by two-phase strategy, i.e., FRCN+ft-full) and Meta R-CNN over iterations. As shown in <ref type="figure">Fig 8.1</ref>, Meta R-CNN presents as an envelope that upper bounds Faster R-CNN. It indicates meta-learning encouraging faster performance improvement to novel-class object detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Attentive vector analysis</head><p>As we mentioned in the paper, Meta R-CNN takes class attentive vectors to remodel Faster R-CNN, while class attentive vectors are inferred by averaging the object attentive vectors in each class. It implies that learning good representation of object attentive vectors would lead to the success of Meta R-CNN. To this end, we visualize the object attentive vectors used for testing by t-SNE <ref type="bibr" target="#b28">[29]</ref>, and compare the same visualization when Meta R-CNN is trained without meta-loss L meta (φ) . All are illustrated in <ref type="figure">Fig  8.</ref>2. First, we find that object attentive vectors tend to cluster together when they belong to the same class and repulse those from the other classes (See <ref type="figure">Fig 8.2 (a)</ref>). These object attentive vectors produce more deterministic class attentive vector (less inter-class variance when choosing different objects to induce class attentive vectors). To this Meta R-CNN is endowed with more stable performance, since class attentive vectors would not significant change when objects change. Distinct from this, when Meta R-CNN is trained without meta-loss <ref type="figure">Fig 8.2 (b)</ref> , object attentive vectors become more diverse and the inter-class variance is very large.</p><p>These object attentive vectors bring about two negative effects to Meta R-CNN: 1). Due to the large inter-class variance, the trained model suffers unstable performances: if we change the objects, the according class attentive vectors will significantly change. 2). The inferred class attentive vectors are probably close, resulting ambiguous object detection produced by the corresponding class-specific predictor heads.</p><p>In <ref type="figure">Fig 8.2 (a)</ref>, it is also observed that the classes with similar semantics would be closer to those with different semantics. For instance, 'Car', 'Bus', 'Train' are close together, as they all belong to vehicle. The observation unveils that Meta R-CNN may achieve novel-class object detection by the aid of the base-class objects that share similar semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Few-shot object detection</head><p>In <ref type="table" target="#tab_6">Table 8</ref>, we conduct the PASCAL VOC experimental results based on few-shot object detection in details. These experiments are based on three different novel / base-class split settings: Novel-class Split-1 ("bird", "bus", "cow", "mbike", "sofa"/ rest); Novel-class Split-2 ("aero", "bottle","cow","horse","sofa" / rest) and Novel-class Split-3 ("boat", "cat", "mbike","sheep", "sofa"/ rest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Few-shot object segmentation</head><p>In <ref type="table" target="#tab_7">Table 9</ref> 10, we conduct the COCO experiments based on few-shot object segmentation in two different novel/base-class split settings. In novel-class split-1, the novel class selection follows the classes in PAS-CAL VOC. In novel-class split-2, we randomly choose ('person','car', 'motorcycle', 'airplane', 'bus', 'train', 'cow','elephant','zebra','tennis racket','bed', 'refrigerator','pizza', 'toilet','microwave','truck','umbrella', 'handbag', 'parking meter', 'teddy bear') as novel classes. In the 5-/10-shot experiment in Split-1, we develop two variants from our Meta R-CNN, i.e., (224x224) and (600x600). They indicate different resolution of the input in meta  (reference)-set D meta . Since object segmentation concerns more detailed semantic than object detection, increasing the resolution of reference image can significantly improve the segmentation performance on those objects in the datastarve categories. For a fair comparison with other baselines, the images used for training (D train ) and evaluation (D test ) are consistent in 224x224 across all baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Construction Ablation of PRN</head><p>We additionally test four designs to model a predictor head in different manners: concate (Concatenate the class attentive vector and RoI feature for the class-specific prediction), plus (elementwise-plus of class attentive feature and RoI feature for the class-specific prediction), unshare (The parameters of PRN and R-CNN counterpart are not shared), limited meta set (Only use the image-related classes to generate D meta ). Results are shown in <ref type="table">Table.</ref>11. concate shows superior in "Base" object detection while ours (channel-wise attention) performs better in "Novel" object detection.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of novel-class objects detected by FRCN+ft-full and Meta R-CNN. Compared with Meta R-CNN, FRCN+ft-full is inferior: bboxes in the first two columns are missed; in the middle column is duplicate and the classes are wrong in the last two columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Normalized mAP w.r.t. novel-class object detection over iterations. The mean and variance values of Normalized mAP are computed by class-specific Normalized AP, which is normarlized by the converged value of AP against number of training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>The t-SNE visualization of object attentive vectors with respect to Meta R-CNN trained w/wo meta-loss. For each class, 10 objects are taken to produce the object attentive vectors for visualization. Color indicates class (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Few-shot detection mAP on VOC2007 test set in novel classes. We evaluate the baselines under three different splits of novel classes. RED and BLUE indicate state-of-the-art (SOTA) and the second best. (Best viewd in color) Few-shot [21] 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 39.2 19.2 21.7 25.7 40.6 41.3 19.6 32.8 41.5 45.6 7.9 15.3 26.2 31.6 39.1 9.8 11.3 19.1 35.0 45.1 Meta R-CNN (ours) 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1</figDesc><table><row><cell></cell><cell cols="4">Novel-class Setup 1</cell><cell cols="3">Novel-class Setup 2</cell><cell></cell><cell cols="3">Novel-class Setup 3</cell></row><row><cell>Method/Shot</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5 10 1</cell><cell>2</cell><cell>3</cell><cell>5 10</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5 10</cell></row><row><cell>YOLO-FRCN+joint</cell><cell cols="11">2.7 3.1 4.3 11.8 29.0 1.9 2.6 8.1 9.9 12.6 5.2 7.5 6.4 6.4 6.4</cell></row><row><cell>FRCN+ft</cell><cell cols="11">11.9 16.4 29.0 36.9 36.9 5.9 8.5 23.4 29.1 28.8 5.0 9.6 18.1 30.8 43.4</cell></row><row><cell>FRCN+ft-full</cell><cell>13.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The ablation study of backbones (mAP on VOC2007 testset in novel classes and base classes of the first base/novel split based on FRCN).</figDesc><table><row><cell>Shot</cell><cell>Baselines</cell><cell>Base Novel</cell></row><row><cell></cell><cell cols="2">ResNet-34+ft-full 57.9 19.6</cell></row><row><cell>3</cell><cell cols="2">ResNet-34+Ours 57.6 25.3 ResNet-101+ft-full 63.6 32.8</cell></row><row><cell></cell><cell cols="2">ResNet-101+Ours 64.8 35.0</cell></row><row><cell></cell><cell cols="2">ResNet-34+ft-full 61.1 40.2</cell></row><row><cell>10</cell><cell cols="2">ResNet-34+Ours 61.3 44.5 ResNet-101+ft-full 61.3 45.6</cell></row><row><cell></cell><cell cols="2">ResNet-101+Ours 67.9 51.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>AP and mAP on VOC2007 test set for novel classes and base classes of the first base/novel split. We evaluate the performance for 3/10-shot novel-class examples with FRCN under ResNet-101. RED/BLUE indicate the SOTA/the second best. (Best viewd in color) Meta R-CNN (ours) 30.1 44.6 50.8 38.8 10.7 35.0 67.6 70.5 59.8 50.0 75.7 81.4 44.9 57.7 76.3 74.9 76.9 34.7 58.7 74.7 67.8 64.8 57.3 10 YOLO-Few-shot [21] 30.0 62.7 43.2 60.6 39.6 47.2 65.3 73.5 54.7 39.5 75.7 81.1 35.3 62.5 72.8 78.8 68.6 41.5 59.2 76.2 69.2 63.6 59.5 FRCN+joint 14.6 20.3 19.2 24.3 2.2 16.1 78.1 80.0 65.9 64.1 86.0 87.1 56.9 69.7 84.1 80.0 78.4 44.8 74.6 82.7 74.1 73.8 59.4 FRCN+ft 31.3 36.5 54.1 26.5 36.2 36.9 68.4 75.2 59.2 54.8 74.1 80.8 42.8 56.0 68.9 77.8 75.5 34.7 66.1 71.2 66.2 64.8 57.8 FRCN+ft-full 40.1 47.8 45.5 47.5 47.0 45.6 65.7 69.2 52.6 46.5 74.6 73.6 40.7 55.0 69.3 73.5 73.2 33.8 56.5 69.8 65.1 61.3 57.4 Meta R-CNN (ours) 52.5 55.9 52.7 54.6 41.6 51.5 68.1 73.9 59.8 54.2 80.1 82.9 48.8 62.8 80.1 81.4 77.2 37.2 65.7 75.8 70.6 67.9 63.8</figDesc><table><row><cell>Shot</cell><cell>Baselines</cell><cell>Novel classes bird bus cow 8 55.2 Base classes mAP</cell></row><row><cell></cell><cell>FRCN+joint</cell><cell>13.7 0.4 6.4 0.8 0.2 4.3 75.9 80.0 65.9 61.3 85.5 86.1 54.1 68.4 83.3 79.1 78.8 43.7 72.8 80.8 74.7 72.7 55.6</cell></row><row><cell></cell><cell>FRCN+ft</cell><cell>31.1 24.9 51.7 23.5 13.6 29.0 65.4 56.4 46.5 41.5 73.3 84.0 40.2 55.9 72.1 75.6 74.8 32.7 60.4 71.2 71.2 61.4 53.3</cell></row><row><cell></cell><cell>FRCN+ft-full</cell><cell>29.1 34.1 55.9 28.6 16.1 32.8 67.4 62.0 54.3 48.5 74.0 85.8 42.2 58.1 72.0 77.8 75.8 32.3 61.0 73.7 68.6 63.6 55.9</cell></row></table><note>mbike sofa mean aero bike boat bottle car cat chair table dog horse person plant sheep train tv mean 3 YOLO-Few-shot [21] 26.1 19.1 40.7 20.4 27.1 26.7 73.6 73.1 56.7 41.6 76.1 78.7 42.6 66.8 72.0 77.7 68.5 42.0 57.1 74.7 70.7 64.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Few-shot detection performance on COCO minival set for novel classes. We evaluate the performance for different shot examples of novel classes under FRCN pipeline with ResNet-50. RED/BLUE indicate the SOTA/the second best. (Best viewd in color) Meta R-CNN (ours) 8.7 +2.2 19.1 +5.7 6.6 +0.7 2.3 +0.5 7.7 +2.4 14.0 +2.7 12.6 +0 17.8 +0.1 17.9 +0.1 7.8 +1.3 15.6 +1.2 27.2 −1.4 Meta R-CNN (ours) 12.4 +1.3 25.3 +4.3 10.8 +0.5 2.8 −0.1 11.6 +2.8 19.0 +1.0 15.0 +0 21.4 +0.3 21.7 +0.4 8.6 −1.5 20.0 +2.1 32.1 −1.4</figDesc><table><row><cell>Shot</cell><cell>Baselines</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AR 1</cell><cell>AR 10</cell><cell>AR 100</cell><cell>AR S</cell><cell>AR M</cell><cell>AR L</cell></row><row><cell></cell><cell>YOLO-Few-shot [21]</cell><cell>5.6</cell><cell>12.3</cell><cell>4.6</cell><cell>0.9</cell><cell>3.5</cell><cell>10.5</cell><cell>10.1</cell><cell>14.3</cell><cell>14.4</cell><cell>1.5</cell><cell>8.4</cell><cell>28.2</cell></row><row><cell>10</cell><cell>FRCN+ft FRCN+ft-full</cell><cell>1.3 6.5</cell><cell>4.2 13.4</cell><cell>0.4 5.9</cell><cell>0.4 1.8</cell><cell>0.9 5.3</cell><cell>2.1 11.3</cell><cell>5.5 12.6</cell><cell>8.0 17.7</cell><cell>8.0 17.8</cell><cell>2.4 6.5</cell><cell>6.4 14.4</cell><cell>13.0 28.6</cell></row><row><cell></cell><cell>YOLO-Few-shot [21]</cell><cell>9.1</cell><cell>19.0</cell><cell>7.6</cell><cell>0.8</cell><cell>4.9</cell><cell>16.8</cell><cell>13.2</cell><cell>17.7</cell><cell>17.8</cell><cell>1.5</cell><cell>10.4</cell><cell>33.5</cell></row><row><cell>30</cell><cell>FRCN+ft FRCN+ft-full</cell><cell>1.5 11.1</cell><cell>4.8 21.6</cell><cell>0.5 10.3</cell><cell>0.3 2.9</cell><cell>1.8 8.8</cell><cell>2.0 18.9</cell><cell>7.0 15.0</cell><cell>10.1 21.1</cell><cell>10.1 21.3</cell><cell>5.8 10.1</cell><cell>8.3 17.9</cell><cell>13.5 33.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>The ablation of image-level and RoI-level meta-learning Ablation studies of (1) meta-learning and (2) meta-loss (mAP on VOC2007 test set for novel classes and base classes of the first base/novel split under FRCN pipeline with ResNet-101) .</figDesc><table><row><cell></cell><cell>shot</cell><cell></cell><cell>Method</cell><cell>Base Novel</cell></row><row><cell></cell><cell>3</cell><cell cols="2">full-image meta-learning 43.4 8.1 RoI meta-learning 64.8 35.0</cell></row><row><cell></cell><cell>10</cell><cell cols="2">full-image meta-learning 61.2 32.0 RoI meta-learning 67.9 51.5</cell></row><row><cell>shot</cell><cell cols="2">Ablation (1)</cell><cell>Base Novel Ablation (2) Base Novel</cell></row><row><cell>3</cell><cell cols="3">meta-learning (w/o) 38.5 9.0 meta-loss (w/o) 24.2 57.7 meta-learning (w) 64.8 35.0 meta-loss (w) 35.0 64.8</cell></row><row><cell>10</cell><cell cols="3">meta-learning (w/o) 56.9 40.5 meta-loss (w/o) 46.6 64.3 meta-learning (w) 67.9 51.5 meta-loss (w) 51.5 67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>+2.2 9.9 +6.9 1.2 +0.1 1.2 +0.9 3.9 +2.<ref type="bibr" target="#b7">8</ref> 5.8 +3.4 2.8 +1.5 6.9 +4.2 1.7 +0.6 0.3 +0.0 2.3 +1.7 4.7 +2.5 +3.1 14.2 +8.5 3.0 +1.1 2.0 +0.0 6.6 +3.9 8.8 +4.9 4.4 +2.5 10.6 +5.9 3.3 +2.0 0.5 +0.3 3.6 +2.2 7.2 +4.0 +1.7 16.6 +6.8 2.5 −0.9 1.7 −0.3 6.7 +2.1 9.6 +3.4 6.4 +2.7 14.8 +6.3 4.4 +1.5 0.7 +0.4 4.9 +2.4 9.3 +3.5 different backbones (large margins of 35.0% vs. 32.8% with ResNet-34 and 51.5% vs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Box</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shot</cell><cell>method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell cols="3">MRCN+ft-full MRCN+ft-full Meta R-CNN (ours) 3.5 10 1.3 5 2.5 MRCN+ft-full 4.5 Meta R-CNN (ours) 5.6 20 Meta R-CNN (ours) 6.2</cell><cell>3.0 5.7 9.8</cell><cell>1.1 1.9 3.4</cell><cell>0.3 2.0 2.0</cell><cell>1.1 2.7 4.6</cell><cell>2.4 3.9 6.2</cell><cell>1.3 1.9 3.7</cell><cell>2.7 4.7 8.5</cell><cell>1.1 1.3 2.9</cell><cell>0.3 0.2 0.3</cell><cell>0.6 1.4 2.5</cell><cell>2.2 3.2 5.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>AP and mAP on VOC2007 test set for novel classes and base classes of the first base/novel split. We evaluate the performance for different shots novel-class examples with FRCN under ResNet-101. RED/BLUE indicate the SOTA/the second best. (Best viewd in color) Shot Baselines bird bus cow mbike sofa mean aero bottle cow horse sofa mean boat cat mbike sheep sofa mean 1 YOLO-Few-shot[21] 13.5 10.6 31.5 13.8 4.3 14.8 11.8 9.1 15.6 23.7 18.2 15.7 10.8 44.0 17.8 18.1 5.3 19.2 CNN (ours) 6.1 32.8 15.0 35.4 0.2 19.9 23.9 0.8 23.6 3.1 0.7 10.4 0.6 31.1 28.9 11.0 0.1 14.3 2 YOLO-Few-shot[21] 21.2 12.0 16.8 17.9 9.6 15.5 28.6 0.9 27.6 0.0 19.5 15.3 5.3 46.4 18.4 26.1 12.4 21.7 25.9 49.3 13.0 1.5 19.6 3.5 0.1 36.1 35.7 1.1 15.3 2.2 25.6 13.9 13.9 0.9 11.3 Meta R-CNN (ours) 17.2 34.4 43.8 31.8 0.4 25.5 12.4 0.1 44.4 50.1 0.1 19.4 10.6 24.0 36.2 19.2 0.8 18.2 3 YOLO-Few-shot [21] 26.1 19.1 40.7 20.4 27.1 26.7 29.4 4.6 34.9 6.8 37.9 22.7 11.2 39.8 20.9 23.7 33.0 25.7 34.1 55.9 28.6 16.1 32.8 31.9 0.3 45.2 50.4 3.4 26.2 10.6 27.2 16.5 31.7 9.5 19.1 Meta R-CNN (ours) 30.1 44.6 50.8 38.8 10.7 35.0 25.2 0.1 50.7 53.2 18.8 29.6 16.3 39.7 32.6 38.8 10.3 27.5 5 YOLO-Few-shot[21] 31.5 21.1 39.8 40.0 37.0 33.9 33.1 9.4 38.4 25.4 44.0 30.1 14.2 57.3 50.8 38.9 41.6 40.6 33.5 37.2 41.5 23.1 3.9 44.7 54.0 32.2 31.6 11.0 51.8 36.0 41.3 34.6 35.0 Meta R-CNN (ours) 35.8 47.9 54.9 55.8 34.0 45.7 28.5 0.3 50.4 56.7 38.0 34.8 16.6 45.8 53.9 41.5 48.1 41.2 10 YOLO-Few-shot [21] 30.0 62.7 43.2 60.6 39.6 47.2 43.2 13.9 41.5 58.1 39.2 39.2 20.1 51.8 55.6 42.4 36.6 41.3 FRCN+joint 14.6 20.3 19.2 24.3 2.2 16.1 17.6 9.1 13.8 21.6 0.8 12.6 2.3 43.0 17.4 12.6 1.0 15.3 FRCN+ft 31.3 36.5 54.1 26.5 36.2 36.9 46.5 4.5 34.0 57.9 1.1 28.8 15.5 65.2 53.6 40.9 41.9 43.4 FRCN+ft-full 40.1 47.8 45.5 47.5 47.0 45.6 44.3 3.0 42.9 59.4 46.2 39.1 19.4 64.3 57.3 40.9 43.4 45.1 Meta R-CNN (ours) 52.5 55.9 52.7 54.6 41.6 51.5 52.8 3.0 52.1 70.0 49.2 45.4 13.9 72.6 58.3 47.8 47.6 48.1</figDesc><table><row><cell></cell><cell>Novel-class Split-1</cell><cell>Novel-class Split-2</cell><cell>Novel-class Split-3</cell></row><row><cell>FRCN+joint</cell><cell cols="3">9.7 0.0 1.5 0.5 1.8 2.7 1.6 0.3 3.2 3.6 0.8 1.9 0.2 21.9 0.0 1.1 3.0 5.2</cell></row><row><cell>FRCN+ft</cell><cell cols="3">13.4 14.8 4.9 25.6 0.7 11.9 0.5 0.2 15.9 12.2 0.6 5.9 10.4 7.3 13.1 3.5 0.6 5.0</cell></row><row><cell>FRCN+ft-full</cell><cell cols="3">14.3 16.6 16.4 18.7 2.9 13.8 0.5 0.4 22.7 15.0 0.7 7.9 0.8 26.4 12.3 9.3 0.1 9.8</cell></row><row><cell>Meta R-FRCN+joint</cell><cell cols="3">12.4 0.1 2.2 0.3 0.5 3.1 2.3 0.2 3.9 5.4 1.0 2.6 1.3 25.0 0.2 9.7 1.5 7.5</cell></row><row><cell>FRCN+ft</cell><cell cols="3">5.4 19.0 39.8 16.6 1.2 16.4 3.6 1.3 13.1 23.3 1.4 8.5 5.3 16.9 10.2 14.3 1.1 9.6</cell></row><row><cell cols="4">FRCN+ft-full 8.1 FRCN+joint 13.7 0.4 6.4 0.8 0.2 4.3 16.7 0.2 7.4 15.7 0.5 8.1 0.2 37.2 0.6 17.2 0.1 11.1</cell></row><row><cell>FRCN+ft</cell><cell cols="3">31.1 24.9 51.7 23.5 13.6 29.0 29.8 0.1 40.3 43.8 2.9 23.4 3.7 32.8 18.2 30.7 5.0 18.1</cell></row><row><cell cols="4">FRCN+ft-full 29.1 FRCN+joint 17.4 7.9 9.6 14.0 9.1 11.8 3.2 4.5 16.1 24.8 0.6 9.9 1.6 39.7 3.2 16.4 3.4 12.9</cell></row><row><cell>FRCN+ft</cell><cell cols="3">31.3 36.5 54.1 26.5 36.2 36.9 17.5 2.3 39.6 55.0 31.2 29.1 5.1 41.7 33.1 36.2 37.9 30.8</cell></row><row><cell>FRCN+ft-full</cell><cell>36.1 44.6 56.0</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Few-shot detection and instance segmentation performance on COCO minival set for novel classes under Mask R-CNN with ResNet-50. The evaluation based on 5/10/20-shot-object in novel classes. +1.1 5.8 +2.<ref type="bibr" target="#b7">8</ref> 1.5 +0.4 0.8 +0.5 2.5 +1.4 3.7 +1.3 2.2 +0.9 4.9 +2.2 1.7 +0.6 0.2 −0.1 1.7 +1.1 3.6 +1.4Meta R-CNN (600x600) 3.5 +2.2 9.9 +6.9 1.2 +0.1 1.2 +0.9 3.9 +2.<ref type="bibr" target="#b7">8</ref> 5.8 +3.4 2.8 +1.5 6.9 +4.2 1.7 +0.6 0.3 +0.0 2.3 +1.7 4.7 +2.5 +1.8 9.4 +3.7 3.3 +1.4 1.3 −0.7 0.4 −2.3 6.4 +2.5 3.7 +1.8 8.4 +3.7 2.9 +1.6 0.3 +0.1 0.2 −1.2 5.6 +2.4 Meta R-CNN (600x600) 5.6 +3.1 14.2 +8.5 3.0 +1.1 2.0 +0.0 6.6 +3.9 8.8 +4.9 4.4 +2.5 10.6 +5.9 3.3 +2.0 0.5 +0.3 3.6 +2.2 7.2 +4.0 CNN (224x224) 6.2 +1.7 16.6 +6.8 2.5 −0.9 1.7 −0.3 6.7 +2.1 9.6 +3.4 6.4 +2.7 14.8 +6.3 4.4 +1.5 0.7 +0.4 4.9 +2.4 9.3 +3.5</figDesc><table><row><cell cols="2">COCO Novel-class Split-1</cell><cell></cell><cell></cell><cell>Box</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shot</cell><cell>method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell></cell><cell>MRCN+ft-full</cell><cell>1.3</cell><cell>3.0</cell><cell>1.1</cell><cell>0.3</cell><cell>1.1</cell><cell>2.4</cell><cell>1.3</cell><cell>2.7</cell><cell>1.1</cell><cell>0.3</cell><cell>0.6</cell><cell>2.2</cell></row><row><cell cols="3">5 Meta R-CNN (224x224) 2.4 10 MRCN+ft-full 2.5 MRCN+ft-full 4.5 Meta R-CNN (224x224) 4.3 20 Meta R-</cell><cell>5.7 9.8</cell><cell>1.9 3.4</cell><cell>2.0 2.0</cell><cell>2.7 4.6</cell><cell>3.9 6.2</cell><cell>1.9 3.7</cell><cell>4.7 8.5</cell><cell>1.3 2.9</cell><cell>0.2 0.3</cell><cell>1.4 2.5</cell><cell>3.2 5.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Few-shot detection and instance segmentation performance on COCO minival set for novel classes under Mask R-CNN with ResNet-50. The evaluation based on 5/10/20-shot-object in novel classes.CNN  (224x224) 3.3 +1.0 9.4 +5.0 1.1 −1.2 1.7 +1.1 3.9 +1.6 4.4 +1.2 2.3 +0.2 5.1 +1.2 1.8 −0.2 0.4 +0.1 2.2 +0.4 3.8 +0.7Meta R-CNN (600x600) 3.1 +0.<ref type="bibr" target="#b7">8</ref> 8.9 +4.5 1.1 −1.2 1.1 +0.6 3.0 +0.7 5.1 +1.9 2.2 +0.1 4.7 +0.8 1.9 −0.1 0.4 +0.1 1.7 −0.1 3.2 +0.1 CNN (224x224) 3.9 +1.3 11.2 +5.2 1.4 −0.4 1.9 +0.7 4.0 +1.3 5.9 +2.3 2.9 +0.1 6.3 +0.6 2.1 −0.2 0.5 +0.0 2.8 +0.2 5.0 +0.9 Meta R-CNN (600x600) 3.9 +1.3 11.0 +5.0 1.7 −0.1 1.7 +0.5 3.9 +1.2 6.2 +2.6 2.8 +0.0 6.4 +0.7 2.1 −0.2 0.5 +0.0 2.7 +0.1 4.5 +0.4 +1.3 10.2 +2.1 3.8 +1.5 2.8 +0.6 5.4 +1.7 7.2 +2.3 4.5 +1.2 9.4 +2.0 3.8 +1.5 1.1 +0.3 4.5 +1,3 7.8 +2.3</figDesc><table><row><cell cols="2">COCO Novel-class Split-2</cell><cell></cell><cell></cell><cell>Box</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mask</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shot</cell><cell>method</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP S</cell><cell>AP M</cell><cell>AP L</cell></row><row><cell cols="2">MRCN+ft-full MRCN+ft-full Meta R-10 5 MRCN+ft-full Meta R-20 Meta R-CNN (ours)</cell><cell>2.3 2.6 3.4 4.7</cell><cell>4.4 6.0 8.1</cell><cell>2.3 1.8 2.3</cell><cell>0.6 1.2 2.2</cell><cell>2.3 2.7 3.7</cell><cell>3.2 3.6 4.9</cell><cell>2.1 2.8 3.3</cell><cell>3.9 5.7 7.4</cell><cell>2.0 2.3 2.3</cell><cell>0.3 0.5 0.8</cell><cell>1.8 2.6 3.2</cell><cell>3.1 4.1 5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>The ablation of different variations on PRN</figDesc><table><row><cell>shot</cell><cell>Variations</cell><cell cols="2">Base Novel shot</cell><cell>Variations</cell><cell>Base Novel</cell></row><row><cell></cell><cell>concate</cell><cell>67.0 33.6</cell><cell></cell><cell>concate</cell><cell>68.4 50.5</cell></row><row><cell></cell><cell>plus</cell><cell>64.1 32.9</cell><cell></cell><cell>plus</cell><cell>67.9 48.7</cell></row><row><cell>3</cell><cell>unshare</cell><cell>59.8 21.2</cell><cell>10</cell><cell>unshare</cell><cell>67.3 40.5</cell></row><row><cell></cell><cell cols="2">limited meta set 55.8 33.4</cell><cell></cell><cell cols="2">limited meta set 61.4 49.9</cell></row><row><cell></cell><cell>ours</cell><cell>64.8 35.0</cell><cell></cell><cell>ours</cell><cell>67.9 51.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In a normal setup, meta-learning includes two phases, meta-train and meta-test. The first phase only use a subset of C base to train a meta-learner.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">RoIAlign operation is first introduced by Mask R-CNN yet can be used by Faster R-CNN. Faster R-CNNs in our work are based on RoIAlign.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01244</idno>
		<title level="m">Meta-learning by adjusting priors based on extended pac-bayes theory</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bottom-up instance segmentation using deep higher-order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02583</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Masklab: Instance segmentation by refining object detection with semantic and direction features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised one-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to segment every thing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01866</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Provable guarantees for gradient-based meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10644</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="75" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attentive recurrent comparators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dukkipati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Lowshot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to Learn: Model Regression Networks for Easy Small Sample Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bridging categorylevel and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph rcnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2614" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
