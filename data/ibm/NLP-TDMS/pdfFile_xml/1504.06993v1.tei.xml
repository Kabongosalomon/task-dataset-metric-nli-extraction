<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Change</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loy</forename><forename type="middle">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution [4], we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar "easy to hard" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use case (i.e. Twitter). In addition, we show that our method can be applied as preprocessing to facilitate other low-level vision routines when they take compressed images as input.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Lossy compression (e.g. JPEG, WebP and HEVC-MSP) is one class of data encoding methods that uses inexact approximations for representing the encoded content. In this age of information explosion, lossy compression is indispensable and inevitable for companies (e.g. Twitter and Facebook) to save bandwidth and storage space. However, compression in its nature will introduce undesired complex artifacts, which will severely reduce the user experience (e.g. <ref type="figure">Figure 1</ref>). All these artifacts not only decrease perceptual visual quality, but also adversely affect various low-level image processing routines that take compressed images as input, e.g. contrast enhancement <ref type="bibr" target="#b13">[14]</ref>, super-resolution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref>, and edge detection <ref type="bibr" target="#b1">[2]</ref>. However, under such a huge demand, effective compression artifacts reduction remains an open problem.</p><p>We take JPEG compression as an example to explain compression artifacts. JPEG compression scheme divides (a) Left: the JPEG-compressed image, where we could see blocking artifacts, ringing effects and blurring on the eyes, abrupt intensity changes on the face. Right: the restored image by the proposed deep model (AR-CNN), where we remove these compression artifacts and produce sharp details.</p><p>(b) Left: the Twitter-compressed image, which is first re-scaled to a small image and then compressed on the server-side. Right: the restored image by the proposed deep model (AR-CNN) <ref type="figure">Figure 1</ref>. Example compressed images and our restoration results on the JPEG compression scheme and the real use case -Twitter.</p><p>an image into 8×8 pixel blocks and applies block discrete cosine transformation (DCT) on each block individually. Quantization is then applied on the DCT coefficients to save storage space. This step will cause a complex combination of different artifacts, as depicted in <ref type="figure">Figure 1(a)</ref>. Blocking artifacts arise when each block is encoded without considering the correlation with the adjacent blocks, resulting in discontinuities at the 8×8 borders. Ringing effects along the edges occur due to the coarse quantization of the high-frequency components (also known as Gibbs phenomenon <ref type="bibr" target="#b7">[8]</ref>). Blurring happens due to the loss of high-frequency components. To cope with the various compression artifacts, different approaches have been proposed, some of which can only deal with certain types of artifacts. For instance, deblocking oriented approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref> perform filtering along the block boundaries to reduce only blocking artifacts. Liew et al. <ref type="bibr" target="#b14">[15]</ref> and Foi et al. <ref type="bibr" target="#b4">[5]</ref> use thresholding by wavelet transform and Shape-Adaptive DCT transform, respectively. These approaches are good at removing blocking and ringing artifacts, but tend to produce blurred output. Jung et al. <ref type="bibr" target="#b11">[12]</ref> propose restoration method based on sparse representation. They produce sharpened images but accompanied with noisy edges and unnatural smooth regions.</p><p>To date, deep learning has shown impressive results on both high-level and low-level vision problems . In particular, the SRCNN proposed by Dong et al. <ref type="bibr" target="#b3">[4]</ref> shows the great potential of an end-to-end DCN in image super-resolution.</p><p>The study also points out that conventional sparse-codingbased image restoration model can be equally seen as a deep model. However, we find that the three-layer network is not well suited in restoring the compressed images, especially in dealing with blocking artifacts and handling smooth regions. As various artifacts are coupled together, features extracted by the first layer is noisy, causing undesirable noisy patterns in reconstruction.</p><p>To eliminate the undesired artifacts, we improve the SR-CNN by embedding one or more "feature enhancement" layers after the first layer to clean the noisy features. Experiments show that the improved model, namely "Artifacts Reduction Convolutional Neural Networks (AR-CNN)", is exceptionally effective in suppressing blocking artifacts while retaining edge patterns and sharp details (see <ref type="figure">Figure 1</ref>). However, we are met with training difficulties in training a deeper DCN. "Deeper is better" is widely observed in high-level vision problems, but not in low-level vision tasks. Specifically, "deeper is not better" has been pointed out in super-resolution <ref type="bibr" target="#b2">[3]</ref>, where training a five-layer network becomes a bottleneck. The difficulty of training is partially due to the sub-optimal initialization settings.</p><p>The aforementioned difficulty motivates us to investigate a better way to train a deeper model for low-level vision problems. We find that this can be effectively solved by transferring the features learned in a shallow network to a deeper one and fine-tuning simultaneously <ref type="bibr" target="#b0">1</ref> . This strategy has also been proven successful in learning a deeper CNN for image classification <ref type="bibr" target="#b21">[22]</ref>. Following a similar general intuitive idea, easy to hard, we discover other interesting transfer settings in this low-level vision task: (1) We transfer the features learned in a high-quality compression model (easier) to a low-quality one (harder), and find that it converges faster than random initialization. (2) In the real use case, companies tend to apply different compression strategies (including re-scaling) according to their purposes (e.g. <ref type="figure">Figure 1</ref>(b)). We transfer the features learned in a standard compression model (easier) to a real use case (harder), and find that it performs better than learning from scratch.</p><p>The contributions of this study are three-fold: (1) We formulate a new deep convolutional network for efficient reduction of various compression artifacts. Extensive experiments, including that on real use cases, demonstrate the effectiveness of our method over state-of-the-art methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref> both perceptually and quantitatively. (2) We verify that reusing the features in shallow networks is helpful in learning a deeper model for compression artifact reduction. Under the same intuitive idea -easy to hard, we reveal a number of interesting and practical transfer settings. Our study is the first attempt to show the effectiveness of feature transfer in a low-level vision problem. (3) We show the effectiveness of AR-CNN in facilitating other low-level vision routines (i.e. super-resolution and contrast enhancement), when they take JPEG images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Existing algorithms can be classified into deblocking oriented and restoration oriented methods. The deblocking oriented methods focus on removing blocking and ringing artifacts. In the spatial domain, different kinds of filters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed to adaptively deal with blocking artifacts in specific regions (e.g., edge, texture, and smooth regions). In the frequency domain, Liew et al. <ref type="bibr" target="#b14">[15]</ref> utilize wavelet transform and derive thresholds at different wavelet scales for denoising. The most successful deblocking oriented method is perhaps the Pointwise Shape-Adaptive DCT (SA-DCT) <ref type="bibr" target="#b4">[5]</ref>, which is widely acknowledged as the state-of-the-art approach <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. However, as most deblocking oriented methods, SA-DCT could not reproduce sharp edges, and tend to overly smooth texture regions. The restoration oriented methods regard the compression operation as distortion and propose restoration algorithms. They include projection on convex sets based method (POCS) <ref type="bibr" target="#b29">[30]</ref>, solving an MAP problem (FoE) <ref type="bibr" target="#b22">[23]</ref>, sparse-coding-based method <ref type="bibr" target="#b11">[12]</ref> and the Regression Tree Fields based method (RTF) <ref type="bibr" target="#b10">[11]</ref>, which is the new state-ofthe art method. The RTF takes the results of SA-DCT <ref type="bibr" target="#b4">[5]</ref> as bases and produces globally consistent image reconstructions with a regression tree field model. It could also be optimized for any differentiable loss functions (e.g. SSIM), but often at the cost of other evaluation metrics.</p><p>Super-Resolution Convolutional Neural Network (SR-CNN) <ref type="bibr" target="#b3">[4]</ref> is closely related to our work. In the study, independent steps in the sparse-coding-based method are formulated as different convolutional layers and optimized in a unified network. It shows the potential of deep model in low-level vision problems like super-resolution. However, the model of compression is different from super-resolution in that it consists of different kinds of artifacts. Designing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature enhancement Mapping Reconstruction</head><p>Compressed image (Input)</p><p>Reconstructed image (Output) "noisy" feature maps "cleaner" feature maps "restored" feature maps <ref type="figure" target="#fig_6">Figure 2</ref>. The framework of the Artifacts Reduction Convolutional Neural Network (AR-CNN). The network consists of four convolutional layers, each of which is responsible for a specific operation. Then it optimizes the four operations (i.e., feature extraction, feature enhancement, mapping and reconstruction) jointly in an end-to-end framework. Example feature maps shown in each step could well illustrate the functionality of each operation. They are normalized for better visualization.</p><p>a deep model for compression restoration requires a deep understanding into the different artifacts. We show that directly applying the SRCNN architecture for compression restoration will result in undesired noisy patterns in the reconstructed image. Transfer learning in deep neural networks becomes popular since the success of deep learning in image classification <ref type="bibr" target="#b12">[13]</ref>. The features learned from the ImageNet show good generalization ability <ref type="bibr" target="#b32">[33]</ref> and become a powerful tool for several high-level vision problems, such as Pascal VOC image classification <ref type="bibr" target="#b17">[18]</ref> and object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. Yosinski et al. <ref type="bibr" target="#b31">[32]</ref> have also tried to quantify the degree to which a particular layer is general or specific. Overall, transfer learning has been systematically investigated in high-level vision problems, but not in low-level vision tasks. In this study, we explore several transfer settings on compression artifacts reduction and show the effectiveness of transfer learning in low-level vision problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our proposed approach is based on the current successful low-level vision model -SRCNN <ref type="bibr" target="#b3">[4]</ref>. To have a better understanding of our work, we first give a brief overview of SRCNN. Then we explain the insights that lead to a deeper network and present our new model. Subsequently, we explore three types of transfer learning strategies that help in training a deeper and better network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of SRCNN</head><p>The SRCNN aims at learning an end-to-end mapping, which takes the low-resolution image Y (after interpolation) as input and directly outputs the high-resolution one F (Y). The network contains three convolutional layers, each of which is responsible for a specific task. Specifically, the first layer performs patch extraction and representation, which extracts overlapping patches from the input image and represents each patch as a high-dimensional vector. Then the non-linear mapping layer maps each high-dimensional vector of the first layer to another highdimensional vector, which is conceptually the representation of a high-resolution patch. At last, the reconstruction layer aggregates the patch-wise representations to generate the final output. The network can be expressed as:</p><formula xml:id="formula_0">F i (Y) = max (0, W i * Y + B i ) , i ∈ {1, 2};</formula><p>(1)</p><formula xml:id="formula_1">F (Y) = W 3 * F 2 (Y) + B 3 .<label>(2)</label></formula><p>where W i and B i represent the filters and biases of the ith layer respectively, F i is the output feature maps and ' * ' denotes the convolution operation. The W i contains n i filters of support n i−1 × f i × f i , where f i is the spatial support of a filter, n i is the number of filters, and n 0 is the number of channels in the input image. Note that there is no pooling or full-connected layers in SRCNN, so the final output F (Y) is of the same size as the input image. Rectified Linear Unit (ReLU, max(0, x)) <ref type="bibr" target="#b16">[17]</ref> is applied on the filter responses. These three steps are analogous to the basic operations in the sparse-coding-based super-resolution methods <ref type="bibr" target="#b28">[29]</ref>, and this close relationship lays theoretical foundation for its successful application in super-resolution. Details can be found in the paper <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional Neural Network for Compression Artifacts Reduction</head><p>Insights. In sparse-coding-based methods and SRCNN, the first step -feature extraction -determines what should be emphasized and restored in the following stages. However, as various compression artifacts are coupled together, the extracted features are usually noisy and ambiguous for accurate mapping. In the experiments of reducing JPEG compression artifacts (see Section 4.1.2), we find that some quantization noises coupled with high frequency details are further enhanced, bringing unexpected noisy patterns around sharp edges. Moreover, blocking artifacts in flat areas are misrecognized as normal edges, causing abrupt intensity changes in smooth regions. Inspired by the feature enhancement step in super-resolution <ref type="bibr" target="#b26">[27]</ref>, we introduce a feature enhancement layer after the feature extraction layer in SRCNN to form a new and deeper network -AR-CNN. This layer maps the "noisy" features to a relatively "cleaner" feature space, which is equivalent to denoising the feature maps.</p><p>Formulation. The overview of the new network AR-CNN is shown in <ref type="figure" target="#fig_6">Figure 2</ref>. The three layers of SRCNN remain unchanged in the new model. We also use the same annotations as in Section 3.1. To conduct feature enhancement, we extract new features from the n 1 feature maps of the first layer, and combine them to form another set of feature maps. This operation F 1 can also be formulated as a convolutional layer:</p><formula xml:id="formula_2">F 1 (Y) = max (0, W 1 * F 1 (Y) + B 1 ) ,<label>(3)</label></formula><p>where W 1 corresponds to n 1 filters with size n 1 × f 1 × f 1 . B 1 is an n 1 -dimensional bias vector, and the output F 1 (Y) consists of n 1 feature maps. Overall, the AR-CNN consists of four layers, namely the feature extraction, feature enhancement, mapping and reconstruction layer.</p><p>It is worth noticing that AR-CNN is not equal to a deeper SRCNN that contains more than one non-linear mapping layers 2 . Rather than imposing more non-linearity in the mapping stage, AR-CNN improves the mapping accuracy by enhancing the extracted low-level features. Experimental results of AR-CNN, SRCNN and deeper SRCNN will be shown in Section 4.1.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Learning</head><p>Given a set of ground truth images {X i } and their corresponding compressed images {Y i }, we use Mean Squared Error (MSE) as the loss function:</p><formula xml:id="formula_3">L(Θ) = 1 n n i=1 ||F (Y i ; Θ) − X i || 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">Θ = {W 1 , W 1 , W 2 , W 3 , B 1 , B 1 , B 2 , B 3 },</formula><p>n is the number of training samples. The loss is minimized using stochastic gradient descent with the standard backpropagation. We adopt a batch-mode learning method with a batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Easy-Hard Transfer</head><p>Transfer learning in deep models provides an effective way of initialization. In fact, conventional initialization strategies (i.e. randomly drawn from Gaussian distributions with fixed standard deviations <ref type="bibr" target="#b12">[13]</ref>) are found not suitable for training a very deep model, as reported in <ref type="bibr" target="#b8">[9]</ref>. To address 2 Adding non-linear mapping layers has been suggested as an extension of SRCNN in <ref type="bibr" target="#b3">[4]</ref>. this issue, He et al. <ref type="bibr" target="#b8">[9]</ref> derive a robust initialization method for rectifier nonlinearities, Simonyan et al. <ref type="bibr" target="#b21">[22]</ref> propose to use the pre-trained features on a shallow network for initialization.</p><p>In low-level vision problems (e.g. super resolution), it is observed that training a network beyond 4 layers would encounter the problem of convergence, even that a large number of training images (e.g. ImageNet) are provided <ref type="bibr" target="#b3">[4]</ref>. We are also met with this difficulty during the training process of AR-CNN. To this end, we systematically investigate several transfer settings in training a low-level vision network following an intuitive idea of "easy-hard transfer". Specifically, we attempt to reuse the features learned in a relatively easier task to initialize a deeper or harder network. Interestingly, the concept "easy-hard transfer" has already been pointed out in neuro-computation study <ref type="bibr" target="#b6">[7]</ref>, where the prior training on an easy discrimination can help learn a second harder one.</p><p>Formally, we define the base (or source) task as A and the target tasks as B i , i ∈ {1, 2, 3}. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the base network baseA is a four-layer AR-CNN trained on a large dataset dataA, of which images are compressed using a standard compression scheme with the compression quality qA. All layers in baseA are randomly initialized from a Gaussian distribution. We will transfer one or two layers of baseA to different target tasks (see <ref type="figure" target="#fig_0">Figure 3</ref>). Such transfers can be described as follows.</p><p>Transfer shallow to deeper model. As indicated by <ref type="bibr" target="#b2">[3]</ref>, a five-layer network is sensitive to the initialization parameters and learning rate. Thus we transfer the first two layers of baseA to a five-layer network targetB 1 . Then we randomly initialize its remaining layers <ref type="bibr" target="#b2">3</ref> and train all layers toward the same dataset dataA. This is conceptually similar to  that applied in image classification <ref type="bibr" target="#b21">[22]</ref>, but this approach has never been validated in low-level vision problems.</p><p>Transfer high to low quality. Images of low compression quality contain more complex artifacts. Here we use the features learned from high compression quality images as a starting point to help learn more complicated features in the DCN. Specifically, the first layer of targetB 2 are copied from baseA and trained on images that are compressed with a lower compression quality qB.</p><p>Transfer standard to real use case. We then explore whether the features learned under a standard compression scheme can be generalized to other real use cases, which often contain more complex artifacts due to different levels of re-scaling and compression. We transfer the first layer of baseA to the network targetB 3 , and train all layers on the new dataset.</p><p>Discussion. Why the features learned from relatively easy tasks are helpful? First, the features from a welltrained network can provide a good starting point. Then the rest of a deeper model can be regarded as shallow one, which is easier to converge. Second, features learned in different tasks always have a lot in common. For instance, <ref type="figure" target="#fig_0">Figure 3</ref>.4 shows the features learned under different JPEG compression qualities. Obviously, filters a, b, c of high quality are very similar to filters a , b , c of low quality. This kind of features can be reused or improved during finetuning, making the convergence faster and more stable. Furthermore, a deep network for a hard problem can be seen as an insufficiently biased learner with overly large hypothesis space to search, and therefore is prone to overfitting. These few transfer settings we investigate introduce good bias to enable the learner to acquire a concept with greater generality. Experimental results in Section 4.2 validate the above analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We use the BSDS500 database <ref type="bibr" target="#b0">[1]</ref> as our base training set. Specifically, its disjoint training set (200 images) and test set (200 images) are all used for training, and its validation set (100 images) is used for validation. As in other compression artifacts reduction methods (e.g. RTF <ref type="bibr" target="#b10">[11]</ref>), we apply the standard JPEG compression scheme, and use the JPEG quality settings q = 20 (mid quality) and q = 10 (low quality) in MATLAB JPEG encoder. We only fo- </p><formula xml:id="formula_5">X = {X i } n i=1 . Then the compressed sam- ples Y = {Y i } n i=1</formula><p>are generated from the training samples with MATLAB JPEG encoder <ref type="bibr" target="#b10">[11]</ref>. The sub-images are extracted from the ground truth images with a stride of 10. Thus the 400 training images could provide 537,600 training samples. To avoid the border effects caused by convolution, AR-CNN produces a 20 × 20 output given a 32 × 32 input Y i . Hence, the loss (Eqn. (4)) was computed by comparing against the center 20 × 20 pixels of the ground truth sub-image X i . In the training phase, we follow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> and use a smaller learning rate (10 −5 ) in the last layer and a comparably larger one (10 −4 ) in the remaining layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with the State-of-the-Arts</head><p>We use the LIVE1 dataset <ref type="bibr" target="#b20">[21]</ref> (29 images) as test set to evaluate both the quantitative and qualitative performance. The LIVE1 dataset contains images with diverse properties. It is widely used in image quality assessment <ref type="bibr" target="#b24">[25]</ref> as well as in super-resolution <ref type="bibr" target="#b27">[28]</ref>. To have a comprehensive qualitative evaluation, we apply the PSNR, structural similarity (SSIM) <ref type="bibr" target="#b24">[25]</ref>  <ref type="bibr" target="#b4">5</ref> , and PSNR-B <ref type="bibr" target="#b30">[31]</ref> for quality assessment. We want to emphasize the use of PSNR-B. It is designed specifically to assess blocky and deblocked images, thus is more sensitive to blocking artifacts than the perceptual-aware SSIM index. The network settings are f 1 = 9, f 1 = 7, f 2 = 1, f 3 = 5, n 1 = 64, n 1 = 32, n 2 = 16 and n 3 = 1, denoted as AR-CNN (9-7-1-5) or simply AR-CNN. A specific network is trained for each JPEG quality. Parameters are randomly initialized from a Gaussian distribution with a standard deviation of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with SA-DCT</head><p>We first compare AR-CNN with SA-DCT <ref type="bibr" target="#b4">[5]</ref>, which is widely regarded as the state-of-the-art deblocking oriented method <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref>. The quantization results of PSNR, SSIM and PSNR-B are shown in <ref type="table" target="#tab_0">Table 1</ref>. On the whole, our AR-  CNN outperforms the SA-DCT on all JPEG qualities and evaluation metrics by a large margin. Note that the gains on PSNR-B is much larger than that on PSNR. This indicates that AR-CNN could produce images with less blocking artifacts. To compare the visual quality, we present some restored images 6 with q = 10 in <ref type="figure" target="#fig_8">Figure 10</ref>. From <ref type="figure" target="#fig_8">Figure 10</ref>, we could see that the result of AR-CNN could produce much sharper edges with much less blocking and ringing artifacts compared with SA-DCT. The visual quality has been largely improved on all aspects compared with the state-of-the-art method. Furthermore, AR-CNN is superior to SA-DCT on the implementation speed. For SA-DCT, it needs 3.4 seconds to process a 256 × 256 image. While AR-CNN only takes 0.5 second. They are all implemented using C++ on a PC with Intel I3 CPU (3.1GHz) with 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with SRCNN</head><p>As discussed in Section 3.2, SRCNN is not suitable for compression artifacts reduction. For comparison, we train two SRCNN networks with different settings. (i) The original SRCNN (9-1-5) with f 1 = 9, f 3 = 5, n 1 = 64 and n 2 = 32. (ii) Deeper SRCNN (9-1-1-5) with an additional non-linear mapping layer (f 2 = 1, n 2 = 16). They all use the BSDS500 dataset for training and validation as in Section 4. The compression quality is q = 10. The AR-CNN is the same as in Section 4.1.1. Quantitative results tested on LIVE1 dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>. We could see that the two SRCNN networks are inferior on all evaluation metrics. From convergence curves shown in <ref type="figure" target="#fig_3">Figure 5</ref>, it is clear that AR-CNN achieves higher PSNR from the beginning of the learning stage. Furthermore, from their restored images <ref type="bibr" target="#b5">6</ref> in <ref type="figure">Figure 11</ref>, we find out that the two SRCNN networks all produce images with noisy edges and unnatural smooth regions. These results demonstrate our statements in Section 3.2. In short, the success of training a deep model needs comprehensive understanding of the problem and careful design of the model structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparison with RTF</head><p>RTF <ref type="bibr" target="#b10">[11]</ref> is the recent state-of-the-art restoration oriented method. Without their deblocking code, we can only compare with the released deblocking results. Their model is trained on the training set (200 images) of the BSDS500 dataset, but all images are down-scaled by a factor of 0.5 <ref type="bibr" target="#b10">[11]</ref>. To have a fair comparison, we also train new AR-CNN networks on the same half-sized 200 images. Testing is performed on the test set of the BSDS500 dataset (images scaled by a factor of 0.5), which is also consistent with <ref type="bibr" target="#b10">[11]</ref>. We compare with two RTF variants. One is the plain RTF, which uses the filter bank and is optimized for PSNR. The other is the RTF+SA-DCT, which includes the SA-DCT as a base method and is optimized for MAE. The later one achieves the highest PSNR value among all RTF variants <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="table" target="#tab_2">Table 3</ref>, we obtain superior performance than the plain RTF, and even better performance than the combination of RTF and SA-DCT, especially under the more representative PSNR-B metric. Moreover, training on such a small dataset has largely restricted the ability of AR-CNN. The performance of AR-CNN will further improve given more training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Easy-Hard Transfer</head><p>We show the experimental results of different "easy-hard transfer" settings, of which the details are shown in <ref type="table" target="#tab_3">Table 4</ref>. Take the base network as an example, the base-q10 is a four-layer AR-CNN (9-7-1-5) trained on the BSDS500 <ref type="bibr" target="#b0">[1]</ref> dataset (400 images) under the compression quality q = 10. Parameters are initialized by randomly drawing from a Gaussian distribution with zero mean and standard deviation 0.001. <ref type="figure" target="#fig_4">Figures 6 -8</ref> show the convergence curves on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Transfer shallow to deeper model</head><p>In <ref type="table" target="#tab_3">Table 4</ref>, we denote a deeper (five-layer) AR-CNN as "9-7-3-1-5", which contains another feature enhancement layer (f 1 = 3 and n 1 = 16). Results in <ref type="figure" target="#fig_4">Figure 6</ref> show that the  <ref type="table">transfer  short  network  training  initialization  strategy  form  structure  dataset  strategy  base</ref> base-q10 9-7-1-5 BSDS-q10 Gaussian (0, 0.001) network base-q20 9-7-1-5 BSDS-q20 Gaussian (0, 0.001) shallow base-q10 9-7-1-5 BSDS-q10 Gaussian (0, 0.001) to transfer deeper 9-7-3-1-5 BSDS-q10 1,2 layers of base-q10 deep</p><p>He <ref type="bibr" target="#b8">[9]</ref> 9-7-3-1-5 BSDS-q10 He et al. <ref type="bibr" target="#b8">[9]</ref> high base-q10 9-7-1-5 BSDS-q10 Gaussian (0, 0.001) to transfer 1 layer 9-7-1-5 BSDS-q10 1 layer of base-q20 low transfer 2 layers 9-7-1-5 BSDS-q10 1,2 layer of base-q20 standard base-Twitter 9-7-1-5 Twitter Gaussian (0, 0.001) to transfer q10 9-7-1-5 Twitter 1 layer of base-q10 real transfer q20 9-7-1-5 Twitter 1 layer of base-q20      transferred features from a four-layer network enable us to train a five-layer network successfully. Note that directly training a five-layer network using conventional initialization ways is unreliable. Specifically, we have exhaustively tried different groups of learning rates, but still have not observed convergence. Furthermore, the "transfer deeper" converges faster and achieves better performance than using He et al.'s method <ref type="bibr" target="#b8">[9]</ref>, which is also very effective in training a deep model. We have also conducted comparative experiments with the structure "9-7-1-1-5" and observed the same trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transfer high to low quality</head><p>Results are shown in <ref type="figure" target="#fig_5">Figure 7</ref>. Obviously, the two networks with transferred features converge faster than that training from scratch. For example, to reach an average PSNR of 27.77dB, the "transfer 1 layer" takes only 1.54 × 10 8 backprops, which are roughly a half of that for "base-q10". Moreover, the "transfer 1 layer" also outperforms the 'base-q10" by a slight margin throughout the training phase. One reason for this is that only initializing the first layer provides the network with more flexibility in adapting to a new dataset. This also indicates that a good starting point could help train a better network with higher convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Transfer standard to real use case -Twitter</head><p>Online Social Media like Twitter are popular platforms for message posting. However, Twitter will compress the uploaded images on the server-side. For instance, a typical 8 mega-pixel (MP) image (3264 × 2448) will result in a compressed and re-scaled version with a fixed resolution of 600 × 450. Such re-scaling and compression will introduce very complex artifacts, making restoration difficult for existing deblocking algorithms (e.g. SA-DCT). However, AR-CNN can fit to the new data easily. Further, we want to show that features learned under standard compression schemes could also facilitate training on a completely different dataset. We use 40 photos of resolution 3264 × 2448 taken by mobile phones (totally 335,209 training subimages) and their Twitter-compressed version 7 to train three networks with initialization settings listed in <ref type="table" target="#tab_3">Table 4</ref>. From <ref type="figure" target="#fig_7">Figure 8</ref>, we observe that the "transfer q10" and "transfer q20" networks converge much faster than the "base-Twitter" trained from scratch. Specifically, the "transfer q10" takes 6 × 10 7 backprops to achieve 25.1dB, while the "base-Twitter" uses 10 × 10 7 backprops. Despite of fast convergence, transferred features also lead to higher PSNR values compared with "base-Twitter". This observation suggests that features learned under standard compression schemes are also transferrable to tackle real use case problems. Some restoration results 6 are shown in <ref type="figure" target="#fig_6">Figure 12</ref>. We could see that both networks achieve satisfactory quality improvements over the compressed version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Application</head><p>In the real application, many image processing routines are affected when they take JPEG images as input. Blocking artifacts could be either super-resolved or enhanced, causing significant performance decrease. In this section, we show the potential of AR-CNN in facilitating other lowlevel vision studies, i.e. super-resolution and contrast enhancement. To illustrate this, we use SRCNN <ref type="bibr" target="#b3">[4]</ref> for superresolution and tone-curve adjustment <ref type="bibr" target="#b13">[14]</ref> for contrast enhancement <ref type="bibr" target="#b1">[2]</ref>, and show example results when the input is a JPEG image, SA-DCT deblocked image, and AR-CNN restored image. From results shown in <ref type="figure">Figure 9</ref>, we could see that JPEG compression artifacts have greatly distorted the visual quality in super-resolution and contrast enhancement. Nevertheless, with the help of AR-CNN, these effects     <ref type="figure">Figure 9</ref>. AR-CNN can be applied as pre-processing to facilitate other low-level routines when they take JPEG images as input.</p><p>have been largely eliminated. Moreover, AR-CNN achieves much better results than SA-DCT. The differences between them are more evident after these low-level vision processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Applying deep model on low-level vision problems requires deep understanding of the problem itself. In this paper, we carefully study the compression process and propose a four-layer convolutional network, AR-CNN, which is extremely effective in dealing with various compression artifacts. We further systematically investigate several easy-to-hard transfer settings that could facilitate training a deeper or better network, and verify the effectiveness of transfer learning in low-level vision problems. As discussed in SRCNN <ref type="bibr" target="#b3">[4]</ref>, we find that larger filter sizes also help improve the performance. We will leave them to further work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Easy-hard transfer settings. First row: The baseline 4layer network trained with dataA-qA. Second row: The 5-layer AR-CNN targeted at dataA-qA. Third row: The AR-CNN targeted at dataA-qB. Fourth row: The AR-CNN targeted at Twitter data. Green boxes indicate the transferred features from the base network, and gray boxes represent random initialization. The ellipsoidal bars between weight vectors represent the activation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) High compression quality (quality 20 in Matlab encoder) (b) Low compression quality (quality 10 in Matlab encoder)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>First layer filters of AR-CNN learned under different JPEG compression qualities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparisons with SRCNN and Deeper SRCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Transfer shallow to deeper model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Transfer high to low quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Transfer standard to real use case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Original PSNR /SSIM /PSNR-B JPEG 32.46 dB /0.8558 /29.64 dB SA-DCT 33.88 dB /0.9015 /33.02 dB AR-CNN 34.37 dB /0.9079 /34.10 dB Results on image "parrots" show that AR-CNN is better than SA-DCT on removing blocking artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>JPEG</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Results on image "monarch" show that AR-CNN is better than SRCNN on removing ringing effects. Original / PSNR Twitter / 26.55 dB Transfer q10 / 27.92 dB Restoration results of AR-CNN on Twitter compressed images. The origina image (8MP version) is too large for display and only part of the image is shown for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The average results of PSNR (dB), SSIM, PSNR-B (dB) on the LIVE1 dataset.</figDesc><table><row><cell>Eval. Mat</cell><cell>Quality</cell><cell>JPEG</cell><cell>SA-DCT</cell><cell>AR-CNN</cell></row><row><cell>PSNR</cell><cell>10 20</cell><cell>27.77 30.07</cell><cell>28.65 30.81</cell><cell>28.98 31.29</cell></row><row><cell>SSIM</cell><cell>10 20</cell><cell>0.7905 0.8683</cell><cell>0.8093 0.8781</cell><cell>0.8217 0.8871</cell></row><row><cell>PSNR-B</cell><cell>10 20</cell><cell>25.33 27.57</cell><cell>28.01 29.82</cell><cell>28.70 30.76</cell></row><row><cell cols="5">cus on the restoration of the luminance channel (in YCrCb space) in this paper.</cell></row><row><cell cols="5">The training image pairs {Y, X} are prepared as follows -Images in the training set are decomposed into 32 × 32 sub-images 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The average results of PSNR (dB), SSIM, PSNR-B (dB) on the LIVE1 dataset with q = 10 .</figDesc><table><row><cell></cell><cell>Eval. Mat</cell><cell></cell><cell>JPEG</cell><cell></cell><cell>SRCNN</cell><cell cols="2">Deeper SRCNN</cell><cell></cell><cell>AR-CNN</cell></row><row><cell></cell><cell>PSNR SSIM PSNR-B</cell><cell></cell><cell>27.77 0.7905 25.33</cell><cell></cell><cell>28.91 0.8175 28.52</cell><cell cols="2">28.92 0.8189 28.46</cell><cell></cell><cell>28.98 0.8217 28.70</cell></row><row><cell>AverageRtestRPSNRR(dB)</cell><cell>27.4 27.5 27.6 27.7 27.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AR−CNN deeperRSRCNN SRCNN</cell></row><row><cell></cell><cell>0.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell>3</cell><cell>3.5</cell><cell>4</cell><cell>4.5</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">NumberRofRbackprops</cell><cell></cell><cell></cell><cell></cell><cell>xR10 8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The average results of PSNR (dB), SSIM, PSNR-B (dB) on the test set BSDS500 dataset.</figDesc><table><row><cell>Eval. Mat</cell><cell>Quality</cell><cell>JPEG</cell><cell>RTF</cell><cell>RTF +SA-DCT</cell><cell>AR-CNN</cell></row><row><cell>PSNR</cell><cell>10 20</cell><cell>26.62 28.80</cell><cell>27.66 29.84</cell><cell>27.71 29.87</cell><cell>27.71 29.87</cell></row><row><cell>SSIM</cell><cell>10 20</cell><cell>0.7904 0.8690</cell><cell>0.8177 0.8864</cell><cell>0.8186 0.8871</cell><cell>0.8192 0.8857</cell></row><row><cell>PSNR-B</cell><cell>10 20</cell><cell>23.54 25.62</cell><cell>26.93 28.80</cell><cell>26.99 28.80</cell><cell>27.04 29.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Experimental settings of "easy-hard transfer".</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>30.12 dB /0.8817 /26.86 dB SRCNN 32.60 dB /0.9301 /31.47 dB Deeper SRCNN 32.58 dB /0.9298 /31.52 dB AR-CNN 32.88 dB /0.9343 /32.22 dB</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Generally, the transfer learning method will train a base network first, and copy the learned parameters or features of several layers to the corresponding layers of a target network. These transferred layers can be left frozen or fine-tuned to the target dataset. The remaining layers are randomly initialized and trained to the target task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Random initialization on remaining layers are also applied similarly for tasks B 2 , and B 3 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use sub-images because we regard each sample as an image rather than a big patch.<ref type="bibr" target="#b4">5</ref> We use the unweighted structural similarity defined over fixed 8 × 8 windows as in<ref type="bibr" target="#b25">[26]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">More qualitative results are provided in the supplementary file.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We will share this dataset on our project page.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.00092</idno>
		<title level="m">Image super-resolution using deep convolutional networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hippocampal mediation of stimulus representation: A computational theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hippocampus</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="516" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital image processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural image denoising with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image deblocking via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="663" to="677" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A contrast enhancement framework with jpeg artifacts suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Blocking artifacts suppression in block-coded images using overcomplete wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="450" to="461" />
		</imprint>
		<respStmt>
			<orgName>TCSVT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>List</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lainema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karczewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive deblocking filter. TCSVT</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="614" to="619" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reduction of blocking effects in image coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Reeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230134" to="230134" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<title level="m">Live image quality assessment database release</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Postprocessing of low bit-rate block DCT coded images based on a fields of experts prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2743" to="2751" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive non-local means filter for image deblocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual quantities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image hallucination with feature enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2074" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-image superresolution: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Projection-based spatially adaptive reconstruction of blocktransform compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="896" to="908" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quality assessment of deblocked images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="98" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
