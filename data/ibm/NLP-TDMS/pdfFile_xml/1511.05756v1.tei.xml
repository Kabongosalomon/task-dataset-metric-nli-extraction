<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
							<email>hyeonwoonoh@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hongsuck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo</forename><surname>Bohyung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network-joint network with the CNN for ImageQA and the parameter prediction networkis trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the ultimate goals in computer vision is holistic scene understanding <ref type="bibr" target="#b29">[30]</ref>, which requires a system to capture various kinds of information such as objects, actions, events, scene, atmosphere, and their relations in many different levels of semantics. Although significant progress on various recognition tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> has been made in recent years, these works focus only on solving relatively simple recognition problems in controlled settings, where each dataset consists of concepts with similar level of understanding (e.g. object, scene, bird species, face identity, action, texture etc.). There has been less efforts made on solving various recognition problems simultaneously, which is more complex and realistic, even though this is a crucial step toward holistic scene understanding. <ref type="bibr">Figure 1</ref>. Sample images and questions in VQA dataset <ref type="bibr" target="#b0">[1]</ref>. Each question requires different type and/or level of understanding of the corresponding input image to find correct answers.</p><p>Image question answering (ImageQA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref> aims to solve the holistic scene understanding problem by proposing a task unifying various recognition problems. ImageQA is a task automatically answering the questions about an input image as illustrated in <ref type="figure">Figure 1</ref>. The critical challenge of this problem is that different questions require different types and levels of understanding of an image to find correct answers. For example, to answer the question like "how is the weather?" we need to perform classification on multiple choices related to weather, while we should decide between yes and no for the question like "is this picture taken during the day?" For this reason, not only the performance on a single recognition task but also the capability to select a proper task is important to solve ImageQA problem.</p><p>ImageQA problem has a short history in computer vision and machine learning community, but there already exist several approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>. Among these methods, simple deep learning based approaches that perform classification on a combination of features extracted from image and question currently demonstrate the state-of-the-art accuracy on public benchmarks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b15">16]</ref>; these approaches extract image features using a convolutional neural network (CNN), and use CNN or bag-of-words to obtain feature descriptors from question. They can be interpreted as a method that the answer is given by the co-occurrence of a particular combination of features extracted from an image and a question.</p><p>Contrary to the existing approaches, we define a different recognition task depending on a question. To realize this idea, we propose a deep CNN with a dynamic parameter layer whose weights are determined adaptively based on questions. We claim that a single deep CNN architecture can take care of various tasks by allowing adaptive weight assignment in the dynamic parameter layer. For the adaptive parameter prediction, we employ a parameter prediction network, which consists of gated recurrent units (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights for the dynamic parameter layer. The entire network including the CNN for ImageQA and the parameter prediction network is trained end-to-end through back-propagation, where its weights are initialized using pre-trained CNN and GRU. Our main contributions in this work are summarized below:</p><p>• We successfully adopt a deep CNN with a dynamic parameter layer for ImageQA, which is a fully-connected layer whose parameters are determined dynamically based on a given question.</p><p>• To predict a large number of weights in the dynamic parameter layer effectively and efficiently, we apply hashing trick <ref type="bibr" target="#b2">[3]</ref>, which reduces the number of parameters significantly with little impact on network capacity.</p><p>• We fine-tune GRU pre-trained on a large-scale text corpus <ref type="bibr" target="#b13">[14]</ref> to improve generalization performance of our network. Pre-training GRU on a large corpus is natural way to deal with a small number of training data, but no one has attempted it yet to our knowledge.</p><p>• This is the first work to report the results on all currently available benchmark datasets such as DAQUAR, COCO-QA and VQA. Our algorithm achieves the state-of-the-art performance on all the three datasets.</p><p>The rest of this paper is organized as follows. We first review related work in Section 2. Section 3 and 4 describe the overview of our algorithm and the architecture of our network, respectively. We discuss the detailed procedure to train the proposed network in Section 5. Experimental results are demonstrated in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There are several recent papers to address ImageQA <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>; the most of them are based on deep learning except <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Malinowski and Fritz [17]</ref> propose a Bayesian framework, which exploits recent advances in computer vision and natural language processing. Specifically, it employs semantic image segmentation and symbolic question reasoning to solve ImageQA problem. However, this method depends on a pre-defined set of predicates, which makes it difficult to represent complex models required to understand input images.</p><p>Deep learning based approaches demonstrate competitive performances in ImageQA <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref>. Most approaches based on deep learning commonly use CNNs to extract features from image while they use different strategies to handle question sentences. Some algorithms employ embedding of joint features based on image and question <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>. However, learning a softmax classifier on the simple joint features-concatenation of CNN-based image features and continuous bag-of-words representation of a question-performs better than LSTM-based embedding on COCO-QA <ref type="bibr" target="#b22">[23]</ref> dataset. Another line of research is to utilize CNNs for feature extraction from both image and question and combine the two features <ref type="bibr" target="#b15">[16]</ref>; this approach demonstrates impressive performance enhancement on DAQUAR <ref type="bibr" target="#b16">[17]</ref> dataset by allowing fine-tuning the whole parameters.</p><p>The prediction of the weight parameters in deep neural networks has been explored in <ref type="bibr" target="#b1">[2]</ref> in the context of zeroshot learning. To perform classification of unseen classes, it trains a multi-layer perceptron to predict a binary classifier for class-specific description in text. However, this method is not directly applicable to ImageQA since finding solutions based on the combination of question and answer is a more complex problem than the one discussed in <ref type="bibr" target="#b1">[2]</ref>, and ImageQA involves a significantly larger set of candidate answers, which requires much more parameters than the binary classification case. Recently, a parameter reduction technique based on a hashing trick is proposed by Chen et al. <ref type="bibr" target="#b2">[3]</ref> to fit a large neural network in a limited memory budget. However, applying this technique to the dynamic prediction of parameters in deep neural networks is not attempted yet to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm Overview</head><p>We briefly describe the motivation and formulation of our approach in this section. networks are designed to solve two or more tasks jointly by constructing multiple branches connected to a common CNN architecture. In this work, we hope to solve the heterogeneous recognition tasks using a single CNN by adapting the weights in the dynamic parameter layer. Since the task is defined by the question in ImageQA, the weights in the layer are determined depending on the question sentence. In addition, a hashing trick is employed to predict a large number of weights in the dynamic parameter layer and avoid parameter explosion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Problem Formulation</head><p>ImageQA systems predict the best answerâ given an image I and a question q. Conventional approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> typically construct a joint feature vector based on two inputs I and q and solve a classification problem for ImageQA using the following equation:</p><formula xml:id="formula_0">a = argmax a∈Ω p(a|I, q; θ)<label>(1)</label></formula><p>where Ω is a set of all possible answers and θ is a vector for the parameters in the network. On the contrary, we use the question to predict weights in the classifier and solve the problem. We find the solution bŷ</p><formula xml:id="formula_1">a = argmax a∈Ω p(a|I; θ s , θ d (q))<label>(2)</label></formula><p>where θ s and θ d (q) denote static and dynamic parameters, respectively. Note that the values of θ d (q) are determined by the question q. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the overall architecture of the proposed algorithm. The network is composed of two subnetworks: classification network and parameter prediction network. The classification network is a CNN. One of the fully-connected layers in the CNN is the dynamic parameter layer, and the weights in the layer are determined adaptively by the parameter prediction network. The parameter prediction network has GRU cells and a fully-connected layer. It takes a question as its input, and generates a realvalued vector, which corresponds to candidate weights for the dynamic parameter layer in the classification network. Given an image and a question, our algorithm estimates the weights in the dynamic parameter layer through hashing with the candidate weights obtained from the parameter prediction network. Then, it feeds the input image to the classification network to obtain the final answer. More details of the proposed network are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification Network</head><p>The classification network is constructed based on VGG 16-layer net <ref type="bibr" target="#b23">[24]</ref>, which is pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref>. We remove the last layer in the network and attach three fullyconnected layers. The second last fully-connected layer of the network is the dynamic parameter layer whose weights are determined by the parameter prediction network, and the last fully-connected layer is the classification layer whose output dimensionality is equal to the number of possible answers. The probability for each answer is computed by applying a softmax function to the output vector of the final layer.</p><p>We put the dynamic parameter layer in the second last fully-connected layer instead of the classification layer because it involves the smallest number of parameters. As the number of parameters in the classification layer increases in proportion to the number of possible answers, predicting the weights for the classification layer may not be a good op-tion to general ImageQA problems in terms of scalability. Our choice for the dynamic parameter layer can be interpreted as follows. By fixing the classification layer while adapting the immediately preceding layer, we obtain the task-independent semantic embedding of all possible answers and use the representation of an input embedded in the answer space to solve an ImageQA problem. Therefore, the relationships of the answers globally learned from all recognition tasks can help solve new ones involving unseen classes, especially in multiple choice questions. For example, when not the exact ground-truth word (e.g., kitten) but similar words (e.g., cat and kitty) are shown at training time, the network can still predict the close answers (e.g., kitten) based on the globally learned answer embedding. Even though we could also exploit the benefit of answer embedding based on the relations among answers to define a loss function, we leave it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Parameter Prediction Network</head><p>As mentioned earlier, our classification network has a dynamic parameter layer. That is, for an input vector of the dynamic parameter layer</p><formula xml:id="formula_2">f i = f i 1 , . . . , f i N T , its output vector denoted by f o = [f o 1 , . . . , f o M ] T is given by f o = W d (q)f i + b (3)</formula><p>where b denotes a bias and W d (q) ∈ R M ×N denotes the matrix constructed dynamically using the parameter prediction network given the input question. In other words, the weight matrix corresponding to the layer is parametrized by a function of the input question q.</p><p>The parameter prediction network is composed of GRU cells <ref type="bibr" target="#b3">[4]</ref> followed by a fully-connected layer, which produces the candidate weights to be used for the construction of weight matrix in the dynamic parameter layer within the classification network. GRU, which is similar to LSTM, is designed to model dependency in multiple time scales. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, such dependency is captured by adaptively updating its hidden states with gate units. However, contrary to LSTM, which maintains a separate memory cell explicitly, GRU directly updates its hidden states with a reset gate and an update gate. The detailed procedure of the update is described below.</p><p>Let w 1 , ..., w T be the words in a question q, where T is the number of words in the question. In each time step t, given the embedded vector x t for a word w t , the GRU encoder updates its hidden state at time t, denoted by h t , using the following equations: where r t and z t respectively denote the reset and update gates at time t, andh t is candidate activation at time t. In addition, indicates element-wise multiplication operator and σ(·) is a sigmoid function. Note that the coefficient matrices related to GRU such as W r , W z , W h , U r , U z , and U h are learned by our training algorithm. By applying this encoder to a question sentence through a series of GRU cells, we obtain the final embedding vector h T ∈ R L of the question sentence.</p><formula xml:id="formula_3">r t = σ(W r x t + U r h t−1 ) (4) z t = σ(W z x t + U z h t−1 ) (5) h t = tanh(W h x t + U h (r t h t−1 )) (6) h t = (1 − z t ) h t−1 + z t h t<label>(7)</label></formula><p>Once the question embedding is obtained by GRU, the candidate weight vector, p = [p 1 , . . . , p K ] T , is given by applying a fully-connected layer to the embedded question h T as p = W p h T</p><p>where p ∈ R K is the output of the parameter prediction network, and W p is the weight matrix of the fully-connected layer in the parameter prediction network. Note that even though we employ GRU for a parameter prediction network since the pre-trained network for sentence embeddingskip-thought vector model <ref type="bibr" target="#b13">[14]</ref>-is based on GRU, any form of neural networks, e.g., fully-connected and convolutional neural network, can be used to construct the parameter prediction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Parameter Hashing</head><p>The weights in the dynamic parameter layers are determined based on the learned model in the parameter prediction network given a question. The most straightforward approach to obtain the weights is to generate the whole matrix W d (q) using the parameter prediction network. However, the size of the matrix is very large, and the network may be overfitted easily given the limited number of training examples. In addition, since we need quadratically more parameters between GRU and the fully-connected layer in the parameter prediction network to increase the dimensionality of its output, it is not desirable to predict full weight matrix using the network. Therefore, it is preferable to construct W d (q) based on a small number of candidate weights using a hashing trick.</p><p>We employ the recently proposed random weight sharing technique based on hashing <ref type="bibr" target="#b2">[3]</ref> to construct the weights in the dynamic parameter layer. Specifically, a single param-eter in the candidate weight vector p is shared by multiple elements of W d (q), which is done by applying a predefined hash function that converts the 2D location in W d (q) to the 1D index in p. By this simple hashing trick, we can reduce the number of parameters in W d (q) while maintaining the accuracy of the network <ref type="bibr" target="#b2">[3]</ref>.</p><p>Let w d mn be the element at (m, n) in W d (q), which corresponds to the weight between m th output and n th input neuron. Denote by ψ(m, n) a hash function mapping a key (m, n) to a natural number in {1, . . . , K}, where K is the dimensionality of p. The final hash function is given by</p><formula xml:id="formula_5">w d mn = p ψ(m,n) · ξ(m, n)<label>(9)</label></formula><p>where ξ(m, n) : N × N → {+1, −1} is another hash function independent of ψ(m, n). This function is useful to remove the bias of hashed inner product <ref type="bibr" target="#b2">[3]</ref>. In our implementation of the hash function, we adopt an open-source implementation of xxHash 1 .</p><p>We believe that it is reasonable to reduce the number of free parameters based on the hashing technique as there are many redundant parameters in deep neural networks <ref type="bibr" target="#b6">[7]</ref> and the network can be parametrized using a smaller set of candidate weights. Instead of training a huge number of parameters without any constraint, it would be advantageous practically to allow multiple elements in the weight matrix to share the same value. It is also demonstrated that the number of free parameter can be reduced substantially with little loss of network performance <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training Algorithm</head><p>This section discusses the error back-propagation algorithm in the proposed network and introduces the techniques adopted to enhance performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Training by Error Back-Propagation</head><p>The proposed network is trained end-to-end to minimize the error between the ground-truths and the estimated answers. The error is back-propagated by chain rule through both the classification network and the parameter prediction network and they are jointly trained by a first-order optimization method.</p><p>Let L denote the loss function. The partial derivatives of L with respect to the k th element in the input and output of the dynamic parameter layer are given respectively by</p><formula xml:id="formula_6">δ i k ≡ ∂L ∂f i k and δ o k ≡ ∂L ∂f o k .<label>(10)</label></formula><p>The two derivatives have the following relation:</p><formula xml:id="formula_7">δ i n = M m=1 w d mn δ o m<label>(11)</label></formula><p>1 https://code.google.com/p/xxhash/ Likewise, the derivative with respect to the assigned weights in the dynamic parameter layer is given by</p><formula xml:id="formula_8">∂L ∂w d mn = f i n δ o m .<label>(12)</label></formula><p>As a single output value of the parameter prediction network is shared by multiple connections in the dynamic parameter layer, the derivatives with respect to all shared weights need to be accumulated to compute the derivative with respect to an element in the output of the parameter prediction network as follows:</p><formula xml:id="formula_9">∂L ∂p k = M m=1 N n=1 ∂L ∂w d mn ∂w d mn ∂p k = M m=1 N n=1 ∂L ∂w d mn ξ(m, n)I[ψ(m, n) = k],<label>(13)</label></formula><p>where I[·] denotes the indicator function. The gradients of all the preceding layers in the classification and parameter prediction networks are computed by the standard backpropagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Using Pre-trained GRU</head><p>Although encoders based on recurrent neural networks (RNNs) such as LSTM <ref type="bibr" target="#b10">[11]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref> demonstrate impressive performance on sentence embedding <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, their benefits in the ImageQA task are marginal in comparison to bag-of-words model <ref type="bibr" target="#b22">[23]</ref>. One of the reasons for this fact is the lack of language data in ImageQA dataset. Contrary to the tasks that have large-scale training corpora, even the largest ImageQA dataset contains relatively small amount of language data; for example, <ref type="bibr" target="#b0">[1]</ref> contains 750K questions in total. Note that the model in <ref type="bibr" target="#b24">[25]</ref> is trained using a corpus with more than 12M sentences.</p><p>To deal with the deficiency of linguistic information in ImageQA problem, we transfer the information acquired from a large language corpus by fine-tuning the pre-trained embedding network. We initialize the GRU with the skipthought vector model trained on a book-collection corpus containing more than 74M sentences <ref type="bibr" target="#b13">[14]</ref>. Note that the GRU of the skip-thought vector model is trained in an unsupervised manner by predicting the surrounding sentences from the embedded sentences. As this task requires to understand context, the pre-trained model produces a generic sentence embedding, which is difficult to be trained with a limited number of training examples. By fine-tuning our GRU initialized with a generic sentence embedding model for ImageQA, we obtain the representations for questions that are generalized better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fine-tuning CNN</head><p>It is very common to transfer CNNs for new tasks in classification problems, but it is not trivial to fine-tune the CNN in our problem. We observe that the gradients below the dynamic parameter layer in the CNN are noisy since the weights are predicted by the parameter prediction network. Hence, a straightforward approach to fine-tune the CNN typically fails to improve performance, and we employ a slightly different technique for CNN fine-tuning to sidestep the observed problem. We update the parameters of the network using new datasets except the part transferred from VGG 16-layer net at the beginning, and start to update the weights in the subnetwork if the validation accuracy is saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Training Details</head><p>Before training, question sentences are normalized to lower cases and preprocessed by a simple tokenization technique as in <ref type="bibr" target="#b28">[29]</ref>. We normalize the answers to lower cases and regard a whole answer in a single or multiple words as a separate class.</p><p>The network is trained end-to-end by back-propagation. Adam <ref type="bibr" target="#b12">[13]</ref> is used for optimization with initial learning rate 0.01. We clip the gradient to 0.1 to handle the gradient explosion from the recurrent structure of GRU <ref type="bibr" target="#b21">[22]</ref>. Training is terminated when there is no progress on validation accuracy for 5 epochs.</p><p>Optimizing the dynamic parameter layer is not straightforward since the distribution of the outputs in the dynamic parameter layer is likely to change significantly in each batch. Therefore, we apply batch-normalization <ref type="bibr" target="#b11">[12]</ref> to the output activations of the layer to alleviate this problem. In addition, we observe that GRU tends to converge fast and overfit data easily if training continues without any restriction. We stop fine-tuning GRU when the network start to overfit and continue to train the other parts of the network; this strategy improves performance in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We now describe the details of our implementation and evaluate the proposed method in various aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets</head><p>We evaluate the proposed network on all public Im-ageQA benchmark datasets such as DAQUAR <ref type="bibr" target="#b16">[17]</ref>, COCO-QA <ref type="bibr" target="#b22">[23]</ref> and VQA <ref type="bibr" target="#b0">[1]</ref>. They collected question-answer pairs from existing image datasets and most of the answers are single words or short phrases.</p><p>DAQUAR is based on NYUDv2 <ref type="bibr" target="#b19">[20]</ref> dataset, which is originally designed for indoor segmentation using RGBD images. DAQUAR provides two benchmarks, which are distinguished by the number of classes and the amount of data; DAQUAR-all consists of 6,795 and 5,673 questions for training and testing respectively, and includes 894 categories in answer. DAQUAR-reduced includes only 37 answer categories for 3,876 training and 297 testing questions. Some questions in this dataset are associated with a set of multiple answers instead of a single one.</p><p>The questions in COCO-QA are automatically generated from the image descriptions in MS COCO dataset <ref type="bibr" target="#b14">[15]</ref> using the constituency parser with simple question-answer generation rules. The questions in this dataset are typically long and explicitly classified into 4 types depending on the generation rules: object questions, number questions, color questions and location questions. All answers are with one-words and there are 78,736 questions for training and 38,948 questions for testing.</p><p>Similar to COCO-QA, VQA is also constructed on MS COCO <ref type="bibr" target="#b14">[15]</ref> but each question is associated with multiple answers annotated by different people. This dataset contains the largest number of questions: 248,349 for training, 121,512 for validation, and 244,302 for testing, where the testing data is splited into test-dev, test-standard, testchallenge and test-reserve as in <ref type="bibr" target="#b14">[15]</ref>. Each question is provided with 10 answers to take the consensus of annotators into account. About 90% of answers have single words and 98% of answers do not exceed three words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Metrics</head><p>DAQUAR and COCO-QA employ both classification accuracy and its relaxed version based on word similarity, WUPS <ref type="bibr" target="#b16">[17]</ref>. It uses thresholded Wu-Palmer similarity <ref type="bibr" target="#b27">[28]</ref> based on WordNet <ref type="bibr" target="#b8">[9]</ref> taxonomy to compute the similarity between words. For predicted answer set A i and groundtruth answer set T i of the i th example, WUPS is given by <ref type="bibr" target="#b13">(14)</ref> where µ (·, ·) denotes the thresholded Wu-Palmer similarity between prediction and ground-truth. We use two threshold values (0.9 and 0.0) in our evaluation.</p><formula xml:id="formula_10">WUPS = 1 N N i=1 min a∈A i max t∈T i µ (a, t), t∈T i max a∈A i µ (a, t) ,</formula><p>VQA dataset provides open-ended task and multiplechoice task for evaluation. For open-ended task, the answer can be any word or phrase while an answer should be chosen out of 18 candidate answers in the multiple-choice task. In both cases, answers are evaluated by accuracy reflecting human consensus. For predicted answer a i and target answer set T i of the i th example, the accuracy is given by</p><formula xml:id="formula_11">Acc VQA = 1 N N i=1 min t∈T i I [a i = t] 3 , 1<label>(15)</label></formula><p>where I [·] denotes an indicator function. In other words, a predicted answer is regarded as a correct one if at least three annotators agree, and the score depends on the number of agreements if the predicted answer is not correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>We test three independent datasets, VQA, COCO-QA, and DAQUAR, and first present the results for VQA dataset in <ref type="table" target="#tab_0">Table 1</ref>. The proposed Dynamic Parameter Prediction network (DPPnet) outperforms all existing methods nontrivially. We performed controlled experiments to analyze the contribution of individual components in the proposed algorithm-dynamic parameter prediction, use of pre-trained GRU and CNN fine-tuning, and trained 3 additional models, CONCAT, RAND-GRU, and CNN-FIXED. CNN-FIXED is useful to see the impact of CNN fine-tuning since it is identical to DPPnet except that the weights in CNN are fixed. RAND-GRU is the model without GRU pre-training, where the weights of GRU and word embedding model are initialized randomly. It does not fine-tune CNN either. CONCAT is the most basic model, which predicts answers using the two fully-connected layers for a combination of CNN and GRU features. Obviously, it does not employ any of new components such as parameter prediction, pre-trained GRU and CNN fine-tuning.</p><p>The results of the controlled experiment are also illustrated in <ref type="table" target="#tab_0">Table 1</ref>. CONCAT already outperforms LSTM Q+I by integrating GRU instead of LSTM <ref type="bibr" target="#b3">[4]</ref> and batch normalization. RAND-GRU achieves better accuracy by employing dynamic parameter prediction additionally. It is interesting that most of the improvement comes from yes/no questions, which may involve various kinds of tasks since it is easy to ask many different aspects in an input image for binary classification. CNN-FIXED improves accuracy further by adding GRU pre-training, and our final model DPPnet achieves the state-of-the-art performance on VQA dataset with large margins as illustrated in <ref type="table" target="#tab_0">Table 1 and 2.  Table 3</ref>, 4, and 5 illustrate the results by all algorithms including ours that have reported performance on COCO-QA, DAQUAR-reduced, DAQUAR-all datasets. The proposed  algorithm outperforms all existing approaches consistently in all benchmarks. In <ref type="table">Table 4</ref> and 5, single answer and multiple answers denote the two subsets of questions divided by the number of ground-truth answers. Also, the numbers (0.9 and 0.0) in the second rows are WUPS thresholds.</p><p>To understand how the parameter prediction network understand questions, we present several representative questions before and after fine-tuning GRU in a descending order based on their cosine similarities to the query question in <ref type="table">Table 6</ref>. The retrieved sentences are frequently determined by common subjective or objective words before fine-tuning while they rely more on the tasks to be solved after fine-tuning.</p><p>The qualitative results of the proposed algorithm are presented in <ref type="figure">Figure 4</ref>. In general, the proposed network is successful to handle various types of questions that need different levels of semantic understanding. <ref type="figure">Figure 4</ref>(a) shows that the network is able to adapt recognition tasks depending on questions. However, it often fails in the questions asking the number of occurrences since these questions involve the difficult tasks (e.g., object detection) to learn only with image level annotations. On the other hand, the proposed network is effective to find the answers for the same question on different images fairly well as illustrated in <ref type="figure">Figure 4</ref>(b). Refer to our project website 2 for more comprehensive qualitative results. <ref type="table">Table 6</ref>. Retrieved sentences before and after fine-tuning GRU</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query question</head><p>What body part has most recently contacted the ball? Is the person feeding the birds?</p><p>Before fine-tuning What shape is the ball? Is he feeding the birds? What colors are the ball?</p><p>Is the reptile fighting the birds? What team has the ball?</p><p>Does the elephant want to play with the birds? How many times has the girl hit the ball?</p><p>What is the fence made of behind the birds? What number is on the women's Jersey closest to the ball?</p><p>Where are the majority of the birds? What is unusual about the ball?</p><p>What colors are the birds? What is the speed of the ball?</p><p>Is this man feeding the pigeons?</p><p>After fine-tuning What body part is the boy holding the bear by? Is he feeding the birds? What body part is on the right side of this picture?</p><p>Is the person feeding the sheep? What human body part is on the table?</p><p>Is the man feeding the pigeons? What body parts appear to be touching?</p><p>Is she feeding the pigeons? What partial body parts are in the foreground?</p><p>Is that the zookeeper feeding the giraffes? What part of the body does the woman on the left have on the ramp?</p><p>Is the reptile fighting the birds? Name a body part that would not be visible if the woman's mouth was closed? Does the elephant want to play with the birds?  <ref type="figure">Figure 4</ref>. Sample images and questions in VQA dataset <ref type="bibr" target="#b0">[1]</ref>. Each question requires a different type and/or level of understanding of the corresponding input image to find correct answer. Answers in blue are correct while answers in red are incorrect. For the incorrect answers, ground-truth answers are provided within the parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed a novel architecture for image question answering based on two subnetworks-classification network and parameter prediction network. The classification network has a dynamic parameter layer, which enables the classification network to adaptively determine its weights through the parameter prediction network. While predicting all entries of the weight matrix is infeasible due to its large dimensionality, we relieved this limitation using parameter hashing and weight sharing. The effectiveness of the proposed architecture is supported by experimental results showing the state-of-the-art performances on three different datasets. Note that the proposed method achieved outstanding performance even without more complex recognition processes such as referencing objects. We believe that the proposed algorithm can be extended further by integrating attention model <ref type="bibr" target="#b28">[29]</ref> to solve such difficult problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of the proposed Dynamic Parameter Prediction network (DPPnet), which is composed of the classification network and the parameter prediction network. The weights in the dynamic parameter layer are mapped by a hashing trick from the candidate weights obtained from the parameter prediction network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of GRU and LSTM. Contrary to LSTM that contains memory cell explicitly, GRU updates the hidden state directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Result of the proposed algorithm on multiple questions for a single image (b) Results of the proposed algorithm on a single common question for multiple images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results on VQA test-dev in terms of AccVQA Num Others All Y/N Num Others Question [1] 48.09 75.66 36.70 27.14 53.68 75.71 37.05 38.64 Image [1] 28.13 64.01 00.42 03.77 30.53 69.87 00.45 03.76 Q+I [1] 52.64 75.55 33.67 37.37 58.97 75.59 34.35 50.33 LSTM Q [1] 48.76 78.20 35.68 26.59 54.75 78.22 36.82 38.78 LSTM Q+I [1] 53.74 78.94 35.24 36.42 57.17 78.95 35.80 43.41 CONCAT 54.70 77.09 36.62 39.67 59.92 77.10 37.48 50.31 RAND-GRU 55.46 79.58 36.20 39.23 61.18 79.64 38.07 50.63 CNN-FIXED 56.74 80.48 37.20 40.90 61.95 80.56 38.32 51.40 DPPnet 57.22 80.71 37.24 41.69 62.48 80.79 38.94 52.16</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell></cell><cell cols="2">Multiple-Choice</cell></row><row><cell cols="8">All Y/N Table 2. Evaluation results on VQA test-standard</cell></row><row><cell></cell><cell></cell><cell cols="2">Open-Ended</cell><cell></cell><cell></cell><cell cols="2">Multiple-Choice</cell></row><row><cell></cell><cell cols="8">All Y/N Num Others All Y/N Num Others</cell></row><row><cell cols="5">Human [1] 83.30 95.77 83.39 72.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">LSTM Q+I [1] 54.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DPPnet</cell><cell cols="8">57.36 80.28 36.92 42.24 62.69 80.35 38.79 52.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results on COCO-QA</figDesc><table><row><cell></cell><cell>Acc</cell><cell></cell><cell cols="2">WUPS 0.9</cell><cell cols="2">WUPS 0.0</cell></row><row><cell>IMG+BOW [23]</cell><cell>55.92</cell><cell></cell><cell cols="2">66.78</cell><cell>88.99</cell></row><row><cell>2VIS+BLSTM [23]</cell><cell>55.09</cell><cell></cell><cell cols="2">65.34</cell><cell>88.64</cell></row><row><cell>Ensemble [23]</cell><cell>57.84</cell><cell></cell><cell cols="2">67.90</cell><cell>89.52</cell></row><row><cell>ConvQA [16]</cell><cell>54.95</cell><cell></cell><cell cols="2">65.36</cell><cell>88.58</cell></row><row><cell>DPPnet</cell><cell>61.19</cell><cell></cell><cell cols="2">70.84</cell><cell>90.61</cell></row><row><cell cols="6">Table 4. Evaluation results on DAQUAR reduced</cell></row><row><cell></cell><cell cols="3">Single answer</cell><cell cols="3">Multiple answers</cell></row><row><cell></cell><cell>Acc</cell><cell>0.9</cell><cell>0.0</cell><cell>Acc</cell><cell>0.9</cell><cell>0.0</cell></row><row><cell>Multiworld [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">12.73 18.10 51.47</cell></row><row><cell cols="7">Askneuron [18] 34.68 40.76 79.54 29.27 36.50 79.47</cell></row><row><cell cols="4">IMG+BOW [23] 34.17 44.99 81.48</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">2VIS+BLSTM [23] 35.78 46.83 82.15</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ensemble [23]</cell><cell cols="3">36.94 48.15 82.68</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ConvQA [16]</cell><cell cols="6">39.66 44.86 83.06 38.72 44.19 79.52</cell></row><row><cell>DPPnet</cell><cell cols="6">44.48 49.56 83.95 44.44 49.06 82.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Evaluation results on DAQUAR all ] 19.43 25.28 62.00 17.49 23.28 57.76 ConvQA [16] 23.40 29.59 62.95 20.69 25.89 55.48 DPPnet 28.98 34.80 67.81 25.60 31.03 60.77</figDesc><table><row><cell></cell><cell cols="3">Single answer</cell><cell cols="3">Multiple answers</cell></row><row><cell></cell><cell>Acc</cell><cell>0.9</cell><cell>0.0</cell><cell>Acc</cell><cell>0.9</cell><cell>0.0</cell></row><row><cell>Human [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">50.20 50.82 67.27</cell></row><row><cell>Multiworld [17]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">07.86 11.86 38.79</cell></row><row><cell>Askneuron [18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://cvlab.postech.ac.kr/research/dppnet/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeCAF: a deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wordnet: An electronic database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00333</idno>
		<title level="m">Learning to answer questions from image using convolutional neural network</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
