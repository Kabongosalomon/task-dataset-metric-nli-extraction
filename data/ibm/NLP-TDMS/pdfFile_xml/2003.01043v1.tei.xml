<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GATED MECHANISM FOR ATTENTION BASED MULTIMODAL SENTIMENT ANALYSIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jithendra</forename><surname>Vepa</surname></persName>
						</author>
						<title level="a" type="main">GATED MECHANISM FOR ATTENTION BASED MULTIMODAL SENTIMENT ANALYSIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-sentiment analysis</term>
					<term>multimodal fusion</term>
					<term>gated mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal sentiment analysis has recently gained popularity because of its relevance to social media posts, customer service calls and video blogs. In this paper, we address three aspects of multimodal sentiment analysis; 1. Cross modal interaction learning, i.e. how multiple modalities contribute to the sentiment, 2. Learning long-term dependencies in multimodal interactions and 3. Fusion of unimodal and cross modal cues. Out of these three, we find that learning cross modal interactions is beneficial for this problem. We perform experiments on two benchmark datasets, CMU Multimodal Opinion level Sentiment Intensity (CMU-MOSI) and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Our approach on both these tasks yields accuracies of 83.9% and 81.1% respectively, which is 1.6% and 1.34% absolute improvement over current state-ofthe-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Sentiment analysis has been one of the widely studied problems in spoken language understanding that aims to determine the opinion of the speaker towards a product, topic or event. With the proliferation of social media platforms such as Facebook, Whatsapp, Instagram and YouTube, huge volume of data is being generated in the forms of podcasts, vlogs, interviews, commentary etc. Multimodal data offer parallel acoustic (vocal expressions like intensity, pitch) and visual cues (facial expressions, gestures) along with the textual information (spoken words), which in particular, provides advanced understanding of affective behavior.</p><p>Several approaches have been proposed for multimodal sentiment analysis that attempt to effectively leverage multimodal information. These are categorised into three types, 1. Methods that learn the modalities independently and fuse the output of modality specific representations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, 2. Methods that jointly learn the interactions between two or three modalities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, and 3. Methods that explicitly learn contributions from these unimodal and cross modal cues, typically using attention based techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Most of the existing approaches propose either fusion at different granularities <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> or use a cross interaction block that couple the features from different modalities <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Combining features from different modalities is necessary as they offer parallel information for same source and help in disambiguation of affective behavior. For example, while uttering sarcastic statements, the speaker shows a distinct intonation which aids in determining the correct sentiment of the speaker. It is imperative that all modalities in multimodal sentiment analysis do not contribute equally, rather act as cues to reinforce or rectify the information from the other modalities. This is more evident in the case of imperfect modalities, for example; errors in automatic speech recognition might corrupt the textual information, or poor recording distort the acoustic information, or improper lighting might negatively impact visual features. Therefore, to learn better cross modal information, we introduce novel conditional gating mechanism to modulate the information during cross interactions. Proposed gating mechanism selectively learns the relative importance of different modalities based on the linguistic information, tone of the speaker and facial expressions of an utterance. Furthermore, to capture long term dependencies across the utterances in the video, we apply a self attention layer on unimodal contextual representations. The major advantage of self attention is that it induces direct interaction between any two utterances and hence offers unrestricted information flow in the network. Finally, we feed the self attended unimodal contextual representations and the gated cross interaction representations to a recurrent layer to obtain deep multimodal contextual feature vectors for each utterance.</p><p>The main contributions of our proposed approach are: 1) Learnable gating mechanism to control information flow during cross interaction, 2) Self attended contextual representation to capture long term dependencies, and 3) Recurrent layer based fusion of self and gated cross fusion feature vectors to obtain modality specific deep multimodal feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED APPROACH</head><p>In our proposed model, we aim to learn the interaction between different modalities controlled by learnable gates. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of the system outlining the main components in the model: contextual utterance representation, self attention, cross attention, gating mechanism c 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. for cross interaction and, deep multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Contextual Utterance Representation</head><p>We feed a sequence of utterance level features for each modality to a separate Bi-GRU <ref type="bibr" target="#b10">[11]</ref> and obtain modality specific contextual utterance representation, H. Formally, contextual utterance representations (H T ∈R u×d ) for a sequence of utterances (U 1 , U 2 , ..., U u ) for a Text modality can be defined as:</p><formula xml:id="formula_0">H T = Bi-GRU (U 1 , U 2 , ..., U u )<label>(1)</label></formula><p>Subscript T denotes Text modality, A and V represent Audio and Video modalities respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self Attention</head><p>In order to capture long term dependencies, a bilinear attention <ref type="bibr" target="#b11">[12]</ref> based self matching layer on contextual utterance representations is employed. Since we have sequences of up to 100 utterances in a video, self attention allows us to capture the long context. For a Text modality, self attention can be represented as: </p><formula xml:id="formula_1">M T = H T W H T T , M T ∈ R u×u (2a) A T (i, ) = sof tmax(M Ti, ) (2b) S T = A T .H T , S T ∈ R u×d<label>(2c)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cross Attention</head><p>Multimodal sentiment analysis provides an opportunity to learn interactions between different modalities. Similar to approaches mentioned for intermodal attention in Ghosal et al <ref type="bibr" target="#b9">[10]</ref>, we propose a method to learn cross-interaction vectors. For a pair of Text (H T ) and Video (H V ) modalities, co-attention matrix (M T V ∈R u×u ) can be defined as:</p><formula xml:id="formula_2">M T V = H T W H T V ; W ∈ R d×d<label>(3)</label></formula><p>Cross attentive representations of Text (C V T ∈R u×d ) and Video (C T V ∈R u×d ) can be represented as:</p><formula xml:id="formula_3">A T V (i :) = sof tmax(M T Vi: ) (4a) A V T (: j) = sof tmax(M T V:j ) (4b) C V T = A V T .H T , C T V = A T V .H V (4c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Gating Mechanism for Cross Interaction</head><p>As much as there is an opportunity to leverage cross modal interactions, it brings in challenges of fusing imperfect modalities. To overcome the noise present in individual modalities, we propose a gating mechanism to selectively learn the cross fused vector <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The gated cross fused vector (F P Q ∈R u×d ) for a pair of Text-Video modalities can be obtained as:</p><formula xml:id="formula_4">F V T = f usion(C V T , H T ) (5a) F T V = f usion(C T V , H V )<label>(5b)</label></formula><p>We define fusion kernel f usion(·, ·) to be gated combination of cross interaction and contextual representation. Cross interaction, X(P, Q), is a non-linear transformation on cross attended vector (P ) and contextual representation (Q). Gating  <ref type="figure">Q)</ref>, modulates the information to be passed from cross interaction to next layer.</p><formula xml:id="formula_5">X(P, Q) = tanh(W F .[P, Q, P -Q, P •Q] + b F ) (6a) G(P, Q) = σ(W T G .[P, Q, P -Q, P •Q] + b G ) (6b) F P Q = G(P, Q).X(P, Q) + (1 − G(P, Q)).Q (6c) where, W F , b F , W T G , b G are trainable parameters and • rep- resents element wise product.</formula><p>If features from participating modalities are complementary, gating function favours cross interaction and hence would have higher value. On the other hand, if the features from participating modalities is not rich enough or unimodal representation is self-sufficient, the gating function would favor contextual representation and hence would have lower value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Deep Multimodal Fusion</head><p>To aggregate the information from the self and gated cross interactions, we use a Bi-GRU layer to learn deep multimodal feature vector for each modality.</p><formula xml:id="formula_6">Deep T = Bi-GRU (S T , F V T , F AT )<label>(7)</label></formula><p>Finally, deep multimodal feature vector for each modality for an utterance is concatenated and fed to the prediction layer containing a fully connected layer followed by softmax layer for final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We evaluated our system on two standard multimodal sentiment analysis datasets from CMU multimodal SDK 1 <ref type="bibr" target="#b5">[6]</ref>, 1) CMU-MOSI: CMU Multimodal Opinion level Sentiment Intensity <ref type="bibr" target="#b14">[15]</ref> and; 2) CMU-MOSEI: CMU Multimodal Opinion Sentiment and Emotion Intensity <ref type="bibr" target="#b6">[7]</ref>. To compare with the existing approaches, we report results on the binary sentiment classification setup, where values ≥ 0 signify positive sentiments and values &lt; 0 signify negative sentiments. There are 1284, 229 and 686 utterances in the training, validation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>In our experiments, we used same features mentioned in Ghosal et al <ref type="bibr" target="#b9">[10]</ref>. Specifically, for CMU-MOSEI dataset, we used Glove embeddings for word features, Facets 2 for visual features and CovaRep <ref type="bibr" target="#b15">[16]</ref> for acoustic features. For MOSI dataset, we used output of a CNN network for utterance level features, 3D CNN features for visual and openSMILE <ref type="bibr" target="#b16">[17]</ref> for acoustic features.</p><p>We trained Bi-GRUs with hidden size of 100 for CMU-MOSI dataset and 200 for CMU-MOSEI dataset, also used a dropout of 0.4 for regularization and ReLU activation <ref type="bibr" target="#b17">[18]</ref> in dense layers. We used Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with a learning rate 0.0005 and a batch size 16 for CMU-MOSI and 32 for CMU-MOSEI dataset and, finally train the network for 75 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Baselines and Ablation Study</head><p>We carried out several experiments to analyze the contribution of the proposed approach <ref type="table" target="#tab_1">(Table 1)</ref>. We frame a unimodal (B1) and a bimodal baseline (B3) to compare the impacts of self attention (B2) and gating mechanism (B4). Further we also evaluate the model with deep multimodal fusion (B6). We see that by using self attention, the performance of model improves by 0.54% on MOSI and MOSEI corpora. Gating mechanism improves the accuracy by absolute 1% on MOSI while multimodal fusion adds additional 0.54% and 0.26% accuracy on two corpora.</p><p>The gains in performance over these baselines clearly validates our main hypotheses that attention focused gating selectively learns the noise-robust interactions between different modalities and self attention is required to exploit long term context dependencies present in the video. Finally, deep multimodal feature representations learned using self attended representations and gated cross interactions provides additional gains in the accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU-MOSI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU-MOSEI Approach</head><p>Accuracy F1-Score Approach Accuracy F1-Score Zadeh et al <ref type="bibr" target="#b2">[3]</ref> 77.  <ref type="bibr" target="#b4">[5]</ref> 76.5 73. <ref type="bibr" target="#b3">4</ref> Zadeh et al <ref type="bibr" target="#b6">[7]</ref> 76.9 77.0 Georgiou et al <ref type="bibr" target="#b8">[9]</ref> 76.9 76.9 Poria et al <ref type="bibr" target="#b1">[2]</ref> 77.64 -Ghosal et al <ref type="bibr" target="#b9">[10]</ref> 82.31 80.69 Ghosal et al <ref type="bibr" target="#b9">[10]</ref> 79.80 -Sun et al <ref type="bibr" target="#b3">[4]</ref> 80   <ref type="bibr" target="#b6">[7]</ref>, ( ¦ ) results are obtained on CMU-MOSEI dataset after excluding the utterances with sentiment score of 0. We mention the results of proposed model with this setup in the parenthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance Gold Label</head><p>Predicted Label Remark I really really loved it Pos.</p><p>Pos.</p><p>ST u = 0.91, which justifies that text is self sufficient cross-interaction score for this utterance is 0.43 i was just thinking about um how its the performances in it were sort of over overlooked at the academy awards</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pos.</head><p>Pos.</p><p>Text modality suggests it to be a negative sentiment. Contribution of T-A and T-V cross-interaction is less (0.12 and 0.05). SV u = 0.75 and SA u = 0.67 suggests that V, A modality drives the prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Benchmarking</head><p>To comprehensively compare our method, we list several baselines for multimodal sentiment analysis. Tensor fusion network <ref type="bibr" target="#b2">[3]</ref> uses a 3-fold cartesian product on unimodal embeddings; Context-dependent sentiment analysis <ref type="bibr" target="#b1">[2]</ref> learns context dependent multimodal feature representations; Memory fusion network (MFN) <ref type="bibr" target="#b7">[8]</ref> proposes a 3-step architecture for multi-view sequential learning using attention network and gated memory; Graph-MFN <ref type="bibr" target="#b6">[7]</ref> replaces attention network in MFN with dynamic fusion graph to learn modal dynamics; Gated multimodal-embedding with temporal attention <ref type="bibr" target="#b4">[5]</ref> performs word level modality fusion using gating; Hierarchical fusion [9] performs 3-step fusion at word, sentence and high level for sentiment classification; Deep canonical correlation analysis (DCCA) based multi-modal embeddings <ref type="bibr" target="#b3">[4]</ref>; and Contextual inter-modal attention based network <ref type="bibr" target="#b9">[10]</ref> that proposes a multi-modal attention framework to learn joint-association between multiple modalities &amp; utterances.</p><p>In <ref type="table" target="#tab_4">Table 2</ref>, we present the comparison of our proposed method with other state-of-the-art approaches. Our proposed method outperforms the state-of the-art by 1.6% (absolute) points for CMU-MOSI corpus and 1.34% points for CMU-MOSEI corpus. Qualitative analysis of our results is presented in <ref type="table" target="#tab_5">Table 3</ref> with a few examples. The analysis demonstrates the effectiveness of the model in selectively attending to the relevant modalities by adjusting the modality specific scores (self attention) as well as cross interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we propose an approach to improve the multimodal sentiment analysis using self attention to capture long term context and gating mechanism to selectively learn cross attended features. The gating function emphasize on cross interactions when unimodal information is insufficient to decide the sentiment while it assigns lower weightage to cross modal information when unimodal information is sufficient to predict the sentiment. Evaluations on two well known benchmark datasets (CMU-MOSI and CMU-MOSEI) show that our proposed method is significantly better than the state-of-theart. In future, we will extend the proposed techniques for real world data, e.g. call center customer conversations, where noise in both Text and Audio modalities is high due to poor audio quality, thus resulting in lower speech recognition accuracies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Architectural diagram of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>https://github.com/A2Zadeh/CMU-MultimodalSDK and the test set for CMU-MOSI dataset while CMU-MOSEI has 16216, 1835 &amp; 4625 utterances in training, validation &amp; test set respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>maybe only 5</head><label>5</label><figDesc>jokes made me laugh Neg. Neg. All three modalities are correlated in this utterance of the video, evident by cross-interaction contributions of T-A, A-V and T-V to be 0.78, 0.69 and 0.83 respectively. oh oh my gosh i was blown away Pos. Pos. Audio (SA u = 0.62) and video (SV u = 0.49) contributes in all crossinteractions (0.74) to reinforce their learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of performance of each step in the proposed model. Accuracy values are mentioned in the table function, G(P,</figDesc><table><row><cell>Sl. No.</cell><cell>Model</cell><cell>CMU-MOSI</cell><cell>CMU-MOSEI</cell></row><row><cell>B1</cell><cell>Contextual Unimodal (Unimodal Baseline)</cell><cell>80.57</cell><cell>78.58</cell></row><row><cell>B2</cell><cell>B1 + Self Attention</cell><cell>81.11</cell><cell>79.12</cell></row><row><cell>B3</cell><cell>Cross Interaction w/o gating (Bimodal Baseline)</cell><cell>81.91</cell><cell>80.00</cell></row><row><cell>B4</cell><cell>Cross Interaction w/ gating</cell><cell>82.91</cell><cell>80.59</cell></row><row><cell>B5</cell><cell>B2 + B4 w/o deep multimodal fusion</cell><cell>83.37</cell><cell>80.88</cell></row><row><cell>B6</cell><cell>Proposed: B2 + B4 w/ multimodal fusion</cell><cell>83.91</cell><cell>81.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparative results on CMU-MOSI and CMU-MOSEI multimodal sentiment analysis. ( * ) results are taken from Zadeh et al</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Qualitative analysis of the proposed model. T, A, V refers to text, audio and video respectively. S Mu denotes self attention score for utterance u in modality M. Cross-interaction score are average values of gate G(P, Q) for a pair of modalities P, Q.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://https://pair-code.github.io/facets/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis using deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Bucy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep hierarchical fusion with application in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1646" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual inter-modal attention for multi-modal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3454" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ruminating reader: Reasoning with gated multi-hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Reading for Question An-swering@ACL</title>
		<meeting>the Workshop on Machine Reading for Question An-swering@ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno>abs/1606.06259</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">COVAREP -A collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recent developments in opensmile, the munich opensource multimedia feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="835" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
