<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-21">21 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of MEIE</orgName>
								<orgName type="institution">China University of Mining and Technology (Beijing)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-21">21 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>FGVC</term>
					<term>Classification</term>
					<term>Attention</term>
					<term>Location</term>
					<term>Scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fan Zhang [0000−0003−2504−4651] , Meng Li [0000−0002−1543−5029] , Guisheng Zhai [0000−0001−9901−010X] , and Yizhao Liu [0000−0002−9549−7317]</p><p>Abstract. ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most authoritative academic competitions in the field of Computer Vision (CV) in recent years. But applying ILSVRC's annual champion directly to fine-grained visual categorization (FGVC) tasks does not achieve good performance. To FGVC tasks, the small interclass variations and the large intra-class variations make it a challenging problem. Our attention object location module (AOLM) can predict the position of the object and attention part proposal module (APPM) can propose informative part regions without the need of boundingbox or part annotations. The obtained object images not only contain almost the entire structure of the object, but also contains more details, part images have many different scales and more fine-grained features, and the raw images contain the complete object. The three kinds of training images are supervised by our multi-branch network. Therefore, our multi-branch and multi-scale learning network(MMAL-Net) has good classification ability and robustness for images of different scales. Our approach can be trained end-to-end, while provides short inference time. Through the comprehensive experiments demonstrate that our approach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft and Stanford Cars datasets. Our code will be available at https://github.com/ZF1044404254/MMAL-Net</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How to tell a dog's breed? This is a frequently asked question because dogs have similar characteristics. The FGVC direction of CV research focuses on such issues, and it is also called sub-category recognition. In recent years, it is a very popular research topic in CV, pattern recognition and other fields. It's purpose is to make a more detailed sub-class division for coarse-grained large categories (e.g. classifying bird species <ref type="bibr" target="#b0">[1]</ref>, aircraft models <ref type="bibr" target="#b1">[2]</ref>, car models <ref type="bibr" target="#b2">[3]</ref>, etc.).</p><p>Many papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have shown that the key to fine-grained visual categorization tasks lies in developing effective methods to accurately identify informative regions in an image. They leverage the extra annotations of bounding box and part annotations to localize significant regions. However, obtaining such dense annotations of bounding box and part annotations is labor-intensive, which limits both scalability and practicality of real-world fine-grained applications. Other methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> use an unsupervised learning scheme to localize informative regions. They eliminate the need for the expensive annotations, but how to focus on the right areas and use them is still worth investigating.</p><p>An overview of our MMAL-Net is shown in <ref type="figure">Fig. 1</ref>. Our method has three branches in training phase, whose raw branch mainly studies the overall characteristics of the object, and AOLM needs to obtain the object's bounding box information with the help of the feature maps of the raw image from this branch. As the input of object branch, the finer scale of object image is very helpful for classification, because it contains the structural features of the target as well as the fine-grained features. Then, APPM proposes several part regions with the most discrimency and less redundancy according to the feature maps of object image. The part branch sends the part image clipped from the object image to the network for training. It enables the network to learn fine-grained features of different parts in different scales. Unlike RA-CNN <ref type="bibr" target="#b11">[12]</ref>, the parameters of CNN and FC in our three branches are shared. Therefore, through the common learning process of the three branches, the trained model has a good classification ability for different scales and parts of object. In the testing phase, unlike RA-CNN <ref type="bibr" target="#b11">[12]</ref> and NTS-Net <ref type="bibr" target="#b9">[10]</ref> need to calculate the feature vectors of the multiple part region images and then concat these vectors for classification. After our repeated experiments, the best classification performance is simply obtained by the result of object branch, so our approach can reduce some calculations and inference time while achieving high accuracy.</p><p>Our main contributions can be summarized as follows:</p><p>• Our multi-branch network can be trained end-to-end and learn object's discriminative regions for recognition effectively. • The AOLM does not increase the number of parameters, so we do not need to train a proposal network like RA-CNN <ref type="bibr" target="#b11">[12]</ref>. The accuracy of object localization is achieved by only using category labels. • We present an attention part proposal method(APPM) without the need of part annotations. It can select multiple ordered discriminative part images, so that the model can effectively learn different scales parts's fine-grained features. • State-of-the-art performances are reported on three standard benchmark datasets, where our method stable outperforms the state-of-the-art methods and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>In the past few years, the accuracy of benchmark on open datasets has been improved based on deep learning and fine-grained classification methods. They can be classified as follows: 1) By end-to-end feature encoding; 2) By localizationclassification subnetworks. <ref type="figure">Fig. 1</ref>. The overview of our proposed MMAL-Net in the training phase, The red branch is raw branch, the orange branch is object branch, and the blue branch is part branch.</p><p>In the dotted green box is the network structure for the test phase. The CNN (Convolutional Neural Networks) and FC (Fully Connection) layer of the same color represent parameter sharing. Our multi-branch all use cross entropy loss as the classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">By End-to-End Feature Encoding</head><p>This kind of method directly learns a more discriminative feature representation by developing powerful deep models for fine-grained recognition. The most representative method among them is Bilinear-CNN <ref type="bibr" target="#b3">[4]</ref>, which represents an image as a pooled outer product of features derived from two bilinear models, and thus encodes higher order statistics of convolutional activations to enhance the mid-level learning capability. Thanks to its high model capacity, it achieves clear performance improvement on a wide range of visual tasks. However, the high dimensionality of bilinear features still limits its further generalization. In order to solve this problem, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> try to aggregate low-dimensional embeddings by applying tensor sketching. They can reduce the dimensionality of bilinear features and achieve comparable or higher classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">By Localization-Classification Subnetworks</head><p>This kind of method trains a localization subnetwork with supervised or weakly supervised to locates key part regions. Then the classification subnetwork uses the information of fine-grained regions captured by the localization subnetwork to further enhance its classification capability. Earlier works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> belong to full supervision method depend on more than image-level annotations to locate semantic key parts. <ref type="bibr" target="#b6">[7]</ref> trained a region proposal network to generate proposals of informative image parts and concatenate multiple part-level features as a whole image representation toward final fine-grained recognition. However, maintaining such dense part annotations increases additional location labeling cost. Therefore, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> take advantage of attention mechanisms to avoid this problem. There is no need of bounding box annotations and part annotations except image-level annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention Object Location Module (AOLM)</head><p>This method was inspired by SCDA <ref type="bibr" target="#b14">[15]</ref>. SCDA uses a pre-trained model to extract image features for fine-grained image retrieval tasks. We improve it's positioning performance as much as possible through some measures. At the first, we describe the process of generating object location coordinates by processing the CNNs feature map as the <ref type="figure" target="#fig_0">Fig. 2</ref> illustrated. We use F ∈ R C×H×W to represent feature maps with C channels and spatial size H × W output from the last convolutional layer of an input image X and f i is the i-th feature map of the corresponding channel. As shown in Equ 1,</p><formula xml:id="formula_0">A = C−1 i=0 f i<label>(1)</label></formula><p>activation map A can be obtained by aggregating the feature maps F . We can visualizes where the deep neural networks focus on for recognition simply and locates the object regions accurately from A. As shown in Equ 2, a is the mean value of A.</p><formula xml:id="formula_1">a = W −1 x=0 H−1 y=0 A(x, y) H × W (2)</formula><p>a is used as the threshold to determine whether the element at that position in A is a part of object, and (x, y) is a particular position in a H × W activation map. Then we initially obtained a coarse mask map M conv 5c from the last convolutional layer Conv 5c of ResNet-50 <ref type="bibr" target="#b15">[16]</ref> according to Equ 3.</p><formula xml:id="formula_2">M (x,y) = 1 if A (x,y) &gt; a 0 otherwise<label>(3)</label></formula><p>On the basis of the experimental results, we find that the object is often in the largest connected component of M conv 5c , so the smallest bounding box containing the largest connected area is used as the result of our object location. Only using a pre-trained VGG16 <ref type="bibr" target="#b16">[17]</ref> in SCDA <ref type="bibr" target="#b14">[15]</ref> achieved better position accuracy, but our pre-trained ResNet-50 does not reach a similar accuracy rate and dropped significantly. So we use the training set to train ResNet-50 for improving object location accuracy and experiments in subsection 4.5 verify the effectiveness of this approach. Then, inspired by <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b17">[18]</ref> the performance of their methods all benefit from the ensembel of Multiple layers. So we get the activation map of the output of Conv 5b according to Equ 1, which is one block in front of Conv 5c. Then we can get M conv 5b according to Equ 3, and finally we can get a more accurate mask M after taking the intersection of M conv 5c and M conv 5b . As shown in Equ 4.</p><formula xml:id="formula_3">M = M conv 5c ∩ M conv 5b<label>(4)</label></formula><p>Subsequent experimental results prove the effectiveness of these approaches to improve object location accuracy. This improved weakly supervised object location method can achieve higher localization accuracy than ACOL <ref type="bibr" target="#b18">[19]</ref>, ADL <ref type="bibr" target="#b19">[20]</ref> and SCDA <ref type="bibr" target="#b14">[15]</ref>, without adding trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention Part Proposal Module(APPM)</head><p>Although AOLM can achieve higher localization accuracy, but there are some positioning results are part of the object. We improve the robustness of the model to this situation through APPM and part branch. It will be demonstrated in the next section. By observing the activation map A, we find that the area with high activation value of the activation map are often the area where the key part are located, such as the head area in the example. Using the idea of sliding window in object detection to find the windows with information as part images. Moreover, we implemented the traditional sliding window approach with full convolutional network to reduce the amount of calculation, just like Overfeat <ref type="bibr" target="#b20">[21]</ref> gets the feature map of different windows from the feature map output from the previous branch. Then we aggregate each window's activation map A w in the channel dimension and get its activation mean value a w according to Equ 5,</p><formula xml:id="formula_4">a w = Ww−1 x=0 Hw−1 y=0 A w (x, y) H w × W w<label>(5)</label></formula><p>H w , W w are the height and width of a window's feature map. We sort by the a w value of all windows. The larger the a w is, the larger the informativeness of this part region is, as shown in <ref type="figure">Fig. 3</ref>. However, we cannot directly select the first few windows, because they are often adjacent to the largest a w windows and contain approximate the same part, but we hope to select as many different parts as possible. In order to reduce region redundancy, we adopt non-maximum suppression (NMS) to select a fixed number of windows as part images with different scales. By visualizing the output of this module in <ref type="figure" target="#fig_1">Fig. 4</ref>, it can be seen that this method proposed some ordered, different importance degree part regions. <ref type="figure">Fig. 3</ref>. The simple pipeline of the APPM. We use red, orange, yellow, green colors to indicate the order of windows' aw.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture of MMAL-Net</head><p>In order to make the model fully and effectively learn the images obtained through AOLM and APPM. During the training phase, we construct a three branches network structure consisting of raw branch, object branch, and part branch, as shown in <ref type="figure">fig. 1</ref>. The three branches share a CNN for feature extraction and a FC layer for classification. Our three branches all use cross entropy loss as the classification loss. As shown in Equ 6, 7, and 8, respectively.</p><formula xml:id="formula_5">L raw = − log(P r (c))<label>(6)</label></formula><formula xml:id="formula_6">L object = − log(P o (c))<label>(7)</label></formula><formula xml:id="formula_7">L parts = − N −1 n=0 log(P p ( n) (c))<label>(8)</label></formula><p>Where c is the ground truth label of the input image, P r , P o are the category probabilities of the last softmax layer output of the raw branch and object branch, respectively , P p ( n) is the output of the softmax layer of the part branch corresponding to the nth part image, N is the number of part images. The total loss is defined as:</p><formula xml:id="formula_8">L total = L raw + L object + L parts<label>(9)</label></formula><p>The total loss is the sum of the losses of the three branches, which work together to optimize the performance of the model during backpropagation. It enables the final convergent model to make classification predictions based on the overall structural characteristics of the object or the fine-grained characteristics of a part. The model has good object scale adaptability, which improves the robustness in the case of inaccurate AOLM localization. During the testing phase, we removed the part branch so as to reduce a large amount of calculations, so our method will not take too long to predict in practical applications. Due to our reasonable and efficient framework. MMAL-Net can achieves the state-of-the-art performance so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>These three datasets are widely used as benchmarks for fine-grained classification(shown in <ref type="table" target="#tab_0">Table 1</ref>). In our experiments, we only use the image classification labels provided by these datasets.  <ref type="figure" target="#fig_0">N 2</ref> and N 3 are the number of three broad categories of scales windows mentioned above. ResNet-50 <ref type="bibr" target="#b15">[16]</ref> pre-trained on ImageNet is used as the backbone of our network structure. During training and testing, we do not use any other annotations other than image-level labels. Our optimizer is SGD with the momentum of 0.9 and the weight decay of 0.0001, and a mini-batch size of 6 on a Tesla P100 GPU. The initial learning rate is 0.001 and multiplied by 0.1 after 60 epoch. We use Pytorch as our code-base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison</head><p>We compared the baseline methods mentioned above on three commonly used fine-grained classification datasets. The experimental results are shown in the <ref type="table">Table 2</ref>. By comparison, we can see that our method achieves the best accuracy currently available on these three datasets. <ref type="table">Table 2</ref>. Comparison results on three common datasets. Train Anno. represents using bounding box or part annotations in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Train Anno. CUB AIR CAR ResNet-50 Bilinear-CNN <ref type="bibr" target="#b3">[4]</ref> 84.1 84.1 91.3 SPDA-CNN <ref type="bibr" target="#b4">[5]</ref> 85.1 --KP <ref type="bibr" target="#b13">[14]</ref> 86.2 86.9 92.4 RA-CNN <ref type="bibr" target="#b11">[12]</ref> 85.3 -92.5 MA-CNN <ref type="bibr" target="#b8">[9]</ref> 86.5 89.9 92.8 OSME+MAMC <ref type="bibr" target="#b21">[22]</ref> 86.5 -93.0 PC <ref type="bibr" target="#b22">[23]</ref> 86.9 89.2 92.9 HBP <ref type="bibr" target="#b23">[24]</ref> 87.1 90.3 93.7 Mask-CNN <ref type="bibr" target="#b5">[6]</ref> 87.3 --DFL-CNN <ref type="bibr" target="#b24">[25]</ref> 87.4 92.0 93.8 HSnet <ref type="bibr" target="#b6">[7]</ref> 87.5 --NTS-Net <ref type="bibr" target="#b9">[10]</ref> 87.5 91.4 93.9 MetaFGNet <ref type="bibr" target="#b25">[26]</ref> 87.6 --DCL <ref type="bibr" target="#b26">[27]</ref> 87.8 92.2 94.5 TASN <ref type="bibr" target="#b10">[11]</ref> 87.9 -93.8 Ours 89.6 94.7 95.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>The ablation study is performed on the CUB dataset. Without adding any of our proposed methods, the ResNet-50 <ref type="bibr" target="#b15">[16]</ref> obtained an accuracy of 84.5% under the condition that the input image resolution is 448 × 448. In order to verify the rationality of the training structure of our three branches, we remove the object branch and part branch respectively. After removing the object branch, the best accuracy is 85.0% from the raw branch, a drop of 4.6%. This proves the great contribution of AOLM and object branch to improve the classification accuracy. After removing the part branch, the best accuracy is 87.3% from the object branch, down 2.3% . The results of the experiment show that part branch and APPM can improve the robustness of the model when facing AOLM unstable positioning results. The above experiment has shown that the three branches of our method all have a significant contribution to the final accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Object Localization Performance</head><p>Percentage of Correctly Localized Parts (PCP) metric is the percentage of predicted boxes that are correctly localized with more than 50% IOU with the ground-truth bounding boxes. On the CUB dataset, the best result of AOLM in terms of the PCP metric for object localization is 85.1%. AOLM clearly exceeds SCDA's <ref type="bibr" target="#b14">[15]</ref> 76.8% and the recent weakly supervised object location methods ACOLs <ref type="bibr" target="#b18">[19]</ref> 46.0% and ADLs <ref type="bibr" target="#b19">[20]</ref> 62.3%. As shown in <ref type="table" target="#tab_1">Table 3</ref>, the ensemble of multiple layers significantly improves object location accuracy. Through the experiment, we find that the object location accuracy using the pre-trained model directly is 65.0%. However, it rise to 85.1% after one iteration of training. And as the training progressed, CNN paid more and more attention to the most discerning region to improve classification accuracy which leads to localization accuracy drops to 71.1%. Even so, due to part branch and APPM make model has good adaptability to object's scale, it still achieved excellent classification performance. Methods localization Accuracy ACOL <ref type="bibr" target="#b18">[19]</ref> 46.0% ADL <ref type="bibr" target="#b19">[20]</ref> 62.3% SCDA <ref type="bibr" target="#b14">[15]</ref> 76.8% AOLM(conv 5c) 82.2% AOLM(conv 5b &amp; conv 5c) 85.1%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model and Inference Time Complexity</head><p>Unlike RA-CNN <ref type="bibr" target="#b11">[12]</ref> and NTS-Net <ref type="bibr" target="#b9">[10]</ref>, the former has three branches with independent parameters and needs to train a subnetwork to propose finer scale images and the latter needs to train a navigator network to propose regions with large amount of information (such as different body parts of birds </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Visualization of Object and Part Regions</head><p>In order to visually analyze the areas of concern for our AOLM and APPM, we draw the object's bounding boxes and part regions proposed by AOLM and APPM in <ref type="figure" target="#fig_1">Fig. 4</ref>. In the first column, we use red and green rectangles to denote the ground truth and predicted bounding box in raw image. It is very helpful for classification that the positioned areas of objects often cover an almost complete object. In columns two through four, we use red, orange, yellow, and green rectangles to represent the regions with the highest average activation values in different scales proposed by APPM, with red rectangle denoting the highest one. <ref type="figure" target="#fig_1">Fig. 4</ref> conveys that the proposed area does contain more fine-grained information and the order is more reasonable on the same scale, which are very helpful for model's robustness to scale. We can find that the most discriminative regions of birds are head firstly , then is body, which is similar to human cognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an effective method for fine-grained classification without bounding box or part annotations. The multi-branch structure can make full use of the images obtained by AOLM and APPM to achieve excellent performance. Our algorithm is end-to-end trainable and achieves state-of-the-art results in CUB-200-2001 <ref type="bibr" target="#b0">[1]</ref>, FGVC Aircraft <ref type="bibr" target="#b1">[2]</ref> and Stanford Cars <ref type="bibr" target="#b2">[3]</ref> datasets.The future work is how to set the number and size of windows adaptively to further improve the classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The pipeline of the AOLM, we first get an activation map by aggregating the feature maps in the channel dimension, then obtain a bounding box according to activation map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of object and part regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Introduction of the three datasets used in this paper.In all our experiments, we first preprocess images to size 448 × 448 to get input image for raw branch and object branch. The object image is also scaled into 448 × 448 , but all part images are resized to 224 × 224 for part branch. We construct windows with three broad categories of scales: {[4 × 4, 3 × 5], [6 × 6, 5 × 7], [8 × 8, 6 × 10, 7 × 9, 7 × 10]} for activation map of 14 × 14 size, and the number of a raw image's part images is N = 7 , among them N 1 = 2, N 2 = 3, N 3 = 2. N 1 ,</figDesc><table><row><cell>Datasets</cell><cell cols="2">Object Class Train Test</cell></row><row><cell cols="3">CUB-200-2011(CUB) [1] Bird 200 5994 5794</cell></row><row><cell cols="3">FGVC-Aircraft(AIR) [2] Aircraft 100 6667 3333</cell></row><row><cell cols="2">Stanford Cars(CAR) [3] Car</cell><cell>196 8144 8041</cell></row><row><cell>4.2 Implementation Details</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Object localization accuracy on CUB-200-2011.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Our MMAL-Net has some advantages in terms of parameter volume over them. First, the three branches parameters are shared, and secondly the AOLM and APPM modules do not require training, do not add additional parameter amounts. Thirdly their calculation amount is relatively smaller. Finally, better classification performance is achieved by MMAL-Net. Compared with the ResNet-50 baseline, our method yields a significantly better result (+4.1%) with the same parameter volume. As for inference time, RA-CNN needs to calculate the output of three branches and fuse them; NTS-Net needs to extract and fuse the 4 proposal local image features of the input image. Above reasons make their inference time relatively longer and our method has a shorter inference time. It only needs to calculate the output of the first two branches and does not need to fuse them, because the classification results are based on the output of the second branch (object branch). For more accurate comparison, we conducted an inference time test on Tesla P100 and the input image size is 448 × 448. The inference time of MMAL-Net and NTS-Net are summarized as follows: the running time on an image of NTS-Net is about 5.61 ms, and ours methods running time on an image is about 1.80 ms. Based on the above analysis, lower model and inference time complexity both add extra practical value to our proposed method.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<title level="m">Fine-grained visual classification of aircraft</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06878</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2520" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T IMAGE PROCESS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization using meta-learning optimization with sample selection of auxiliary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="233" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
