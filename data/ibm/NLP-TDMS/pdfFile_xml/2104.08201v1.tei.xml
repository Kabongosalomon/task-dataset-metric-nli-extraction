<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><forename type="middle">Sun</forename><surname>Hkust</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuaishou</forename><surname>Technology</surname></persName>
						</author>
						<title level="a" type="main">Semantic Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural image matting separates the foreground from background in fractional occupancy which can be caused by highly transparent objects, complex foreground (e.g., net or tree), and/or objects containing very fine details (e.g., hairs). Although conventional matting formulation can be applied to all of the above cases, no previous work has attempted to reason the underlying causes of matting due to various foreground semantics.</p><p>We show how to obtain better alpha mattes by incorporating into our framework semantic classification of matting regions. Specifically, we consider and learn 20 classes of matting patterns, and propose to extend the conventional trimap to semantic trimap. The proposed semantic trimap can be obtained automatically through patch structure analysis within trimap regions. Meanwhile, we learn a multi-class discriminator to regularize the alpha prediction at semantic level, and content-sensitive weights to balance different regularization losses. Experiments on multiple benchmarks show that our method outperforms other methods and has achieved the most competitive state-of-theart performance. Finally, we contribute a large-scale Semantic Image Matting Dataset with careful consideration of data balancing across different semantic classes. Code and dataset are available at https://github.com/nowsyn/ SIM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The matting equation models an image I as a linear combination of a foreground image F and a background image B modulated by an alpha matte α:</p><formula xml:id="formula_0">I = αF + (1 − α)B.</formula><p>(1)</p><p>The natural image matting problem is to extract the alpha matte α from a given image, which has a wide range of applications in image/video editing. Typical foreground objects can belong to a great variety of categories, such as This work was done when Yanan Sun was a student intern at Kuaishou Technology. This work was supported by Kuaishou Technology and the Research Grant Council of the Hong Kong SAR under grant no. 16201420. humans, furry animals, glass objects with transparent/semitransparent regions, or objects with complex shapes such as net or tree, thus making this research problem still challenging, see <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Natural image matting is an ill-posed problem, so most existing methods take both image and user-supplied clues (scribbles, trimaps, etc.) as input to solve the problem. Class-agnostic trimap comprising of foreground, background, and unknown regions is the most common choice among traditional and deep learning-based methods.</p><p>Traditional methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25</ref>] mainly rely on low-level features related to image colors or structures. As traditional methods do not take into consideration any semantic information of foreground objects or background scenes, they fail easily in images where foreground pixels mingle deeply with background pixels. This issue has been addressed to a considerable extent with the introduction of deep neural networks. In <ref type="bibr" target="#b0">[2]</ref> images are segmented into soft transitional regions based on semantics. However, this method can easily fail in images with entangled colors or large transitional regions. To extract alpha mattes for general objects, deep learning-based methods <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b24">26]</ref> have contributed various convolution network designs to greatly improve performance benefiting from high-level semantic representations. Recent works such as human matting and transparent object matting <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b5">7]</ref> have focused on solving specific instances of the matting problem, leveraging prior domain knowledge, and contributing to excellent performance for these class-specific matting tasks. However, these deep learning-based methods still utilize semantic information only at a data level and do not adequately consider the underlying cause of matting due to different object semantics. Given the unknown region surrounding a foreground object, different types of boundaries or patterns may exist. For example, a portrait usually has both fuzzy hair and sharp boundaries.</p><p>In this paper, we propose to incorporate semantic classification of matting regions into our matting framework for extracting better alpha matte. Specifically, we first cluster 20 different matting classes based on the regional matting patterns. Our matting classes cover most typical matting scenarios for various foreground objects. Then, we extend the conventional class-agnostic trimap to semantic trimap, which consists of a 2D confidence map for each matting class in the unknown region. The automatically generated semantic trimap with RGB image is then fed into our framework as input. Meanwhile, we learn a multi-class discriminator for supervision, providing regularization from a semantic level for alpha predictions. In particular, to improve prediction with semantics incorporation, we introduce gradient constraints with content-sensitive weights to balance different regularization losses.</p><p>In summary, our main contributions are: 1. We introduce semantics into the matting task and demonstrate how semantic information can be used to achieve the most competitive performance. To our knowledge, this is the first paper to consider semantic classification of matting patterns in a natural image matting framework.</p><p>2. Our main technical contributions include: the introduction of semantic trimap, the proposal of learnable content-sensitive weights, and the usage of multi-class discriminator to regularize the matting results.</p><p>3. We contribute the first large-scale class-balanced Semantic Image Matting Dataset covering a wide range of matting patterns to benefit future matting research.</p><p>Our new dataset provides new insight and in-depth analysis to the performance of different matting algorithms on different matting classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first briefly review representative works on general image matting. Then, we will discuss methods that are tailored for humans and other class-specific objects.</p><p>Natural Image Matting. Traditional methods on natural image matting are all based on the linear combination equation Eq. 1 except for <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b21">23]</ref>. Since this is an illposed problem, traditional methods often make use of different priors. They can be further divided into samplingbased methods <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b34">36]</ref> and propagation-based methods <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b0">2,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25]</ref> or a combination of both. Sampling-based methods assume color similarities between unknown regions and definitely foreground/background regions to estimate foreground and background colors followed by alpha matte estimation. Propagation-based methods utilize the color line model proposed by <ref type="bibr" target="#b22">[24]</ref> to estimate alpha by propagating its values from known regions to unknown regions. A comprehensive review on traditional matting methods can be found in <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b9">11]</ref>.</p><p>For deep-learning based methods, Cho et al. <ref type="bibr" target="#b9">[11]</ref> proposed to apply deep neural networks to fuse the results produced by closed-form matting <ref type="bibr" target="#b22">[24]</ref> and KNN matting <ref type="bibr" target="#b8">[10]</ref>. Xu et al. <ref type="bibr" target="#b41">[43]</ref> introduced the Composite-1K dataset and a two-stage encoder-decoder network for general object matting. Lutz et al. <ref type="bibr" target="#b29">[31]</ref> proposed a generative adversarial network in the encoder-decoder framework to improve matting performance. Cai et al. <ref type="bibr" target="#b4">[6]</ref> introduced trimap adaptation and applied multi-task learning to matting. Hou et al. <ref type="bibr" target="#b18">[20]</ref> simultaneously estimated foreground and alpha matte with local matting encoder and global context encoder. Li et al. <ref type="bibr" target="#b24">[26]</ref> designed a guided contextual attention module to propagate high-level opacity information globally based on the learned low-level affinity. Recently, a number of works <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b31">33]</ref> have been proposed focusing on relaxing trimap input. Lastly, Forte and Pitié <ref type="bibr" target="#b13">[15]</ref> introduced the FBA-net which simultaneously estimates foreground, background and alpha. See supplementary material for comparison with <ref type="bibr" target="#b13">[15]</ref> 1 .</p><p>Human Matting. Human matting was first introduced by Shen et al. <ref type="bibr" target="#b36">[38]</ref> to extract alpha mattes of portrait images. Different from general object matting, human matting only considers hairs and sharp boundaries of human body. The authors formulated the solution as a two-step approach: human detection followed by human matting <ref type="bibr" target="#b47">[49]</ref>. Since foreground objects are already known to be human, the trimap can be eliminated <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b27">29]</ref>. Although these approaches give excellent performance for human subjects, they fall short of matting general objects. In addition, since they were trained using portrait images, the performance would significantly drop when the human detector fails.</p><p>Class-Specific Matting. For works targeting at other specific classes of matting, Lin et al. <ref type="bibr" target="#b25">[27]</ref> introduced motion regularization for matting motion blurred moving objects. Köhler et al. <ref type="bibr" target="#b20">[22]</ref> proposed to separate motion blurred foreground through explicit modeling of object motion. Amin et al. <ref type="bibr" target="#b2">[4]</ref> applied image matting to segment out-of-focus regions. Yeung et al. <ref type="bibr" target="#b42">[44]</ref> proposed attenuation-refraction matte to mask out and re-composite transparent object with refraction consideration. Chen et al. <ref type="bibr" target="#b5">[7]</ref> proposed a multiscale encoder-decoder network to estimate the refractive flow for transparent object matting. In summary, traditional matting methods rely on lowlevel image cues for solving the matting equation, while more recent methods do not adequately consider or provide a principled approach to semantic image matting, or are limited to one or few classes such as humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Although there exist several large-scale matting datasets for training deep neural networks, they exhibit severe biases with respect to our matting classes.</p><p>Matting Classes. We study all existing publicly available matting datasets. Alpha mattes can exhibit diverse patterns. We propose 20 matting classes with distinct pattern as shown in <ref type="figure">Figure 2</ref>. These 20 classes cover almost all of the matting scenarios in existing matting datasets. While there exist numerous matting cases in the real world, most of them can be categorized into these 20 classes while the rest are quite rare. <ref type="figure">Figure 3</ref> shows the t-SNE visualizations of the 20 classes.</p><p>Based on these classes, we keep part of foreground objects in Adobe Image Matting Dataset <ref type="bibr" target="#b41">[43]</ref>. Additionally, we collect sufficient clean images covering objects with corresponding patterns, and carefully extract their alpha matte with Photoshop. Finally, our Semantic Image Matting Dataset consists of 20 classes with 726 training foregrounds and 89 testing foregrounds.</p><p>To generate the composited training set, we combine  <ref type="figure">Figure 5</ref>. Left: class-specific alpha ratio distribution. Alpha ratio is the quotient between the number of alpha pixels and the total number of all pixels within a region. Right: Alpha gradient magnitude distribution. Y-axis is the mean gradient magnitude of alpha pixels within a region.</p><p>training foregrounds with randomly selected background images from COCO <ref type="bibr" target="#b26">[28]</ref> dataset in an online manner during training. For the testing set, we follow <ref type="bibr" target="#b41">[43]</ref> and composite each testing foreground with 10 background images from PASCAL VOC <ref type="bibr" target="#b11">[13]</ref> dataset.</p><p>Comparison with Other Matting Datasets. Although large-scale matting datasets, e.g., Adobe Image Matting Dataset <ref type="bibr" target="#b41">[43]</ref> and Distinctions-646 <ref type="bibr" target="#b31">[33]</ref>, have been proposed as benchmarks in matting task, they are heavily biased toward common objects such as humans or animals. <ref type="figure">Figure 4</ref> compares the statistics of these two representative datasets and our dataset, showing that our Semantic Image Matting Dataset is class-balanced across the 20 matting classes. Feat. Recon. Loss <ref type="figure">Figure 6</ref>. Overall framework. Our method takes as input an RGB image and its semantic trimap automatically generated from a patch-based classifier, which simultaneously predicts the alpha, foreground, background as well as learnable regularization weights. During training, a multi-class discriminator is used to provide supervision including classification loss and feature reconstruction loss from a semantic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch-Based Classifier</head><p>Analysis. In <ref type="figure">Figure 5</ref>, we compute the average gradient magnitude of alpha mattes as well as foreground alpha ratios within the unknown region in all classes and plot their distributions. From the statistics, we can observe that while some classes have smooth texture with small gradients, such as fire, smoke, and silk, other classes exhibit many fine features and sharp structures, such as net and spider web. This motivates us to exploit more useful information from classrelated gradient constraints in training our model. More detailed statistics are included in the supplementary materials. Different matting classes exhibit quite distinctive alpha patterns and distributions, indicating that taking into consideration such semantics caused by different objects should boost matting performance. <ref type="figure">Figure 6</ref> shows our whole framework which is an encoder-decoder structure that takes an RGB image as well as its semantic trimap as input, and outputs the alpha predictions. Notably, this network is supervised by our multi-class discriminator as well as reconstruction and gradient-related losses. The network architecture of this multi-class discriminator is the same as the patch-based classifier which generates the semantic trimap, comprising of standard CNN, max-pooling layers, and ResBlocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Framework</head><p>Patch-Based Classifier. Semantic trimap is the concatenation of a conventional trimap and n-channel score maps where n is the number of matting classes. The conventional trimap defines definite foreground, definite background and unknown region while the score maps indicate the confidence each unknown pixel belongs to a certain matting class. The score maps are automatically generated through semi-supervised patch-based structure analysis within unknown regions.</p><p>Our patch-based classifier is trained on the composited dataset where the images are partitioned with patch labels indicating their respective matting classes. An alpha image usually contains more than one matting pattern, so it is partitioned into multiple regions each of which only belongs to a certain class. When training the classifier, we randomly crop a square patch from unknown regions of a composited image. To make the classifier robust to scale variation, the crop size is random, uniformly chosen within a range of [160, 640] and then the cropped patch is resized to 320. Then, the classifier takes as input this given patch with the corresponding conventional trimap, and predicts the matting class for the given patch. After the classifier is well-trained, we compute the n-channel score maps for a patch by multiplying the last convolutional feature map with the fully connected weights, which is also known as class activation map <ref type="bibr" target="#b46">[48]</ref>.</p><p>During inference, we obtain the semantic trimap for the entire input image through multi-scale patch analysis. In detail, we partition the input image into multi-scale overlapped patches and stitch the score maps of these patches together. Then we treat the stitched scores maps with the conventional trimap as our semantic trimap.</p><p>When we train our matting network, the semantic trimap will be concatenated with the RGB image and fed into the encoder, as shown in <ref type="figure">Figure 6</ref>. Compared to conventional trimap, semantic trimap provides prior knowledge for the network to reduce search space for each class, focusing the model to predict more reliable alpha matte compatible to patterns caused by pertinent matting object semantics.</p><p>Encoder-Decoder Structure. Our framework for inference stage consists of U-Net <ref type="bibr" target="#b33">[35]</ref> like structure. The encoder is adapted from ResNet-50 <ref type="bibr" target="#b17">[19]</ref> with a downsampling stride Predicted Alpha Region</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth Alpha Region</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Reconstruction Loss</head><p>Multi-classification Loss</p><formula xml:id="formula_1">Conv+MaxPool ResBlock FC ! " # $ % " ! " " " # " $ " %F igure 7.</formula><p>Multi-class discriminator. After feeding the predicted alpha region and groundtruth region to the well-trained discriminator, we compute the classification loss and feature reconstruction loss in order to preserve the distribution of predictions. of 8. Dilation convolution layers are used to enlarge receptive fields. Before sending encoded features to the decoder, an atrous spatial pyramid pooling (ASPP) module <ref type="bibr" target="#b6">[8]</ref> is applied to aggregate features of different receptive fields in order to enhance the feature representation capability. Afterward, three up-conv layers are utilized to recover the spatial information by integrating high-level features as well as high-resolution features from shallow layers. We simultaneously predict F, B, α through 3 prediction heads comprised of two 3 × 3 convolutional layers.</p><p>Multi-Class Discriminator. Previous works <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b31">33]</ref> also adopt a discriminator to provide additional supervision for obtaining better alpha predictions. However, all of them have not taken inter-class difference into consideration. In contrast, we apply a multi-class discriminator to make the model aware of specific structures and preserve the statistics of relevant alpha patterns. The regularization imposed by the multi-class discriminator enforces the model to better learn the characteristics of each class and consequently give more reasonable predictions.</p><p>Overall, the multi-class discriminator shares the same architecture with our semantic trimap classifier, except taking alpha matte as input during training and inference. <ref type="figure">Figure 7</ref> shows its detailed structure with data flows.</p><p>Before training the matting network, we first train this classifier on the groundtruth alpha mattes in Semantic Image Matting Dataset. The training process is similar to that of our semantic trimap classifier. Specifically, in each iteration, we randomly crop a square patch from the unknown class-specific regions and feed it to the classifier. After the classifier has been well trained, it will be deployed as a discriminator in our matting framework to generate classification loss and feature reconstruction loss.</p><p>During training, after obtaining the predicted alpha patch from the auto encoder-decoder matting network, the predicted alpha patch and its groundtruth patch are sent to the well-trained classifier so that we can extract for the two patches the respective multi-classification probabilitiesp and p, as well as the deep encoded multi-level featuresf k and f k where k ∈ {1, 2, 3, 4, 5}.</p><p>To compute the classification loss, we treatp as logit and p as label, and apply a cross-entropy loss on them. Note that even though the prediction for the groundtruth patch p is different from its original label, for instance, the original label is hair easy while the predicted label is hair hard, we still use p as the label in the classification loss, since we suppose the distribution of the predicted alpha region should be as close as possible to that of the groundtruth alpha region.</p><p>To further improve the consistency, we compute the feature reconstruction loss between the features of predicted alpha and groundtruth alpha, which is a part of the perceptual loss proposed in <ref type="bibr" target="#b19">[21]</ref>. Conventionally, L 2 loss is performed betweenf k and f k on each level separately.</p><p>Content-Sensitive Weights. Each matting class represents a distinct appearance and structure and thus its respective color and alpha exhibit different gradient distributions from other classes. For instance, Hair consists of fine structures with large gradients along hair boundaries, while Fire exhibits smooth transition across its foreground region.</p><p>Based on this observation, we introduce gradient constraints in our framework. The gradient of Eq. 1 is</p><formula xml:id="formula_2">∇I = (F − B)∇α + α∇F + (1 − α)∇B (2)</formula><p>Given an image, within the unknown region, F − B and α are not available to the model. Thus, we learn 2D contentsensitive weights as different regularization coefficients to balance the gradient contributions among F, B, α for I. Specifically, the gradient constraint Eq. 2 is re-formulated as Eq. 3 with 2D learnable weights λ 1 , λ 2 .</p><formula xml:id="formula_3">∇I = λ 1 ∇α + (1 − λ 2 )∇F + λ 2 ∇B<label>(3)</label></formula><p>By introducing gradient constraints with learnable contentsensitive weights, our framework learns the implicit relation between the gradient contribution and semantic representation, which guides the model to distinguish the source of the image structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Losses</head><p>We jointly employ multiple losses, including reconstruction losses, classification loss, feature reconstruction loss, and gradient-related losses.</p><p>Reconstruction Losses. The reconstruction losses include alpha reconstruction loss L α and F, B reconstruction loss L F B . Alpha reconstruction loss is composed of difference loss and composition loss as well as Laplacian pyramid loss L lap proposed by Hou et al. <ref type="bibr" target="#b18">[20]</ref>, which is defined as Eq. 4. For foreground and background reconstruction loss, we only consider the pixels within F and B respectively, as </p><formula xml:id="formula_4">L α = 1 |U | i∈U α i − α i 1 + 1 |U | i∈U Î i − I i 1 + L lap (4) L F B = 1 | F | i∈ F F i − F i 1 + 1 | B| i∈ B B i − B i 1 (5)</formula><p>Classification and Feature Reconstruction Loss. As discussed above, our multi-class discriminator regularizes the model from a semantic level by classification loss L c and feature reconstruction loss L f defined as below:</p><formula xml:id="formula_5">L c = − jp j log p j (6) L f = k 1 |f k | f k − f k 2<label>(7)</label></formula><p>Gradient-related Loss. To enforce the model to obey the gradient constraint, we utilize the loss with learnable content-sensitive regularization weights as Eq. 8:</p><formula xml:id="formula_6">L g = 1 |U | i∈U ∇Î i − ∇I i 1<label>(8)</label></formula><formula xml:id="formula_7">∇Î i = λ 1 ∇α i + (1 − λ 2 )∇F i + λ 2 ∇B i<label>(9)</label></formula><p>In addition, to fully exploit the gradient constraint, an exclusion loss used in a similar form in <ref type="bibr" target="#b43">[45]</ref> is defined in Eq. 10 to avoid the structure of the image leaking into both foreground and background: Total Loss. The total loss is thus the weighted sum of the above losses defined as:</p><formula xml:id="formula_8">L e = 1 |U | i∈U ∇F i 1 ∇B i 1 + ∇α i 1 ∇B i 1<label>(</label></formula><formula xml:id="formula_9">L = L α + 0.2(L F B + L f + L g + L e ) + 0.1L c (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We train our models on the Semantic Image Matting Dataset. Specifically, we take the images in COCO dataset <ref type="bibr" target="#b26">[28]</ref> as background images. For each foreground object, we randomly select an image from the backgrounds and composite them using Eq. 1. Before composition, we perform various augmentations on both foreground and background. For the foreground, we randomly crop a square patch from size [320, 480, 640] and apply random scaling, rotation, color jittering, and horizontal flipping on the patch. Meanwhile, we randomly crop a patch from the background image and then generate inputs from the two patches. To extend training samples, we also randomly merge two foregrounds into one following <ref type="bibr" target="#b24">[26]</ref>. We dilate and erode alpha matte with a random kernel size within <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b7">9]</ref> and a random iteration within <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b13">15]</ref> for trimap generation. More details can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIM</head><p>IndexNet GCA SIM (Ours) <ref type="figure">Figure 9</ref>. Qualitative results on the real-world images.</p><p>Methods SAD MSE(10 3 ) Grad Conn Closed-Form <ref type="bibr" target="#b22">[24]</ref> 168.1 91.0 126.9 167.9 KNN-Matting <ref type="bibr" target="#b8">[10]</ref> 175 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Main Results</head><p>Results on Semantic Image Matting Dataset. <ref type="table">Table 1</ref> tabulates the quantitative comparisons with other methods on our Semantic Image Matting Dataset. Our method outperforms them and achieves state-of-the-art performance. <ref type="table" target="#tab_4">Table 3</ref> further lists the quantitative results on each matting class. Our method also performs better than other methods on all matting classes. <ref type="figure" target="#fig_2">Figure 8</ref> show the qualitative comparisons of some classes on the test set.</p><p>Results on Composition-1K. We also evaluate our model on the Composition-1K dataset proposed by Xu et al. <ref type="bibr" target="#b41">[43]</ref>, which has 50 unique foreground objects and 1000 testing samples. <ref type="table">Table 2</ref> shows the quantitative results. Our method also achieves the most competitive state-of-the-art performance on this dataset.</p><p>Results on alphamatting.com. <ref type="table">Table 4</ref> shows the evaluation results on alphamatting.com [1] benchmark proposed in <ref type="bibr" target="#b32">[34]</ref>. Our method outperforms other methods and ranks first on three metrics. Detailed comparisons are provided in supplementary materials.</p><p>Results on Real-World Images. To evaluate the generalization ability of our framework, we collect from the Internet a number of free real-world images of different matting classes with user-labeled trimaps. <ref type="figure">Figure 9</ref> shows the qualitative results which further demonstrates the effectiveness and generalization ability of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>Analysis of Semantic Trimap. <ref type="figure" target="#fig_0">Figure 10</ref> visualizes an example semantic trimap, which is the combination of a conventional trimap and an automatically generated 20-channel score maps. The conventional trimap is divided into foreground, background, and unknown regions, which does not provide any semantics of the unknown pixels. Although deep neural networks are capable of encoding the implicit semantics of alpha patterns into high-level features, many fail cases still cannot be avoided due to restricted receptive fields and complex background, especially when the conventional trimap is too coarse to provide enough clues for extracting the relevant foreground object. The semantics score maps on the other hand provide the much-needed prior knowledge for indicating the potential class of unknown pixels, which guides the network to generate more accurate predictions.</p><p>Analysis of Multi-Class Discriminator. The arguably most widely used criterion in matting optimization is pixelto-pixel distance which unfortunately does not take any dis-  <ref type="table">Table 5</ref>. Ablation studies. The basic model is trained with reconstruction losses. "S", "D", "G" denotes semantic trimap , multiclass discriminator and gradient-related losses respectively. tribution of a region into consideration. Thus, we propose a multi-class discriminator to make the model learn the distribution of different classes by taking into consideration class-specific statistics of alpha mattes. Previous works <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b30">32]</ref> had exploited natural image statistics and utilized them to boost performance in image reconstruction or translation. As for matting, if the predicted mattes are satisfactory, such predictions are supposed to have similar distributions to those of the groundtruth. The classification and feature reconstruction losses provided by the multi-class discriminator enforces the model to preserve the statistics of different patterns and consequently improves the performance.</p><p>Analysis of Content-Sensitive Weights. During the training stage, we introduce the gradient constraints with learnable weights to regularize the model on different classes. <ref type="figure" target="#fig_0">Figure 11</ref> visualizes sample learnable weights. From this example, we observe that for the semi-transparent smooth textures such as silk, the alpha matte and the background contribute most to the image gradient. With the learnable content-sensitive weights, the performance is improved by 1.97 on SAD as shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we make use the semantics underlying different alpha mattes, and develop a method incorporating semantic classification of matting regions for extracting better alpha matte. Specifically, we first cluster 20 classes according to regional alpha patterns, which cover a wide range of matting scenarios. Based on the classes, we extend traditional trimap to semantic trimap which is automatically extracted by a classifier to provide valuable semantic information for predictions. To further preserve alpha statistics for each class, a novel multi-class discriminator is designed to regularize the model according to class-specific distributions. Finally, we introduce an alpha gradient constraint with learnable content-sensitive weights as a new regularization to achieve better optimization of different classes. We conduct extensive experiments on Semantic Image Matting Dataset, Composition-1K dataset, and real-world images. Quantitative and qualitative results demonstrate the clear advantages of our method over existing methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Alpha mattes can have different local shapes and patterns depending on the underlying foreground. Left shows a human matte with both soft hair and sharp body boundaries. Right shows a complex matte where the foreground exhibits different degrees of transparency. Deep reasoning of object semantics in natural image matting can advance the state-of-the-art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3</head><label>23</label><figDesc>The 20 matting classes with high diversity in appearance across different classes. . t-SNE visualizations of the class-specific features extracted from our discriminator (for its design see Method section).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FFigure 8 .</head><label>8</label><figDesc>= F + U and B = B + U : Qualitative results on the Semantic Image Matting test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>An example semantic trimap. We visualize two channels of the score maps: a. hair hard; b. sharp. Visualization of learnable weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Quantitative results of 20 classes on Semantic Image Matting Dataset.</figDesc><table><row><cell>Classes</cell><cell cols="3">defocus fire</cell><cell>fur</cell><cell cols="2">glass ice</cell><cell>hair easy</cell><cell>hair hard</cell><cell cols="2">insect motion</cell><cell>net</cell><cell>flower</cell></row><row><cell>DIM [43]</cell><cell></cell><cell cols="2">25.91 60.53</cell><cell>9.88</cell><cell></cell><cell>91.36</cell><cell>11.23</cell><cell>13.01</cell><cell>111.21</cell><cell>6.78</cell><cell>87.09</cell><cell>65.40</cell></row><row><cell cols="4">IndexNet [30] 22.86 97.85</cell><cell>9.99</cell><cell></cell><cell>91.95</cell><cell>8.33</cell><cell>13.24</cell><cell>130.52</cell><cell>6.68</cell><cell>91.43</cell><cell>59.60</cell></row><row><cell>GCA [26]</cell><cell></cell><cell cols="2">18.33 46.29</cell><cell>8.12</cell><cell></cell><cell>76.20</cell><cell>8.24</cell><cell>11.31</cell><cell>99.11</cell><cell>6.08</cell><cell>83.71</cell><cell>44.86</cell></row><row><cell>SIM (Ours)</cell><cell></cell><cell cols="2">13.49 35.44</cell><cell>5.90</cell><cell></cell><cell>49.19</cell><cell>5.68</cell><cell>7.72</cell><cell>96.85</cell><cell>4.04</cell><cell>50.35</cell><cell>37.10</cell></row><row><cell>Classes</cell><cell></cell><cell>leaf</cell><cell cols="2">tree plastic bag</cell><cell></cell><cell>sharp</cell><cell cols="2">smoke cloud spider web</cell><cell>lace</cell><cell>silk</cell><cell>water drop water spray</cell></row><row><cell>DIM [43]</cell><cell></cell><cell cols="2">45.43 91.71</cell><cell>65.44</cell><cell></cell><cell>2.96</cell><cell>48.21</cell><cell>145.57</cell><cell cols="2">101.78 51.89</cell><cell>32.48</cell><cell>41.96</cell></row><row><cell cols="4">IndexNet [30] 43.85 99.26</cell><cell>89.70</cell><cell></cell><cell>3.32</cell><cell>35.31</cell><cell>145.62</cell><cell cols="2">114.47 62.81</cell><cell>33.90</cell><cell>34.92</cell></row><row><cell>GCA [26]</cell><cell></cell><cell cols="2">41.12 87.61</cell><cell>47.40</cell><cell></cell><cell>3.35</cell><cell>41.18</cell><cell>107.14</cell><cell>80.51</cell><cell>51.93</cell><cell>25.83</cell><cell>31.12</cell></row><row><cell>SIM (Ours)</cell><cell></cell><cell cols="2">20.98 34.14</cell><cell>36.70</cell><cell></cell><cell>1.39</cell><cell>27.42</cell><cell>63.79</cell><cell>51.08</cell><cell>41.78</cell><cell>16.94</cell><cell>20.53</cell></row><row><cell>Methods</cell><cell></cell><cell cols="6">SAD Overall S L U Overall Overall MSE Grad</cell><cell></cell><cell></cell></row><row><cell>AdaMatting [6]</cell><cell></cell><cell cols="3">7.6 6.9 6.5 9.4</cell><cell>8.5</cell><cell>8.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SampleNet [40]</cell><cell cols="4">8.2 6.5 7.6 10.5 9.2</cell><cell>9.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Background [37] 7.9 5.9 5.4 12.4 7.4</cell><cell>6.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCA [26]</cell><cell></cell><cell>9</cell><cell cols="3">10 6.4 10.8 9.9</cell><cell>8.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIM (Ours)</cell><cell></cell><cell cols="3">2.5 2.6 1.8 3</cell><cell>2.9</cell><cell>3.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">Table 4. Quantitative results of our method and several representa-</cell><cell></cell><cell></cell></row><row><cell cols="8">tive state-of-the-art methods on alphamatting.com [1] benchmark.</cell><cell></cell><cell></cell></row><row><cell cols="8">"S", "L", "U" denote three trimap sizes and scores denote average</cell><cell></cell><cell></cell></row><row><cell cols="7">rank across 8 test samples. Best results are shown in bold.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="6">SAD MSE(10 3 ) Grad Conn</cell><cell></cell><cell></cell></row><row><cell>Basic</cell><cell></cell><cell cols="2">32.04</cell><cell>5.9</cell><cell cols="3">12.05 26.20</cell><cell></cell><cell></cell></row><row><cell>Basic + S</cell><cell></cell><cell cols="2">30.24</cell><cell>5.4</cell><cell cols="3">11.60 23.83</cell><cell></cell><cell></cell></row><row><cell>Basic + S + D</cell><cell></cell><cell cols="2">29.84</cell><cell>5.3</cell><cell cols="3">12.37 23.33</cell><cell></cell><cell></cell></row><row><cell cols="4">Basic + S + D + G 27.87</cell><cell>4.7</cell><cell cols="3">11.57 20.83</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b13">[15]</ref> has not been published in recognized, peer-reviewed venues.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Designing effective inter-pixel information flow for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tunc Ozan Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A hybrid defocused region segmentation approach using image matting. Multidimensional Systems and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benish</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Muhammad Mohsin Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghafoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="561" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tom-net: Learning transparent object matting from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A cluster sampling method for image matting via sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">F</forename><surname>Matting</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07711</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shared sampling for real-time alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="575" to="584" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schiwietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Shmuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rüdiger</forename><surname>Westermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving alpha matting and motion blurred foreground estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transductive multi-label learning via alpha matting. Unpublished manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Nan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spectral matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Motion regularization for matting motion blurred objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ting Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2329" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Boosting semantic human matting with coarse annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Alphagan: Generative adversarial networks for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Amplianitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The contextual loss for image transformation with non-aligned data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Background matting: The world is your green screen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Jayaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Soft color segmentation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1537" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yagiz</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunc Ozan</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Statistics of natural image categories. Network: computation in neural systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="391" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image and video matting: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="175" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Matting and compositing of transparent and refractive objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Xuaner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning based digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Kambhamettu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast deep matting for portrait animation on mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
