<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Daedeok-daero 989beon-gil</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">A</forename><surname>Bratholm</surname></persName>
							<email>*lars.bratholm@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Chemistry</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Cantock&apos;s Close</addrLine>
									<postCode>BS8 1TS</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Fry Building, Woodland Road</addrLine>
									<postCode>BS1 1UG</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Gerrard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Chemistry</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Cantock&apos;s Close</addrLine>
									<postCode>BS8 1TS</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
								<address>
									<postCode>15222</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Choi</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Institute of Science and Technology Information</orgName>
								<orgName type="institution">National Institute of Supercomputing and Network</orgName>
								<address>
									<addrLine>245 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon, Republic</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Dang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Hanchar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Addison</forename><surname>Howard</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Gangnam Finance Center</orgName>
								<address>
									<addrLine>Google Inc., Mountain View, US; 11 Ebay Korea, 34F, 152 Teheran Ro, Gangnam Gu</addrLine>
									<settlement>Kaggle, Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Huard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
								<address>
									<postCode>15222</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<addrLine>5730 S Ellis Ave</addrLine>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">14 Center for Computational Mathematics, Flatiron Institute</orgName>
								<orgName type="department" key="dep2">15 Bosch Research and Technology Center</orgName>
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<addrLine>5747 S Ellis Ave, 162 5th Ave</addrLine>
									<postCode>60637, 10010, 02139</postCode>
									<settlement>Chicago, New York, Cambridge</settlement>
									<region>IL, NY, MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mordechai</forename><surname>Kornbluth</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhan</forename><surname>Lee</surname></persName>
							<affiliation key="aff10">
								<orgName type="department" key="dep1">Department of Chemical and Biomolecular Engineering</orgName>
								<orgName type="department" key="dep2">Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>291 Daehak-ro, Yuseong-gu</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country>Korea, Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsoo</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Mailoa</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Tu</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>Popovic</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Totient Inc</orgName>
								<address>
									<addrLine>Sindjeliceva 9</addrLine>
									<postCode>11000</postCode>
									<settlement>Belgrade</settlement>
									<country key="RS">Serbia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Rakocevic</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Totient Inc</orgName>
								<address>
									<addrLine>Sindjeliceva 9</addrLine>
									<postCode>11000</postCode>
									<settlement>Belgrade</settlement>
									<country key="RS">Serbia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Reade</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Gangnam Finance Center</orgName>
								<address>
									<addrLine>Google Inc., Mountain View, US; 11 Ebay Korea, 34F, 152 Teheran Ro, Gangnam Gu</addrLine>
									<settlement>Kaggle, Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Song</surname></persName>
							<affiliation key="aff12">
								<orgName type="laboratory">KAIST Web Security &amp; Privacy Lab</orgName>
								<address>
									<addrLine>291, Daehak-ro, Yuseong-gu, 20 Medbravo.org</addrLine>
									<postCode>34141</postCode>
									<settlement>Daejeon, Alicante</settlement>
									<country>Republic of Korea;, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka</forename><surname>Stojanovic</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Totient Inc</orgName>
								<address>
									<addrLine>Sindjeliceva 9</addrLine>
									<postCode>11000</postCode>
									<settlement>Belgrade</settlement>
									<country key="RS">Serbia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">H</forename><surname>Thiede</surname></persName>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Statistics</orgName>
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<addrLine>5730 S Ellis Ave</addrLine>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">14 Center for Computational Mathematics, Flatiron Institute</orgName>
								<orgName type="department" key="dep2">15 Bosch Research and Technology Center</orgName>
								<orgName type="institution">The University of Chicago</orgName>
								<address>
									<addrLine>5747 S Ellis Ave, 162 5th Ave</addrLine>
									<postCode>60637, 10010, 02139</postCode>
									<settlement>Chicago, New York, Cambridge</settlement>
									<region>IL, NY, MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Tijanic</surname></persName>
							<affiliation key="aff11">
								<orgName type="institution">Totient Inc</orgName>
								<address>
									<addrLine>Sindjeliceva 9</addrLine>
									<postCode>11000</postCode>
									<settlement>Belgrade</settlement>
									<country key="RS">Serbia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Torrubia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Willmott</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
								<address>
									<postCode>15222</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">P</forename><surname>Butts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Chemistry</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Cantock&apos;s Close</addrLine>
									<postCode>BS8 1TS</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Glowacki</surname></persName>
							<email>drglowacki@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Chemistry</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Cantock&apos;s Close</addrLine>
									<postCode>BS8 1TS</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Merchant Venturer&apos;s Building</addrLine>
									<postCode>BS8 1UB</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Intangible Realities Laboratory</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<addrLine>Cantock&apos;s Close</addrLine>
									<postCode>BS8 1TS</postCode>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaggle</forename><surname>Participants</surname></persName>
						</author>
						<title level="a" type="main">Daedeok-daero 989beon-gil</title>
					</analytic>
					<monogr>
						<title level="j" type="main">MINDS AND COMPANY</title>
						<meeting> <address><addrLine>Yuseong-gu, Daejeon, Republic of Korea; Nambusunhwan-ro, Gangnam-gu, Seoul 06267, Republic of Korea</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">111</biblScope>
							<biblScope unit="issue">34057</biblScope>
						</imprint>
					</monogr>
					<note>21 Participants are listed at</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rise of machine learning (ML) has created an explosion in the potential strategies for using data to make scientific predictions. For physical scientists wishing to apply ML strategies to a particular domain, it can be difficult to assess in advance what strategy to adopt within a vast space of possibilities. Here we outline the results of an online community-powered effort to swarm search the space of ML strategies and develop algorithms for predicting atomic-pairwise nuclear magnetic resonance (NMR) properties in molecules. Using an open-source dataset, we worked with Kaggle to design and host a 3-month competition which received 47,800 ML model predictions from 2,700 teams in 84 countries. Within 3 weeks, the Kaggle community produced models with comparable accuracy to our best previously published 'in-house' efforts. A meta-ensemble model constructed as a linear combination of the top predictions has a prediction accuracy which exceeds that of any individual model, 7-19x better than our previous state-of-the-art. The results highlight the potential of transformer architectures for predicting quantum mechanical (QM) molecular properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rise of machine learning (ML) in the physical sciences has created a number of notable successes, (1-7) and the number of published outputs is increasing substantially. (8) This explosion is perhaps not entirely surprising, given that ML 'search space' is effectively infinite. For example, the performance of a particular ML algorithm strategy depends sensitively on at least four components: (a) the dataset used for training (and the corresponding methodology used for dataset curation); (b) the feature selection used to construct ML inputs; (c) the choice of ML algorithm; and (d) the values of the optimal constituent hyperparameters. For components (b) and (c), the space of possibilities is continually expanding; for components (a) and (d), the space of possibilities is potentially infinite. Given the sensitivity of ML approaches to each of the items outlined above, ML's explosion within the scientific literature has led to warnings of an emerging computational reproducibility crisis, a risk exacerbated by the fact that many peerreviewed ML publications do not include the data and algorithms required to reproduce their results. <ref type="formula">(9)</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* lars. <ref type="bibr">bratholm@bristol.ac.uk, drglowacki@gmail.com</ref> The rise of machine learning (ML) has created an explosion in the potential strategies for using data to make scientific predictions. For physical scientists wishing to apply ML strategies to a particular domain, it can be difficult to assess in advance what strategy to adopt within a vast space of possibilities. Here we outline the results of an online community-powered effort to swarm search the space of ML strategies and develop algorithms for predicting atomic-pairwise nuclear magnetic resonance (NMR) properties in molecules. Using an open-source dataset, we worked with Kaggle to design and host a 3-month competition which received 47,800 ML model predictions from 2,700 teams in 84 countries. Within 3 weeks, the Kaggle community produced models with comparable accuracy to our best previously published 'in-house' efforts. A meta-ensemble model constructed as a linear combination of the top predictions has a prediction accuracy which exceeds that of any individual model, <ref type="bibr">7-19x</ref> better than our previous state-of-the-art. The results highlight the potential of transformer architectures for predicting quantum mechanical (QM) molecular properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rise of machine learning (ML) in the physical sciences has created a number of notable successes, <ref type="bibr" target="#b46">(1)</ref><ref type="bibr">(2)</ref><ref type="bibr">(3)</ref><ref type="bibr">(4)</ref><ref type="bibr">(5)</ref><ref type="bibr">(6)</ref><ref type="bibr">(7)</ref> and the number of published outputs is increasing substantially. <ref type="bibr">(8)</ref> This explosion is perhaps not entirely surprising, given that ML 'search space' is effectively infinite. For example, the performance of a particular ML algorithm strategy depends sensitively on at least four components: (a) the dataset used for training (and the corresponding methodology used for dataset curation); (b) the feature selection used to construct ML inputs; (c) the choice of ML algorithm; and (d) the values of the optimal constituent hyperparameters. For components (b) and (c), the space of possibilities is continually expanding; for components (a) and (d), the space of possibilities is potentially infinite. Given the sensitivity of ML approaches to each of the items outlined above, ML's explosion within the scientific literature has led to warnings of an emerging computational reproducibility crisis, a risk exacerbated by the fact that many peerreviewed ML publications do not include the data and algorithms required to reproduce their results. <ref type="bibr">(9)</ref> The difficulty of searching an enormous ML space is compounded by the fact that the training of even simple neural networks has been shown to be an NP-complete problem. <ref type="bibr">(10)</ref> Deciphering whether any global optima lurk within an effectively infinite ML search space has been the topic of a great deal of research; however, there seems to be a consensus emerging that it is practically impossible to demonstrate that any particular ML strategy is in fact optimal or bias-free, even for very simple systems. <ref type="bibr">(11)</ref> Broadly speaking, the parameter spaces in which a particular ML strategy can be constructed are non-convex, and characterized by multiple local minima and saddle points in which optimization algorithms can get trapped. <ref type="bibr">(12)</ref> Nevertheless, ML algorithms can produce useful results. In a nod to the 1950 Japanese period drama "Rashomon" (where various characters provide subjective, alternative, self-serving, yet compelling versions of the same incident), ML's tendency to produce many accurate-but-different models has been referred to as the "Rashomon effect" in machine learning. <ref type="bibr">(13)</ref> In such a vast space, any individual agent has a chance of stumbling upon a reasonable ML model. Given the difficulty of rationalizing the uniqueness of any particular ML model or approach, individual models are increasingly being used as constituents within ensemble models, whose combined accuracy outperforms that of any individual model. <ref type="bibr">(14)</ref> Over the last several years, a number of studies have demonstrated the utility of 'crowd-sourced' approaches for solving scientific problems which involve searching hyperdimensional spaces. <ref type="bibr">(15)</ref><ref type="bibr">(16)</ref><ref type="bibr">(17)</ref><ref type="bibr">(18)</ref><ref type="bibr">(19)</ref> Inspired by recent attempts within both particle physics <ref type="bibr">(20,</ref><ref type="bibr">21)</ref> and materials science <ref type="bibr">(22)</ref> using community power to develop ML algorithms, we worked with Kaggle (an online platform for ML competitions), to design a competition encouraging participants to develop ML models able to accurately predict QM nuclear magnetic resonance (NMR) properties from 3D molecular structure information. <ref type="bibr">(23)</ref> The fact that some of our authorship team had worked in this area over several years <ref type="bibr">(24)</ref> meant that we had quantitative and qualitative benchmarks to analyse competition progress in relation to what conventional academic research approaches had achieved. The so-called 'Champs Kaggle Competition' (CKC) ran from 29-May-19 through 28-Aug-19. The 5 models which achieved the highest accuracy were awarded respective prize money of $12.5k, $7.5k, $5k, $3k and $2k. Over ~13 weeks, the CKC received 47,800 model predictions from 2,700 teams in 84 countries <ref type="figure" target="#fig_0">(Fig 1a)</ref>, representing the most exhaustive search to date of ML strategies aimed at predicting QM NMR properties from 3D molecular structure information. The number of participants who engaged with the CKC was amongst the highest for any physical science challenge which Kaggle has hosted to date. <ref type="figure" target="#fig_0">Fig 1b and 1c</ref> show a steady increase in the number of participants who joined the CKC versus time. CKC participants reported being drawn to the competition because it: (a) facilitated progress on an important research problem; (b) involved a rich, noise-less dataset whose structure was easy to understand; and (c) had a dataset which was manageable using standard data processing tools, workflows, and hardware. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Competition Design 2.1. Domain</head><p>NMR is the dominant spectroscopic technique for determining 2D and 3D molecular structure in solution. Amongst the most important data obtained in an NMR spectrum are the chemical shifts (which describe the position/frequency of a signal in the spectrum) and the scalar couplings (which determine the splitting/shape of the signal in the spectrum). ML methods to predict NMR properties are established in academic and commercial workflows for determining 2D molecular structure from experimental NMR datasets. <ref type="bibr">(25)</ref><ref type="bibr">(26)</ref><ref type="bibr">(27)</ref><ref type="bibr">(28)</ref> Despite this success, these 2D approaches often fail when the NMR properties are affected by 3D structure, for example atoms are separated by several bonds yet remain close in 3D space (ring current effects, hydrogen bonding etc.). This is an inherently difficult problem as the 3D molecular structure is simply not well described by 2D representations and there are not enough highquality experimental data available to accurately infer most 3D relationships from a 2D structural representation alone.</p><p>The most accurate computed predictions of NMR properties use QM methods like density functional theory (DFT) to get a one-toone mapping between a 3D structure and the contribution it has to the experimentally observed NMR property. Accurate QM methods for NMR property predictions are powerful but expensive. Recent work has thus focussed on developing ML algorithms which can efficiently reproduce the results of costly QM methods, achieving results in seconds rather than hours or days. <ref type="bibr">(24,</ref><ref type="bibr">29)</ref> ML approaches have the added appeal that they can be trained using large datasets of DFT-computed NMR parameters, which are not limited to experimental structural observations. With a large enough training database, we have shown in previous work that an ML strategy can approach the accuracy of DFT calculations of atom-centered NMR parameters such as chemical shift for 3D structure analysis, but with several orders of magnitude reduction in time. <ref type="bibr">(24)</ref> Beyond NMR, the last decade has seen considerable effort focused on machine learning QM molecular properties. <ref type="bibr">(30)</ref><ref type="bibr">(31)</ref><ref type="bibr">(32)</ref><ref type="bibr">(33)</ref><ref type="bibr">(34)</ref><ref type="bibr">(35)</ref><ref type="bibr">(36)</ref> Broadly speaking, this work has tended to focus on predicting atomic properties such as partial charges, or molecular properties such as energies and dipoles. Relatively little work has been carried out designing ML models which are able to predict pairwise atomic properties such as scalar coupling constants. Our earlier work to develop pairwise property prediction algorithms were effectively independent-atom treatments, in which atomic feature vectors describing the local environment of each atom were concatenated. <ref type="bibr">(24)</ref> However, this approach loses information about the relative position/orientation of each atom's respective environment, which is important for multiple-bond couplings. The CKC represents an attempt to kickstart research into ML methods able to make accurate prediction of pairwise properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset &amp; Scoring</head><p>Scalar couplings are critically dependent on the 3D structure of the molecule for which they are being measured; however at the time we carried out this work, we were unaware of accurate experimental databases linking pair-wise mutiple-bond NMR scalar couplings to well-defined 3D molecular structures. Therefore, we decided to run the CKC utilizing molecular structures included in the QM9 dataset, a publicly available benchmark for developing ML models of 3D structure-property relationships. (37) QM9 includes ~134k molecules comprised of carbon, fluorine, nitrogen, oxygen and hydrogen. The molecules included within QM9 have no more than 9 heavy atoms (non-hydrogen), with a maximum of 29 total atoms. To obtain a corresponding set of scalar couplings, we extended the QM9 computational methodology, using the B3LYP functional <ref type="bibr">(38)</ref> and the 6-31g(2df,p) basis set <ref type="bibr">(39)</ref><ref type="bibr">(40)</ref><ref type="bibr">(41)</ref><ref type="bibr">(42)</ref> to compute NMR parameters on the optimized QM9 structures. The computed QM9 scalar coupling constants are available under Creative Commons CC-NC-BY 4.0, enabling others to build on this work.</p><p>To remove the possibility of CKC participants overfitting their models to the entire set of computed QM9 scalar couplings, 65% of molecules in the dataset were randomly partitioned into a training set and the other 35% to a testing set. The test set was further split, with 29% of the data in a 'public' test set, and 71% of the data in a 'private' test set (competitors were unaware of the specifics of the private/public split). Both the training and test sets included the molecular geometries and indices of the coupling atoms. Unlike the test set, the training set included a range of other data, including the calculated scalar coupling values, their breakdown into Fermi contact (FC), spin-dipole (SD), paramagnetic spin-orbit (PSO) and diamagnetic spin-orbit (DSO) components, and a range of auxiliary information obtained from the QM computations (e.g., potential energy, dipole moment vectors, magnetic shielding tensors and Mulliken charges). As the CKC progressed, participating teams continually iterated and improved their models. A regularly updated and publicly visible leaderboard enabled each team to see where their model ranked in predicting the public test set data compared to the model predictions made by all of the other teams.</p><p>The leaderboard scores were determined using a function which accounted for the 8 different types of coupling constants included in the training and testing datasets: 1 JHC, 1 JHN, 2 JHH, 2 JHC, 2 JHN, 3 JHH, <ref type="bibr">3</ref> JHC and 3 JHN (where the superscript indicates the number of covalent bonds separating the atom pairs indicated by the subscript). Since the number of couplings of each type differed (e.g., the molecular composition of the QM9 test set included 811,999 3 JHC couplings compared to 24,195 1 JHN couplings) and spanned different value ranges, the scoring function used the average of the logarithm of the mean absolute error for each type of coupling constant:</p><formula xml:id="formula_0">score = 1 ( ) log , 1 -! )|/ " − / 1 " | # ! " 2 Eq (1) $ !</formula><p>where t is an index that runs over the T = 8 different scalar coupling types, i is an index that spans 1..nt, the number of observations of type t, yi is the scalar coupling constant for observation i, and / 1 " is the predicted scalar coupling constant for observation i. This scoring function ensures, for example, that a 10% improvement in one type of coupling will improve the score by the same amount as a 10% improvement in another type of coupling, so that no coupling class dominates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Leaderboard time evolution</head><p>Over the course of the CKC, <ref type="figure" target="#fig_1">Fig 2a</ref> shows the evolution of the best score whose source code was publicly available (public notebooks), and its relationship to the top score versus time. <ref type="figure" target="#fig_1">Fig 2b  shows</ref> that the time trace of the top score is well fit by a biexponential curve with two distinct phases. Phase 1 lasted for the first week, during which time the accuracy increased by ~12x (~2.5 improvement in score), with a time constant of ~1.29 ± 0.18 days. Phase 2 lasted for the next 12 weeks, during which time the accuracy improved more gradually by a factor of ~4x (~1.5 improvement in score), with a time constant of ~50.0 ± 16.6 days.</p><p>To determine which models were awarded prize money, the final set of model rankings were assessed using Eq (1) to evaluate how well each of the models predicted the scalar coupling values in the private test set (preventing competitors inferring the target property from the leaderboard scores rather than from the training set). Due to the large amount of noise-less data, the positioning in the top 37 submissions was the same on the public and private leaderboard at the end of the CKC. Several teams commented that the stability between the public &amp; private leaderboards made for an enjoyable competition.</p><p>The top-scoring method achieved a geometric mean error (exponential of the score) of 0.039 Hz which was 6-16x more accurate than what could be achieved using our own recently developed methodology (see SI for details). <ref type="bibr">(24)</ref> In addition to the final score, Kaggle also rewards participants who make the best contributions to: (1) publicly available code, and (2) the discussion forums. As a result of these incentives, a number of participants opted to voluntarily publish their source code (public notebooks). In many cases, the public notebooks were then utilized and adapted by other CKC participants. As shown in <ref type="figure" target="#fig_1">Fig 2a,</ref> the best score achieved using these public notebooks follows a time trace which is similar to the leading score, but less accurate by ~1.5. A number of participants made instructional web posts, scripts, and videos outlining specific approaches which they had taken during the CKC. For example, video presentations by Andrey Lukyanenko <ref type="bibr" target="#b43">(43)</ref> and the NVIDIA team (44) discuss the approaches which they utilized to develop the 8 th and 33 rd place solutions, respectively. The CKC summary features insightful write-ups by several top teams in which they describe their various model approaches. <ref type="bibr" target="#b45">(45)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meta-Ensemble Model</head><p>To assess the extent to which the prize-winning submissions differed from one another (and other highly ranked submissions), we used the top 400 submissions to construct a meta ensemble (ME) model as a linear combination of the top scoring models:</p><formula xml:id="formula_1">y ",&amp;' = ) 9 ( y ",( Eq (2) )** (+,</formula><p>Given that many of the top models (and all of the prize winners) were ensemble models, we have adopted the term "meta-ensemble" (ME) to emphasize the fact that Eq <ref type="formula">(2)</ref> is an ensemble of ensemble models. In Eq (2), the ME prediction yi,ME of the i'th scalar coupling constant is a linear combination of the predictions yi,j of the j'th ranked model. The index k specifies the lowest ranked model to be included within the optimized ME model. When k = 1, Eq (2) runs over the entire list of the top 400 models. When k &gt; 1, Eq (2) neglects top-scoring models. Setting k = 6 for example, the Eq (2) ME model excludes all of the prize-winning models (ranks #1 -#5). For ME models constructed using Eq (2), the weights wj were determined by minimizing yi,ME using half of the test set, under the constraint that the weights were positive and summed to unity. While a range of different ME models can be constructed (e.g., different ensembles for each type of coupling, median averaging etc.), this simple mean is easy to interpret. Different classes of machine learning algorithms (or even the same algorithm with different hyperparameters) may be able to learn different regions of the data better than others. Thus, by combining the highest scoring model predictions that have the least correlation for a meta-ensemble, the strengths of various models may be accumulated, a result confirmed by the ME analysis shown in <ref type="figure" target="#fig_2">Fig  3a</ref> as a function of k. As expected (for k = 1..300) the optimized ME model achieves an accuracy which always surpasses that of the best individual model. In the regime where the top scorers are incrementally being eliminated from Eq (2) (k = 1..50), <ref type="figure" target="#fig_2">Fig 3a</ref> shows that the ME model has a score that is ~0.2 lower than the "best" model. For example, the k = 7 ME model (which neglects the top 6 models) still outperforms the winning solution, and the k = 11 ME model outperforms the winning solution when the per-type ensemble mentioned above is used. <ref type="figure" target="#fig_2">Fig 3b shows</ref> how many contributors to the ME model at a particular k value had an optimized weight greater than 0.01. Broadly speaking, the <ref type="figure" target="#fig_2">Fig 3a  results</ref> can be lumped into three regimes. In the first regime (k ~ 1..40) the best performing methods dominate the ME and there is little to be gained by including within the ME methods that are very different if they perform worse. In the second regime (k ~ 41..200), <ref type="figure" target="#fig_2">Fig 3a</ref> shows that the gap between the top score and the ME model widens to ~0.4. Here there are many similarly performing yet different methods, so there is much to be gained by combining their different approaches into a ME. In the third regime (k ~ 201..300) the gain from a ME decreases, presumably because many of the models are similar variants of the public notebooks. The relative benefit of constructing a ME model (versus using a top-scoring model) thus appears to be more significant outside of the band of top-scoring and low-scoring models.</p><p>For the k = 1 ME model, which was 7-19x more accurate than our previously published model, <ref type="bibr">(24)</ref> we analysed in further detail its constituents. The results in <ref type="table" target="#tab_0">Table 1</ref> show the k = 1 ME constituents with weights wi &gt; 0.02, along with the relative rankings j of the constituent ME models. <ref type="table" target="#tab_0">Table 1</ref> shows that there is no particular model which is dominant: there are five models with a weighting greater than 0.11, and three with a weighting greater than 0.20. Of the six models in <ref type="table" target="#tab_0">Table 1</ref>, one (#12) falls outside the top 5. Its 0.149 contribution is larger than prize winning models #3 and #5. <ref type="figure" target="#fig_3">Fig 4a</ref> shows the submission history of the <ref type="table" target="#tab_0">Table 1</ref> models, and their relationship to the overall public leader board. </p><formula xml:id="formula_2">N N Y N N N Translational invariance? Y Y N Y Y Y Rotational invariance? Y N N Y Y Y Previous Kaggle experience? N Y Y N Y N Included additional input features? Y N N Y Y N Number of model parameters ~105M ~60M ~70M ~60M ~66M ~250K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correlation Analysis</head><p>To further understand the relationship between the winning submissions within the k = 1 ME model, we carried out a correlation analysis on the top 50 team submissions. The submissions were then ordered using a hierarchical clustering analysis (see SI). The results in <ref type="figure" target="#fig_3">Fig 4b</ref> show that the #1 -#5 teams are part of the same sub-cluster i.e. all relatively similar to each other. <ref type="figure" target="#fig_3">Fig 4c specifically</ref> highlights the low correlation between models #1 -#5 compared to model #12, which shows that this team's approach exists within a region of ML strategy space that appears relatively distinct from the prize-winning models, and also from the top 50 solutions.</p><p>Compared to the others in <ref type="table" target="#tab_0">Table 1</ref>, team #12 was a relative latecomer to the CKC as shown in <ref type="figure" target="#fig_3">Fig 4a.</ref> In addition, the number of parameters in their model is ~100x smaller than the others. The low correlation of team #12 compared to the other teams in <ref type="table" target="#tab_0">Table 1</ref> appears to have arisen because they utilized the 'Cormorant' rotationally covariant neural network strategy. (46) Originally developed for learning molecular potential energy surfaces (PESs), Cormorant takes advantage of rotational symmetries in order to enforce physical relationships in the resultant neural network, by using spherical tensors to encode local geometric information around each atom's environment, which transform in a predictable way under rotation. The use of spherical tensors allows for a network architecture that is covariant to rotations, so that if a rotation is applied to a layer, all activations at the next layer will automatically inherit that rotation. As such, a rotation to a Cormorant input will propagate through the network to ensure that the output transforms as well. This captures local geometric information while still maintaining the desired transformation properties under rotations. Team #12's sophisticated input processing strategy contrasts with the approaches taken by other teams, which tended to utilize far simpler encoding strategies, either by restricting the input features to have translational and rotational invariance (e.g., using internal distances), adding translational and rotational noise to make the inputs robust to rotation and translation, or allowing the model learn invariance on its own. Team #12's approach is grounded in domain specific physics knowledge, and characteristic of the emphases which physical scientists tend to apply in ML contexts.  <ref type="table" target="#tab_0">Table 1</ref> teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion &amp; Conclusions</head><p>Community-powered approaches offer a powerful tool for searching ML strategy space and providing accurate predictions for physical science problems like the prediction of 2-body QM NMR properties. Within 3 weeks, the best score on the Kaggle public leader board achieved an accuracy which surpassed our own previously published approaches, <ref type="bibr">(24)</ref> suggesting that an open source community-powered 'swarm search' of ML strategy space may in some cases be significantly faster and more cost-efficient than conventional academic research strategies where a single agent (e.g., a PhD student or post-doctoral researcher) spends several years hunting for solutions in an infinite search space. ME model construction combined with correlation analysis highlights the strength of the CKC 'swarm search' approach, in line with the "Rashomon effect".</p><p>Whereas our earlier approaches to predicting NMR structure coupling constants (24) had relied on kernel-ridge regression approaches (47) where the internal distances and angles in the molecules were systematically encoded to a feature vector for the coupling atom pairs using predetermined basis functions, the community which emerged around the CKC pioneered a new application of transformer neural nets (48) to QM molecular property prediction. While such networks have found extensive use for sequence modelling and transduction problems such as language modelling and machine translation, they represent a relatively new approach to predicting QM properties like NMR shifts or scalar couplings, and it will be interesting to explore their further application to other QM properties and more general 2-body property prediction problems, which are relevant in several domains across the physical sciences. The rich portfolio of open source blog posts, data, insight, source code, and discussions arising from the CKC offers an excellent foundation for subsequent research and follow-up studies, through community initiatives or more conventional academic research approaches.</p><p>Teams #2 and #5 had no domain specific expertise, and yet outperformed participants with domain expertise, including our own previous attempts. <ref type="bibr">(24)</ref> This contrasts with previously published Kaggle competitions in particle physics <ref type="bibr">(20,</ref><ref type="bibr">21)</ref> and materials science, <ref type="bibr">(22)</ref> where the winners tended to be domain experts. <ref type="table" target="#tab_0">Table  1</ref> shows that teams with prior domain expertise (e.g., #1 and #4) used their insight to calculate additional input features beyond those which we provided, and which they then used as model input. For example, team #1 used Mulliken charges and atomic valency, while team #4 used electronegativity, first ionization energy, electron affinity, mulliken charge, and bond types. Despite this added complexity, team #1 only narrowly managed (i.e., within the CKC's final hours) to improve on the approach of team #2, which used a simple cartesian input representation with no additional data.</p><p>All of the prize winning teams utilized deep neural networks where the encoder learned the pair-feature vectors from the coordinates, atom types, distances, etc. A separate feed forward neural network (decoder) was then used to make scalar coupling predictions per coupling type or sub coupling type. The relatively simple input descriptions used by many of the top teams transferred to the neural network the challenge of learning an effective input representation. Such approaches contrast with those favored by physical scientists, which utilize more complex descriptors constructed so as to include domain specific insight (e.g., rotational symmetries for team #12). Taking advantage of the variance in approaches, the various model predictions can be combined into a ME model whose combined accuracy surpasses that of any individual model, 7-19x more accurate than what our previous methods were able to achieve. The benefit of a ME model seems to be most significant in the regime where there are many independent individual models with similar performance. <ref type="figure" target="#fig_1">Fig 2a</ref> shows that the average benefit which new models contributed to the overall improvement in prediction accuracy decreased versus time, with a rapid improvement over the first week, followed by a much more gradual improvement over the next 13 weeks. <ref type="figure" target="#fig_0">Fig 1c shows</ref> that the number of model predictions was approximately constant versus time with an increase over the final 20 days. These observations indicate an overall decrease in the relative cost/benefit ratio as a function of time. This cost/benefit decrease is qualitatively compatible with conclusions drawn from previous meta-analyses of scientific progress, (49) which suggest that search strategies for scientific discovery tend to become less efficient with time. In our case, these results suggest that a shorter competition may have furnished similar insights. The results also highlight potential shortcomings in the elaborate scheme of awards and prizes which scientific disciplines utilize to incentivize progress and recognize 'top-performers' -e.g., the fact that solution #12 played a more important role in the optimized ME model compared to some of the prize winning models offers an important reminder that scientific progress is a community effort that depends on a range of important contributions, which can often go unrecognized in conventional indicators of prestige.</p><p>The results of this study demonstrate how community science initiatives in conjunction with open data can enable rapid scientific progress in ML domains, reaffirming the community benefits that can arise when scientific workers make their data and algorithms open. Web-based platforms enable distributed community efforts to build engagement with scientific concepts at a time where scientific approaches face mounting challenges across media and political landscapes. Given the constraints on conventional scientific collaboration which have arisen as a result of social distancing, distributed scientific community efforts like these may become more prevalent in the near term. For example, there has been a steady increase in the number of scientific stack exchanges, which (like Kaggle) incentivize scientific communities to share knowledge and expertise. Digital platforms which benefit from the ubiquity of cloud computing and which enable distributed communities to engage with one another to undertake collective problem solving are likely to play an important role in our emerging scientific future. Such approaches may be particularly useful for problems like ML, where the strategy spaces are effectively infinite. Moving forward, it will be interesting to explore the extent to which search efficiencies might be enhanced by combining the intelligence of human agents with machine agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>LAB and DRG devised the project concept, conceived various analysis strategies, and organized overall execution of the project work strands (LAB early phases, DRG late phases). LAB, WG, DRG, CPB, AH and WR worked together to design the form of the CKC. LAB computed and curated the CKC data, managed the CKC, responded on the forums, engaged with the winners, and organized a discussion workshop. LAB carried out data analyses with assistance of WR and WG, and guidance from DRG. AH and WR mounted the project on the Kaggle platform, and advised on how to run the CKC. The following CKC participants contributed to discussions which fed into the paper, as well as contributed to solution code and write-ups which are included in the SI: ZK, DW, JPM, MK &amp; SB (team #1); AT &amp; PH (team #2); SC, SK, Youhan Lee, Youngsoo Lee &amp; WS (team #3); GR, MP, NT &amp; LS (team #4); LD, GH &amp; TTN (team #5); and BA &amp; ET (team #12). DRG wrote the initial paper draft based on the data analysis, with subsequent input from LAB, CPB, and WG. DRG and CPB organized project funding.</p><p>Competing Interests: This competition was made possible through financial support from Kaggle, where AH and WR are employees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and materials availability:</head><p>All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. Additional data related to this paper may be requested from the authors. Supporting Information for A community-powered search of machine learning strategy space to find NMR property prediction models S1 Overview Section S2 and S3 contains details on how the dataset was generated, and what considerations went into the choices made. Section S4 describes the data set available to the partitipants, as well as details on how submissions were scored. Section S5 contains various plots and analysis strategies that didn't make it into the main paper, as well as details on how the ensembles were fitted and the correlations were calculated. Section S7, S8, S9, S10, S11 and S12 contain detailed write-ups explaining the model architecture of the five winning teams as well as the 12th placed team. Finally, section S13 outlines the code and data available for download, including all the winning solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Dataset considerations</head><p>We opted to use structures from the QM9 dataset <ref type="bibr" target="#b46">[1]</ref> to construct our own dataset as well as the same computational methodology. QM9 consists of around 130,000 molecules comprised of carbon, fluorine, nitrogen, oxygen and hydrogen. Each molecule has up to 9 heavy atoms (non-hydrogen) and up to 29 total number of atoms. Each structure was optimized by the authors using Density functional theory (DFT), with the B3LYP functional <ref type="bibr">[2]</ref> and the 6-31g(2df,p) basis set <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>. There were several advantages of basing our dataset on QM9, as well as several possible disadvantages that needed to be considered. QM9 has historically had an important role as a benchmark dataset in the development of machine learning models for 3D structure-property relationships, and we deemed it beneficial to augment this with magnetic properties. This is particularly true for pairwise properties like scalar coupling constants, as there to our knowledge currently does not exist any dataset that includes pair-wise properties. Using an existing set of structures also eased the workload of generating a dataset considerably. We initially had two primary concerns about using 1 QM9 structures specifically and one primary concern about using computational methods in general.</p><p>• The QM9 molecules are quite small compared to molecules commonly used as e.g. medicines, and the solutions to the competition might not work (due to memory use etc.) on larger molecules. However, since the strength of atomic interactions decay with distance (with the exception of large aromatic systems), we believe that we could ultimately modify the solutions to scale better with system size by introducing an interaction distance cuto↵. At this time, we have not researched the current size limit of the winning solutions.</p><p>• All of the QM9 molecules have been optimized to form a local minimum on the potential energy surface (equilibrium). In the future we might be interested in studying non-equilibrium structures, and the solutions might perform less well on these (such as methods ignoring 3D relationships, and that relies entirely on connectivity). All of the top solutions incorporated 3D relationships, and at this time we have no reason to believe that any of them will perform poorly on a non-equilibrium dataset.</p><p>• By relying on quantum chemistry calculations that directly relate a molecular 3D structure to the target property, participants could guess the specific methodology and compute the target properties of the test set, thus knowing the correct answer. Due to the computational program not being widely available, but more importantly due to the high computational cost (which directly translates to monetary cost), we expected that it was unlikely that anyone would 'cheat' in this way. This is particularly true as it was a requirement of the competition that the source code to the solution was made accessible to the organizers for a team to be eligible for any prize money.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3 Dataset generation</head><p>Of the 130,831 molecules in QM9 that passed the geometry consistency check, we removed an additional 42 molecules as these had no hydrogen atoms. The remaining structures were modified to a valid xyz format and input files for Gaussian NMR computations were constructed. The scalar coupling constants (including contributions from separate terms), dipole moment vectors, nuclear shielding tensors, mulliken charges and potential energy were parsed from the Gaussian output files and the dataset was written in an easily accessible csv format. The molecule structures were made available in xyz formatted files as well as a single csv file. While the target observable was the scalar coupling constants, the auxiliary data was included in case additional learning from these could be achieved. A further 14 molecules were removed due to one or more of the scalar couplings being a big outlier compared to the range seen in the remaining molecules. For a small subset of the remaining molecules (108) Gaussian automatically enabled symmetry in the DFT computations. This had little e↵ect on most of the observables, however the direction of the dipole vector and the nuclear shielding tensor became arbitrary as a result of this (however the norm of the vector and trace of the matrix was still correct). This was discovered when the competition was ongoing, but since it only a↵ected a small subset of the auxiliary data, the competition data was not updated with data computed with symmetry disabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4 Competition details</head><p>The goal of the competition was to predict the scalar coupling constants from the interaction between a hydrogen and a hydrogen, carbon or nitrogen atom 1,2 or 3 bonds apart. This was to be done without the competitors knowing any information other than the coordinates and type of element of the atoms in the molecules. The dataset was randomly split into a 65%/35% training/test split. The scalar coupling distributions seemed to be very similar with random splits so stratified splits did not seem necessary.</p><p>The competitors were provided the following files:</p><p>• structures.csv, which contains the coordinates and element type of each of the 2,358,657 atoms of the 130,775 molecules that constitutes the combined test and training set.</p><p>• train.csv, which contains the values in Hz of the 4,658,147 scalar coupling constants in the training set as well as which atom pairs in which molecule the coupling is between.</p><p>• test.csv, which contains 2,505,542 pairs of atoms and their respective molecules, which the competitors had to predict the scalar coupling constants of.</p><p>• potential energy.csv, which contains the potential energy in Hartree of the 85,003 molecules in the training set.</p><p>• magnetic shielding tensors.csv, which contains the XX, XY, XZ, YX, YY, YZ, ZX, ZY and ZZ components of the magnetic shielding tensors in ppm of the 1,533,537 atoms in the training set.</p><p>• mulliken charges.csv, which contains the Mulliken charges in atomic units of the 1,533,537 atoms in the training set.</p><p>• dipole moment.csv, which contains the dipole moments in Debye of the 85,003 molecules in the training set.</p><p>• scalar coupling contributions.csv, which contains the FC, SD, PSO and DSO components in Hz of the 4,658,147 scalar coupling constants in the training set. The sum of these equates to the values in train.csv.</p><p>The test set was further split into a 29%/71% public/private test set. All submissions were scored immediately on both splits. The public score was made available on the public leaderboard, while the private score (which the winners were determined from) were hidden until the end of the competition. here were no changes in the top 37 placements between the public and private leaderboard, indicating that there was little noise in the data. We opted to use a custom score function to evaluate the submissions. The dataset included 8 di↵erent types of coupling: 1JHC (coupling between a hydrogen and a carbon separated by 1 covalent bond), 1JHN, 2JHH, 2JHC, 2JHN, 3JHH, 3JHC and 3JHN. Since the number of couplings of each type di↵ered dramatically (811,999 3JHC couplings in the test set, but only 24,195 1JHN) and spanned di↵erent ranges, a bad choice of scoring metric could easily be dominated by the performance on the 3JHC coupling constants. Our loss function is the logarithm of the geometric mean of the mean absolute error for each type:</p><formula xml:id="formula_3">score = 1 T T X t log 1 n t nt X i |y i ŷ i | ! (S1)</formula><p>Where:</p><p>• T is the number of scalar coupling types.</p><p>• n t is the number of observations of type t.</p><p>• y i is the actual scalar coupling constant for the observation i.</p><p>•ŷ i is the predicted scalar coupling constant for the observation i.</p><p>This way, a 10% improvement in one type of coupling will improve the score by the same amount as a 10% improvement in another type of coupling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5 Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.1 Ensembling</head><p>We created an ensemble of the top 400 submissions to see which submissions contributed the most. Of the top 400 submissions, 18 were removed due to being duplicates. The data points (the test set referenced above) were split into a training and test set of equal size, stratified by coupling type using the Scikitlearn library <ref type="bibr">[7]</ref>. We did a linear fit to the submissions using TensorFlow <ref type="bibr">[8]</ref>, minimizing the loss function in equation S1 under the constraint that the weights were positive and summed to 1. 6 submissions had weights of larger than 0.02 as shown in <ref type="table" target="#tab_0">Table S1</ref>. Similarly we did separate linear fits for each coupling type that minimized the mean absolute error. A comparison of the performance of the individual teams and the ensembling strategies described above is shown in <ref type="table">Table S2</ref> Table S1: Competition rank, team name and individual weight of 6 teams whose submission had a weight greater than 0.02 in the ensemble.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.2 Correlation</head><p>We investigated how similar the submissions from the top teams were by computing the correlation between a scaled subset of their submissions on the test data. 20,000 data points for each coupling type were drawn randomly from the test set. For each coupling type the submissions were the scaled by their individual root mean square error (RMSE). These two pre-processing steps were done to make each coupling type have an equal impact on the correlation. The same analysis was done on the top 50 team submissions where the submissions were clustered with hierarchical clustering using the 'complete' linkage in the Scipy module <ref type="bibr">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.3 Manifold</head><p>In a similar analysis, we projected the top 100 submissions of a scaled subset of the test set down on a two-dimensional manifold. Again, 20,000 data points for each coupling type were drawn randomly from the test set. As the mean absolute error (MAE) is a more natural metric in relation to the scoring metric used in the competition, the submissions were for each coupling type scaled by their individual MAE. This pre-processing is important as if the submissions weren't scaled appropriately, the dimensionality reduction might mainly capture the average performance of a given submission, rather than how the methods di↵er in general. <ref type="figure" target="#fig_0">Figure S1</ref> shows the submissions projected onto a two-dimensional Multidimensional Scaling (MDS) manifold (which tries to preserve the Manhattan distance between submissions) <ref type="bibr">[7,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.4 Comparison with previous competitions</head><p>To get an idea of how engaging the competition was to the community (and earlier how much participation we could expect), we looked at how the number of participating teams have historically depended on the prize pool. We retrieved the number of participating teams for all previous competitions with a monetary prize and converted the prize into US dollars. Additionally we restricted ourselves to competitions where everyone could enter, where the number of teams were listed, that were not of recruitment or code-type, and competitions that took place within the last five years. <ref type="figure" target="#fig_1">Figure S2</ref> shows how this competition compare to previous ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5.5 Participant engagement</head><p>We looked at how many days separated each participating teams' first and last submission, to get an overview over the average engagement. <ref type="figure" target="#fig_2">Figure S3</ref> shows that many teams concentrated their e↵orts (or did only a single submission) over 1-2 days. However, <ref type="figure" target="#fig_3">Figure S4</ref> truncates any team that made continuous submissions over a period longer than 3 weeks into a single bin, showing that the majority of the teams were engaged throughout the competition <ref type="figure" target="#fig_2">Figure S3</ref>: Histogram showing the number of days between a teams' first and last submission. <ref type="figure" target="#fig_3">Figure S4</ref>: Histogram showing the number of days between a teams' first and last submission, where the rightmost bin indicate teams with more than 3 weeks between first and last submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6 Comparison to IMPRESSION</head><p>In the manuscript we provide comparisons to our previously published methodology (IMPRESSION <ref type="bibr">[13]</ref>). We report ratio's in a range, where the upper bound are the best models that we have trained on the training set. These could likely be improved upon using higher memory machines. As the error in kernel methods tend to scale as a power law with number of data points, we extrapolated what the error theoretically could be in the limit of using the entire training data set. This estimate is used as the lower bound. <ref type="table">Table S3</ref> shows the score contribution of each coupling type for the best ensemble, the winning submission as well as scores for IMPRESSION best trained models and interpolated errors. Similarly <ref type="table" target="#tab_5">Table S4</ref> shows the mean absolute errors.   Of the provided data, we use only the element and position of each atom and the scalar coupling type of each bond. From these, we use the open source package RDKit <ref type="bibr">[14]</ref> to generate additional features for each atom, specifically bond order and partial charge. Below we list the total set of features of each molecular object that we use in the network.</p><p>• Atoms: element, number of neighbors, order of neighboring bonds, partial charge, angle with nearest neighbors. All atoms are included.</p><p>• Bonds: each atom's element, coupling type (if applicable), bond order (if applicable), distance. These are included regardless of whether there is a chemical bond or not.</p><p>• Triplets: each atom's element, bond angle. These are included only if there is a chemical bond (one central atom bonded to a pair of other atoms).</p><p>The methods described can be generalized to include quadruplets (and dihedral angles) as well, although the addition of quadruplets was found to not be helpful in this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.1.1 Molecular Representation</head><p>Deep learning approaches to problems in molecular modeling generally represent the molecule as a graph, with nodes representing atoms and edges representing bonds. Many of these are then passed through networks that can be described using the general framework of message passing neural networks (MPNN) <ref type="bibr">[15]</ref>, where information is usually passed through alternating convolutions, first from edges to nodes and then from nodes to edges. This representation is restrictive for several reasons. Most importantly, information may only be passed locally, as direct connections do not exist between, for example, pairs of atoms, or an atom and a bond far away in the molecule. Further, this molecular representation only allows us to use features that directly correspond to a particular atom or bond; for example, it is not clear how to incorporate features of atom triplets as input in a typical MPNN. In contrast to this framework, we represent each molecular object of interest (in this case, each atom, bond, and triplet, but we may incorporate other objects or properties of the molecule as we like) as a node in a complete graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2 Model Architecture</head><p>Our architecture is a deep learning approach with three stages: an embedding layer, several graph transformer layers, and element-wise group convolutions. Each of these is detailed in the subsections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2.1 Embedding</head><p>We use three di↵erent embedding layers, each corresponding to a type of molecular object (atom, bond, and triplet). All of these objects have a mixture of discrete and continuous features, and some of these features are hierarchical in nature (for example, bonds' coupling type and sub-type, which includes some local bond-order information). As such, we use hierarchical embeddings <ref type="bibr">[16]</ref>, which embed each feature separately, and then linear combine these embeddings to yield a single representation of the object. Discrete features are embedded in the usual fashion, while for continuous features we use a sine filter embedding similar to the position embeddings employed by Transformer models <ref type="bibr">[17]</ref>. We embed all features described in Section S7.1, with the notable exception of atom positions. Instead, we use these positions to construct a matrix of relative distances to be used later in the network; this ensures that network output is invariant to translation and rotation of the molecule. Subsequent layers in the network will convolve over all nodes in the graph; as such, we require that the output size of each of these embeddings be the same. It is worthwhile to note that after the embedding layer, all nodes are treated identically by the network -that is, the graph transformer layer makes no distinction between nodes represented by atoms, bonds, and triplets. Rather, all representation of molecular objects exist in the same space, and it is the job of the embedding layer parameters to learn representations that can be meaningfully distinguished by the remainder of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2.2 Graph Transformer Layers</head><p>Throughout this section, we let Z be a matrix that represents the hidden layer of a molecule, where each column of Z represents a particular node. The layer describe here, which we call the graph transformer layer, forms the foundational building block of our architecture. This layer is an extension of a graph convolution layer, which, in its most general form, may be represented as</p><formula xml:id="formula_4">GraphConvolution(Z) = (W ZA) (S2)</formula><p>where is an activation function, W is a matrix of learnable weights, and A is a static mixing matrix that encodes the structure of the graph; common choices for A include a normalized adjacency matrix or a graph Laplacian. Our first modification is the replacement of the mixing matrix A with a self-attention mechanism, which was popularized by the Transformer model <ref type="bibr">[17]</ref> for sequence modeling. In self-attention, the strength of message passing is computed via a parametrized inner product between nodes:</p><formula xml:id="formula_5">GraphSelfAttention = (W 3 Z softmax(Z T W T 1 W 2 Z))<label>(S3)</label></formula><p>Under this architecture, W3 learns the message to be passed, while the weight matrices W1 and W2 learn the strength of each message. Replacing the mixing matrix A in this manner removes the only structural information the network <ref type="figure">Figure S5</ref>: Graphical representation of the model architecture.</p><p>receives about the graph. To reintroduce this information, we incorporate a scaling based on the distance between each pair of nodes in the graph. Let D be a matrix such that D i,j represents distance between nodes i and j, and let be a learnable scalar parameter. Distance-scaled self-attention is given by</p><formula xml:id="formula_6">ScaledGraphAttention(Z) = (W 3 Z softmax(Z T W T 1 W 2 Z D)) (S4)</formula><p>This scaling has the e↵ect of reducing the strength of interaction between pairs of faraway nodes. The learnable parameter allows us to learn the intensity of this scaling: as approaches 0, relative distances are ignored, and as approaches 1, we operate on a sparse graph, with information only flowing between directly adjacent molecular objects. The scaled graph self-attention transformation is shown in <ref type="figure">Figure S5</ref>. As in the original Transformer model, we follow self-attention with a linear transformation, a residual connection, and layer normalization <ref type="bibr">[18]</ref>. The full graph Transformer layer is given by</p><formula xml:id="formula_7">GraphTransformerLayer(Z) (S5) = LayerNorm(W 4 ScaledGraphSelfAttention(Z) + Z) (S6) = LayerNorm(W 4 (W 3 Zsoftmax(Z T W T 1 W 2 Z D)) + Z) (S7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2.3 Defining Distances</head><p>The self-attention scaling described above requires a matrix D of relative distances between objects represented in the graph. However, since bonds and triplets do not have a well-defined position in space, it is not necessarily trivial to define distances among these objects. To do so, we begin with distances between two atoms a and a', denoted d atom (a, a 0 ), which we define as simple Euclidean distance between their positions. Distance involving larger sets of atoms are defined recursively, and with the guiding principle that the distance between two sets of atoms should be 0 if and only if one set is contained in the other. Distances are defined as below. In the following, let a be an atom, B be a bond connecting atoms b1 and b2, and C be a triplet containing center atom c1, other atoms c2 and c3, and bonds c12 and c13 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2.4 Group Convolutions</head><p>After being passed through some number of graph transformer layers, we take the nodes representing coupling bonds and pass them through a series of 1 ⇥ 1 convolutions inside of a residual block. These convolutions are grouped according to coupling type and bond order, such that the set of convolutional filters applied is di↵erent for each group. A final grouped convolution outputs the predicted scalar coupling constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.3 Training and Ensembling</head><p>In total, 13 of the models described in the previous section were trained on the task of predicting magnetic scalar coupling constants. Model size varied, but generally had between 12 and 18 graph transformer layers, and hidden dimension between 600 and 800. These models were initially trained using absolute loss instead of the loss defined in equation S1. The model was implemented in PyTorch <ref type="bibr">[19]</ref> and network training was done using the ADAM optimizer <ref type="bibr">[20]</ref> with a cosine annealing learning rate scheduler <ref type="bibr">[21]</ref> for 200 epochs. For some of our best models, we additionally fine-tuned by training for approximately 40 epochs using the loss defined in equation S1 to improve their single model scores by around -0.020 to -0.030. For each coupling subtype (i.e. coupling type, plus further breakdowns by element and chemical environment information), the targets were scaled to 0 mean and one standard deviation to simplify the learning process. These 13 models were ensembled with a mix of mean and median ensembling. First, for each coupling type, we observed how frequently each model's prediction was the median among all 13 predictions. The 9 best models according to this metric were selected, with the remaining 4 discarded. Finally, for each coupling constant, we selected the center 5 median values from among the 9 model predictions; the mean of these 5 values was used as the final prediction. We found that this ensembling strategy was more e↵ective than strict mean or median ensembling, and did not require a validation set to estimate individual models' test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 Solution 2 -Quantum Uncertainty</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8.1 Data Augmentation</head><p>The key to the success of this model lies in the data augmentation, and comes from the insight that pair-wise properties can be modelled as atom-wise properties, given that relative position to the paired atom is encoded in the input. This is achieved by making duplicate 'sibling' molecules that are translated to have a di↵erent atom as origin. For a given molecule of N atoms, where M of these are of atom type H, C or N, M siblings are constructed, each centered on a di↵erent H, C or N atom. Furthermore the sibling molecules are padded with dummy atoms to make all molecules have the same number of atoms (29 for this competition), which are later masked in the model. Each atom in the sibling molecules are then matched with the corresponding coupling constant label, where dummy values are used for coupling types that are not present in the dataset, and these are similarly masked later in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8.2 Input Features</head><p>A feature vector is constructed for each atom in the sibling molecules, that contain the Cartesian coordinate of the atom, the atom type index (C=0, H=1, ...) as well as an index for the type of coupling formed with the origin atom (1JCH=0, 2JHH=1, ...). In the feature vector 3 elements will be the X, Y and Z component of the Cartesian coordinate, while the remaining elements are used <ref type="figure">Figure S6</ref>: Overview of the overall architecture for embedding the atom type and coupling type. For a feature vector of e.g. length 1024, 510 elements will be repetitions of the atom type index, 511 will be repetitions of the coupling type index, while the remaining 3 elements will contain the Cartesian coordinates. Note that there is no graph information nor any other manually engineered features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8.3 Model Architecture</head><p>The model used a standard transformer architecture <ref type="bibr">[22]</ref> utilizing the fast.ai library <ref type="bibr">[23]</ref>, where each feature vector are updated with several transformer layers followed by a feed forward neural network to decode the scalar coupling constants from the transformed feature vectors (See <ref type="figure">Fig S6)</ref>. Adding rotations during training didn't improve the model performance, indicating that the molecules were either systematically aligned in a way that enabled e cient training, or that rotation invariance were easily learned by the model. <ref type="figure">Figure S7</ref>: Overview of the ensembling strategy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8.4 Training and Ensembling</head><p>14 models were trained, with varying dimensions from 512 to 2048 and layers from 6 to 24, and with scores of -2.9 to -3.1 Each model parameter size ranged from 12M to 100M (biggest model). A small validation set were kept separate to fit the ensemble model. Four di↵erent regressors were trained on the validation set predictions of the 14 models: k-nearest neighbors, linear, RANSAC <ref type="bibr">[24]</ref> and Theil-Sen <ref type="bibr">[25]</ref> regression models. A voting regressor were using to combine predictions of the four models, which resulted in a score of -3.21. Finally the single model with the best score (6 layers, 1024 dimensional feature vector) were trained on all the training data (including validation set), yielding a score of -3.16. Averaging this best single model with the voting regressor model yielded a final score of -3.23 on the public testing set. An overview of the ensembling strategy is shown in <ref type="figure">Fig S7.</ref> Predictions from the single best model takes ⇠10 minutes for the test set of 46K molecules (⇠15 ms per molecule).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9 Solution 3 S9.1 Features</head><p>Due to the graphical nature of molecular systems, graph-based architectures are often applied to many chemical problems including this competition <ref type="bibr">[26]</ref>. However, the nature of problem in this competition is slightly di↵erent to typical chemical problems, since atom-pair properties of three-dimensional chemical structures has not been modelled before and only a subset of atomic pairs in the molecules have target values (coupling constants). For example, the largest molecule in the training set (nonane) has 29 atoms and there are 406 atomic pairs. Among those atomic pairs, only 127 atomic pairs have target values. For this reason, an alternative representation for molecular systems than the graph representation can be utilized. Here, we use a set of only a part of pair features as a descriptor for a molecule. Even though we replace the graphical representation, the size-extensiveness and permutation-invariant nature should be preserved. A similarity-based attention (or simply attention) is one of the mechanisms that satisfies both conditions:</p><formula xml:id="formula_8">Attention(Q, K, V ) = softmax( QK T p d k )V (S13)</formula><p>where Q,K,V and d k are query, key, value, and feature dimension of the key value respectively. An output of attention is a weighted average of input values where the weights are determined by similarity of queries and keys. Attention layers hold permutational invariance and size-extensivity because if the size or order of the input vector is changed, those of corresponding key and query vectors are changed. In many di↵erent contexts of machine learning, a large number of variants of attention have been proposed. A multi-head attention is one of the most frequently employed ones. It generalizes conventional attentions with concatenating results of attention whose inputs are linearly transformed with di↵erent weights.</p><formula xml:id="formula_9">Multihead(Q, K, V ) = Concat(O 1 , O 2 , ..., O h )W 0 (S14) O i = Attention(QW Q i , KW K i , V W V i ) (S15) where W Q i , W K i , W V i and W O</formula><p>are weights for query, key, value, and outputs of attentions respectively. One of the benefits of using (multi-head) attention is to minimize sequential computing which is an obstacle for parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.2 Model Architecture</head><p>Transformers, one of the well-known natural language processing architectures, recorded overwhelming performance in the Seq2Seq <ref type="bibr">[27]</ref> problem with replacing sequential computing to multi-head attention. The original transformer model <ref type="bibr">[22]</ref> includes positional embedding to impose sequential information on input and output feature vectors before performing the encoding and decoding process, since encoder and decoder are composed of permutation-invariant layers. Also, like ordinary Attention and Linear blocks, they are size extensive which means di↵erent lengths of sequence can be treated. Hence, we employ encoder of Transformer to embedding pair sequence information to construct the latent space that contains interaction among atomic pairs. In terms of NLP, atomic pair information and pair sequences become words and sentences respectively. We refer to the original transformer paper for more details on the attention and transformer architecture <ref type="bibr">[22]</ref>. Once again important features of the model is size-extensivity and permutation-invariance.</p><p>Our model (shown in <ref type="figure">Figure S9</ref>) adopts the encoder of Transformer architecture because of its size extensivity and permutation invariance. We make an atomic pair sequence to represent a molecule and calculate a target value by considering interactions among atomic pairs. The Transformer encoder (grey part in <ref type="figure">Figure S9</ref>) is employed to transfer input feature to latent space which is   <ref type="table">Table S8</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.3 Readout stage</head><p>Readout stage evaluates the spin coupling constant (SC in <ref type="figure">Figure S9</ref>) from the output sequence, output of the encoding layer. Here, we employ physical condition; the spin coupling constant is the sum of 4 components, Fermi Contact contribution (FC), Spin-dipolar contribution (SD), Paramagnetic spin-orbit contribution (PSO) and Diamagnetic spin-orbit contribution (DSO). The Readout stage is designed to predict a target value as mean of two di↵erently evaluated values. One directly comes from the latent space and the other one is a sum of 4 components which comes from latent space. The loss function for the whole </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.4 Features</head><p>An atomic pair feature is composed of two atoms' properties and pair properties. (See <ref type="figure" target="#fig_0">Figure S11</ref> ) For atomic properties, the position of atom, atomic number and atomic charge, available in QM9 dataset are used and distance between them and type of coupling are for the pair properties. Those values are embedded and concatenated to build feature vector. Unlike the sum operation, concatenation is not a commutative operation so our feature vectors are dependent on the order of atoms in a pair. To make the feature vector independent of the order of atoms, test time augmentation or data augmentation can be applied, but in the data set, the order of atoms in a pair are sorted. Therefore, we use none of them. For clarity, it should be noticed that this order-dependency does not necessarily lead to permutation dependency of atomic pairs belonging to molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.4.1 Data Augmentation</head><p>As described in the previous section, positions of atoms are used in input sequences, therefore rotational and translational invariances are not conserved in our model. Although both invariances are not mathematically preserved, by training augmented data, we make parameters of our model guided to have pseudo-invariance in a certain range of translational and rotational changes. Fortunately, positions of atoms belonging to the QM9 set do not have extreme values, which means this kind of less rigorous strategy can cover all the test cases in the competition. If we need to cover general molecular geometry, this kind of strategy might not work. Generation of noise can be done in every epoch with low computational cost, the size of the original training database being enlarged 10 times with randomly selected noise. The selected noise is not changed during the training. The translational perturbations on each axis and angle of rotational perturbations are sampled from a Normal distribution. By this augmentation of the training data, we observed that the trained model recorded small enough disagreements with both changes and a better score in leaderboard. Mean and standard deviation values for both translational and rotational perturbation can be found in <ref type="table">Table S8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.4.2 Dummy types</head><p>Some of the geometric information of molecule is missing in the input sequences because not all atomic pairs are included. It may cause an incomplete description of chemical circumstances. In order to overcome this limit, dummy pairs are introduced. The dummy pairs are a part of the input sequences but do not have coupling constant values. The predicted values of dummy types are not used to evaluate the loss but the feature vectors of dummy types participate in process of building latent space even for meaningful pairs. By this, the geometric information of non-activated pairs can be included in the input sequence. Thousands of pair types exists in the given training set. Among them, 7 additional atomic types are included in input sequences: 1JHO, 1JCO, 1JCN, 1JNO, 1JCC, 1JNN, and 1JFC. The selection of these dummy pairs is a tunable hyperparameter but due to a lack of computing power, we did not test various combinations of dummy types. We choose to add neighboring types first because nearby pairs may have a strong impact on chemical circumstances. If more dummy types are added, more geometric information can be included in input sequences but an increase of computational costs follows as the length of sequence increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.5 Training and Ensembling</head><p>In order to achieve a high score, we employed an ensemble technique and pseudolabeling. An ensemble technique is to merge the results of various models and pseudo-labeling is to use new dataset which contains unlabeled data whose label is assumed as the results of previously trained model. We originally planned to use a seed ensemble which uses a number of identical models with di↵erent random seed numbers but because of inequalities of teammates' computing resources. Therefore, models with di↵erent N values (in <ref type="figure">Figure S9</ref>) and epochs are used. An overall training process is illustrated in <ref type="figure" target="#fig_0">Figure S12</ref>. At first stage, 15 models are trained using only training dataset with dropout. At second stage, parameters of models were fine-tuned by turning o↵ the dropouts. By using models trained up to the second phase, labels for test sets (so-called pseudolabel) are predicted. To minimize error of pseudo-label, results of 4-8 trained models up to the second phase are employed. In the third phase, newly constructed training sets which including both original training dataset and pseudolabeled test set are used. Because of due date of competition, we only take 8 models and further trained with the expanded datasets. At phase 3 and 4, 20 epochs are progressed. To mitigate the unpredicted bias from pseudo-labeling, at the last phase, the models are trained with only the original training dataset. Adam optimizer, typical choice, is employed with gradient clipping. Linear warmup and linear decay are used for the stable convergence of training. All weights are initialized with uniform distribution (mean=0.0, std.=0.02) and 0 is used for initial bias values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.5.1 Specification of models</head><p>Training in each ensemble is composed of 4 di↵erent steps. First step, training is performed with training set data and dropout. In the second step, further training is proceeded without dropout. Using the obtained model from the second step, the coupling constants for testing set are predicted; these predicted values are called pseudo label. The pseudo-labeled data in addition to training data is used in the third training step. To mitigate overfitting, further training is performed with only training set.</p><p>Output values of each type are normalized because each type shows di↵erent distribution of target values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9.6 Performance</head><p>Our final model has around 75M parameters. With two V100 graphic cards, training the model takes around 2 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22</head><p>Figure S12: Pictoral representation of the training procedure 23 S10 Solution 4 S10.1 Model Architecture</p><p>The model we used is a development of the Graph Attention Network <ref type="bibr">[28]</ref> architecture with Gated Residual Connections <ref type="bibr">[29]</ref>. The model operates on molecular graphs (nodes being the atoms, and edges being the bonds), augmented by artificial links between the nodes representing atoms 2 and 3 bonds apart in the molecular structure. Both graph nodes and graph edges hold feature vectors (in contrast to the more standard approach where only the nodes hold feature vectors). Size of the node and edge feature vectors are equal in each layer. The network is implemented in the PyTorch <ref type="bibr">[30]</ref> framework, using the Deep Graph Library <ref type="bibr">[31]</ref>, which requires all graph edges to be directed. We therefore represent each bond (as well as each 1-3 and 1-4 link) as a pair of graph edges (one in each direction).</p><p>The network consists of multiple layers, where each layer updates both the node and the edge feature vectors through the following steps:</p><p>1. Fully connected layers applied to nodes and edges:</p><formula xml:id="formula_10">⌘ l = W l node n l 1 , ✏ l = W l edge e l 1</formula><p>where n l 1 and e l 1 represent the values of the node and edge embeddings from the previous layer, while ⌘ and ✏ represent intermediate values used within the rest of the layer.</p><p>2. Attention based convolutional update applied to each node:</p><formula xml:id="formula_11">↵ l ij = sof tmax j ( (A l [⌘ l i k ✏ l ij k ⌘ l j ])) n l i = ( X j (↵ l ij ⌘ l i ✏ l ij ))</formula><p>where is a non-linearity (in the final model we used the Leaky Rectified Linear Unit with the slope set to 0.2). Furthermore, we employ the multihead extension to the attention mechanism (as described in <ref type="bibr">[28]</ref>), where multiple independent attention mechanisms are applied, and their results concatenated:</p><formula xml:id="formula_12">n l i = K n k=1 ( X j (↵ l ijk ⌘ l ik )✏ l ijk )</formula><p>3. Fully connected update to each edge, based on the concatenation of the source node feature vector, edge feature vector, and the destination node feature vector: e l ij = W l triplet [n l i k ✏ l ij k n l j ])) 4. At the end of each layer we apply layer normalization <ref type="bibr">[32]</ref> to node and edge feature vectors.</p><p>Each layer is followed by a Gated (Parametric) Residual connection <ref type="bibr">[29]</ref>, on both the node and the edge feature vectors. The outputs of the residual values are passed through Parametric Rectified Linear Units <ref type="bibr">[33]</ref>.</p><p>The final model is made up of an embedding layer, eight residual layers, and the two fully connected output layers applied to edge feature vectors of the final layer. Embedding size for both the nodes and the edges in each residual layer is 1152, and the number of heads is 24 (each head accounting for 48 parameters). The output sizes of the two final fully connected output layers were 512 and 8 (one output for each of the coupling types).</p><p>Coupling constant between two atoms are predicted by taking the corresponding output from the edges linking the graph nodes representing the two atoms. For each pair of atoms the graph will incorporate two such edges (one in each direction, due to the directed nature of the graph). During training, we treat these as two independent predictions (calculating and back-propagating the loss as if these were two data points). In prediction mode we output the average of these two predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10.2 Features</head><p>In addition to the molecular graph structure (atoms and bonds as inferred by OpenBabel <ref type="bibr">[34]</ref>) the model uses a number of atom and bond features. For atoms we use: electronegativity, first ionization energy, and electron a nity for each atom type, as well as atom Mulliken charge taken from the QM9 dataset <ref type="bibr">[35,</ref><ref type="bibr">36]</ref>. For edges we use features: bond length, bond angle (for artificial 1-3 edges), and the dihedral angle (for artificial 1-4 edges). All features were standardized based on the training set statistics. Labels were standardised for all models, except those used for predicting 1JHC, where we only subtract the mean (which we empirically determined to produce better results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10.3 Training and ensembling</head><p>We trained the model using a modified version of the LAMB optimizer <ref type="bibr">[37]</ref>, where we decouple the weight decay term from the trust region calculations, similarly to the AdamW modification of the Adam optimizer <ref type="bibr">[38]</ref>. The training was done using a mini-batch size of 80 molecules. Models were first pre-trained to jointly predict the scalar coupling constants for all coupling types. In the fine-tuning step we train separate models for JHC and JHH types, and continue training the joint model for the JHN types. The training was done using the Mean Absolute Error loss.</p><p>During training we used the following dynamic learning rate schedule:</p><p>• 30 epoch cycle, linearly varying the learning rate from 0.001 to 0.01 and back (pre-train phase)</p><p>• 70 epochs with a constant learning rate of 0.001 (pre-train phase)</p><p>• Constant learning rate of 0.001 until convergence; 90-100 cycles, depending on coupling type (fine tuning phase)</p><p>The model was regularized using weight decay set to 0.05 for the first 30 epochs and 0.01 afterwards. Final model parameters are derived by running Stochastic Weight Averaging <ref type="bibr">[39]</ref> over the models from the final 25 epochs of training. Final predictions are produced as the mean of the outputs from two folds (two sets of models, both using the same architecture, but with a di↵erent train/validation split, with 90% of the data used in training, and 10% used for validation). Each of the two model sets consists of 6 models:</p><p>• a model per coupling type, for each of 1JHC, 2JHC, 3JHC, 2JHH, 3JHH</p><p>coupling types</p><p>• 1 model specialized for 1JHN, 2JHN, and 3JHN coupling types</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10.4 Performance</head><p>The training procedure for the final ensemble took 200 GPU hours on systems with 2080Ti cards. The ensemble achieved a score of -3.18667 on the public test set, and -3.18085 on the private test set provided by the competition. In order to illuminate the architecture's performance, in addition to the final model (denoted FULL) we also benchmarked a single model trained to predict all of the coupling type interaction jointly (denoted SINGLE), and an ensemble where single model is specialized for each coupling type (denoted TYPE). The results are presented in <ref type="table" target="#tab_7">Table S5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11 Solution 5</head><p>Our model is an ensemble model built from 16 graph-based deep learning models. Our base models are inspired from MatErials Graph Network (MEGNet) <ref type="bibr">[40]</ref> which is an architecture used to predict properties of either molecules or crystals. In the following we will describe our best single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.1 Input features</head><p>We created features from the raw atom elements and coordinates using OpenBabel <ref type="bibr">[34]</ref>. OpenBabel provides numerous chemical features for each atoms, bonds, and for the whole molecule, which are all included in our input features. We also added translation and rotation invariant features such as ACSFs <ref type="bibr">[41]</ref>, distances of bonds, angle between bonds and the raw coordinates. Random rotations on the molecule coordinates were applied as a way of data augmentation. The most important features sorted by a permutation feature importance are the following:</p><p>• Atom: size of smallest ring that contains the atom, heteroatom valence, number of ring connection to the atom, average angle of bonds connected to the atom, whether a neighboring atom has an unsaturated bond to a third atom, count of hydrogen bound to the atom.</p><p>• Bond: angle formed by the neighboring bond with the closest atom, minimum distance of neighboring bond, bond distance, scalar coupling type.</p><p>• Molecule: number of atoms, number of non-hydrogen atoms.</p><p>We also noticed that the most important features of our preliminary models, according to a permutation feature importance, were the ring topology in the molecule and the angles between two bonds. This inspired us to modify MEG-Net by adding two operations related to rings and bond-bond angles. We will describe these modifications below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.2 Model architecture</head><p>Our models consists of three stages :</p><p>1. a preprocessing operation that transforms molecular, bonds and atomic features into a graph representation.</p><p>2. multiple steps of a graph update operation.</p><p>3. a readout operation that transform the graph representation to a set of scalar coupling values.</p><p>The architecture is described in <ref type="figure" target="#fig_0">Figure S13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.2.1 Preprocessing operation</head><p>The preprocessing operation builds a graph representation of the molecule. Each atom in the molecule represents a node in the graph. We choose to represent the molecule with a dense graph rather that limiting the edges to chemical bonds. This choice helped the information flows better in our network. Each node, edge and the whole graph are associated to a state vector. To build each state vector, we apply a multi-layer perceptron to the features of each considered object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.2.2 Graph update operation</head><p>The graph update operation takes a graph as input and outputs the same graph structure with updated state vector values. It consists of three steps :</p><p>1. update the edge state vectors We denote V = {v i } i=1:Nv the set of node vectors, with N V the node count, E = {e k } k=1:N E the set of edge vectors, with N E the edge count, and u the global graph vector. G = (V, E, u) is the input graph representation and G 0 = (V 0 , E 0 , u 0 ) is the output graph representation of the graph update operation.</p><p>In the following, we denote all functions as multi-layer perceptron with two hidden layers, SoftPlus activation and LayerNormalization that takes a vector as input and outputs another vector. If two functions share the same subscript it means that the multi-layer perceptrons share their parameters. If the function has no subscript it means that its parameters are not shared with any other multi-layer perceptron.</p><p>All functions are vector aggregation functions that outputs one vector from a set of vectors. In our architecture, all aggregations are attention functions that takes two parameters :</p><p>• a set of vectors to aggregate, which is used as the keys and values in the attention mechanism</p><p>• a vector which is used as query in the attention mechanism</p><p>The L operator is the vector concatenation operator. In the following we denote e the edge vector to be updated and b the bond associated with it. Firstly we introduce an intermediary vector r which is an aggregation of the edge vectors contained in all rings b is a part of, where a ring denote a simple cycle of atoms and chemical bonds in a molecule. The purpose of r is to allow a better flow of the rings related information. We proposed this intermediate vector because we observed that rings features had an high impact on preliminary scalar coupling models.</p><p>We denote N R the number of rings containing b in the molecule. For the ith ring containing b, R i = { e (e k )} k=1:N R i is the processed edge vectors of the ring. The vector r is computed as follows :</p><formula xml:id="formula_13">r i = ⇣ R i , e (e) ⌘ r = ⇣ {r i } i=1:N R , e (e)</formula><p>⌘ r i is an aggregated edge vector along the ith ring containing b and r is an aggregated edge vector along in all the rings containing b. An example of such computation is displayed in <ref type="figure" target="#fig_0">Figure S14</ref>. Secondly we introduce another intermediary vector a which is an aggregation of neighboring edge vectors and local geometric features. The purpose of a is firstly to allow to integrate angular edge-edge features into our architecture and secondly to provide a better flow of the edge-edge information. We proposed this intermediate vector because we observed that the engineered edge feature "angles between one edge and the edges formed with the closest atom" feature had an high impact on our preliminary scalar coupling models.</p><p>We denote A = {(e 1 i , e 2 i )} i=1:N V 2 is all couples of edges formed between an atom in the molecule and the two atoms in e. f i is the feature vector characterising the triangle of edges (e, e 1 i , e 2 i ) which is 5-dimensional and contains the 3 angles in the triangle as well as the length of edges e 1 i and e 2 i . The vector a is defined as follows :</p><formula xml:id="formula_14">a = ⇣n (f i ) M e (e) M e (e 1 i ) M e (e 2 i ) o i=1:N V 2 , e (e) ⌘</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.2.3 Readout stage</head><p>To predict a scalar coupling associated with an edge, we concatenated the edge vector, the two node vectors associated with the edge, and the global state vector and passed it through multi-layer perceptron to generate the final output. Since there was 8 di↵erent types of scalar coupling, our model had 8 di↵erent multilayer perceptrons to enforce a di↵erent readout for each type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.3 Training and ensembling</head><p>We trained our model with the Adam <ref type="bibr">[20]</ref> optimizer with a fixed learning rate and the original article default parameters, for about 150 epochs then reduced the learning rate by a factor 2 each 3 epochs for 15 epochs. The dimension of our graph vectors was set at 300. Our batch size was 20, a relatively small number due to the high memory requirements of the computation of the neighboring edge vectors aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.3.1 Experiments and model variations</head><p>We observed that even models with a modest score could help to contribute to a better overall performance with an ensemble model. As such we tried various architectures with di↵erent modifications. The following experiments were tested and integrated as base models:</p><p>• Di↵erent activation functions in our multi-layer perceptron: Softplus provided a boost of performance in comparison to a ReLU baseline.</p><p>• Normalization: LayerNormalization worked better that BatchNormalization in our experiments and helped improve convergence.</p><p>• In the readout stage, rather that using only the edge and two nodes associated with a scalar coupling, we concatenated all the edges and nodes vectors in the chemical bond path of the two atoms we compute the scalar coupling. We observed faster training with this method but could not integrate it in our best single model in time.</p><p>• We iterated with di↵erent number of graph update operations or di↵erent number of hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.3.2 Ensemble learning</head><p>Ensemble learning is a technique that aggregates multiple models into a single one to obtain a better performance. It has become a standard in machine learning competitions. To build our final prediction, we fitted a linear model and a gradient boosting model that we averaged. Rather than using only our 16 base models in this ensemble, we also integrating intermediary models checkpoints of those 16 models to further improve the performance. In the end there was about 50 input models in the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.4 Improvements</head><p>We can think of few improvements for our best single model architecture:</p><p>• Prune the amount of edges considered in the neighboring edge vectors aggregation by keeping only the edges closest to the updated edge. This would greatly reduce the memory consumption of the architecture, allow a bigger batch size and fasten the model. We suppose that this would not degrade the performance as the angular features that had high permutation importance in our preliminary models were related to the closest edges.</p><p>• Integrate the full chemical bond path in the readout stage mentioned in S11.3.1 to increase the training speed as it proved e cient on other similar architectures. S12 Solution 12</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12.1 Introduction</head><p>There has been extensive work recently using graph neural networks for predicting properties of molecular systems. Many problems along this direction require the use of configuration information (i.e., the positions of all atoms in a molecule.) A key challenge in applying machine learning techniques to these problems is that of capturing local geometric information (such as bond or torsion angles), while preserving symmetries of the overall system. Symmetries such as rotation and translation invariance are fundamental properties of molecular systems, and therefore must be exactly preserved in order for a machine learning architecture to e↵ectively use training data and make meaningful predictions. Our recent paper proposed the COvaRiant MOleculaR Artificial Neural neTwork (Cormorant) to help solve these issues <ref type="bibr">[42]</ref>. Cormorant uses spherical tensors as activation to encode local geometric information around each atom's environment. A spherical tensor is an object F`which transforms in a predictable way under a rotation. Specifically, for a rotation R 2 SO (3), there is a matrix D`(R), such that F`! D`(R) F`. Here, D`(R) is known as a Wigner-D matrix, and`is known as the order of the representation.</p><p>A tensor with`= 0 is a scalar quantity (that is, invariant under rotations), whereas`= 1 is a vector quantity such as a dipole, and`= 2 is a second-order tensor like a quadrupole, and so on. This structure is well known in physics. For example, the multipole expansion is a decomposition of an electrostatic potential V (r) = P 1 =0 Q`Y`(r) /r`+ 1 into multipole moments Q`and spherical harmonics Y`(r) oriented in the direction of the vector r. For a more in-depth discussion of spherical tensors, we point the reader to Ref. <ref type="bibr">[42]</ref>.</p><p>The use of spherical tensors allows for a network architecture that is "covariant to rotations." This means that if level s, a rotation is applied to all activations F s,`! D`(R) F s,`, then at the next level s + 1, all activations will automatically inherit that rotation so F s+1,`! D`(R) F s+1,`. As such, a rotation to the input of Cormorant will propagate through the network to ensure the output transforms as well. In this way, we can capture local geometric information, but can still maintain the desired transformation properties under rotations.</p><p>A key point is that the quantum algebra (SU(2)) used in the definition of NMR couplings is for our purposes equivalent (homomorphic) to the SO(3) algebra used in Cormorant 1 . The J -Coupling Hamiltonian H = 2⇡I i · J ij · I j describes the couplings J ij between spin operators I i . The spin operators are themselves objects that generate the Lie algebra SU (2), and thus transform according to Wigner-D matrices. Cormorant therefore naturally incorporates the structure of J -Coupling Hamiltonian, and we expect is a natural platform learning NMR couplings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12.2 Model architecture</head><p>We now briefly summarize our Cormorant architecture; for a more in-depth description of our architecture, see the original paper <ref type="bibr">[42]</ref>.</p><p>The structure of of Cormorant is similar to a graph neural network, with the key di↵erence that vertex activations F i and edge activations G ij for atoms i, j are promoted to lists of spherical tensors F i = ⇣ F 0 i , . . . , F`m ax i ⌘ and G ij = ⇣ G 0 ij , . . . , G`m ax ij ⌘ , where Fì 2 C (2`+1)⇥n`a nd Gì j 2 C (2`+1)⇥n`a re spherical tensors of order`, and n`is the multiplicity (number of channels) of the tensor.</p><p>In order to maintain covariance, we must carefully choose our non-linearity. The core non-linearity in Cormorant is dictated by the structure of the D`(R) matrices, and is known as the Clebsch-Gordan product <ref type="bibr">[CG]</ref>. We express the CG product of ⌦ cg of two spherical tensors as:</p><formula xml:id="formula_15">[A`1 ⌦ cg B`2 ]`=`1 +`2 M =|`1 `2| C`1`2`(A`1 ⌦ B`2 )</formula><p>where ⌦ denotes a Kronecker product, and C`1`2`are the famous Clebsch-Gordan coe cients <ref type="bibr">[CG]</ref>.</p><p>Our vertex activations F s i at level s are chosen to be</p><formula xml:id="formula_16">F s i = h F s 1 i F s 1 i ⌦ cg F s 1 i ⇣ X j G s ij ⌦ cg F s 1 j ⌘i · W vertex s,ẁ</formula><p>here denotes concatenation and W vertex s,`i s a linear mixing layer that acts on the multiplicity index. We choose the edge activations to have the form of G s,ì j = g s,ì j ⇥ Y`(r ij ), where r ij is a vector pointing from atom i to atom j. The scalar-valued edge terms are given by with µ s (r ij ) a learnable mask function, ⌘ s,`( r ij ) a learnable set of radial basis functions, and W edge s,`a linear layer along the multiplicity index. We iterate this architecture for s = 0, . . . , s max . Finally, at the last layer of Cormorant, we take the`= 0 component of the edge g smaxì j , and use it as a prediction of the J-coupling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12.3 Training and ensembling</head><p>To predict J-coupling constants, we first observed that each J-coupling was a rotationally invariant feature on pairs of atoms. Consequently, it was natural to read the values of the J-couplings o↵ the values of g s,ì j . We therefore constructed a neural net with five cormorant layers, with a maximum`value of 3. In each layer, we used 48 vertex activations for each value of`. We then took a linear combination of the values of g s,ì j for each layer and took a learned linear combination of the values as our prediction for the scalar coupling constant. Initial features were constructed using the radial distance between atoms, the atomic identity, and the atomic charge. In general, almost exactly the same network was used as for the QM9 tests in the Cormorant paper <ref type="bibr">[42]</ref>. We also used the Mulliken charges as initial features for pretraining. Our training proceeded in three stages.</p><p>1. Three nets were trained on the 1J, 2J, and 3J couplings with the Mulliken Charges on an internal split on the training data.</p><p>2. Eight nets were trained on the 1JHC, 1JHN, 2JHH, 2JHC, etc. couplings with the Mulliken Charges on an internal split on the training data, with weights initialized to be the values in stage 1.</p><p>3. Each of the nets in stage 2 was trained on the full dataset, without access to the Mulliken charges.</p><p>The internal split was constructed using an 80/10/10 train/validation/test split on the training dataset provided in the Kaggle competition. The model was optimized using the AMSGrad optimizer for 200 epochs. The learning rate was varied using cosine annealing. For stage one the initial and final learning was 5 ⇤ 10 4 and 5 ⇤ 10 6 , and for stages two and three the initial and final learning rates were 3 ⇤ 10 4 and 3 ⇤ 10 6 , respectively. We repeated stages two and three with di↵erent random seeds in attempt to average over multiple trained networks, however this had negligible e↵ect on the results. In table S6, we give the final results on the Kaggle Test/Train split. We achieve considerably stronger results for the 2J and 3J couplings than for the 1J couplings. However, we were pleasantly surprised by the minimal fine-tuning </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S13 Code and data</head><p>Code and data is available on http://osf.io/kcaht and http://github.com/ larsbratholm/champs_kaggle. These contain the following</p><p>• List of molecules removed as they did not contain hydrogens.</p><p>• List of molecules removed due to one or more couplings being outliers.</p><p>• List of training and test molecules.</p><p>• Script to convert QM9 extended XYZ-files into both regularly formatted XYZ-files as well as input files for Gaussian NMR computations.</p><p>• Script to create the Kaggle dataset from the output files of Gaussian NMR computations.</p><p>• Archive of all extended XYZ-files of QM9 that passed the consistency check of the original paper.</p><p>• Archive of all regular formatted XYZ files parsed from the extended XYZ file format of the QM9 dataset.</p><p>• Archive of all Gaussian input files.</p><p>• Archive of all Gaussian output files.</p><p>• Archive of the Kaggle dataset.</p><p>• Archive of the top 400 submissions.</p><p>• Script to create an ensemble from the top submissions.</p><p>• Script to create the plots used in the paper and SI.</p><p>• Code used by the 1st, 2nd, 3rd, 4th, 5th and 12th placed teams to create their models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) map showing the number teams participating from different countries over the duration of the CKC (countries with less than 5 participants are shown light gray); (b) the number of CKC participants vs. time; (c) number of submissions per day</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) score evolution vs. time. Black line shows the best performing method vs. time. Blue line shows the best performing public notebook. Red lines shows the best submission by each team; (b) best fit of the time dependent leader (black) score to an biexponential curve of the form ! • #$%(−(/* ! ) + -• #$%(−(/* " ) + . (A = 2.11; B = 2.97; t1 = 50.0 days; t2 = 1.29 days; C = -3.59). Blue indicates the best ME model score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) comparison of the top individual score (orange) to the ME model score (blue) as function of the k value in Eq (2); (b) the number of contributors to the ME model at a particular k value that had an optimized weight greater than 0.01.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>(a) score evolution vs. time for Table 1 teams. Black line shows the best performing method at a current time. Colored lines show the best submission by each team; (b) correlation amongst the top 50 submissions. Red indicates high correlation, and blue low. Bottom and right side shows the ranking of the submission, while top and left features a dendrogram depicting the hierarchical clustering; (c) correlation between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>S2: The performance of the ensemble fitted on all coupling types ('E'), the ensemble fitted on each coupling type separately ('E*'), and the submitted solutions (individual contributions and the total score per coupling type). Rank 1JHC 1JHN 2JHH 2JHC 2JHN 3JHH 3JHC 3JHN Total E -0.296 -0.311 -0.509 -0.432 -0.463 -0.502 -0.418 -0.483 -3.414 E* -0.297 -0.332 -0.515 -0.433 -0.463 -0.507 -0.421 -0.485 -3.453 1 -0.284 -0.288 -0.477 -0.410 -0.440 -0.477 -0.400 -0.464 -3.241 2 -0.277 -0.291 -0.458 -0.410 -0.450 -0.474 -0.394 -0.472 -3.226 3 -0.269 -0.244 -0.480 -0.411 -0.443 -0.488 -0.397 -0.461 -3.193 4 -0.267 -0.286 -0.482 -0.401 -0.433 -0.470 -0.390 -0.455 -3.184 5 -0.275 -0.289 -0.468 -0.400 -0.428 -0.452 -0.387 -0.450 -3.149 12 -0.221 -0.278 -0.448 -0.357 -0.414 -0.444 -0.336 -0.437 -2.936</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure S1 :</head><label>S1</label><figDesc>Projections of top 100 submissions to a two-dimensional MDS manifold. Coloring indicate submissions score as indicated by the colorbar. A subset of 7 submissions are marked with the competition rank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S2 :</head><label>S2</label><figDesc>How the prize pool are related to number of participating teams on Kaggle in the last five years (log-log plot). This competition highlighted in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>S3: Score contributions for each coupling type for the best ensemble, the winner and IMPRESSION lower and upper bounds. (lower, upper)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>d 4 X 4 Xc 0 13 d</head><label>4413</label><figDesc>atom,bond (a, B) = min(d atom (a, b1), d atom (a, b2)) (S8) d bond,bond (B, B 0 ) = 1 b2B,b 0 2B 0 d atom,bond (b, b 0 ) (S9) d atom,trip (a, C) = min c2c1,c2,c3 (d atom,atom (a, c)) + d atom,atom (a, c 1 ) (S10) d bond,trip (B, C) = min(d bond,bond (B, c 12 ), d bond,bond (B, c 12 )) (S11) d trip,trip (C, C 0 ) = 1 c2c12,c13,c 0 2c 0 12 ,bond,bond (c, c 0 ) (S12)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S8 :</head><label>S8</label><figDesc>Detailed solution information read by Readout (purple part in Figure S9, more detailed information in Figure S10). For each type, weights of Readout are di↵erent but structures and dimensions of weights are the same. (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>18 Figure S9 :</head><label>18S9</label><figDesc>Pictoral representation of the overall architecture Figure S10: Pictoral representation of the overall model model is constructed as the log of the sum of MAEs of outputs of two Readout layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S11 :</head><label>S11</label><figDesc>Pictoral representation of the input features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure S13: Best single model architecture 2 .</head><label>2</label><figDesc>update the node state vectors 3. update the global state vector</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 Figure S14 :</head><label>2S14</label><figDesc>Computation of r with two rings Edge vector update The edge vector update consists of four operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>g s,ì j = µ s (r ij ) h⇣ g s 1 ,ì j F s 1 i· F s 1 j</head><label>111</label><figDesc>⌘ s,`( r ij ) ⌘ · W edge s,`i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary descriptions for the six models in the final ME.</figDesc><table><row><cell>"Use of</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Advances in Neural Information Processing Systems, 14510-14519 (2019). 47. F. A. Faber, A. S. Christensen, B. Huang, O. A. v. Lilienfeld, Alchemical and structural distribution based representation for universal quantum machine learning. The Journal of Chemical Physics 148, 241717 (2018). 48. A. Vaswani et al., Attention is all you need. Advances in neural information processing systems, 5998-6008 (2017). 49.</figDesc><table><row><cell>https://www.kaggle.com/c/champs-scalar-</cell></row><row><cell>coupling/discussion/106565, (2019).</cell></row><row><cell>46.</cell></row></table><note>B. Anderson, T. S. Hy, R. Kondor, Cormorant: Covariant molecular neural networks.A. Rzhetsky, J. G. Foster, I. T. Foster, J. A. Evans, Choosing experiments to accelerate collective discovery. Proceedings of the National Academy of Sciences 112, 14569 (2015).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S4 :</head><label>S4</label><figDesc>Mean absolute error for each coupling type for the best ensemble, the winner and IMPRESSION lower and upper bounds.</figDesc><table><row><cell cols="2">S7 Solution 1 -Hybrid</cell><cell></cell></row><row><cell>S7.1 Features</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell cols="2">Ensemble* Winner</cell><cell>IMPRESSION</cell></row><row><cell>1JHC</cell><cell>0.0937</cell><cell cols="2">0.1031 (0.3002, 0.9728)</cell></row><row><cell>1JHN</cell><cell>0.0702</cell><cell cols="2">0.0999 (0.1292, 0.2404)</cell></row><row><cell>2JHH</cell><cell>0.0162</cell><cell cols="2">0.0220 (0.1886, 0.3872)</cell></row><row><cell>2JHC</cell><cell>0.0313</cell><cell cols="2">0.0376 (0.1488, 0.8682)</cell></row><row><cell>2JHN</cell><cell>0.0246</cell><cell cols="2">0.0296 (0.1040, 0.3180)</cell></row><row><cell>3JHH</cell><cell>0.0173</cell><cell cols="2">0.0220 (0.6815, 1.1675)</cell></row><row><cell>3JHC</cell><cell>0.0345</cell><cell cols="2">0.0408 (0.3751, 1.4811)</cell></row><row><cell>3JHN</cell><cell>0.0207</cell><cell cols="2">0.0244 (0.1780, 0.4257)</cell></row><row><cell>Geometric mean</cell><cell>0.0317</cell><cell cols="2">0.0391 (0.2183, 0.6069)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S5 :</head><label>S5</label><figDesc>Scores achieved by di↵erent ensembling choices.</figDesc><table><row><cell>Model</cell><cell cols="3">Private score Public score Train time (GPU h)</cell></row><row><cell>FULL</cell><cell>-3.18085</cell><cell>-3.18667</cell><cell>200</cell></row><row><cell>PER-TYPE</cell><cell>-3.13853</cell><cell>-3.14362</cell><cell>85</cell></row><row><cell>SINGLE</cell><cell>-2.96183</cell><cell>-2.96443</cell><cell>24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S6</head><label>S6</label><figDesc>results close to state of the art. Future directions include directly connecting the tensors learned by cormorant with the tensor-valued data in NMR experiments to provide more detailed inferences.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">: Performance</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Dataset 1JHC 1JHN 2JHH 2JHC 2JHN 3JHH 3JHC 3JHN</cell></row><row><cell cols="2">Training -2.736</cell><cell>-4.834</cell><cell>-4.366</cell><cell cols="3">-3.481 -5.4889 -4.632</cell><cell>-3.275</cell><cell>-5.557</cell></row><row><cell>Test</cell><cell>-1.768</cell><cell>-2.224</cell><cell>-3.584</cell><cell>-2.856</cell><cell>-3.312</cell><cell>-3.552</cell><cell>-2.688</cell><cell>-3.496</cell></row><row><cell cols="2">required to get</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More precisely, SU(2) is a double cover of SO(3), and both groups share a Lie algebra.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>Part of this work was carried out using the computational facilities of the Advanced Computing Research Centre, University of Bristol. WG is partially supported by the EPSRC National Productivity Investment Fund (NPIF) for Doctoral Studentship funding. LAB thanks the Alan Turing Institute under the EPSRC grant EP/N510129/1. DRG is supported by the Leverhulme Trust (Philip Leverhulme Prize) and Royal Society (URF/R/180033). LAB and DRG acknowledge support of this work through the "CHAMPS" EPSRC programme grant EP/P021123/1. SC was supported by National Research Foundation of Korea (2018R1D1A1B07049981, 2019M3E5D4065968) funded by the Ministry of Science and ICT. We furthermore thank Prof. Jan H. Jensen for access to additional computing resources and Dr. Christopher Sutton for helpful advice on the design of the CKC.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighboring atom 1</head><p>Neighboring atom 2 <ref type="figure">Figure S15</ref>: Computation of a for the edge e in a four node system.</p><p>An example of such computation is displayed in <ref type="figure">Figure S15</ref>. Finally we compute the updated edge vector e 0 . We denote the v 1 and v 2 the first and second atom vector of e.</p><p>e 0 contains a skip connection to enable deeper and faster model training.</p><p>Node update We denote v the node vector to be updated, v i the ith node vector di↵erent from v and e 0 i the updated edge vector between the nodes associated with v and v i . Let C = {v i L e 0 i } i=1:N V 1 be the set of concatenated node and updated edge vectors. The vector v 0 is defined as follows :</p><p>As for the edge update, we integrate a skip connection.</p><p>Global state update We denote V 0 = {v 0 k } k=1:N V the set of updated node vectors and E 0 = {e 0 k } k=1:N E the set of updated edges vectors. The updated global state vector is defined as follows :</p><p>As for the edge and node update, we integrate a skip connection.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page">1147</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page">1026</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-level performance in 3D multiplayer games with population-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="859" to="865" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A robotic platform for flow synthesis of organic compounds informed by AI planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page">1566</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Superhuman AI for multiplayer poker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="885" to="890" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crystal symmetry determination in electron diffraction using machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaufmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="564" to="568" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Facial expressions of emotion states and their neuronal correlates in mice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dolensek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Gehrlach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gogolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page" from="89" to="94" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">We analyzed 16,625 papers to figure out where AI is headed next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hao</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/s/612768/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artificial intelligence faces reproducibility crisis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="725" to="726" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training a 3-node neural network is NP-complete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lessons for artificial intelligence from the study of natural stupidity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Gureckis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="174" to="180" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01755</idno>
		<title level="m">A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Masulli</surname></persName>
		</author>
		<title level="m">Italian workshop on neural nets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting protein structures with a multiplayer online game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="page" from="756" to="760" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithm discovery by protein folding game players</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crowd science user contribution patterns and their implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sauermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Franzoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">679</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opinion: Toward an international definition of citizen science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heigl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kieslinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dörler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page">8089</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Remote optimization of an ultracold atoms experiment by experts and citizen scientists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">11231</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Higgs Machine Learning Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Adam-Bourdarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">664</biblScope>
			<biblScope unit="page">72015</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The TrackML high-energy physics tracking challenge on Kaggle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Web Conf</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page">6037</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00085</idno>
		<title level="m">kaggle competition: solving materials science challenges through crowd sourcing</title>
		<meeting><address><addrLine>Nomad</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="https://www.kaggle.com/c/champs-scalar-coupling" />
		<title level="m">Predicting Molecular Properties</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">IMPRESSION -prediction of NMR parameters for 3-dimensional chemical structures using machine learning with near quantum chemical accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gerrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="508" to="515" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acd/Labs</surname></persName>
		</author>
		<ptr target="https://www.acdlabs.com/products/adh/nmr/nmr_pred" />
	</analytic>
	<monogr>
		<title level="j">NMR Predictior Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Research</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Predict</surname></persName>
		</author>
		<ptr target="https://mestrelab.com/software/mnova/nmr-predict" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask Ernö&quot;: a self-learning tool for assignment and prediction of nuclear magnetic resonance spectra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dieden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Patiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Brandolini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMRPredict Modgraph Consultants</title>
		<imprint>
			<biblScope unit="page">1348</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Contact company for pricing information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Place</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Escondido</surname></persName>
		</author>
		<ptr target="http://www.modgraph-usa.com" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Chemical Society</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="13313" to="13313" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chemical shifts in molecular solids by machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Paruzzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4501</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Machine Learning Predictions of Molecular Properties: Accurate Many-Body Potentials and Nonlocality in Chemical Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2326" to="2331" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Meets Quantum Chemistry Approximations: The Δ-Machine Learning Approach</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2087" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<title level="m">the Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">FCHL revisited: Faster and more accurate quantum machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Bratholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A V</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">44107</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parrinello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">146401</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Atom-centered symmetry functions for constructing high-dimensional neural network potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page">74106</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ab Initio Calculation of Vibrational Absorption and Circular Dichroism Spectra Using Density Functional Force Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chabalowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="11623" to="11627" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-consistent molecular-orbital methods. IX. An extended Gaussian-type basis for molecular-orbital studies of organic molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ditchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Hehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="724" to="728" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods. XII. Further extensions of Gaussian-type basis sets for use in molecular orbital studies of organic molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Hehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ditchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="2257" to="2261" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods. XX. A basis set for correlated wave functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="650" to="654" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods 25. Supplementary functions for Gaussian basis sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3265" to="3269" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Lukyanenko</surname></persName>
		</author>
		<ptr target="https://youtu.be/sdIR8i0f_5A?t=1344" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Accelerating Molecular Property Prediction</title>
		<ptr target="https://info.nvidia.com/accelerating-molecular-property-prediction-reg-page.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting Molecular Properties -Competition Finalized</title>
	</analytic>
	<monogr>
		<title level="m">Congratulations &amp; Takeaways</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ab initio calculation of vibrational absorption and circular dichroism spectra using density functional force fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Chabalowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Frisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="11623" to="11627" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Self-consistent molecularorbital methods. ix. an extended gaussian-type basis for molecular-orbital studies of organic molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ditchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Hehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="724" to="728" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods 25. supplementary functions for gaussian basis sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Stephen</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3265" to="3269" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods. xii. further extensions of gaussian-type basis sets for use in molecular orbital studies of organic molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Hehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ditchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2257" to="2261" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-consistent molecular orbital methods. xx. a basis set for correlated wave functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="650" to="654" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je↵rey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo↵rey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stéfan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mayorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">İlhan</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Polat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Vand Erplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Laxalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Perktold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cimrman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Paul van Mulbregt, and SciPy 1. 0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antônio</forename><forename type="middle">H</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">P</forename><surname>Groenen</surname></persName>
		</author>
		<title level="m">modern multidimensional scaling. theory and applications</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nonmetric multidimensional scaling: a numerical method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Impression-prediction of nmr parameters for 3-dimensional chemical structures using machine learning with near quantum chemical accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Gerrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">A</forename><surname>Bratholm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">J</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig P</forename><surname>Glowacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Butts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="508" to="515" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<ptr target="http://www.rdkit.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Joint learning of hierarchical word embeddings from a corpus and a taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Knowledge Base Construction (AKBC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo↵rey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<ptr target="https://github.com/fastai/fastai" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Theil-sen estimators in a multiple linear regression model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Olemiss Edu</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Residual gated graph convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<title level="m">Deep graph library: Towards e cient and scalable deep learning on graphs</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo↵rey E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Open babel: An open chemical toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo↵rey R</forename><surname>Vandermeersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hutchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">970 million druglike small molecules for virtual screening in the chemical universe database GDB-13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Chem. Soc</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">8732</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<title level="m">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Graph networks as a universal machine learning framework for molecules and crystals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weike</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxing</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyue Ping</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemistry of Materials</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3564" to="3572" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Atom-centered symmetry functions for constructing highdimensional neural network potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Behler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">74106</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14510" to="14519" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
