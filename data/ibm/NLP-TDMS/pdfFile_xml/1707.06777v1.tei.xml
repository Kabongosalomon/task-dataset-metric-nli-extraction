<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Person Search Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karlekar</forename><surname>Jayashree</surname></persName>
							<email>karlekar.jayashree@sg.panasonic.com</email>
							<affiliation key="aff3">
								<orgName type="department">Panasonic R&amp;D Center</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
							<email>zhaobo@my.swjtu.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Southwest Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
							<email>jgjiang@hfut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<email>yanshuicheng@360.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Person Search Machines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the problem of person search in the wild in this work. Instead of comparing the query against all candidate regions generated in a query-blind manner, we propose to recursively shrink the search area from the whole image till achieving precise localization of the target person, by fully exploiting information from the query and contextual cues in every recursive search step. We develop the Neural Person Search Machines (NPSM) to implement such recursive localization for person search. Benefiting from its neural search mechanism, NPSM is able to selectively shrink its focus from a loose region to a tighter one containing the target automatically. In this process, NPSM employs an internal primitive memory component to memorize the query representation which modulates the attention and augments its robustness to other distracting regions. Evaluations on two benchmark datasets, CUHK-SYSU Person Search dataset and PRW dataset, have demonstrated that our method can outperform current state-of-the-arts in both mAP and top-1 evaluation protocols.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person search <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref> aims to localize a specific person matching the provided query in gallery images or video frames. It is a new and challenging task that requires to address person detection and re-identification simultaneously. It has many important applications in video surveillance and security, such as cross-camera visual tracking <ref type="bibr" target="#b21">[22]</ref> and person verification <ref type="bibr" target="#b36">[37]</ref>. But it is difficult in real-world scenarios due to various distracting factors including large appearance variance across multiple cameras, low resolution, cluttered background, unfavorable camera setting, etc.</p><p>To date, only a few methods have been proposed to address this task. In the pioneer work <ref type="bibr" target="#b32">[33]</ref>, Xiao et al. adopted the end-to-end person search model based on the proposed Online Instance Matching (OIM) loss function to jointly (a) Search process of previous methods (b) Search process of the NPSM <ref type="figure">Figure 1</ref>. Demonstration of person search process for one gallery image in previous methods and our proposed method. (a) The search process of previous methods. The query person is one-byone compared with the detection results for one gallery image; then the search result ranked at the first place is obtained. The red boxes indicate the wrong matched results while the green box represents the truly matched person. (b) The search process of the NPSM. When a target person is searched within a whole scene, the search scope on which attention is focused is recursively shrinking with guidance from memory of the query person's appearance, which is marked in red boxes. train person detection and re-identification networks. The recent work <ref type="bibr" target="#b40">[41]</ref> also follows a similar pipeline. Generally, all of the previous person search methods are based on such a simple two-stage search strategy: first to detect all candidate persons within an image and then to perform exhaustive comparison between all possible pairs of the query and the candidates to output a search result ranked at the first place within the searched images. This pipeline has some drawbacks. Firstly, if the target person has distracting factors around, e.g., another person with similar appearance, the search accuracy would be affected by the distracting factors. Secondly, extra error, such as inaccurate detection, would be introduced by the two isolated frameworks, i.e. person detection and re-identification. See <ref type="figure">Figure 1</ref> (a) for demonstration. The red boxes indicate the wrong matched results while the green box represents the truly matched person.</p><p>For person search, it is commonly assumed that within an image, the target person only appears at a single location. Such instance-level exclusive cues imply that instead of examining all possible persons, a more effective strategy is to only search within the regions possibly containing the target person in a coarse-to-fine manner. This is similar to human neural system for processing complex visual information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. More concretely, after seeing and remembering the appearance of a target person, one usually shrinks his search area from a large scope to a small one and performs matching with his memory in details within the small scope with more effort. Such a coarse-to-fine search process is intuitively useful for existing person search solutions.</p><p>Inspired by above observations, we propose a new and more effective person search strategy and develop the Neural Person Search Machines (NPSM). Compared to the search process in previous methods, our NPSM <ref type="figure">(Figure 1</ref> (b)) takes the query person as memory to recursively guide the model to shrink the search region and judge whether the current region contains the target person or not. This process would include more contextual cues beneficial for person matching. In <ref type="figure">Figure 1</ref> (b), the red box in each image from left to right corresponds to a region that can be focused on, and the arrow indicates a search process which can be considered as the continuous shrinkage of the focus region. Additionally, those irrelevant regions can be ignored after every shrinkage of a subregion from a big region, which can reduce the interference of other unimportant regions.</p><p>To model the above person search process, we need to solve the following two non-trivial problems: 1) integrating information of the query person into the search process as memory to exclude interference from impossible candidates; 2) judging which subregion should be focused on in the bigger region at each recursive step in the coarse-to-fine search process under the guidance of memory.</p><p>For localizing the target person in a sequence correctly and fully exploiting the context information, we propose a neural search architecture to selectively concentrate on an effective subregion of the input region, and meanwhile ignore other perceived information from distracting subregions in a recursive way. Take the third subregion of the search process in <ref type="figure">Figure 1</ref> (b) for example, the proposed NPSM would highlight the truly matched person at the left side of the region and ignore the similar person at the right side. Considering the specific ability of Long Short-Term Memory (LSTM) <ref type="bibr" target="#b9">[10]</ref> to partially allow or deny information to flow into or out of its memory component, we build our Neural Search Networks (NSN) upon Convolutional LSTM (Conv-LSTM) <ref type="bibr" target="#b33">[34]</ref> units which are capable of preserving spatial information from the spatio-temporal sequences.</p><p>Different from the vanilla Conv-LSTM, we augment our NSN by equipping it with external primitive memory that contains appearance information of the query and helps identify the candidate regions at the coarse level and discards irrelevant regions. The external primitive memory thus enables the query to be involved in the representation learning for person search as well as the recursive search process with region shrinkage.</p><p>To sum up, in this work we go beyond the standard LSTM based models and propose a new person search approach called Neural Person Search Machines (NPSM) based on the Conv-LSTM <ref type="bibr" target="#b33">[34]</ref>, which contains the context information of each person and employs the external memory about the query person to guide the model to attend to the right region. Our approach is able to achieve better performance compared with other methods, as validated by experimental results.</p><p>We make the following contributions to person search: 1) We redefine the person search process as a detection free procedure of recursively focusing on the right regions. 2) We coin a novel method more robust to distracting factors benefiting from contextual information.</p><p>3) We propose a new neural search model that can integrate the query person information into primitive memory to guide the model to recursively focus on the effective regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person search can be regarded as the combination of person re-identification and person detection. Most of existing works of person re-identification focus on designing hand-crafted discriminative features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref>, learning deep learning based high-level features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> and learning distance metrics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref>. Recent deep learning based person re-identification methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> re-design the structure of the deep network to improve performance. For instance, <ref type="bibr" target="#b0">[1]</ref> designed two novel layers to capture relationships between two views of a person pair. Among distance metric learning methods, <ref type="bibr" target="#b11">[12]</ref> proposed KISSME (KISS MEtric) to learn a distance metric from equivalence constraints. Additionally, <ref type="bibr" target="#b37">[38]</ref> proposed to solve the person re-id problem by learning a discriminative null space of the training samples while <ref type="bibr" target="#b14">[15]</ref> proposed a method learning a shared subspace across different scales to address the low resolution person re-identification problem.</p><p>For person detection, Deformable Part Model (DPM) <ref type="bibr" target="#b5">[6]</ref>, Aggregated Channel Features (ACF) <ref type="bibr" target="#b3">[4]</ref> and Locally Decorrelated Channel Features (LDCF) <ref type="bibr" target="#b20">[21]</ref> are three representative methods relying on hand-crafted features and linear classifiers to detect pedestrians. Recently, several deep learning-based frameworks have been proposed. In <ref type="bibr" target="#b28">[29]</ref>, DeepParts was proposed to handle occlusion with an extensive part pool. Besides, <ref type="bibr" target="#b2">[3]</ref> proposed the CompACT boosting algorithm learning complexity-aware detector cascades for person detection. In our knowledge, two previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref> address person search by fusing person re-identification and detection into an integral pipeline to consider whether any complementarity exists between the two tasks. <ref type="bibr" target="#b32">[33]</ref> developed an end-to-end person search framework to jointly handle both aspects with the help of Online Instance Matching (OIM) loss while <ref type="bibr" target="#b40">[41]</ref> proposed ID-discriminative Embedding (IDE) and Confidence Weighted Similarity (CWS) to improve the person search performance. However, these two works simply focus on how the interplay of pedestrian detection and person re-identification affects the overall performance, and they still isolate the person search into two individual components (detection and re-identification), which would introduce extra error, e.g. inaccurate detection. In this paper, we regard person search as a detection-free process of gradually removing interference or irrelevant target persons for the query person.</p><p>Recently, LSTM based attention methods have shown good performance in image description <ref type="bibr" target="#b34">[35]</ref>, action recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> and person re-identification <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, Xu et al. showed how the learned attention can be exploited to give more interpretability into the model generation process, while <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> adopted attention methods to recognize important elements in video frames based on the action that is being performed. Moreover, <ref type="bibr" target="#b17">[18]</ref> formulated an attention model as a triplet recurrent neural network which dynamically generates comparative attention location maps for person re-identification. Analogously, our proposed NPSM also has such a locally emphasizing property, but NPSM is a query-aware model while the above attention-based methods all adopt a blind attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Neural Person Search Machines</head><p>In this section, we present the architecture details of the proposed Neural Person Search Machines (NPSM), and explain how it works with the primitive memory modeling to facilitate person search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>The overall architecture is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. It consists of two components, i.e. Primitive Memory and Neural Search Networks. We propose to solve person search by recursively shrinking the search area from the whole image to the precise bounding box of the person of interest. And each region in the shrinking search process would contain the contextual information of the final search result. Besides recursively utilizing the contextual cues, NPSM pro-vides extra robustness to interference from other distracting subregions for the model in the search process.</p><p>The proposed NPSM is trained end-to-end to learn to make a decision on the subregion attention at each recursive step and finally localize the person of interest. The Neural Search Network enables the model to automatically focus on relevant regions, and the Primitive Memory that models the representation of the query person continuously provides extra cues for every search step and facilitates more precise localization of persons. After performing the recursive region shrinkage, the model reaches a search result with the biggest confidence as the final search result with an gallery image. Note that, different from previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>, our method is detection-free and includes no Non-Maximum Suppression (NMS), as it is a search process performing simultaneous region shrinking and person identification. When the searching is finished, there will be only one bounding box person search result left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Person Search with NPSM</head><p>As aforementioned, we redefine the person search process as the recursive region shrinking process. It is equivalent to recursively focusing on a subregion containing the person of interest from a bigger region. Here we describe the details of our proposed NPSM and explain how to perform the recursive region shrinking to search the target person for each gallery image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Neural Search Networks</head><p>Learning to search for a person from a big region to a specific person region within the gallery image can be deemed as a sequence modeling problem. Specifically, the shrinking regions constitute a sequence. Thus a natural choice for the model candidates is the Recurrent Neural Network (RNN) or LSTM based RNN. However, the vanilla LSTM <ref type="bibr" target="#b9">[10]</ref> only models sequence information through fully connected layers and requires vectorizing 2D feature maps. This would result in the loss of spatial information, harming person localization performance. In order to preserve the spatial structure of the regions over the shrinking process shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we design a new network called Neural Search Network (NSN) based on Convolutional LSTM (Conv-LSTM) <ref type="bibr" target="#b33">[34]</ref> for each recursive step. Conv-LSTM replaces the fully connected multiplicative operations in an LSTM unit with convolutional operations. Different from it, the NSN has an additional memory component recording the query.</p><p>Conv-LSTM can be used for building attention networks that can learn to pay attention to critical regions within feature maps. Thus, Conv-LSTM based NSN is also equipped with attention mechanism to learn to gradually shrink the region and selectively memorize the contextual information contained in the searched bigger region at each recursive step. However, our neural search model has a unique feature that distinguishes it from a plain attention model: in addition to gallery images, a query illustrating the search target is also input to the search network. Traditional attention networks cannot well model such extra cues. In this work, we propose to model such query information into the primitive memory in order to facilitate person search.</p><p>We now elaborate on the new Neural Search Networks (NSN) of our NPSM, tailored for the person search task. In the NSN component, the query person information, denoted as q, is integrated into the computation within gates and cell states in a way to bias the updating of internal states towards emphasizing information relevant to the query while forgetting irrelevant information. Here the query feauture q is extracted from the query image through the pre-trained "Res50 part1" (conv1 to conv4 3 of ResNet-50 <ref type="bibr" target="#b8">[9]</ref>) which is the same as the one extracting features from gallery images. The cell and gates in the NSN are defined as</p><formula xml:id="formula_0">i t = σ (w xi * x t + w hi * h t−1 + w qi * q + b i ) f t = σ (w xf * x t + w hf * h t−1 + w qf * q + b f ) o t = σ (w xo * x t + w ho * h t−1 + w qo * q + b o ) (1) g t = tanh (w xc * x t + w hc * h t−1 + w qc * q + b c ) c t = f t c t−1 + i t g t h t = o t tanh (c t ) ,</formula><p>where * represents the convolutional operation and is the Hadamard product, w x∼ , w h∼ are two-dimensional convolutional kernels and x t which is the feature map of the re-gion highlighted by the previous time-step denotes the input at time step t. The input gate, forget gate, output gate, hidden state and memory cell are denoted as i t , f t , o t , h t , c t respectively, which are all three-dimensional tensors retaining spatial dimensions. With their control, the contextual information can be selectively memorized. Note the query person information q is independent of the time step t, therefore serving as the global primitive memory that guides the person search procedure continuously. The effect of such memory information over the states is modeled through the parameter w q∼ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Region Shrinkage with Primitive Memory</head><p>As stated above, the goal of NPSM is to effectively shrink regions containing the target person based on the neural search mechanism, guided by the primitive memory. That is, the NPSM will decide which local region should be focused on at each recursive step in the search process as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Through this way, more context information would be included from a large region and the number of irrelevant person candidates with the target person would be recursively reduced in the search process. In this subsection, we introduce how the subregion of each recursive time-step is generated and shrunk from the bigger region of the previous time-step.</p><p>Here we define the region covered by the highlighted proposal bounding boxes induced by current attention map as follows:</p><formula xml:id="formula_1">R = (min(θx1), min(θy1), max(θx2), max(θy2)),</formula><p>where θ x1 , θ y1 , θ x2 , θ y2 are the top left and lower right corner coordinates of all the highlighted bounding boxes from a predefined collection, generated by an unsupervised object proposal model (e.g., Edgeboxes <ref type="bibr" target="#b41">[42]</ref>). Then we separate the region R into several candidate subregions according to the relationship of each contained bounding box in the region R. In this paper, we choose the Euclidean distance as the metric of the relationship defined as</p><formula xml:id="formula_2">d(a, b) = 2 i=1 (a i − b i ) 2 ,<label>(2)</label></formula><p>where a and b are the centre coordinates of two proposal bounding boxes A and B respectively. Specifically, . We denote the parent region to generate subregions R sub (C) as R par . At each recursive step t, the proposed NSN outputs an attention map which predicts the scores (reflecting confidence on containing the target person given the primitive memory information) of shrinking to region R sub t,(C) after NSN taking input the parent region R par t−1 at the previous step t − 1. More specifically, at each time step (corresponding to shrinking to one region), NSN takes input the query person feature q and the region feature x t extracted from pretrained "Res50 part1" which denotes the conv1 to conv4 3 of ResNet-50 <ref type="bibr" target="#b8">[9]</ref>. Here, we add a Region of Interest (ROI) pooling layer following "Res50 part1" to make sure the regions of different sizes can have feature maps of the same size K × K × D. Compared with the standard LSTM based model relying on multi-layer perceptron, NSN uses convolutional layers to integrate the region representation with primitive memory and produce attention maps. Specifically, at each time step t, an attention score map of size K × K for K × K locations is computed:</p><formula xml:id="formula_3">a = (a 1 , a 2 ), b = (b 1 , b 2 ), a 1 = a x1 + 0.5 (a x2 − a x1 ), a 2 = a y1 +0.5 (a y2 − a y1 ), b 1 = b x1 +0.5(b x2 −b x1 ), b 2 = b y1 + 0.5(b y2 − b y1 ). (a x1 ,<label>a</label></formula><formula xml:id="formula_4">z t = w z * tanh (w qa * q + w ha * h t + b a ) (3) l i,j t = exp(z ij t ) i j exp(z ij t )</formula><p>.</p><p>The score for location (i, j) is denoted as l i,j t . Then, in the process of region shrinkage, the NSN computes the average scores of different subregions from the parent region. NSN highlights the subregion with the maximum score as the region to be searched in the next step. This computation would be performed many times until the search path reaches the final proposal. The average score of the subregion is computed as follows:</p><formula xml:id="formula_6">St = 1 m · n m i=1 n j=1 l i,j t ,<label>(5)</label></formula><p>where m and n are the height and the width of the subregion respectively. l i,j t corresponds to the score map defined in Eqn. (4) generated on the parent region. In other words, our model does not stick to the single region. If some regions not highlighted before receive higher attention at certain search step, our model would jump to that region with higher intra-region confidence scores. In this way, the accumulative error in the shrinkage process can be alleviated. Note that our NSN serves as a region shrinkage method. In other words, our NSN only outputs the most similar proposal with the query person in each gallery image. Therefore, the features of the query person image and the final search result are extracted from the "Identification Net" (orange boxes in <ref type="figure" target="#fig_0">Figure 2</ref>) of the trained model when the searching is finished. Here, the "Identification Net" takes input the output of "Res50 Part2" (pink boxes in <ref type="figure" target="#fig_0">Figure 2</ref>) representing the conv4 4 to conv5 3 of ResNet-50. And it consists of a global average pooling layer and a 256-dimension Fully Connected layer. Then the cosine similarity between the features of the query person and the final person search result is computed for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Strategy</head><p>Here we detail the training of the proposed model. Firstly, we use the architecture proposed in OIM <ref type="bibr" target="#b32">[33]</ref> to pretrain the Fully Convolutional Networks (FCN) including both "Res50 part1" and "Res50 part2'. Then for the region at each recursive time-step, the feature is extracted from the ROI pooling layer after the pre-trained "Res50 part1". After that, all the features are fed to the NSN and we add a convolutional layer of size 1 × 1 × 2 after output of each time step to calculate the "region shrinkage loss". Here we adopt segmentation alike softmax loss as the "region shrinkage loss". The supervision label of each time step is defined as</p><formula xml:id="formula_7">Ut = 1, if G ∈ Rt 0, otherwise,<label>(6)</label></formula><p>where G is the ground truth bounding box of the target person while R t is the region box reached at the tth time step. This training strategy enables the proposed network to produce proper attention maps that fall into the region containing the target person as tight as possible. In other words, our NPSM is expected to predict the probability of the target person appearing at each location in a gallery image. Besides, to make the learned feature more discriminative, we add an identification loss following the "Identification Net", which takes input the output feature u of "Identi-fication Net" and is defined as</p><formula xml:id="formula_8">P (z = c|u) = exp(S c u) k exp(S k u) ,<label>(7)</label></formula><p>L iden = −log(P (z = c|u)). <ref type="bibr" target="#b7">(8)</ref> where there are a total of I identities, z is the identity of the person, and S is the softmax weight matrix while S c and S k represent the cth and kth column of it, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Protocol</head><p>We adopt the mean Averaged Precision (mAP) and the top-1 matching rate as performance metrics, which are also used in OIM <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b40">[41]</ref>. Using the mAP metric, person search performance is evaluated in a similar way as detection, reflecting the accuracy of detecting the query person from the gallery images. The top-1 matching rate treats person search as a ranking and localization problem. A matching is counted if a bounding box among the top-1 predicted boxes overlaps with the ground truth larger than the threshold 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In this paper, the Fully Convolutional Networks (FCN) including both "Res50 part1" and "Res50 part2" are pretrained by using the architecture proposed in OIM <ref type="bibr" target="#b32">[33]</ref>. For the input region at each time-step, we apply an ROI pooling layer on the conv4 3 convolutional features of it to normalize all the features to the same size of 14 × 14 × 1024. For query person images, we also extract their 14 × 14 × 1024 convolutional features in the same way. These features are then fed into the NPSM architecture. In particular, within NSN, the convolutional kernels for input-to-input states and state-to-state transitions are fixed as 3 × 3 with 1024 channels. At each recursive search step, we set the number C of subregions covered by clustered proposals to 3. We implement our network using Theano <ref type="bibr" target="#b27">[28]</ref> and Caffe <ref type="bibr" target="#b10">[11]</ref> deep learning framework. The training of the NPSM converges in roughly 50 hours for CUHK-SYSU dataset and 40 hours for PRW dataset on on a machine with 64GB memory, NVIDIA GeForce GTX TITAN X GPU and Intel i7-4790K Processor. The initial learning rate is 0.001 and decays at the rate of 0.9 for the weight updates of RMSProp <ref type="bibr" target="#b29">[30]</ref>. Additionally, we manually augment the data by performing random 2D translation. The speed of our method is close to realtime. For one gallery image, our model takes round 1s to output a final searched result. However, overhead of ranking over gallery is dominating. For the CUHK-SYSU with gallery size of 100, calculating cosine similarity between search result from all the gallery images and query for ranking takes round 20s. For the PRW with 6,112 gallery images, ranking over gallery takes round 15 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this subsection, we perform several analytic experiments on CUHK-SYSU benchmark to investigate the contribution of each component in our proposed NPSM architecture. We analyze attention prediction, contextual cue and primitive memory of query person. In total we have three variants by training the model based on different combinations of the above factors. And the gallery size is set to 100. The details and corresponding results are shown in <ref type="table">Table 1</ref>.</p><p>As aforementioned, we employ the framework in OIM <ref type="bibr" target="#b32">[33]</ref> which involves none of three factors, as the baseline. Based on this framework, the results of OIM <ref type="bibr" target="#b32">[33]</ref> are obtained. In the method named "NPSM w/o C", we remove the "contextual cue and primitive memory integration" part (corresponding to Eqn. (1)) of the NSN in our proposed NPSM. Instead, at each recursive step, we replace the "contextual que and primitive memory integration" part with a 3 × 3 × 1024 convolutional layer followed by the concatenation of the FCN ("Res50 part1") feature map of the query person (primitive memory) and the current step region (q and x t ). Moreover, for each recursive step, we only keep the shrinking region generation method and the attention score prediction model (Eqn. (4) and (5) ) to predict the attention score map. This setting makes our NPSM a simple version without contextual cues involved but still with the attention prediction ability. Furthermore, in the method named "NPSM w/o A&amp;C", we further remove the attention prediction model and only generate the shrinking region as the input of each recursive step and add a 1024dimension fully connected (FC) layer and a 2-dimension <ref type="table">Table 1</ref>. Results of ablation study on CUHK-SYSU dataset with 100 gallery size setting. Legend: A: Attention prediction model, C: Contextual cue, P: Primitive memory of query person. "w/o A&amp;C' and "w/o C" are short for "without Attention prediction model and Contextual cue" and "without Contextual cue but with Attention prediction model" respectively. FC layer after the output (concatenation of the FCN feature map of query person (primitive memory) and the current region) of each recursive step. And the 2-dimension FC layer aims at predicting the score of each highlighted subregion from the parent region. From comparison between the results of "OIM" and "NPSM w/o A&amp;C", we can see that simply using primitive memory of query without contextual cues involved to search for a target person in the recursive way can not achieve satisfactory results. From the result of "NPSM w/o C" , we find that the sightly higher performance is achieved than the baseline due to usage of the attention model which can introduce more spatial location information than the "NPSM w/o A&amp;C". However, both "NPSM w/o A&amp;C" and "NPSM w/o C" lack a contextual cue memory mechanism. In other words, the above methods are unable to memorize the context information provided in a larger region through previous recursive steps. From the result of "NPSM" overtaking the baseline method "OIM" by 2.4% and 2.5% for mAP and top-1 evaluation protocol, we find that the neural search mechanism induced by our proposed NPSM is beneficial for person search performance, and memory of query person can also effectively guide the neural search model to find the right person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We compare NPSM with several state-of-the-arts, including end-to-end person search ones proposed by Xiao et al. <ref type="bibr" target="#b32">[33]</ref> and Zheng et al. <ref type="bibr" target="#b40">[41]</ref> and some other methods combining commonly used pedestrian detectors (DPM <ref type="bibr" target="#b5">[6]</ref>, ACF <ref type="bibr" target="#b3">[4]</ref>, CCF <ref type="bibr" target="#b35">[36]</ref>, LDCF <ref type="bibr" target="#b20">[21]</ref> and their respective R-CNN <ref type="bibr" target="#b6">[7]</ref>) with hand-crafted features (BoW <ref type="bibr" target="#b39">[40]</ref>, LOMO <ref type="bibr" target="#b16">[17]</ref>, DenseSIFT-ColorHist (DSIFT) <ref type="bibr" target="#b38">[39]</ref>) and distance metrics (KISSME <ref type="bibr" target="#b11">[12]</ref>, XQDA <ref type="bibr" target="#b16">[17]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Results on CUHK-SYSU</head><p>We report the person search performance on CUHK-SYSU with 100 gallery size setting in <ref type="table" target="#tab_1">Table 2</ref>, where "CNN" represents the detector part (Faster-RCNN <ref type="bibr" target="#b23">[24]</ref> with ResNet-50) and "IDNet" denotes the re-identification part in the framework of OIM <ref type="bibr" target="#b32">[33]</ref>. Compared with CNN+IDNet, the OIM achieves performance improvement by introducing joint optimization of the detection and identification parts, but still follows the isolated "detection+re-identification" two-stage strategy in the person search process. Comparatively, our proposed NPSM is a detection-free method and solves localization and re-identification of the query person simultaneously by introducing the query-aware region shrinkage mechanism which can include more contextual information beneficial for search accuracy. It can be verified by results shown in <ref type="table" target="#tab_1">Table 2</ref>. NPSM beats all compared methods consistently for both the mAP and top-1 metrics. Moreover, <ref type="figure" target="#fig_3">Figure 3</ref> shows the mAP of the compared methods with different gallery sizes, including [50, 100, 500, 1000, 2000, 4000]. One can see that the mAP drops gradually as the gallery size increases, but our method can still outperform all other methods under different gallery size settings. In particular, NPSM improves average performance per gallery size over OIM <ref type="bibr" target="#b32">[33]</ref> by around 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth Query image</head><p>Attention maps </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Results on PRW</head><p>On PRW dataset, we conduct experiments to compare NPSM with some state-of-the-art methods combining different detectors (respective R-CNN <ref type="bibr" target="#b6">[7]</ref> detectors of DPM <ref type="bibr" target="#b5">[6]</ref>, CCF <ref type="bibr" target="#b35">[36]</ref>,ACF <ref type="bibr" target="#b3">[4]</ref>, LDCF <ref type="bibr" target="#b20">[21]</ref>) and recognizers (LOMO, XQDA <ref type="bibr" target="#b16">[17]</ref>, IDE det , CWS <ref type="bibr" target="#b40">[41]</ref>). Among them, AlexNet <ref type="bibr" target="#b12">[13]</ref> is exploited as the base network for the R-CNN detector. Although VGGNet <ref type="bibr" target="#b25">[26]</ref> and ResNet <ref type="bibr" target="#b8">[9]</ref> have more parameters and are deeper than AlexNet, according to <ref type="bibr" target="#b40">[41]</ref>, AlexNet can achieve better performance than the other two for DPM and ACF incorporating different recognizers. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Because the OIM method is the baseline of our NPSM, we implement the source code provided in OIM <ref type="bibr" target="#b32">[33]</ref> to obtain the baseline result on the PRW dataset. Compared with the result, our NPSM outperforms it by 2.9% and 3.2% for mAP and top-1 accuracy separately. Besides, compared with all other stateof-the-arts considering five bounding boxes per gallery image, our method achieves better performance by only keeping one bounding box for testing per gallery image.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we visualize some attention maps produced by our NPSM for testing samples from CUHK-SYSU and PRW datasets, which are all ranked top 1 in search results. The first three rows are from CUHK-SYSU, while the bottom row is from PRW. We observe that our NPSM can effectively shrink the search region to the correct person region guided by primitive memory of the query person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we introduced a novel neural person search machine solving person search through recursively localiz- ing effective regions, with guidance from the memory of the query person. Extensive experiments on two public benchmarks demonstrated its superiority over state-of-the-arts in most cases and the benefit to recognition accuracy in person search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of our proposed Neural Person Search Machines (NPSM). It consists of two components, i.e. Primitive Memory and Neural Search Networks. It works by recursively shrinking the search area from the whole image to the precise bounding box of the person of interest under the guidance (orange dotted lines) of Primitive Memory. And each region in the shrinking search process would contain the contextual information of the final search result. Red boxes denote the shrinking regions highlighted at different recursive steps in our NPSM. "Res50 Part1" corresponds to the conv1 to conv4 3 of ResNet-50 while "Res50 Part2" represents the conv4 4 to conv5 3 of ResNet-50. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y1 ) and (a x2 , a y2 ) are the top left and lower right coordinates of bounding box A while (b x1 , b y1 ) and (b x2 , b y2 ) are the top left and lower right coordinates of bounding box B. Then the proposal bounding boxes can be grouped into C clusters according to their relationships. The corresponding subregions covered by proposals are R sub (C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Test mAPs of different approaches under different gallery sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Attention maps learned by our NPSM model for different testing person samples in CUHK-SYSU and PRW dataset. The first three rows are from CUHK-SYSU, while the bottom row is from PRW. Green boxes represent the ground truth boxes while red boxes are the region bounding boxes highlighted by our NPSM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of NPSM's performance on CUHK-SYSU with 100 gallery size setting with the state-of-the-arts.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP(%) top-1(%)</cell></row><row><cell>ACF [4]+DSIFT [39]+Euclidean</cell><cell>21.7</cell><cell>25.9</cell></row><row><cell>ACF+DSIFT+KISSME [12]</cell><cell>32.3</cell><cell>38.1</cell></row><row><cell>ACF+BoW [40]+Cosine</cell><cell>42.4</cell><cell>48.4</cell></row><row><cell>ACF+LOMO+XQDA [17]</cell><cell>55.5</cell><cell>63.1</cell></row><row><cell>ACF+IDNet [33]</cell><cell>56.5</cell><cell>63.0</cell></row><row><cell>CCF [36]+DSIFT+Euclidean</cell><cell>11.3</cell><cell>11.7</cell></row><row><cell>CCF+DSIFT+KISSME</cell><cell>13.4</cell><cell>13.9</cell></row><row><cell>CCF+BoW+Cosine</cell><cell>26.9</cell><cell>29.3</cell></row><row><cell>CCF+LOMO+XQDA</cell><cell>41.2</cell><cell>46.4</cell></row><row><cell>CCF+IDNet</cell><cell>50.9</cell><cell>57.1</cell></row><row><cell>CNN [24]+DSIFT+Euclidean</cell><cell>34.5</cell><cell>39.4</cell></row><row><cell>CNN+DSIFT+KISSME</cell><cell>47.8</cell><cell>53.6</cell></row><row><cell>CNN+BoW+Cosine</cell><cell>56.9</cell><cell>62.3</cell></row><row><cell>CNN+LOMO+XQDA</cell><cell>68.9</cell><cell>74.1</cell></row><row><cell>CNN+IDNet</cell><cell>68.6</cell><cell>74.8</cell></row><row><cell>OIM [33](Baseline)</cell><cell>75.5</cell><cell>78.7</cell></row><row><cell>NPSM</cell><cell>77.9</cell><cell>81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of NPSM's performance on PRW with stateof-the-arts.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP(%) top-1(%)</cell></row><row><cell>DPM-Alex+LOMO+XQDA [17]</cell><cell>13.0</cell><cell>34.1</cell></row><row><cell>DPM-Alex+IDE det [41]</cell><cell>20.3</cell><cell>47.4</cell></row><row><cell>DPM-Alex+IDE det +CWS [41]</cell><cell>20.5</cell><cell>48.3</cell></row><row><cell>ACF-Alex+LOMO+XQDA</cell><cell>10.3</cell><cell>30.6</cell></row><row><cell>ACF-Alex+IDE det</cell><cell>17.5</cell><cell>43.6</cell></row><row><cell>ACF-Alex+IDE det +CWS</cell><cell>17.8</cell><cell>45.2</cell></row><row><cell>LDCF+LOMO+XQDA</cell><cell>11.0</cell><cell>31.1</cell></row><row><cell>LDCF+IDE det</cell><cell>18.3</cell><cell>44.6</cell></row><row><cell>LDCF+IDE det +CWS</cell><cell>18.3</cell><cell>45.5</cell></row><row><cell>OIM(Baseline)</cell><cell>21.3</cell><cell>49.9</cell></row><row><cell>NPSM</cell><cell>24.2</cell><cell>53.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61371155, Grant 61174170, and Grant 61632007. The work of Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112 and IDS R-263-000-C67-646.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3908" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cognitive psychology and its implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>WH Freeman/Times Books/Henry Holt &amp; Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning complexityaware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3361" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale learning for low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3765" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Video-based person re-identification with accumulative motion context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00193</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernelized relaxed margin components analysis for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="910" to="914" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local decorrelation for improved pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable semi-automatic annotation for multi-camera person tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niño-Castañeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frías-Velázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slembrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2259" to="2274" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience the Official Journal of the Society for Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4700" to="4719" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person reidentification by dual-regularized kiss metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2726" to="2738" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<idno>abs/1605.02688</idno>
		<title level="m">Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning strong parts for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1904" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Personnet: Person reidentification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07255</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07528</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large scale similarity learning using similar pairs for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02139</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02531</idno>
		<title level="m">Person re-identification in the wild</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
