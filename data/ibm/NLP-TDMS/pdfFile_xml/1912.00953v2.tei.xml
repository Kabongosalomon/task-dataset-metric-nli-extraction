<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LOGAN: LATENT OPTIMISATION FOR GENERATIVE ADVERSARIAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
							<email>yanwu@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
							<email>jeffdonahue@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
							<email>dbalduzzi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
							<email>simonyan@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>UK</roleName><forename type="first">Deepmind</forename><surname>London</surname></persName>
						</author>
						<title level="a" type="main">LOGAN: LATENT OPTIMISATION FOR GENERATIVE ADVERSARIAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training generative adversarial networks requires balancing of delicate adversarial dynamics. Even with careful tuning, training may diverge or end up in a bad equilibrium with dropped modes. In this work, we improve CS-GAN with natural gradient-based latent optimisation and show that it improves adversarial dynamics by enhancing interactions between the discriminator and the generator. Our experiments demonstrate that latent optimisation can significantly improve GAN training, obtaining state-of-the-art performance for the ImageNet (128 × 128) dataset. Our model achieves an Inception Score (IS) of 148 and an Fréchet Inception Distance (FID) of 3.4, an improvement of 17% and 32% in IS and FID respectively, compared with the baseline BigGAN-deep model with the same architecture and number of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Generative Adversarial Nets (GANs) are implicit generative models that can be trained to match a given data distribution. GANs were originally developed by <ref type="bibr" target="#b20">Goodfellow et al. (2014)</ref> for image data. As the field of generative modelling has advanced, GANs remain at the frontier, generating highfidelity images at large scale <ref type="bibr" target="#b7">(Brock et al., 2018;</ref><ref type="bibr" target="#b24">Karras et al., 2019)</ref>. However, despite growing insights into the dynamics of GAN training, much of the progress in GAN-based image generation come from network architecture improvements <ref type="bibr" target="#b38">(Radford et al., 2015;</ref><ref type="bibr" target="#b49">Zhang et al., 2019)</ref>, or regularisation of particular parts of the model .</p><p>Build on the compressed sensing view of GANs (CS-GAN; <ref type="bibr" target="#b48">Wu et al., 2019)</ref>, we improve the efficacy of latent optimisation in adversarial games, using natural gradient descent to optimise the latent variable (usually denoted z) towards the direction favoured by the discriminator during training. This results in a scalable and easy to implement approach that improves the dynamic interaction between the discriminator and the generator. We generally call these approaches latent optimised GANs (LOGAN).</p><p>To summarise our contributions:</p><p>1. We propose an improved, efficient approach to latent optimisation using natural gradient descent.</p><p>2. Our algorithm improves the state-of-the-art BigGAN <ref type="bibr" target="#b7">(Brock et al., 2018)</ref> by a significant margin, without introducing any architectural change, resulting in higher quality images and more diverse samples (see <ref type="table" target="#tab_0">Table 1</ref>, <ref type="figure">Figure 1</ref> and 2).</p><p>3. To provide theoretical insight, we analyse latent optimisation in GANs from the perspective of differentiable games <ref type="bibr" target="#b4">(Balduzzi et al., 2018)</ref>. We argue that latent optimisation can be viewed as improving the dynamics of adversarial training. We use θ D and θ G to denote the vectors representing parameters of the generator and discriminator.</p><p>We use x for images, and z for the latent source generating an image. We use prime to denote a variable after one update step, e.g., θ D = θ D − α ∂f (z;θ D ,θ G ) ∂θ D</p><p>. p(x) and p(z) denote the data distribution and source distribution respectively. E p(x) [f (x)] indicates taking the expectation of function f (x) over the distribution p(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GENERATIVE ADVERSARIAL NETS</head><p>A GAN consists of a generator that generates image x = G(z; θ G ) from a latent source z ∼ p(z), and a discriminator that scores the generated images as D(x; θ D ) <ref type="bibr" target="#b20">(Goodfellow et al., 2014)</ref>. Training GANs involves an adversarial game: while the discriminator tries to distinguish generated samples  <ref type="bibr" target="#b7">Brock et al. (2018)</ref>. "baseline" indicates our reproduced BigGAN-deep with small modifications. The 3rd and 4th columns are from the gradient descent (GD, ablated) and natural gradient descent (NGD) versions of LOGAN respectively. We report the Inception Score (IS, higher is better, <ref type="bibr" target="#b42">Salimans et al. 2016)</ref> and Fréchet Inception Distance (FID, lower is better, <ref type="bibr" target="#b22">Heusel et al. 2017)</ref>.</p><p>FID IS BigGAN-deep 5.7 ± 0.3 124.5 ± 2.0 baseline 4.92 ± 0.05 126.6 ± 1.3 LOGAN (GD)</p><p>4.86 ± 0.09 127.7 ± 3.5 LOGAN (NGD) 3.36 ± 0.14 148.2 ± 3.1</p><formula xml:id="formula_0">x = G (z; θ G ) from data x ∼ p(x)</formula><p>, the generator tries to fool the discriminator. This procedure can be summarised as the following min-max game:</p><formula xml:id="formula_1">min θ D max θ G E x∼p(x) [h D (D (x; θ D ))] + E z∼p(z) [h G (D (G (z; θ G ) ; θ D ))]<label>(1)</label></formula><p>The exact form of h(·) depends on the choice of loss function <ref type="bibr" target="#b20">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b35">Nowozin et al., 2016)</ref>. To simplify our presentation and analysis, we use the Wasserstein loss , so that h D (t) = −t and h G (t) = t. Our experiments with BigGANdeep use the hinge loss <ref type="bibr" target="#b28">(Lim &amp; Ye, 2017;</ref><ref type="bibr" target="#b45">Tran et al., 2017)</ref>, which is identical to this form in its linear regime. Our analysis can be generalised to other losses as in previous theoretical work (e.g., <ref type="bibr" target="#b3">Arora et al. 2017)</ref>. To simplify notation, we abbreviate f (z;</p><formula xml:id="formula_2">θ D , θ G ) = D (G (z; θ G ) ; θ D )</formula><p>, which may be further simplified as f (z) when the explicit dependency on θ D and θ G can be omitted.</p><p>Training GANs requires carefully balancing updates to D and G, and is sensitive to both architecture and algorithm choices <ref type="bibr" target="#b42">(Salimans et al., 2016;</ref><ref type="bibr" target="#b38">Radford et al., 2015)</ref>. A recent milestone is BigGAN <ref type="bibr">(and BigGAN-deep, Brock et al. 2018)</ref>, which pushed the boundary of high fidelity image generation by scaling up GANs to an unprecedented level. BigGANs use an architecture based on residual blocks <ref type="bibr" target="#b21">(He et al., 2016)</ref>, in combination with regularisation mechanisms and self-attention <ref type="bibr" target="#b43">(Saxe et al., 2014;</ref><ref type="bibr" target="#b49">Zhang et al., 2019)</ref>.</p><p>Here we aim to improve the adversarial dynamics during training. We focus on the second term in eq. 1 which is at the heart of the min-max game. For clarity, we explicitly write the losses for D as L D (z) = h D (f (z)) and G as L G (z) = h G (f (z)), so the total loss vector can be written as</p><formula xml:id="formula_3">L(z) = [L D (z), L G (z)] T = [f (z), −f (z)] T<label>(2)</label></formula><p>Computing the gradients with respect to θ D and θ G gives the following vector field, which cannot be expressed as the gradient of any single function <ref type="bibr" target="#b4">(Balduzzi et al., 2018)</ref>:</p><formula xml:id="formula_4">g = ∂L D (z) ∂θ D , ∂L G (z) ∂θ G T = ∂f (z) ∂θ D , − ∂f (z) ∂θ G T<label>(3)</label></formula><p>The fact that g is not the gradient of a function implies that gradient updates in GANs can exhibit cycling behaviour which can slow down or prevent convergence. <ref type="bibr" target="#b4">Balduzzi et al. (2018)</ref> refer to vector fields of this form as the simultaneous gradient. Although many GAN models use alternating update rules (e.g., <ref type="bibr" target="#b20">Goodfellow et al. 2014;</ref><ref type="bibr" target="#b7">Brock et al. 2018)</ref>, following the gradient with respect to θ D and θ G alternatively in each step, they share the same problem from gradients of this form. Therefore, we use the simpler simultaneous gradient (eq. 3) for our analysis (see also <ref type="bibr" target="#b30">Mescheder et al. 2017;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LATENT OPTIMISED GANS</head><p>Inspired by compressed sensing <ref type="bibr" target="#b8">(Candes et al., 2006;</ref><ref type="bibr" target="#b16">Donoho, 2006)</ref>, <ref type="bibr" target="#b48">Wu et al. (2019)</ref> introduced latent optimisation for GANs. Latent optimisation exploits knowledge from D to refine the latent source z. Intuitively, the gradient ∇ z f (z) = ∂f (z) ∂z points in the direction that better satisfies the discriminator D, which implies better samples. Therefore, instead of using the randomly sampled (a) (b) <ref type="figure" target="#fig_4">Figure 3</ref>: (a) Schematic of LOGAN. We first compute a forward pass through G and D with a sampled latent z. Then, we use gradients from the generator loss (dashed red arrow) to compute an improved latent, z . After we use this optimised latent code in a second forward pass, we compute gradients of the discriminator back through the latent optimisation into the model parameters θ D , θ G . We use these gradients to update the model. (b) Truncation curves illustrate the FID/IS trade-off for each model by altering the range of the noise source p(z). GD: gradient descent. NGD: natural gradient descent. Points A, B, C, D correspond to samples shown in <ref type="figure">Figure 1</ref> and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Latent Optimised GANs with Automatic Differentiation</head><p>Input: data distribution p(x), latent distribution p(z), D (·; θ D ), G (·; θ G ), learning rate α, batch size N repeat Initialise discriminator and generator parameters θ D , θ G for i = 1 to N do Sample z ∼ p(z), x ∼ p(x) Compute the gradient ∂D(G(z)) ∂z and use it to obtain ∆z from eq. 4 (GD) or eq. 16 (NGD) Optimise the latent z ← [z + ∆z], <ref type="bibr">[·]</ref> indicates clipping the value between −1 and 1 Compute generator loss L (i)</p><formula xml:id="formula_5">G = −D(G(z )) Compute discriminator loss L (i) D = D(G(z )) − D(x) end for Compute batch losses L G = 1 N N i=1 L (i) G and L D = 1 N N i=1 L (i) D Update θ D and θ G with the gradients ∂L D ∂θ D , ∂L G ∂θ G</formula><p>until reaches the maximum training steps z ∼ p(z), <ref type="bibr" target="#b48">Wu et al. (2019)</ref> uses the optimised latent</p><formula xml:id="formula_6">∆z = α ∂f (z) ∂z z = z + ∆z<label>(4)</label></formula><p>in eq. 1 for training 1 .</p><p>Historically, compressed sensing has been developed as a signal processing technique mostly without any concern on training. However, here we emphasise the influence of this procedure on training, which we will show dominates the effects on large scale models -in contrast, the run-time optimisation that is central in compressed sensing may be unnecessary after training. Therefore, we call this type of models latent-optimised GANs (LOGAN) to avoid any confusion, except when explicitly referring to the results from <ref type="bibr" target="#b48">Wu et al. (2019)</ref>. Latent optimisation has been shown to improve the stability of training as well as the final performance for medium-sized models such as DCGANs and Spectral Normalised GANs <ref type="bibr" target="#b38">(Radford et al., 2015;</ref>. The general algorithm is summarised in Algorithm 1 and illustrated in <ref type="figure" target="#fig_4">Figure 3</ref> a. However, we found that the potential of latent optimisation remained largely untapped in this setting, and develop the natural gradient descent form of latent update in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF THE ALGORITHM</head><p>To understand how latent optimisation interacts with GAN training, we analyse LOGAN as a differentiable game following <ref type="bibr" target="#b4">Balduzzi et al. (2018)</ref>; <ref type="bibr" target="#b19">Gemp &amp; Mahadevan (2018)</ref>; <ref type="bibr" target="#b27">Letcher et al. (2019)</ref>. The Appendix A provides a complementary analysis form the perspective of stochastic approximation <ref type="bibr" target="#b22">(Heusel et al., 2017;</ref><ref type="bibr" target="#b6">Borkar, 1997)</ref>. We can explicitly compute the gradients for the discriminator and generator at z after one step of latent optimisation by differentiating</p><formula xml:id="formula_7">[L D (z ), L G (z )] T = [f (z ), −f (z )] (where z = z + ∆z from eq. 4): dL D dθ D , dL G dθ G T = ∂f (z ) ∂θ D + ∂∆z ∂θ D T ∂f (z ) ∂∆z , − ∂f (z ) ∂θ G − ∂∆z ∂θ G T ∂f (z ) ∂∆z T (5) = ∂f (z ) ∂θ D + α ∂ 2 f (z) ∂z∂θ D T ∂f (z ) ∂z , − ∂f (z ) ∂θ G − α ∂ 2 f (z) ∂z∂θ G T ∂f (z ) ∂z T<label>(6)</label></formula><p>In both equations, the first terms represent how f (z ) depends on the parameters directly, which also appear in the gradients from vanilla GANs (eq. 3). However, the second terms are introduced from latent optimisation, accounting for how f (z ) depends on the parameters via the change ∆z. For the second equality, we substitute ∆z = α ∂f (z) ∂z as the gradient-based update of z and use ∂f (z ) ∂∆z = ∂f (z ) ∂z . Further differentiating ∆z results in the second-order terms ∂ 2 f (z)</p><formula xml:id="formula_8">∂z∂θ D T and ∂ 2 f (z) ∂z∂θ G T .</formula><p>The original GAN's gradient (eq. 3) does not include any second-order term, since ∆z = 0 without latent optimisation. LOGAN computes these extra terms by automatic differentiation when backpropagating through the latent optimisation process (see Algorithm 1). <ref type="bibr" target="#b4">Balduzzi et al. (2018)</ref>; <ref type="bibr" target="#b19">Gemp &amp; Mahadevan (2018)</ref> proposed Symplectic Gradient Adjustment (SGA) to improve the dynamics of gradient-based methods in adversarial games. SGA addresses an important problem with gradient-based optimisation in GANs: the vector-field generated by the losses of the discriminator and generator is not a gradient vector field. It follows that gradient descent is not guaranteed to find a local optimum and can cycle, which can slow down convergence or lead to phenomena like mode collapse and mode hopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RELATION WITH SGA</head><p>For a game with gradient g (eq. 3), the Hessian is the second order derivatives with respect to the parameters, H = ∇ θ g. SGA uses the adjusted gradient</p><formula xml:id="formula_9">g * = g + λ A T g<label>(7)</label></formula><p>where λ is a positive constant and A = 1 2 (H −H T ) is the anti-symmetric component of the Hessian. Applying SGA to GANs yields the adjusted updates (see Appendix A.1 for details):</p><formula xml:id="formula_10">g * = ∂f (z) ∂θ D + λ ∂ 2 f (z) ∂θ G ∂θ D T ∂f (z) ∂θ G , − ∂f (z) ∂θ G + λ ∂ 2 f (z) ∂θ D ∂θ G T ∂f (z) ∂θ D T<label>(8)</label></formula><p>Compared with g in eq. 3, the adjusted gradient g * has second-order terms reflecting the interactions between D and G. SGA significantly improves GAN training in simple examples <ref type="bibr" target="#b4">(Balduzzi et al., 2018)</ref>, allowing faster and more robust convergence to stable fixed points (local Nash equilibria). Unfortunately, SGA is expensive to scale because computing the second-order derivatives with respect to all parameters is expensive. It remains unclear whether SGA can be incorporated into very large scale models using more efficient implementation (e.g., Hessian-vector products from modified back propagation Pearlmutter <ref type="formula" target="#formula_1">(1994)</ref>).</p><p>The SGA updates in eq. 8 and the LOGAN updates in eq. 6 are strikingly similar, suggesting that the latent step used by LOGAN reduces the negative effects of cycling by introducing a symplectic gradient adjustment into the optimisation procedure. The role of the latent step can be formalised in terms of a third player, whose goal is to help the generator (see appendix A for details). Crucially, latent optimisation approximates SGA using only second-order derivatives with respect to the latent z and parameters of the discriminator and generator separately. The second order terms involving parameters of both the discriminator and the generator -which are expensive to compute -are not used. In short, with a simple modification of the original GAN training algorithm, latent optimisation couples the gradients of the discriminator and generator in a way similar to SGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RELATION WITH UNROLLED GANS</head><p>In addition, latent optimisation can be seen as unrolling GANs <ref type="bibr" target="#b32">(Metz et al., 2016)</ref> in the space of the latent source, rather than the parameters. Unrolling in the latent space has the advantages that:</p><p>1. LOGAN is more scalable than Unrolled GANs because it avoids unrolling the parameter updating process, which is prohibitively expensive for models with a large number of parameters. 2. While unrolling the update of D only affects the parameters of G (as in <ref type="bibr" target="#b32">Metz et al. 2016)</ref>, latent optimisation effects both D and G as shown in eq. 6.</p><p>We next formally present this connection by first showing that SGA can be seen as approximating Unrolled GANs <ref type="bibr" target="#b32">(Metz et al., 2016)</ref>. For the update θ D = θ D + ∆θ D , we have the Taylor expansion approximation at θ D :</p><formula xml:id="formula_11">f (z; θ D + ∆θ D , θ G ) ≈ f (z; θ D , θ G ) + ∂f (z; θ D , θ G ) ∂θ D T ∆θ D<label>(9)</label></formula><p>Substitute the gradient descent parameter update</p><formula xml:id="formula_12">∆θ D = −α ∂f (z;θ D ,θ G ) ∂θ D</formula><p>, and take the derivatives with respect to θ G on both sides:</p><formula xml:id="formula_13">∂f (z; θ D + ∆θ D , θ G ) ∂θ G ≈ ∂f (z; θ D , θ G ) ∂θ G − 2α ∂ 2 f (z; θ D , θ G ) ∂θ D ∂θ G T ∂f (z; θ D , θ G ) ∂θ D<label>(10)</label></formula><p>which has the same form as eq. 8 (taking the negative sign). Compared with the exact gradient from the unroll:</p><formula xml:id="formula_14">∂f (z; θ D + ∆θ D , θ G ) ∂θ G = ∂f (z; θ D , θ G ) ∂θ G − 2α ∂ 2 f (z; θ D , θ G ) ∂θ D ∂θ G T ∂f (z; θ D , θ G ) ∂(θ D )<label>(11)</label></formula><p>The approximation in eq. 10 comes from using ∂f</p><formula xml:id="formula_15">(z;θ D ,θ G ) ∂θ D ≈ ∂f (z;θ D ,θ G ) ∂θ D and ∂f (z;θ D ,θ G ) ∂θ G ≈ ∂f (z;θ D ,θ G ) ∂θ G</formula><p>as a result of additional linear approximation.</p><p>At this point, unrolling D update only affects θ D . Although it is expensive to unroll both D and G, in principle, we can unroll G update and compute the gradient of θ D similarly using</p><formula xml:id="formula_16">∆θ G = α ∂f (z;θ D ,θ G ) ∂θ G : ∂f (z; θ D , θ G + ∆θ G ) ∂θ D ≈ ∂f (z; θ D , θ G ) ∂θ D + 2α ∂ 2 f (z; θ D , θ G ) ∂θ G ∂θ D T ∂f (z; θ D , θ G ) ∂θ G<label>(12)</label></formula><p>which gives us the same update rule as SGA (eq. 8). This correspondence based on first order Taylor expansion is unsurprising, as SGA is based on linearising the adversarial dynamics <ref type="bibr" target="#b4">(Balduzzi et al., 2018)</ref>.</p><p>Therefore, given the previous section, we can view LOGAN as further approximating Unrolled GAN, by unrolling the update of latent source z instead of the parameters. Although the information from z is limited compared with all the parameters, the intuition from Unrolled GANs applies here: unrolling the update of z gives D and G extra information to react to their opponents, thus avoiding the circular behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOGAN WITH NATURAL GRADIENT DESCENT</head><p>Our analysis explains why latent optimisation may help GAN training. In practice, we expect more benefit from latent optimisation from stronger optimiser for z, which can better capture the coupling between D and G. <ref type="bibr" target="#b48">Wu et al. (2019)</ref> only used basic gradient descent (GD) with a fixed step-size. This choice limits the size ∆z can take: in order not to overshoot when the curvature is large, the step size would be too conservative when the curvature is small. We hypothesis that GD is more detrimental for larger models, which have complex loss surfaces with highly varying curvatures. Consistent with this hypothesis, we observed only marginal improvement over the baseline using GD (section 5.2.3, <ref type="table" target="#tab_0">Table 1</ref>, <ref type="figure" target="#fig_4">Figure 3</ref> b).</p><p>In this work, we propose using natural gradient descent (NGD, Amari 1998) for latent optimisation. NGD is an approximate second-order optimisation method, and has been applied successfully in many domains <ref type="bibr" target="#b36">(Pascanu &amp; Bengio, 2013;</ref><ref type="bibr" target="#b29">Martens, 2014)</ref>. By using the positive semi-definite (PSD) Gauss-Newton matrix to approximate the (possibly negative definite) Hessian, NGD often works even better than exact second-order methods. NGD is expensive in high dimensional parameter spaces, even with approximations <ref type="bibr" target="#b29">(Martens, 2014)</ref>. However, we demonstrate that it is efficient for latent optimisation, even in very large models.</p><p>Given the gradient of z, g = ∂f (z) ∂z , NGD computes the update as</p><formula xml:id="formula_17">∆z = α F −1 g<label>(13)</label></formula><p>where the Fisher information matrix F is defined as</p><formula xml:id="formula_18">F = E p(t|z) ∇ ln p(t|z) ∇ ln p(t|z) T<label>(14)</label></formula><p>The log-likelihood function ln p(t|z) typically corresponds to commonly used error functions such as the cross entropy loss. This correspondence is not necessary when we interpret NGD as an approximate second-order method, as has long been done <ref type="bibr" target="#b29">(Martens, 2014)</ref>. Nevertheless, Appendix D provides a Poisson log-likelihood interpretation for the hinge loss commonly used in GANs <ref type="bibr" target="#b28">(Lim &amp; Ye, 2017;</ref><ref type="bibr" target="#b45">Tran et al., 2017</ref>). An important difference between latent optimisation and commonly seen scenarios using NGD is that the expectation over the condition (z) is absent. Since each z is only responsible for generating one image, it only minimises the loss L G (z) for this particular instance.</p><p>More specifically, we use the empirical Fisher F with Tikhonov damping, as in TONGA (Roux et al., 2008) F = g · g T + β I (15) F is cheaper to compute compared with the full Fisher, since g is already available. The damping factor β regularises the step size, which is important when F only poorly approximates the Hessian or when the Hessian changes too much across the step. Using the Sherman-Morrison formula, the NGD update can be simplified into the following closed form:</p><formula xml:id="formula_19">∆z = α I β − g g T β 2 + β g T g g = α β + g 2 g<label>(16)</label></formula><p>which does not involve any matrix inversion. Thus, NGD adapts the step size according to the curvature estimate c = 1 β+ g 2 . When β is small, NGD normalises the gradient by its squared L2norm. NGD automatically smooths the scale of updates by down-scaling the gradients as their norm grows, which also contributes to the smoothed norms of updates (Appendix A.2). Since the NGD update remains proportional to g, our analysis based on gradient descent in section 3 still holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADDITIONAL REGULARISATION</head><p>Various regularisation techniques are often necessary to ensure the stable training of GANs. Here we highlight two of them that we found particularly useful in combination with LOGAN. First, we found regularising the Euclidean norm of optimisation step,</p><formula xml:id="formula_20">R z = w r · ∆z 2 2<label>(17)</label></formula><p>where the scalar weight w r is a parameter, as introduced by <ref type="bibr" target="#b48">Wu et al. (2019)</ref> is necessary, especially for large models. This term is added to both the generator loss and discriminator loss in training. <ref type="bibr" target="#b48">Wu et al. (2019)</ref> suggested this term is related to optimal transport; more recently, <ref type="bibr" target="#b44">Tanaka (2019)</ref> formalised this connection in Discriminator Optimal Transport (DOT). We left the exact connection between our work and DOT to future investigation, but here note that while DOT improves evaluation performance, our method mainly focuses on training. Consequently, although the regulariser R z shares the same form as that in DOT, they function differently: it regularises the update of parameters here, but the latent code z in DOT.</p><p>In addition, we found it is more stable to optimise only a portion c of z, leaving some of its elements completely random, which can be seem an additional damping mechanism while preserve more randomness from the latent source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS AND ANALYSIS</head><p>We tested our algorithm for both medium (DCGAN, <ref type="bibr" target="#b38">Radford et al. 2015;</ref>) and large scale <ref type="bibr">(BigGAN, Brock et al. 2018</ref>) models. We use the standard hyper-parameter settings for each GAN model, without further optimising them with LOGAN. We performed grid-search over the four parameters introduced in LOGAN: the latent step size α, damping factor β, the regularisation weight w r , and the portion of z being optimised as c. Details of the grid search are summarised in Appendix E. Additional empirical analysis of latent optimisation is presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTS WITH DCGAN ON CIFAR</head><p>To verify if our proposed NGD optimiser works well for latent optimisation, we first test LOGAN at more moderate scales for direct comparison with <ref type="bibr" target="#b48">Wu et al. (2019)</ref> using basic GD. Here we apply latent optimisation on Spectral Normalised GANs (SN-GANs, .</p><p>The experiments follows the same basic setup and hyper-parameter settings as the CS-GAN in <ref type="bibr" target="#b48">Wu et al. (2019)</ref>. There is no class conditioning in this model. With NGD, we use a large step size of α = 0.9 and the damping factor β = 0.1 for optimising z. We found the weight of 0.1 for the regulariser R z (eq. 17), and optimising 80% of the latent source worked best for SN-GANs. All other parameters are same as in <ref type="bibr" target="#b48">Wu et al. 2019</ref>.</p><p>In addition, we found running extra latent optimisation steps benefited evaluation, so we use ten steps of latent optimisation in evaluation for results in this section, although the models were still trained with a single optimisation step. This is different from in larger models, where optimisation is unnecessary in evaluation (see section 5.2.2 for more details). <ref type="table" target="#tab_1">Table 2</ref> shows the FID and IS alongside SN-GAN and CS-CAN which used the same architecture. The scores are computed based on 10, 000 samples following the same procedure as in <ref type="bibr" target="#b48">Wu et al. (2019)</ref>. We observe that NGD brought significant improvement over CS-GAN (i.e., LOGAN with GD for optimising z). Compared with the baseline SN-GAN model without employing any latent optimisation, there is an improvement of 16.8% in IS and 39.6% in FID. <ref type="figure" target="#fig_1">Figure 4</ref> compares random samples from these two models. Overall, samples from LOGAN (NGD) have higher contrasts and sharper contours.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">MODEL CONFIGURATION</head><p>We used the standard BigGAN-deep architecture with three minor modifications: 1. We increased the size of the latent source from 128 to 256, to compensate the randomness of the source lost when optimising z. 2. We use the uniform distribution U(−1, 1) instead of the standard normal distribution N (0, 1) for p(z) to be consistent with the clipping operation (Algorithm 1). 3. We use leaky ReLU (with the slope of 0.2 for the negative part) instead of ReLU as the non-linearity for smoother gradient flow for ∂f (z) ∂z . Consistent with the detailed findings in <ref type="bibr" target="#b7">Brock et al. (2018)</ref>, our experiment with this baseline model obtains only slightly better scores compared with those in <ref type="bibr" target="#b7">Brock et al. (2018)</ref>  <ref type="table" target="#tab_0">(Table 1</ref>, see also <ref type="figure" target="#fig_0">Figure 12</ref> in Appendix G). We computed the FID and IS as in <ref type="bibr" target="#b7">Brock et al. (2018)</ref>, and computed IS values from checkpoints with the lowest FIDs. Finally, we computed the means and standard deviations for both measures from 5 models with different random seeds.</p><p>To apply latent optimisation with NGD, we use the same large step size of α = 0.9 as in SN-GAN (section 5.1). However, we found much heavier damping is essential for BigGAN, so we use the damping factor β = 5.0, and only optimise 50% of z's elements. Consistent with Tanaka (2019), we found a much larger weight of 300.0 for the regulariser R z (eq. 17) works best, since deeper models generally have larger Lipschitz constants. All other hyper-parameters, including learning rates and a large batch size of 2048, remain the same as in BigGAN-deep. We call this model LOGAN (NGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">BASIC RESULTS</head><p>Employing the same architecture and number of parameters as the BigGAN-deep baseline, LOGAN (NGD) achieved better FID and IS <ref type="table" target="#tab_0">(Table 1)</ref>. As observed by <ref type="bibr" target="#b7">Brock et al. (2018)</ref>, BigGAN training eventually collapsed in every experiment. Training with LOGAN also collapsed, perhaps due to higher-order dynamics beyond the scope we have analysed, but it took significantly longer (600k steps versus 300k steps with BigGAN-deep).</p><p>During training, LOGAN was about 3 times slower per step compared with BigGAN-deep because of the additional forward and backward passes. In contrast to experiments with smaller models (section 5.1), we found that optimising z during evaluation did not improve sample scores (even up to 10 steps), so we do not optimise z for evaluation. Therefore, LOGAN has the same evaluation cost as original BigGAN-deep. To help understand this behaviour, we plot the change from ∆z during training in <ref type="figure" target="#fig_2">Figure 5</ref> a. Although the movement in Euclidean space ∆z grew until training collapsed, the movement in D's output space, measured as f (z + ∆z) − f (z) , remained unchanged (see Appendix F for details). As shown in our analysis, optimising z improves the training dynamics, so LOGANs work well after training without requiring latent optimisation. We reckon that smaller models might not be "over-parametrised" enough to fully amortise the computation from optimising z, which can then further exploit the architecture in evaluation time. Appendix B further illustrates these different behaviours. We aim to further investigate this difference in future studies.</p><p>Given the criticism of FID and IS as heuristics metrics for sample distributions, we further measure how these samples directly contribute to downstream classification task via the recently proposed Classification Accuracy Score (CAS, <ref type="bibr" target="#b39">Ravuri &amp; Vinyals 2019)</ref>. Unlike FID and IS, this metric favours likelihood-based models, which are more likely cover all modes representing different classes. The CAS from LOGAN nearly halved the gap between the state-of-the-art GANs and VQ-VAE2 <ref type="bibr" target="#b40">(Razavi et al., 2019)</ref>. See Appendix C for more details. We verify our theoretical analysis in section 3 by examining key components of Algorithm 1 via ablation studies. First, we experiment with using basic GD to optimising z, as in <ref type="bibr" target="#b48">Wu et al. (2019)</ref>, and call this model LOGAN (GD). A smaller step size of α = 0.0001 was required; larger values were unstable and led to premature collapse of training. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the scores from LOGAN (GD) were worse than LOGAN (NGD) and similar to the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">ABLATION STUDIES</head><p>We then evaluate the effects of removing those terms depending on ∂f (z) ∂z in eq. 6, which are not in the ordinary gradient (eq. 3). Since we computed these terms by back-propagating through the latent optimisation procedure, we removed them by selectively blocking back-propagation with "stop gradient" operations (e.g., in TensorFlow, <ref type="bibr" target="#b0">Abadi et al. 2016</ref>). <ref type="figure" target="#fig_2">Figure 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">TRUNCATION AND SAMPLES</head><p>Truncation is a technique introduced by <ref type="bibr" target="#b7">Brock et al. (2018)</ref> to illustrate the trade-off between the FID and IS in a trained model. For a model trained with z ∼ p(z) from a source distribution symmetric around 0, such as the standard normal distribution N (0, 1) and the uniform distribution U <ref type="figure">(−1, 1)</ref>, down-scaling (truncating) the sourcez = s · z with 0 ≤ s &lt; 1 gives samples with higher visual quality but reduced diversity. We see this quantified in higher IS scores and lower FID when evaluating samples from truncated distributions. has the best sample quality. Interestingly, although LOGAN (GD) and the baseline model have similar scores without truncation (upper-left ends of the curves, see also <ref type="table" target="#tab_0">Table 1)</ref>, LOGAN (GD) was better behaved with increasing truncation, suggesting LOGAN (GD) still converged to a better equilibrium. For further reference, we plot truncation curves from additional baseline models in <ref type="figure" target="#fig_0">Figure 12</ref> (Appendix G). <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 2</ref> show samples from selected points along the truncation curves. In the high IS regime, C and D on the truncation curves both have similarly high IS of near 260. Samples from batches with such high IS have almost photo-realistic image quality. <ref type="figure">Figure 1</ref> shows that while the baseline model produced nearly uniform samples, LOGAN (NGD) could still generate highly diverse samples. On the other hand, A and B from <ref type="figure" target="#fig_4">Figure 3 b</ref> have similarly low FID of near 5, indicating high sample diversity. Samples in <ref type="figure">Figure 2</ref> b show higher quality compared with those in a (e.g., the interfaces between the elephants and ground, the contours around the pandas).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we present LOGAN, which significantly improves the state of the art in large scale GAN training for image generation by optimising the latent source z. Our results illustrate improvements in quantitative evaluation and samples with higher quality and diversity. Moreover, our analysis suggests that LOGAN fundamentally improves adversarial training dynamics. LOGAN is related to the energy-based formulation of a GAN's discriminator <ref type="bibr" target="#b10">(Dai et al., 2017;</ref><ref type="bibr" target="#b26">Kumar et al., 2019;</ref><ref type="bibr" target="#b17">Du &amp; Mordatch, 2019)</ref>, when latent optimisation is viewed as descending the energy function defined by the discriminator. From this view, sampling from the distribution implicitly defined by this energy function, via, e.g., Langevin sampling <ref type="bibr" target="#b47">(Welling &amp; Teh, 2011)</ref>, may bring further benefits. Another class of approaches regularises the entropy of the generator outputs to reduce mode collapse <ref type="bibr" target="#b5">(Belghazi et al., 2018;</ref><ref type="bibr" target="#b12">Dieng et al., 2019)</ref>. Such techniques could be combined with LO-GAN to further improve coverage of the underlying data distribution. Moreover, we expect our method to be useful in other tasks that involve adversarial training, including representation learning and inference <ref type="bibr" target="#b15">(Donahue et al., 2017;</ref><ref type="bibr" target="#b18">Dumoulin et al., 2017;</ref>), text generation <ref type="bibr" target="#b49">(Zhang et al., 2019)</ref>, style learning <ref type="bibr" target="#b50">(Zhu et al., 2017;</ref><ref type="bibr" target="#b24">Karras et al., 2019)</ref>, audio generation  and video generation <ref type="bibr" target="#b46">(Vondrick et al., 2016;</ref><ref type="bibr" target="#b9">Clark et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED ANALYSIS OF LATENT OPTIMISATION</head><p>In this section we present complementary of LOGAN. In particular, we show how the algorithm brings together ideas from symplectic gradient adjustment and stochastic approximation with two time scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 APPROXIMATE SYMPLECTIC GRADIENT ADJUSTMENT</head><p>To analyse LOGAN as a differentiable game we treat the latent step ∆z as adding a third player to the original game played by the discriminator and generator. The third player's parameter, ∆z, is optimised online for each z ∼ p(z). Together the three players (latent player, discriminator, and generator) have losses averaged over a batch of samples:</p><formula xml:id="formula_21">L = [η L G , L D , L G ] T<label>(18)</label></formula><p>where η = 1 N (N is the batch size) reflects the fact that each ∆z is only optimised for a single sample z, so its contribution to the total loss across a batch is small compared with θ D and θ G which are directly optimised for batch losses. This choice of η is essential for the following derivation, and has important practical implication. It means that the per-sample loss L G (z ), instead of the loss summed over a batch N n=1 L G (z n ), should be the only loss function guiding latent optimisation. Therefore, when using natural gradient descent (Section 4), the Fisher information matrix should only be computed using the current sample z.</p><p>The resulting simultaneous gradient is</p><formula xml:id="formula_22">g = η ∂L G (z ) ∂∆z , ∂L D (z ) ∂θ D , ∂L G (z ) ∂θ G T = −η ∂f (z ) ∂∆z , ∂f (z ) ∂θ D , − ∂f (z ) ∂θ G T<label>(19)</label></formula><p>Following <ref type="bibr" target="#b4">Balduzzi et al. (2018)</ref>, we can write the Hessian of the game as:</p><formula xml:id="formula_23">H =     −η ∂ 2 f (z ) ∂∆z 2 −η ∂ 2 f (z ) ∂∆z∂θ D −η ∂ 2 f (z ) ∂∆z∂θ G ∂ 2 f (z ) ∂θ D ∂∆z ∂ 2 f (z ) ∂θ 2 D ∂ 2 f (z ) ∂θ D ∂θ G − ∂ 2 f (z ) ∂θ G ∂∆z − ∂ 2 f (z ) ∂θ G ∂θ D − ∂ 2 f (z ) ∂θ 2 G    <label>(20)</label></formula><p>The presence of a non-zero anti-symmetric component in the Hessian <ref type="figure">Figure 6</ref>: The update speed of the discriminator relative to the generator shown as the difference If we further assume ∆θ G and δθ G are obtained from stochastic gradient descent with identical learning rate,</p><formula xml:id="formula_24">A = 1 2 (H − H T )</formula><formula xml:id="formula_25">∆θ G = α ∂f (z; θ D , θ G ) ∂θ G δθ G = α ∂f (z; θ D , θ G ) ∂θ G<label>(35)</label></formula><p>substituting eq. 35 into eq. 34 gives</p><formula xml:id="formula_26">∆θ G &lt; δθ G +<label>(36)</label></formula><p>The same analysis applies to the discriminator. The similar intuition is that it takes the discriminator additional effort to compensate the exploitation from the optimised z . We then obtain</p><formula xml:id="formula_27">∂f (z; θ D , θ G ) ∂θ D T ∆θ D = ∂f (z; θ D , θ G ) ∂z T ∆z + ∂f (z + ∆z; θ D , θ G ) ∂θ D T δθ D + (37)</formula><p>However, since the adversarial loss</p><formula xml:id="formula_28">L D = −L G , we have ∆θ D = −α ∂f (z;θ D ,θ G ) ∂θ D and δθ D = −α ∂f (z;θ D ,θ G ) ∂θ D</formula><p>taking the opposite signs of eq.35. For sufficiently small ∆z, ∆θ G and δθ G , is close to zero, so ∆θ D &lt; δθ D under our assumptions of small ∆z, ∆θ G and δθ G . Importantly, the bigger the product ∂f (z) ∂z ∆z is, the more robust the inequality is to the error from . Moreover, bigger steps increase the speed gap between updating D and G, further facilitating convergence (in accordance with <ref type="bibr" target="#b22">Heusel et al. (2017)</ref>). Overall, our analysis suggests:</p><p>1. More than one gradient descent step may not be helpful, since ∆z from multiple GD steps may deviate from the direction of ∂f (z) ∂z . 2. A large step of ∆z is helpful in facilitating convergence by widening the gap between D and G updates <ref type="bibr" target="#b22">(Heusel et al., 2017)</ref>. 3. However, the step of ∆z cannot be too large. In addition to the linear approximation we used throughout our analysis, the approximate SGA breaks down when eq.26 is strongly violated when "overshoot" brings the gradients at ∂f (z ) ∂z to the opposite sign of ∂f (z) ∂z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL ANALYSIS OF LATENT OPTIMISATION</head><p>Here we analyse the relationship between the number of latent optimisation steps during evaluation and the final FIDs and inception scores. As in the main paper, we train the SN-GAN model with only 1 latent optimisation step, but test them with {0, 1, 5, 10, 20, 30} steps during evaluation. For lower variance in computing the scores, we use 10, 000 samples for evaluation (as oppose to 5000 samples used in the main paper for direct comparison with other baselines). The inception scores are taken from checkpoints with the best (lowest) FIDs, and the error bars indicate standard deviations obtained from 3 different random seeds. <ref type="figure">Figure 7</ref> shows that the scores can be substantially improved with extra optimisation steps in evaluation. Although only 1 step was used in training, up to around 20 steps at evaluation could still improve sample quality. Beyond that, the return from extra computation became diminishing.</p><p>We did not observe similar improvement with BigGANs in evaluation. To contrast the difference between them, <ref type="figure">Figure 8</ref> and 9 illustrate the change of samples made by latent optimisation. In both cases, 10 latent optimisation steps were applied in evaluation, after the models were trained with 1 optimisation step. While the effect of improvement is clear in <ref type="figure">Figure 8 (from SN-GAN)</ref>, the changes in <ref type="figure">Figure 9</ref> are barely observable from inspecting the samples alone before and after latent optimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EVALUATING CLASSIFICATION ACCURACY SCORE</head><p>To compute CAS, a ResNet classifier <ref type="bibr" target="#b21">(He et al., 2016)</ref> is trained on samples from our model instead of the ImageNet dataset. Each data sample is replaced by a sample from the model conditioned on the same class. The trained model is then evaluated on the ImageNet dataset as in a standard classification task. We use the same schedule as in <ref type="bibr" target="#b39">Ravuri &amp; Vinyals (2019)</ref> for training the ResNet classifier, but stopped earlier at about 10k steps, where the classification accuracy peaked. See Ravuri &amp; Vinyals (2019) for the motivation of CAS and more details of the training and evaluation procedure. We report both top-5 and top-1 classification accuracy in <ref type="table" target="#tab_2">Table 3</ref>. Although higher resolution generally brings better CAS, we use the 128 × 128 model as in the main paper due to limited computational resource. Despite this, the CAS from LOGAN nearly halved the gaps between BigGAN-deep and VQ-VAE2 at a higher resolution of 256 × 256. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D POISSON LIKELIHOOD FROM HINGE LOSS</head><p>Here we provide a probabilistic interpretation of the hinge loss for the generator, which leads naturally to the scenario of a family of discriminators. Although this interpretation is not necessary for our current algorithm, it may provides useful guidance for incorporating multiple discriminators. before difference after <ref type="figure">Figure 9</ref>: ImageNet samples from BigGAN-deep before, after latent optimisation, and the differences between them.</p><p>We introduce the label t = 1 for real data and t = 0 fake samples. This section shows that the generator hinge loss L G = −D (G(z)) (38) can be interpreted as a negative log-likelihood function:</p><formula xml:id="formula_29">L G = − ln p(t = 1; D, G(z))<label>(39)</label></formula><p>Here p(t = 1; z, D, G) is the probability that the generated image G(z) can fool the discriminator D.</p><p>The original GAN's discriminator can be interpreted as outputting a Bernoulli distribution p(t; β G ) = β t G · (1 − β G ) 1−t . In this case, if we parameterise β G = D (G(z)), the generator loss is the negative log-likelihood − ln P t = 1; D, G(z) = − ln p(t = 1; β G ) = − ln β G = − ln D (G(z))</p><p>Bernoulli, however, is not the only valid choice as the discriminator's output distribution. Instead of sampling "1" or "0", we assume that there are many identical discriminators that can independently vote to reject an input sample as fake. The number of votes k in a given interval can be described by a Poisson distribution with parameter λ with the following PMF:</p><formula xml:id="formula_31">p(k; λ) = λ k e −λ k!<label>(41)</label></formula><p>The probability that a generated image can fool all the discriminators is the probability of G(z) receiving no vote for rejection p(k = 0; λ) = e −λ (42) Therefore, we have the following negative log-likelihood as the generator loss if we parameterise λ = −D (G(z)):</p><p>− ln p k = 0; D, G(z) = − ln p(k = 0; λ) = −D (G(z))</p><p>This interpretation has a caveat that when D (G(z)) &gt; 0 the Poisson distribution is not well defined. However, in general the discriminator's hinge loss L D = − min 0, −1 + D(x) − min 0, −1 − D(G(z))</p><p>pushes D (G(z)) &lt; 0 via training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E HYPER-PARAMETER SEARCH</head><p>We first searched the hyper-parameters for the DCGAN (section 5.1) over the following range:</p><p>Base on the results from DCGANs, hyper-parameter search on the following grid was performed for the BigGAN-deep (section 5.2): {0.01, 0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 1.0} β {0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0} w r {0.01, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 2.0, 3.0} c {30%, 50%, 70%, 80%, 90%, 100%}  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DETAILS IN COMPUTING DISTANCES IN FIGURE 5 A</head><p>For a temporal sequence x 1 , x 2 , . . . , x T (e.g. the changes of z or f (z) at each training step in this paper), to normalise its variance while accounting for the non-stationarity, we process it as follows. We first compute the moving average and standard deviation over a window of size N :</p><formula xml:id="formula_34">µ t = 1 N t+N −1 u=t x u (45) σ t = 1 N − 1 t+N −1 u=t (x u − µ u ) 2<label>(46)</label></formula><p>Then normalise the sequence as:</p><formula xml:id="formula_35">x t = x t σ t<label>(47)</label></formula><p>The result in <ref type="figure" target="#fig_2">Figure 5a</ref> is robust to the choice of window size. Our experiments with N from 10 to 50 yielded visually similar plots.</p><p>G ADDITIONAL SAMPLES AND RESULTS <ref type="figure">Figure 1</ref> and 2 provide additional samples, organised similar to <ref type="figure">Figure 1</ref> and 2. <ref type="figure" target="#fig_4">Figure 3</ref> shows additional truncation curves. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Samples from BigGAN-deep (a) and LOGAN (b) with similarly high IS. Samples from the two panels were drawn from truncation levels corresponding to points C and D in Figure 3 b respectively. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9) Samples from BigGAN-deep (a) and LOGAN (b) with similarly low FID. Samples from the two panels were drawn from truncation levels corresponding to points A and B in Figure 3 b respectively. (FID/IS: (a) 5.04/126.8, (b) 5.09/217.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(a) Samples from SN-GAN. (b) Samples from LOGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>(a) The change from ∆z across training, in D's output space and z's Euclidean space. The distances are normalised by their standard derivations computed from a moving window of size 20 (1007 data points in total). (b) Training curves from models with different "stop gradient" operations. For reference, the training curve from an unablated model is plotted as the dashed line. All instances with stop gradient collapsed (FID went up) early in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>b shows the change of FIDs for the three models corresponding to removing ∂∆z ∂θ G terms. As predicted by our analysis (section 3), both terms help stabilise training; training diverged early for all three ablations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 b</head><label>3</label><figDesc>plots the truncation curves for the baseline BigGAN-deep model, LOGAN (GD) and LOGAN (NGD), obtained by varying the truncation (value of s) from 1.0 (no truncation, upper-left ends of the curves) to 0.02 (extreme truncation, bottom-right ends). Each curve shows the tradeoff between FID and IS for an individual model; curves towards the upper-right corner indicate better overall sample quality. The relative positions of curves in figure 3 (b) shows LOGAN (NGD)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>FIDs (a) and Inception Scores (b) obtained with different latent optimisation steps at evaluation. CIFAR samples from SN-GAN before, after latent optimisation, and the differences between them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :Figure 12 :</head><label>101112</label><figDesc>Samples from BigGAN-deep (a) and LOGAN (b) with similarly high Inception scores. Samples from the two panels were drawn from truncations correspond to points C, D in Figure 3. (FID/IS: (a) 27.97/259.4, (b) 8.19/259.9) Samples from BigGAN-deep (a) and LOGAN (b) with similarly low FID. Samples from the two panels were draw from truncations correspond to points A, B in figure 3b. (FID/IS: (a) 5.04/126.8, (b) 5.09/217.0) Truncation curves with additional baselines. In addition to the truncation curves reported inFigure 3b, here we also include the Spectral-Normalised GAN, Self-Attention GAN<ref type="bibr" target="#b49">(Zhang et al., 2019)</ref>, original BigGAN and BigGAN-deep as presented in<ref type="bibr" target="#b7">Brock et al. (2018)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of model scores. BigGAN-deep results are reproduced from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of Scores. The first and second columns are reproduced from and<ref type="bibr" target="#b48">Wu et al. (2019)</ref> respectively. We report the Inception Score (IS, higher is better,<ref type="bibr" target="#b42">Salimans et al. 2016)</ref> and Fréchet Inception Distance (FID, lower is better,<ref type="bibr" target="#b22">Heusel et al. 2017)</ref>.</figDesc><table><row><cell></cell><cell>SN-GAN</cell><cell>CS-GAN</cell><cell>LOGAN (NGD)</cell></row><row><cell cols="2">FID 29.3</cell><cell>23.1 ± 0.5</cell><cell>17.7 ± 0.4</cell></row><row><cell>IS</cell><cell cols="3">7.42 ± 0.08 7.80 ± 0.05 8.67 ± 0.05</cell></row><row><cell>5.2 EXPERIMENTS</cell><cell></cell><cell></cell></row></table><note>WITH BIGGAN ON IMAGENET To illustrate the scalability of our algorithm, we next focus on large scale models based on BigGAN- deep (Brock et al., 2018) trained on 128 × 128 size images from the ImageNet dataset (Deng et al., 2009).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>CAS for different models. Except LOGAN, numbers from all other models are reproduced from<ref type="bibr" target="#b40">Razavi et al. (2019)</ref>.</figDesc><table><row><cell></cell><cell cols="2">Top-5 Accuracy Top-1 Accuracy</cell></row><row><cell>BigGAN-deep (128 × 128)</cell><cell>64.44%</cell><cell>40.64%</cell></row><row><cell>BigGAN-deep (256 × 256)</cell><cell>65.92%</cell><cell>42.65%</cell></row><row><cell>LOGAN (128 × 128)</cell><cell>71.97%</cell><cell>47.86%</cell></row><row><cell>VQ-VAE2 (256 × 256)</cell><cell>77.59%</cell><cell>54.83%</cell></row><row><cell>Real Data (256 × 256)</cell><cell>88.79%</cell><cell>68.82%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameter grid for the DCGAN. Best values from the grid search are highlighted. values α</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameter grid for the BigGAN-deep. Best values from the grid search are highlighted.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although multiple gradient descent steps can be employed for optimising z, we found one step works well in training and justify this choice in section 3.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Mihaela Rosca, Suman Ravuri and James Martens for comments on the draft and insightful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>term to the gradient to obtain g * ← g + λ A T g, which for the discriminator and generator has the form:</p><p>The gradient with respect to ∆z is ignored since the convergence of training only depends on θ D and θ G .</p><p>If we drop the last terms in eq.22 and 23, which are expensive to compute for large models with high-dimensional θ D and θ G , and use ∂f (z ) ∂∆z = ∂f (z ) ∂z , the adjusted updates can be rewritten as</p><p>Because of the third player, there are still the terms depend on ∂f (z ) ∂z to adjust the gradients. Efficiently computing ∂ 2 f (z ) ∂z ∂θ D and ∂ 2 f (z ) ∂z ∂θ D is non-trivial (e.g., Pearlmutter 1994). However, if we introduce the local approximation</p><p>then the adjusted gradient becomes identical to eq. 6 from latent optimisation.</p><p>In other words, automatic differentiation by commonly used machine learning packages can compute the adjusted gradient for θ D and θ G when back-propagating through the latent optimisation process. Despite the approximation involved in this analysis, both our experiments in section 5 and the results from <ref type="bibr" target="#b48">Wu et al. (2019)</ref> verified that latent optimisation can significantly improve GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 STOCHASTIC APPROXIMATION WITH TWO TIME SCALES</head><p>This section shows that latent optimisation accelerates the speed of updating D relative to the speed of updating G, facilitating convergence according to <ref type="bibr" target="#b22">Heusel et al. (2017)</ref> (see also <ref type="figure">Figure 6</ref> b). Intuitively, the generator requires less updating compared with D to achieve the same reduction of loss because latent optimisation "helps" G. <ref type="bibr" target="#b22">Heusel et al. (2017)</ref> used the theory of stochastic approximation to analyse GAN training. Viewing the training process as stochastic approximation with two time scales <ref type="bibr" target="#b6">(Borkar, 1997;</ref><ref type="bibr" target="#b25">Konda &amp; Borkar, 1999)</ref>, they suggest that the update of D should be fast enough compared with that of G. Under mild assumptions, <ref type="bibr" target="#b22">Heusel et al. (2017)</ref> proved that such two time-scale update converges to local Nash equilibrium. Their analysis follows the idea of (τ, δ) perturbation <ref type="bibr" target="#b23">(Hirsch, 1989)</ref>, where ∆θ D − ∆θ G after each update step. Lines are smoothed with a moving average using window size 20 (in total, there are 3007, 1659 and 1768 data points for each curve). All curves oscillated strongly after training collapsed.</p><p>the slow updates (G) are interpreted as a small perturbation over the ODE describing the fast update (D). Importantly, the size of perturbation δ is measured by the magnitude of parameter change, which is affected by both the learning rate and gradients.</p><p>Here we show, in accordance with <ref type="bibr" target="#b22">Heusel et al. (2017)</ref>, that LOGAN accelerates discriminator updates and slows down generator updates, thus helping the convergence of discriminator. We start from analysing the change of θ G . We assume that, without LO, it takes ∆θ G = θ G − θ G to make a small constant amount of reduction in loss L G :</p><p>Now using the optimised z = z + ∆z, we assess the change δθ G required to achieve the same amount of reduction:</p><p>Intuitively, when z "helps" θ G to achieve the same goal of increasing f (z; θ D , θ G ) by ρ, the responsible of θ G becomes smaller, so it does not need to change as much as ∆θ G , thus δθ G &lt; ∆θ G .</p><p>Formally, f (z; θ D , θ G ) and f (z + ∆; θ D , θ G + δθ G ) have the following Taylor expansions around z and θ G :</p><p>Where (·)'s are higher order terms of the increments. Using the assumption of eq. 27 and 28, we can combine eq. 29 and 31: </p><p>Therefore, we have the inequality</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05642</idno>
		<title level="m">The mechanics of n-player differentiable games</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MINE: Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic approximation with two time scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06571</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Calibrating energy-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><forename type="middle">K</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04302</idno>
		<title level="m">Prescribed generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Puckette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04208</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Advesarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Global Convergence to the Equilibrium of GANs using Variational Inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sridhar Mahadevan</surname></persName>
		</author>
		<idno>Arxiv:1808.01531</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convergent activation dynamics in continuous time networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Actor-critic-type learning algorithms for markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vijaymohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on control and Optimization</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="123" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Differentiable game mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Letcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">84</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric GAN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1193</idno>
		<title level="m">New insights and perspectives on the natural gradient method</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6779-the-numerics-of-gans.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3478" to="3487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>abs/1611.02163</idno>
		<ptr target="http://arxiv.org/abs/1611.02163" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">cGANs with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F -Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast exact multiplication by the hessian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="160" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12268" to="12279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Topmoumoute online natural gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discriminator optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6813" to="6823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Rosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6850" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
