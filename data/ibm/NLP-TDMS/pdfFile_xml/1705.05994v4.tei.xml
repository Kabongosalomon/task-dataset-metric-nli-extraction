<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a Hierarchical Latent-Variable Model of 3D Shapes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
							<email>shikun.liu17@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
							<email>giles@ist.psu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a Hierarchical Latent-Variable Model of 3D Shapes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1: Random 3D shapes generated by sampling the learned latent space of the proposed Variational Shape Learner trained from ModelNet40 dataset [45]. See more results in Section 4.3.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the Variational Shape Learner (VSL), a generative model that learns the underlying structure of voxelized 3D shapes in an unsupervised fashion. Through the use of skip-connections, our model can successfully learn and infer a latent, hierarchical representation of objects. Furthermore, realistic 3D objects can be easily generated by sampling the VSL's latent probabilistic manifold. We show that our generative model can be trained end-to-end from 2D images to perform single image 3D model retrieval. Experiments show, both quantitatively and qualitatively, the improved generalization of our proposed model over a range of tasks, performing better or comparable to various state-of-the-art alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the past several years, impressive strides have been made in the generative modelling of 3D objects. Much of this progress can be attributed to recent advances in artificial neural network research. Instead of the usual approach to representing 3D shapes with voxel occupancy vectors, promising recent work has taken to learning simple latent representations of such objects. Neural architectures that have been developed with this goal in mind include those based on deep belief networks <ref type="bibr" target="#b44">[45]</ref>, deep autoencoders <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, and 3D convolutional networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref>. The positive progress made so far with neural networks has also led to the creation of several largescale 3D CAD model benchmarks, notably ModelNet <ref type="bibr" target="#b44">[45]</ref> and ShapeNet <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, despite the progress made so far, one key weakness shared among all previous state-of-the-art approaches is that all of them have focused on learning a single ("flat") vector representation of 3D shapes. These include recent and powerful models such as the autoencoderlike T-L Network <ref type="bibr" target="#b12">[13]</ref> and the probabilistic 3D Generative Adversarial Network (3D-GAN) <ref type="bibr" target="#b43">[44]</ref>, which shared its latent vector representation over multiple tasks. Other models, such as those of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>, further required additional supervision using information about camera viewpoints, shape keypoints, and segmentations.</p><p>To describe the input with only a single layer of latent variables might be too restrictive an assumption, hindering the expressiveness of the underlying generative model learned. Having a multilevel latent structure, on the other hand, would allow for lower-level latent variables to focus on modelling features such as edges and the upper levels to learn to command those lower-level variables as to where to place those edges in order to form curves and shapes. This composition of latent (local) sub-structures would allow us to exploit the fact that most 3D shapes usually have similar structure. Note that this course-to-fine feature extraction process is the very essence of abstraction, yielding representations that can be easily constructed in terms of less abstract ones <ref type="bibr" target="#b1">[2]</ref>. Higher-level variables, or disentangled features, would be modelling complex interactions of low-level patterns. Thus, to encourage the learning of hierarchical features, we explicitly incorporate this as a prior in our model through explicit architectural constraints.</p><p>In this paper, motivated by the argument for a hierarchical representation developed above and the promise shown in work such as that of <ref type="bibr" target="#b8">[9]</ref>, we show how to encourage a latent-variable generative model to learn a hierarchy of latent variables through the use of synaptic skip-connections.</p><p>These skip-connections encourage each layer of latent variables to model exactly one level of abstraction of the data. To efficiently learn such a latent structure, we further exploit recent advances in approximate inference <ref type="bibr" target="#b21">[22]</ref> to develop a variational learning procedure. Empirically, we show that the learned generative model, which we call the Variational Shape Learner, acquires rich representations of 3D shapes that yield significantly improved performance across a multitude of 3D shape tasks.</p><p>The main contributions of this paper are as follows:</p><p>• We propose a novel latent-variable model, which we call the Variational Shape Learner, which is capable of learning expressive feature representations of 3D shapes. We observe impressive performance in shape generation and shape arithmetic in a large dataset.</p><p>• For both general 3D model building and single image reconstruction, we show that our model is fully unsupervised, requiring no extra human-generated information about segmentation, keypoints, or pose information.</p><p>• We show that our model outperforms current state-of-theart approaches in unsupervised (object) model classification while requiring significantly fewer learned feature extractors (a vector with less than 100 dimensions compared to the 3D-GAN's 2.5 million dimensional vector).</p><p>• In real-world image reconstruction, our extensive set of experiments show that the proposed Variational Shape Learner surpasses state-of-the-art in 8 of 10 classes. Half of these the VSL surpasses by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D object recognition is a well-studied problem in the computer vision literature. Early efforts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref> often combined simple image classification methods with handcrafted shape descriptors, requiring intensive effort on the side of the human data annotator. However, ever since the ImageNet contest of 2012 <ref type="bibr" target="#b23">[24]</ref>, deep convolutional networks (ConvNets) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> have swept the vision industry, becoming nearly ubiquitous in countless applications.</p><p>Research in learning probabilistic generative models has also benefited from the advances made by artificial neural networks. Generative Adversarial Networks (GANs), proposed in <ref type="bibr" target="#b13">[14]</ref> and Variational auto-encoders (VAEs), proposed in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>, are some of the most popular and important frameworks that have emerged from improvements in generative modelling. Successful adaptation of these frameworks range from a focus in natural language and speech processing <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36]</ref> to realistic image synthesis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>, yielding promising, positive results. Nevertheless, very little work, outside of <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, has focused on modeling 3D objects, where generative architectures could be used to learn probabilistic embeddings. The model proposed in this paper will offer another step towards constructing powerful generative models of 3D structures.</p><p>One study, amidst the rise of neural network-based approaches to 3D object recognition, that is most relevant to this paper is that of <ref type="bibr" target="#b44">[45]</ref>, which presented promising results and a benchmark for 3D model recognition: ModelNet. Following this key study, researchers have tried applying 3D ConvNets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>, autoencoders <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>, and a variety of probabilistic neural generative models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b31">32]</ref> to the problem of 3D model recognition, with each study progressively advancing state-of-the-art.</p><p>With respect to 3D object generation from 2D images, commonly used methods can be roughly grouped into two categories: 3D voxel prediction <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16]</ref> and mesh-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>. The 3D-R2N2 model <ref type="bibr" target="#b4">[5]</ref> represents a more recent approach to the task, which involves training a recurrent neural network to predict 3D voxels from one or more 2D images. <ref type="bibr" target="#b31">[32]</ref> also takes a recurrent network-based approach, but receives a depth image as input rather than normal 2D images. The learnable stereo system <ref type="bibr" target="#b17">[18]</ref> processes one or more camera views and camera pose information to produce compelling 3D object samples.</p><p>Many of the above methods require multiple images and/or additional human-provided information. Some approaches have attempted to minimize human involvement by developing weakly-supervised schemes, making use of image silhouettes to conduct 3D object reconstruction <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b42">43]</ref>. Of the few unsupervised neural-based approaches that exist, the T-L network <ref type="bibr" target="#b12">[13]</ref> is one of the most important, combining a convolutional autoencoder with an image regressor to encode a unified vector representation of a given 2D image. However, one fundamental issue with the T-L Network is its three-phase training procedure, since jointly training the system components proves to be too difficult. The 3D-GAN <ref type="bibr" target="#b43">[44]</ref> offers a way to train 3D object models in an adversarial learning scheme. However, GANs are notoriously difficult to train <ref type="bibr" target="#b0">[1]</ref>, often due to ill-designed loss functions and the higher chance of zero gradients.</p><p>In contrast to prior work, our approach, which is derived from a variational Bayesian perspective view of learning, naturally allows for joint training of all model parameters. Furthermore, our approach makes use of a well-formulated loss function that circumvents the instability involved with adversarial learning while still being able to produce higherquality samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Variational Shape Learner</head><p>In this section, we introduce our proposed model, the Variational Shape Learner (VSL), which builds on the ideas of the Neural Statistician <ref type="bibr" target="#b8">[9]</ref> and the volumetric convolutional network <ref type="bibr" target="#b26">[27]</ref>, the parameters of which the VSL learns under a variational inference scheme <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Design Philosophy</head><p>It is well known that generative models, learned through variational inference, are excellent at reconstructing complex data but tend to produce blurry samples. This happens because there is uncertainty in the model's predictions when we reconstruct the data from a latent space. As described above, previous approaches to 3D object modelling have focused on learning a single latent representation of the data. However, this simple latent structure might be hindering the model's ability to extract richer structure from the input distribution and thus lead to blurrier reconstructions.</p><p>To improve the quality of the samples of generated objects, we introduce a more complex internal variable structure, with the specific goal of encouraging the learning of a hierarchical arrangement of latent feature detectors. The motivation for a latent hierarchy comes from the observation that objects under the same category usually have similar geometric structure. As can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>, we start from a global latent variable layer (horizontally depicted) that is hardwired to a set of local latent variables layers (vertically depicted), each tasked with representing one level of feature abstraction. The skip-connections tie together the latent codes, and in a top-down directed fashion, local codes closer to the input will tend to represent lower-level features while local codes farther away from the input will tend towards representing higher-level features.</p><p>The global latent vector can be thought of as a large pool of command units that ensures that each local code extracts information relative to its position in the hierarchy, forming an overall coherent structure. This explicit globallocal form, and the way it constrains how information flows across it, lends itself to a straightforward parametrization of the generative model and furthermore ensures robustness, dramatically cutting down on over-fitting. To make things easier for training via stochastic back-propagation, the local codes will be concatenated to a flattened structure when fed into the task-specific models, e.g., a shape classifier or a voxel reconstruction module. Ultimately, more realistic samples should be generated by an architecture supporting this kind of latent-variable design, since the local variable layers will robustly encode hierarchical semantic cues in an unsupervised fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Objective: Variational + Latent Loss</head><p>The variational auto-encoder (VAE) <ref type="bibr" target="#b21">[22]</ref> has recently been introduced as a powerful generative model for unsupervised learning. The generative model p θ (x|z) for a single data point x with a latent variable z can be parameterized by a neural network with parameters θ. The parameters are inferred by maximizing the variational lower bound,</p><formula xml:id="formula_0">log p(x) ≥ E q φ (z|x) log p θ (x|z)p θ (z) q φ (z|x)<label>(1)</label></formula><p>The inference model q φ (z|x) can also be parameterized by a deep neural network. The inference and generative pa-rameters are then jointly trained by optimizing Equation 1 using back-propagation and stochastic gradient ascent. To deal with the stochasticity of the latent variables, which, in VAE models, are typically assumed to be Gaussian distributed, we use the re-parameterization trick in order to back-propagate through the operation of sampling the Gaussian variables. We refer the reader to <ref type="bibr" target="#b7">[8]</ref> for a much more detailed explanation.</p><p>To learn the parameters of the VSL latent-variable model, we will take a variational inference approach, where the goal is to learn a generative model p(x; θ), with generative parameters θ, using a recognition model q(z 0:n |x; φ), with variational parameters φ. The VSL's learning objective contains a standard reconstruction loss term L rec as well as a regularization penalty L reg over the latent variables. Furthermore, the loss contains an additional term for the latent variables L lat , which is particularly relevant and useful for the 3D model retrieval task of Section 4.5. This extra term is a simple L 2 penalty imposed on the difference between the learned features of the image regressor z and true latent features z = [z 0:n ] where [·] denotes concatenation.</p><p>We assume a fixed, spherical unit Gaussian prior, p(z 0 ) = N (0, I). The conditional distribution over each local latent code (z i , i ≥ 2) is defined as follows:</p><formula xml:id="formula_1">p(z i |z i−1 , z 0 ; θ) = N (µ(z i−1 , z 0 ), σ 2 (z i−1 , z 0 )) (2)</formula><p>where the first local code z 1 is simply:</p><formula xml:id="formula_2">p(z 1 |z 0 ; θ) = N (µ(z 0 ), σ 2 (z 0 )).<label>(3)</label></formula><p>Note that p(z 1 |z 0 ; θ) and p(z i |z i−1 , z 0 ; θ) are also spherical Gaussians and θ contains the generative parameters. The (occupancy) probability for one voxel p(x) can then be calculated by, p(x|z 0:n ; θ)p(z 1 |z 0 ; θ)p(z 0 ) n i=2 p(z i |z i−1 , z 0 ; θ) dz 0:n .</p><p>(4) Let the reconstructed voxelx be directly parameterized by occupancy probability. The loss L(x) for the input voxel x of the VSL is then calculated by the following equation:</p><formula xml:id="formula_3">L(x) = L rec + δL reg + γL lat ,<label>(5)</label></formula><p>where each term in the equation above is defined as follows:</p><formula xml:id="formula_4">L rec = x log(x) + (1 − x) log(1 −x) (6) L reg = KL(q(z 0 |x; φ) p(z 0 )) + KL(q(z 1 |z 0 , x; φ) p(z 1 |z 0 ; θ)) + n i=2 KL(q(z i |z i−1 , z 0 , x; φ) p(z i |z i−1 , z 0 ; θ))<label>(7)</label></formula><formula xml:id="formula_5">L lat = − z − z 2 2 .<label>(8)</label></formula><p>Note that δ and γ, which weigh the contributions of the each term towards the overall cost, are tunable hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encoder: 3D-ConvNet + Skip-Connections</head><p>The global latent code z 0 is directly learned from the input voxel through three convolutional layers with kernel sizes {6, 5, 4}, strides {2, 2, 1} and channels {32, 64, 128}.</p><p>Each local latent code z &gt;1 is conditioned on the global latent code, the input voxel x, and the previous latent code (except for z 1 , which does not have a previous latent code) using two fully-connected layers with 100 neurons each. The skip-connections between local codes help ease the process of learning hierarchical features (i.e., improved gradient transmission) and force each local code to learn one level of abstraction.</p><p>The approximate posterior for a single voxel is given by:</p><formula xml:id="formula_6">q(z 0 |x; φ)q(z 1 |z 0 , x; φ) n i=2 q(z i |z i−1 , z 0 , x; φ) (9)</formula><p>where φ, the variational parameters, is parameterized by neural networks. n represents the number of local latent codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder: 3D-DeConvNet</head><p>After we learn the global and local latent codes z 0:n , we then concatenate them into a single vector as shown in <ref type="figure" target="#fig_0">Figure 2</ref> in blue dashed lines.</p><p>A 3D deconvolutional neural network with dimensions symmetrical to the encoder of Section 3.3 is used to decode the learned latent features into a voxel. An element-wise logistic sigmoid is applied to the output layer in order to convert the learned features to occupancy probabilities for each voxel cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Image Regressor: 2D-ConvNet</head><p>We use a standard 2D convolutional network to encode input RGB images into a feature space with the same dimension as the concatenation of global and local latent codes [z 0:n ]. The network contains four fully-convolutional layers with kernel sizes {32, 15, 5, 3}, strides {2, 2, 2, 1}, and channels {16, 32, 64, 128}. The last convolutional layer is flattened and fed into two fully-connected layers with 200 and 100 neurons each. Unlike the encoder described in Section 3.3, we apply dropout <ref type="bibr" target="#b40">[41]</ref> before the last fullyconnected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the quality of our proposed generative model for 3D shapes, we conduct several extensive experiments.</p><p>In Section 4.3, we investigate our model's ability to generalize and synthesize through a shape interpolation experiment and an nearest neighbours analysis of random generated samples from the VSL. Following this, in Section 4.4, we evaluate our model on the task of unsupervised shape classification by directly using the learned latent features on both the ModelNet10 and ModelNet40 datasets. We compare these results to previous supervised and unsupervised state-of-the-art methods. Next, we test our model's ability to reconstruct real-world image in Section 4.5, comparing our results to 3D-R2N2 <ref type="bibr" target="#b4">[5]</ref> and NRSfM <ref type="bibr" target="#b18">[19]</ref>. Finally, we demonstrate the richness of the VSL's learned semantic embeddings through vector arithmetic, using the latent features trained on ModelNet40 for Section 4.6. PASCAL 3D The PASCAL 3D dataset is composed of the images from the PASCAL VOC 2012 dataset <ref type="bibr" target="#b9">[10]</ref>, augmented with 3D annotations using PASCAL 3D+ <ref type="bibr" target="#b45">[46]</ref>. We voxelize the 3D CAD models using resolution [30×30×30] and use the same training and testing splits of <ref type="bibr" target="#b18">[19]</ref>, which was also used in <ref type="bibr" target="#b4">[5]</ref> to conduct real-world image reconstruction (of which the experiment in Section 4.5 is based off of). We use the bounding box information as provided in the dataset. Note that the only pre-processing we applied was image cropping and padding with 0-intensity pixels to create final samples of resolution [100 × 100] (which was required for our model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Protocol</head><p>Training was the same across all experiments, with only minor details that were task-dependent. The architecture of the VSL experimented with in this paper consisted of 5 local latent codes, each made up of 10 variables for ModelNet40 and 5 for ModelNet10. For ModelNet40, the global latent code was set to a dimensionality of 20 variables, while for ModelNet10, it was set to 10 variables.</p><p>The hyper-parameter δ was set to 10 −3 across training on both ModelNet10 and ModelNet40. We optimise parameters by maximizing the loss function defined in Equation 5 using the Adam adaptive learning rate <ref type="bibr" target="#b20">[21]</ref>, with step size set to 5×10 For the experiment in Section 4.5, we use 5 local la-tent codes (each with dimensionality of 5) and a global latent code of 20 variables for the jointly trained model. For the separately trained model, we use 3 local latent codes, each with dimensionality of 2, and a global latent code of dimensionality 5. Mini-batches of 40 samples were use to compute gradients for the joint model while 5 samples were used for the separately trained model. For both model variants, dropout <ref type="bibr" target="#b40">[41]</ref> was to control for over-fitting, with p drop = 0.2, and early stopping was employed (resulting in only 150 epochs). For Section 4.5, which involved image reconstruction and thus required the loss term L lat , instead of searching for an optimal value of the hyper-parameter γ through crossvalidation, we employed a "warming-up" schedule, similar to that of <ref type="bibr" target="#b39">[40]</ref>. "Warming-up" involves gradually increasing γ (on a log-scale as depicted in <ref type="figure" target="#fig_4">Figure 3</ref>), which controls the relative weighting of L lat in <ref type="figure" target="#fig_2">Equation 5</ref>. The schedule is defined as follows,   <ref type="figure" target="#fig_4">Figure 3</ref> depicts, empirically, the benefits of employing a warming-up schedule over using a fixed, externally set coefficient for the L lat term in our image reconstruction experiment. We remark that using a warming-up schedule plays an essential role in acquiring good performance on the image reconstruction task.</p><formula xml:id="formula_7">γ =          10 t/10 −8 t ≤ 50 t − 40 10 · 10 −3 50 &lt; t &lt; 100 5 · 10 −3 t ≥ 100.<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Shape Generation and Learning</head><p>To examine our model's ability to generate highresolution 3D shapes with realistic details, we design a task that involves shape generation and shape interpolation. We add Gaussian noise to the learned latent codes on test data taken from ModelNet40 and then use our model to generate "unseen" samples that are similar to the input voxel. In ef- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Class Interpolation (airplane)</head><p>Inter-Class Interpolation (chair → bed) fect, we generate objects from our VSL model directly from vectors, without a reference object/image. The results of our shape interpolation experiment, from both within-class and across-class perspectives, is presented in <ref type="figure" target="#fig_2">Figure 5</ref>. It can be observed that the proposed VSL shows the ability to smoothly transition between two objects. Our results on shape generation are shown in <ref type="figure" target="#fig_5">Figure  4</ref>. Notably, in our visualizations, darker colours correspond to smaller occupancy probability while lighter corresponds to higher occupancy probability. We further compare to previous state-of-the-art results in shape generation, which are depicted in <ref type="figure">Figure 6</ref>.</p><p>During training, we observed that our model was robust to different choices of the number and dimensionality of its local/global latent codes. We provide the table below as an ablative analysis showing how test reconstruction error is affected by various settings of the latent variables. From the results, we can observe a clear trend that the network with higher dimensionality and greater number of latent variables tends to generate better results. However, increasing the number of network parameters to attain better accuracy also brings about slower training, an important trade-off that one will need to consider in various application scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Shape Classification</head><p>One way to test model expressiveness is to conduct shape classification directly using the learned embeddings. We evaluate the features learned on the ModelNet dataset <ref type="bibr" target="#b44">[45]</ref> by concatenating both the global latent variable with the local latent layers, creating a single feature vector [z 0:m ]. We train a Support Vector Machine with an RBF kernel for classification using these "pre-trained" embeddings. <ref type="table" target="#tab_1">Table 2</ref> shows the performance of previous state-of-theart supervised and unsupervised methods in shape classification on both variants of the ModelNet dataset. Notably, the best unsupervised state-of-the-art results reported so far were from the 3D-GAN of <ref type="bibr" target="#b43">[44]</ref>, which used features from 3 layers of convolutional networks with total dimensions [62 × 32 3 + 128 × 16 3 + 56 × 8 3 ]. This is a far larger feature space than that required by our model, which is simply [5 × 5 + 10] (for 10-way classification) and <ref type="bibr">[5 × 10 + 20]</ref> (for 40-way classification) and reaches the exact same level of performance. The VSL performs comparably to super-</p><formula xml:id="formula_8">Supervision Method Classification Rate ModelNet10 ModelNet40</formula><p>Supervised 3D ShapeNets <ref type="bibr" target="#b44">[45]</ref> 83.5% 77.3% DeepPano <ref type="bibr" target="#b37">[38]</ref> 85.5% 77.6% Geometry Image <ref type="bibr" target="#b38">[39]</ref> 88.4% 83.9% VoxNet <ref type="bibr" target="#b26">[27]</ref> 92.0% 83.0% PointNet <ref type="bibr" target="#b29">[30]</ref> -89.2% ORION <ref type="bibr" target="#b34">[35]</ref> 93.8% -Unsupervised SPH <ref type="bibr" target="#b19">[20]</ref> 79.8% 68.2% LFD <ref type="bibr" target="#b3">[4]</ref> 79.9% 75.5% T-L Network <ref type="bibr" target="#b12">[13]</ref> 74.4% -VConv-DAE <ref type="bibr" target="#b36">[37]</ref> 80.5% 75.5% 3D-GAN <ref type="bibr" target="#b43">[44]</ref> 91.0% 83.3% VSL (ours) 91.0% 84.5% vised state-of-the-art, outperforming models such as 3D ShapeNet <ref type="bibr" target="#b44">[45]</ref>, DeepPano <ref type="bibr" target="#b37">[38]</ref>, and the geometry imagebase approach <ref type="bibr" target="#b38">[39]</ref>, by a large margin, and comes close to models such as VoxNet <ref type="bibr" target="#b26">[27]</ref>.</p><p>In order to visualise the learned feature embeddings, we employ t-SNE <ref type="bibr" target="#b25">[26]</ref> to map our high dimensional feature to a 2D plane. The visualization is shown in <ref type="figure" target="#fig_8">Figure 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Single Image 3D Model Retrieval</head><p>Real-world, single image 3D model retrieval is another application of the proposed VSL model. This is a challenging problem, forcing a model to deal with real-world 2D images under a variety of lighting conditions and resolutions. Furthermore, there are many instances of model occlusion as well as different colour gradings.</p><p>To test our model on this application, we use the PAS-CAL 3D <ref type="bibr" target="#b45">[46]</ref> dataset and utilize the same exact training and testing splits from <ref type="bibr" target="#b18">[19]</ref>. We compare our results with those reported for recent approaches, including the NRSfM <ref type="bibr" target="#b18">[19]</ref> and 3D-R2N2 <ref type="bibr" target="#b4">[5]</ref> models. Note that these also used the exact same experimental configurations we did.</p><p>For this task, we train our model in two different ways:   1) jointly on all categories, and 2) separately on each category. In <ref type="figure" target="#fig_9">Figure 8</ref>, we observe better reconstructions from the (separately-trained) VSL when compared to previous work. Unlike the NRSfM <ref type="bibr" target="#b18">[19]</ref>, the VSL does not require any segmentation, pose information, or keypoints. In addition, the VSL is trained from scratch while the 3D-R2N2 is pre-trained using the ShapeNet dataset <ref type="bibr" target="#b2">[3]</ref>. However, the jointly-trained VSL did not outperform the 3D-R2N2, which is also jointly-trained. The performance gap is due to the fact that the 3D-R2N2 is specifically designed for image reconstruction and employs a residual network <ref type="bibr" target="#b16">[17]</ref> to help the model learn richer semantic features. Quantitatively, we compare our VSL to the NRSfM <ref type="bibr" target="#b18">[19]</ref> and two versions of 3D-R2N2 from <ref type="bibr" target="#b4">[5]</ref>, one with an LSTM structure and another with a deep residual network. Results (Intersection-of-Union) are shown in <ref type="table" target="#tab_3">Table 3</ref>. Observe that our jointly trained model performs comparably to the 3D-R2N2 LSTM variant while the separately trained version surpasses the 3D-R2N2 ResNet structure in 8 out of 10 cat-egories, half of them by a wide margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Shape Arithmetic</head><p>Another way to explore the learned embeddings is to perform various vector operations on the latent space, much what was done in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b12">13]</ref>. We present some results of our shape arithmetic experiment in <ref type="figure" target="#fig_10">Figure 9</ref>. Different from previous results, all of our objects are sampled from the model embeddings which were trained using the whole dataset with 40 classes. Furthermore, unlike the blurrier generations of <ref type="bibr" target="#b12">[13]</ref>, the VSL seems to generate very interesting combinations of the input embeddings without the need for any matching to actual 3D shapes from the original dataset. The resultant objects appear to clearly embody the intuitive meaning of the vector operators. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed the Variational Shape Learner, a hierarchical latent-variable model for 3D shape modelling, learnable through variational inference. In particular, we have demonstrated 3D shape generation results on a popular benchmark, the ModelNet dataset. We also used the learned embeddings of our model to obtain state-ofthe-art in unsupervised shape classification and furthermore showed that we could generate unseen shapes using shape arithmetic. Future work will entail a more thorough investigation of the embeddings learned by our hierarchical latentvariable model as well as integration of better prior distributions into the framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The network structure of the Variational Shape Learner. Solid lines represent synaptic connections for either fully-connected or convolutional layers while dashed lines represent concatenation. Dotted-dashed lines represent possible applications. • means latent features, means concatenated features, and means equivalence relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ModelNet</head><label></label><figDesc>There are two variants of the ModelNet dataset, ModelNet10 and ModelNet 40, introduced in [45], with 10 and 40 target classes respectively. ModelNet10 has 3D shapes which are pre-aligned with the same pose across all categories. In contrast, ModelNet40 (which includes the shapes found in ModelNet10) features a variety of poses. We voxelize both ModelNet10 and ModelNet40 with resolution [30 × 30 × 30]. To test our model's ability to handle 3D shapes of great variety and complexity, we use Model-Net40 for most of the experiments, especially for those in Section 4.3 and 4.6. Both ModelNet10 and ModelNet40 are used to conduct the shape classification experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>− 5 .</head><label>5</label><figDesc>For the experiments of Sections 4.3, 4.4, and 4.6, over 2500 epochs, parameter updates were calculated using mini-batches of 200 samples on ModelNet40 and 100 samples on ModelNet10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>γ = 1 · 10 − 8 γ = 5 ·</head><label>11085</label><figDesc>10 −3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Training the VSL for image reconstruction using a warming-up schedule compared to using constant weights γ = 1 · 10 −8 and γ = 5 · 10 −3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Randomly generated results from the proposed Variational Shape Learner trained on ModelNet40. The nearest neighbors are the ground-truth shapes, fetched from the test data, and placed for reference in the last column of the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Interpolation results of the Variational Shape Learner on ModelNet40. Shape generation from previous state-of-the-art approaches. Up: generated shapes in resolution [30 × 30 × 30] from [45]; Down: generated shapes in resolution [64 × 64 × 64] from<ref type="bibr" target="#b43">[44]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>t-SNE plots of the latent embeddings for Model-Net10 and ModelNet40. Each color represents one class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Reconstruction samples for PASCAL 3D from the separately trained VSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Samples of our shape arithmetic experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Reconstruction error of ModelNet 10/40 with various choices of network structure.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ModelNet classification results for both unsupervised and supervised methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Per-category voxel predictive performance on PASCAL 3D, as measured by Intersection-of-Union (IoU).</figDesc><table><row><cell>Input</cell><cell>GT</cell><cell>VSL</cell><cell>3D-R2N2[5] NRSfM[19]</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<title level="m">Towards principled methods for training generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On visual similarity based 3d model retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00449</idno>
		<title level="m">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimizing the multi-view stereo reprojection error for triangular surface meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delaunoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G I</forename><surname>Piracés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02185</idno>
		<title level="m">Towards a neural statistician</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neocognitron: A hierarchical neural network capable of visual pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimizing the reprojection error in surface reconstruction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08637</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hierarchical surface prediction for 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00710</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05375</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rotation invariant spherical harmonic representation of 3 d shape descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on geometry processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="156" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hough transform and 3d surf for robust three dimensional classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="589" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object detection from large-scale 3d datasets using bottom-up and topdown descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patterson</surname><genName>IV</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="553" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2352" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (fpfh) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation, 2009. ICRA&apos;09. IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03351</idno>
		<title level="m">Orientationboosted voxel nets for 3d object recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Piecewise latent variables for neural variational text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="422" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vconv-dae: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3-d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning 3d shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Silnet : Single-and multi-view reconstruction by learning from silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepshape: Deep learned shape descriptor for 3d shape matching and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1275" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning representation using autoencoder for 3d shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
