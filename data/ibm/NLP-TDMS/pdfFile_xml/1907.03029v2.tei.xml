<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blind Universal Bayesian Image Denoising With Gaussian Noise Level Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020">2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Majed</forename><surname>El Helou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
						</author>
						<title level="a" type="main">Blind Universal Bayesian Image Denoising With Gaussian Noise Level Learning</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING</title>
						<imprint>
							<biblScope unit="volume">29</biblScope>
							<biblScope unit="page">4885</biblScope>
							<date type="published" when="2020">2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TIP.2020.2976814</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Blind and universal image denoising consists of using a unique model that denoises images with any level of noise. It is especially practical as noise levels do not need to be known when the model is developed or at test time. We propose a theoretically-grounded blind and universal deep learning image denoiser for additive Gaussian noise removal. Our network is based on an optimal denoising solution, which we call fusion denoising. It is derived theoretically with a Gaussian image prior assumption. Synthetic experiments show our network's generalization strength to unseen additive noise levels. We also adapt the fusion denoising network architecture for image denoising on real images. Our approach improves real-world grayscale additive image denoising PSNR results for training noise levels and further on noise levels not seen during training. It also improves state-of-the-art color image denoising performance on every single noise level, by an average of 0.1d B, whether trained on or not.</p><p>Index Terms-Additive Gaussian noise removal, Bayesian estimation theory, deep learning, CNN image denoiser optimality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I MAGE denoising is a fundamental image restoration task applied in all image processing pipelines. An image denoiser can also be part of deep network models to improve the training of high-level vision tasks <ref type="bibr" target="#b28">[27]</ref>. However, being an ill-posed inverse problem, denoising is challenging <ref type="bibr" target="#b14">[14]</ref>.</p><p>After the development of the best analytical solution, BM3D <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b18">[18]</ref>, little improvement in denoising performance was achieved until the advent of deep learning denoisers <ref type="bibr" target="#b60">[59]</ref>. Recent Convolutional Neural Network (CNN) based methods achieve state-of-the-art image denoising performance and are even faster than traditional optimizationbased approaches <ref type="bibr" target="#b56">[55]</ref>. The increased capacity of deep CNN models also addresses the limitation of previous multi-layer perceptron methods when it comes to denoising different levels of noise <ref type="bibr" target="#b5">[5]</ref>. Well-designed CNN architectures can also outperform adversarial training methods in image restoration tasks <ref type="bibr" target="#b46">[45]</ref>.</p><p>Neural networks can be deep and wide and thus have large capacity to model complex functions <ref type="bibr" target="#b57">[56]</ref>, <ref type="bibr" target="#b62">[61]</ref>, by leveraging network regularization or normalization <ref type="bibr" target="#b21">[21]</ref> and residual learning <ref type="bibr" target="#b19">[19]</ref>. However, the complex functions modeled by Manuscript received <ref type="bibr">August 5, 2019</ref>; revised January 27, 2020; accepted February 20, 2020. Date of current version March 9, 2020. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Lisimachos P. Kondi. (Corresponding author: Majed El <ref type="bibr">Helou.)</ref> The authors are with the School of Computer and Communication Sciences, EPFL, 1015 Lausanne, Switzerland (e-mail: majed.elhelou@epfl.ch).</p><p>Digital Object Identifier 10.1109/TIP.2020.2976814 the networks are not interpretable and have little connection to stochastic denoising. This is a limitation for training general models for denoising different noise levels. Denoisers are blind when they require no information about the noise level at test time, and universal when a single model can handle all noise levels. Blind universal models are important since knowing the noise level, at test time or ahead of training, is not a practical scenario for most applications. We first mathematically derive a blind and universal denoising function under the theoretical assumption that the image prior is Gaussian. Our denoising function, which is optimal in stochastic expectation, is referred to as fusion denoising because it fuses the input with a prior weighted using the signal-to-noise ratio. It is optimized for additive Gaussian noise removal. Our experimental results show that the stateof-the-art denoiser DnCNN <ref type="bibr" target="#b60">[59]</ref> can model an optimal fusion denoising function. However, it only models it for noise levels that are seen by the network during training. For unseen levels, our synthetic experiment's fusion network, called Fusion Net, far outperforms DnCNN. We show on synthetic data our improved generalization results.</p><p>The assumption that the image prior is Gaussian does not necessarily apply to real-world images. Building on the foundations of our theoretical solution, we adapt our Fusion Net by designing a second network that learns a fusion function for additive Gaussian noise removal. We call this new network Blind Universal Image Fusion Denoiser (BUIFD). BUIFD improves state-of-the-art denoising performance on noise levels seen in training for grayscale and color images on the standard Berkeley test sets (BSD68 and CBSD68) <ref type="bibr" target="#b42">[41]</ref>. Furthermore, we show that our generalization results to unseen noise levels obtained in our synthetic experiment extend to the denoising of the grayscale BSD68 test set. Indeed, the denoising performance on noise levels not trained on improves by multiple PSNR points. We present an extended denoising evaluation that covers other test datasets and other traditional and learning-based denoising methods.</p><p>Our main contributions are: (1) we theoretically derive an optimal fusion denoising function and integrate it into a deep learning architecture (Fusion Net) to evaluate the optimality of deep networks on a theoretical additive Gaussian noise removal task with known prior, (2) we show on synthetic data that the integration of the auxiliary fusion loss into our Fusion Net improves the network's generalization power bringing closer to the optimal solution, and (3) we develop a blind universal image fusion denoiser (BUIFD) network adapted to real images, and show that it outperforms the state of the art for This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/.</p><p>Gaussian noise removal on multiple standard image processing test sets.</p><p>The paper is organized as follows. After a review of related work, we first lay the ground for our theoretical experiments. Our experiment allows us to assess the optimality of the networks on training noise levels and the generalization of trained networks to unseen Gaussian noise levels, in comparison to the optimal Bayesian solution. We then extend the Bayesian framework solution into our network designed for real images (BUIFD) whose exact prior is unknown to improve generalization. Experimental results on standard denoising benchmarks show that our denoising network outperforms the state of the art, especially on unseen noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Image denoising approaches in the literature can be divided into classical methods and the more recent deep-learningbased methods. One common aspect is, however, the leveraging of image priors to improve denoising results. For practical reasons, it is important for a denoiser to be blind and universal since the noise levels in noisy images might not be constant or known.</p><p>Image Priors: Whether they are in the form of assumptions made on image gradients <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b52">[51]</ref>, sparsity <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b15">[15]</ref>, self-similarity within images <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b54">[53]</ref>, hybrid approaches <ref type="bibr" target="#b31">[30]</ref>, or neural network weights given a certain architecture <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b60">[59]</ref>, image priors are essential for denoising. Even traditional methods based on diffusion or filtering (in space <ref type="bibr" target="#b38">[37]</ref> or in other domains <ref type="bibr" target="#b45">[44]</ref>) rely on some priors. They, in all their forms and for multiple image restoration problems, can be discovered and tested heuristically <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b24">[23]</ref>, learned with dictionaries <ref type="bibr" target="#b15">[15]</ref>, with Markov random fields <ref type="bibr" target="#b42">[41]</ref>, or with deep neural networks <ref type="bibr" target="#b60">[59]</ref>. In our network, the prior takes the explicit form of learned feature representations.</p><p>Noise Modeling: Additive white Gaussian noise is not necessarily the best model in practical scenarios such as denoising raw images <ref type="bibr" target="#b2">[3]</ref>. Nevertheless, a large part of the image denoising literature focuses on Gaussian denoising since it remains a fundamental problem. Images with noise following different, potentially data-dependent, distributions can be transformed into images with Gaussian noise, and transformed back <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b39">[38]</ref>. In addition, a Gaussian denoising solution can serve as a proximal <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b37">[36]</ref> for image regularizers. It can be a substitute for the costly step in halfquadratic splitting (HQS) optimization, typically responsible for non-differentiable regularization in image processing. This approach is taken in the recent HQS method that leverages the denoiser for image restoration <ref type="bibr" target="#b61">[60]</ref>. We thus work with the assumption of an additive white Gaussian noise model.</p><p>Image Denoisers: Having to know the exact noise level is a serious limitation in practice for denoisers, and to know it ahead of time, before training, is even more limiting. A fixed and known noise level is also a limitation when denoising images with spatially-varying noise level <ref type="bibr" target="#b62">[61]</ref>. Not having a universal denoising model means that multiple models need to be trained and stored for different noise levels, and that noise level knowledge is required at test time. The recent method <ref type="bibr" target="#b61">[60]</ref> that generalizes to image restoration tasks is a non-universal non-blind denoiser, where 25 denoising networks are used for noise levels below 50, and even training parameters are chosen based on the noise level. Similarly, Remez et al. <ref type="bibr" target="#b40">[39]</ref>, who reach PSNR results on par with the state of the art, is another non-universal non-blind example. To leverage better priors, images are first classified into a set of classes and every single class has its specific deep network. The method is also not blind and is trained per noise level. Zhang et al. <ref type="bibr" target="#b63">[62]</ref> present a universal non-blind network for multiple super-resolution degradations by denoising, deblurring, and super-resolving images. They report that although a blind version is more practical, their blind approach fails to perform consistently well since it cannot generalize.</p><p>Blind Universal Denoisers: The state-of-the-art Gaussian denoiser DnCNN is both universal and blind <ref type="bibr" target="#b60">[59]</ref>. It is a deep network that is jointly trained on randomly-sampled noise level patches to generalize denoising to a range of noise levels. It has not been outperformed yet by other methods, whether blind or not <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b49">[48]</ref>. Only the recent FFDNet <ref type="bibr" target="#b62">[61]</ref> by the same authors of DnCNN <ref type="bibr" target="#b60">[59]</ref> improves on DnCNN for noise levels 50 and 75 by 0.06 and 0.15d B respectively, on the Berkeley BSD68 set, while performing similarly or worse for other levels. It is, however, not a blind network as it requires a noise level map as input. Lefkimmiatis <ref type="bibr" target="#b27">[26]</ref> recently studied universal denoising, building on prior work for modeling patch similarity in CNNs <ref type="bibr" target="#b26">[25]</ref>. His methods are, strictly speaking, not universal as two networks are trained separately, one for low (≤ 30) and one for high noise levels (∈ <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b56">55]</ref>). They are thus non-blind since a noise-levelbased choice must be made at inference time. Furthermore, the published results do not outperform the blind DnCNN denoising results. We thus conduct evaluation comparisons of our BUIFD method with the state-of-the-art DnCNN and the classic BM3D approach <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, which is the best nonlearning-based denoiser. It leverages image self-similarities by jointly filtering similar image patches. The authors also present a blind version of the BM3D algorithm, and we compare to both blind and non-blind versions.</p><p>Our proposed image denoiser BUIFD learns to disentangle its features to predict a prior and a noise level intermediate results. They serve as inputs to the fusion part of the network, responsible for the final denoising. Disentangling the feature space is fundamental for interpretability <ref type="bibr" target="#b6">[6]</ref>, partial transfer learning <ref type="bibr" target="#b58">[57]</ref>, domain translation <ref type="bibr" target="#b55">[54]</ref>, domain adaptation <ref type="bibr" target="#b59">[58]</ref>, specific attribute manipulation <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b64">[63]</ref> and multi-task networks <ref type="bibr" target="#b1">[2]</ref>. In our case, it is fundamental for our theoretical denoising function since the different representations serve as its inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SINGLE-IMAGE FUSION DENOISING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Framework</head><p>Although some specific applications can have a more accurate modeling <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b50">[49]</ref>, an additive white Gaussian noise model is often assumed in denoising tasks, as it models common acquisition channels <ref type="bibr" target="#b53">[52]</ref>. We thus assume that the additive independent and identically distributed noise n follows a Gaussian distribution N (0, σ 2 n ), and is uncorrelated with the data x. The noise standard deviation σ n is called noise level. In a Bayesian framework, the conditional probability distribution of the noiseless data x given a noisy observation y (where y = x + n) is given by the relation</p><formula xml:id="formula_0">p X |Y (x|y) = p Y,X (y, x) p Y (y) = p Y |X (y|x) p X (x) p Y (y) ,<label>(1)</label></formula><p>where X and Y are the random variables corresponding respectively to x and y. We are interested in the conditional distribution as we search for the Maximum Aposteriori Probability (MAP) estimatex of x. The former iŝ</p><formula xml:id="formula_1">x = arg max x p X |Y (x|y).<label>(2)</label></formula><p>We also model the data prior on x as a Gaussian distribution N (x, σ 2 x ) centered atx <ref type="bibr" target="#b41">[40]</ref>. We later modify this assumption in Sec. III-D to the practical case of real-world images. The conditional probability of y given a noiseless x value is</p><formula xml:id="formula_2">p Y |X (y|x) = 1 2πσ 2 n e − (y−x) 2 2σ 2 n ,<label>(3)</label></formula><p>and the probability distribution of y is the convolution of those of x and n, given in the Gaussian case by</p><formula xml:id="formula_3">p Y (y) = p X (x) p N (n) = e − (y−x) 2 2(σ 2 x +σ 2 n ) 2π(σ 2 x + σ 2 n ) ,<label>(4)</label></formula><p>where is the convolution operator. With these probability distribution functions, we can obtain an expression for the conditional distribution of x given its noisy observation y by substituting Eq. (3) and Eq. (4) into Eq. (1). p X |Y (x|y) can also be written in the following form of a Gaussian in x, given an observation y</p><formula xml:id="formula_4">p X |Y (x|y) = 1 2πσ 2 x e − (x−μ) 2 2σ 2 x .<label>(5)</label></formula><p>By matching the expanded expression of p X |Y (x|y) with Eq. (5) for all possible x values, we obtain the expressions forμ andσ 2</p><formula xml:id="formula_5">μ = σ 2 nx + σ 2 x y σ 2 x + σ 2 n ,σ 2 = σ 2 x σ 2 n σ 2 x + σ 2 n .<label>(6)</label></formula><p>For the Gaussian shown in Eq. (5), the MAP estimator is also the conditional expected value (mode and mean being equal) and it is hence given bŷ</p><formula xml:id="formula_6">x = E[x|y] = ∞ −∞ x · p X |Y (x|y)dx,<label>(7)</label></formula><p>which, using Eq. (5), can be directly derived to bê</p><formula xml:id="formula_7">x =x 1 + S + y 1 + 1/S ,<label>(8)</label></formula><p>where S σ 2 x /σ 2 n and stands for Signal-to-Noise Ratio (SNR). We call this operation fusion denoising as it fuses the prior and the noisy image, based on the SNR.</p><p>Image denoising models are typically trained to maximize PSNR or equivalently minimize Mean Squared Error (MSE) loss. This means that with close-to-optimal convergence of a neural network model (MSE loss → 0 + ), its output tends towards the minimum MSE estimator (MMSE). With our Gaussian modeling, this leads to the MAP estimatorx of Eq. (8). Thus, an MSE reconstruction loss in a neural network leads it to the estimatorx, iff S andx are correctly predicted and correctly used in the fusion with the noisy input y, as in Eq. <ref type="bibr" target="#b8">(8)</ref>. The optimal fusion, used as reference in our experimental evaluation in Sec. IV-B, is given the exact S and x values for Eq. (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Net Architecture</head><p>We incorporate the basic structure of the optimal fusion solution into the architecture of a neural network, which we call Fusion Net. We build the main blocks of our Fusion Net based on the blind DnCNN introduced in <ref type="bibr" target="#b60">[59]</ref> and illustrated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the noise-predicting CNN of DnCNN ( <ref type="figure" target="#fig_0">Fig. 1(a)</ref>), the prior-predicting CNN, and the one predicting</p><formula xml:id="formula_8">f (S) (where f (S) 1 1+S</formula><p>) in our Fusion Net ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), all leverage the same DnCNN architecture design. The CNNs are all constituted of a sequence of convolution layers, rectified linear units (ReLU) <ref type="bibr" target="#b35">[34]</ref> and batch normalization blocks <ref type="bibr" target="#b21">[21]</ref>. Note that f (S) is inversely-proportional to the SNR and proportional to the noise level. It is the factor multiplying the prior in Eq. <ref type="bibr" target="#b8">(8)</ref>. To summarize, the f (S) CNN predicts 1 1+S where S is the SNR of the input image (determined by the noise level and the image model used in our theoretical settings), and the prior CNN predictsx defined in Eq. <ref type="formula" target="#formula_6">(7)</ref>.</p><p>Unlike the DnCNN that predicts the noise values in the input noisy image, then subtracts them from the noisy input to yield the final denoised output, our network learns optimal fusion denoising given by the function in Eq. (8), as illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. The same depth and capacity of the DnCNN are retained to learn separately the image prior and the SNR function, f (S), that is required for the weighted fusion of the prior and the noisy input image. Note that SNR learning also contains a form of prior knowledge, but of variance rather than of expectation. We subtract from the prior our noisy input image and multiply the result, pixel-wise, with the SNR function. This yields the noise prediction given a noisy input, which we subtract from the latter to obtain the denoised output. This architecture is mathematically equivalent to Eq. <ref type="bibr" target="#b8">(8)</ref>. However, the wiring of <ref type="figure" target="#fig_0">Fig. 1</ref>(b) allows us to clearly have a residual learning connection and to keep the parallelism between the two aforementioned networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fusion Net Feature Disentangling</head><p>To mimic the optimal fusion between image prior and noisy image based on the SNR, as in Eq. <ref type="formula" target="#formula_7">(8)</ref>, both the architecture and loss function are adapted. For the fusion, the network needs to predict the image priorx and f (S) per pixel ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). We obtain, with close-to-zero MSE reconstruction loss of our Fusion Net, that the ground-truth target and the network output are approximately equal</p><formula xml:id="formula_9">x · f (S) + y · (1 − f (S)) ≈ a · b + y · (1 − b), ∀y ∈ D T ,<label>(9)</label></formula><p>where a and b are the outputs of intermediate layers in the Fusion Net, and y is the noisy input. Specifically, a is the output of the final layer of the prior CNN in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, and b the output of the last layer of f (S) in the same figure. After gradient descent convergence, when the MSE reconstruction loss is close to zero, we get the approximate equality of the left and right terms in Eq. <ref type="bibr" target="#b9">(9)</ref>. We can view this equation as a first-degree polynomial in the variable y. As Eq. <ref type="formula" target="#formula_9">(9)</ref> holds for all y in the training dataset D T , we can apply coefficient equating, where the coefficients are {a ·b, 1−b} and</p><formula xml:id="formula_10">{x · f (S), (1− f (S))}.</formula><p>We thus obtain the approximate equality between a andx and between b and f (S). The network intermediate outputs {a, b} are therefore, respectively, equal to the prior and the SNR function {x, f (S)}, with close-to-zero MSE reconstruction loss ∀y ∈ D T . This extends to other y outside the dataset assuming that the latter is general enough. We can further incorporate optimal denoising information in the Fusion Net, under the theoretical settings described in Sec. III-A, through explicit SNR learning with a dedicated loss term. The fusion representations, i.e. the priorx and f (S), are thus further enforced through a penalty term for predicting f (S) in the loss function. The full loss function L f of the Fusion Net is given by</p><formula xml:id="formula_11">L f = α||a · b + y · (1 − b) − x|| 2 2 + (1 − α)||b − f (S)|| 2 2 ,<label>(10)</label></formula><p>where α is a weight parameter, the first term is the MSE reconstruction loss similar to that of the DnCNN, and the second term is a reconstruction loss for f (S). Following Eq. <ref type="formula" target="#formula_9">(9)</ref>,</p><formula xml:id="formula_12">a · b + y · (1 − b)</formula><p>is the denoised output of the Fusion Net. The Fusion Net therefore minimizes the reconstruction loss over the denoised image by learning to predict the image prior and the SNR function values separately. Unlike the DnCNN residual learning network, which only leverages ground-truth noise-free images during training, the Fusion Net also leverages explicit SNR information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Denoising Non-Gaussian Images</head><p>Here, our main objectives are to (1) design a Blind Universal Image Fusion Denoiser (BUIFD) for real images, by adapting the theoretical fusion strategy integrated in our Fusion Net, (2) evaluate the denoising performance of BUIFD on training noise levels, and (3) assess the generalization to unseen noise levels with real images.</p><p>Since a real image cannot be modeled with a simple Gaussian prior, our image fusion denoising network used for real images (BUIFD), shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, is adapted from the theoretical Fusion Net, shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, by modifying the fusion part. We replace the optimal mathematical fusion by a product fusion step followed by trainable convolution layers. We use three convolution layers to learn the data-dependent fusion function. The optimal fusion function F is to be applied on the noisy input image y, the prior prediction, and the noise level prediction</p><formula xml:id="formula_13">x = F(y, f P (y, θ P ), f N (y, θ N )),<label>(11)</label></formula><p>where the prior-predicting and noise-level-predicting network functions are respectively f P and f N , with their corresponding learned parameters θ P and θ N , and the denoised estimate isx. Intuitively, the prior-predicting network ( f P ) is used to predict the expected value of the unknown real-word distribution out of which the intensity of a given pixel is sampled, and that for each pixel. The noise-level-predicting network ( f N ) predicts the noise level, which is used to control the weighted average between prior and observation. When the noise level is low, the actual observation can be given more weight, and when the noise level is high, the current observation is less reliable and the fusion resorts more to the use of the prior estimation. The optimal fusion F can be approximated byF modeled with three convolution layers. However, we expect F to contain pixel-wise inter-input multiplications similar to the ones of Eq. <ref type="bibr" target="#b8">(8)</ref>. Since such pixel-wise multiplications cannot be replicated with convolutions, we pass two additional inputs into the convolution layers that modelF. These two additional inputs are given by <ref type="bibr" target="#b12">(12)</ref> where is pixel-wise multiplication. They are concatenated with the inputs of F given in Eq. (11), yielding five different inputs that are sent toF. The two additional inputs reduce the learning burden of the convolution layers and improve the denoising performance. Note that we normalize f N (·, ·) ∈ [0, 1]. We call this pixel-wise multiplication step and the concatenation of the additional inputs the product fusion (shown in the pipeline of <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). These two fusion steps, namely  <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">25]</ref>. <ref type="table" target="#tab_0">NOISE LEVELS IN THE RIGHT HALF OF THE TABLE ARE NOT SEEN DURING TRAINING. WE ALSO REPORT  THE OPTIMAL BAYESIAN DENOISING (OPTIMAL FUSION). THE BOTTOM ROW SHOWS THE INDEPENDENT TWO-SAMPLE T-TEST RESULTS  BETWEEN DNCNN AND OUR FUSION NET. THE TWO-TAILED p-VALUES VALIDATE THE NULL HYPOTHESIS OF EQUAL AVERAGE  PSNR BETWEEN DNCNN AND THE FUSION NET ON TRAINING NOISE LEVELS, WITH SIGNIFICANCE LEVEL 0.05</ref> the product fusion and the three convolution layers, formF and realize point (1) above. The BUIFD's optimization loss is given by</p><formula xml:id="formula_14">f P (y, θ P ) f N (y, θ N ), y (1 − f N (y, θ N )),</formula><formula xml:id="formula_15">L f = ||F(C) − x|| 2 2 + || f N (y, θ N ) − N|| 2 2 ,<label>(13)</label></formula><p>where C is the concatenation of the inputs listed in Eq. <ref type="formula" target="#formula_0">(11)</ref> and Eq. <ref type="formula" target="#formula_0">(12)</ref>, namely,</p><formula xml:id="formula_16">{y, f P (y, θ P ), f N (y, θ N ), f P (y, θ P ) f N (y, θ N ), y (1− f N (y, θ N ))},</formula><p>x is the ground-truth original image, and f N (y, θ N ) and N are respectively the predicted and ground-truth noise level values, normalized to [0, 1]. We discuss the relation between BUIFD <ref type="figure" target="#fig_0">(Fig. 1(c)</ref>) and our theoretical Bayesian network Fusion Net ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) in detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Relation With the Bayesian Framework</head><p>The Fusion Net in <ref type="figure" target="#fig_0">Fig. 1(b)</ref> explicitly models the relation with the Bayesian solution in the theoretical experiments.</p><p>We discuss in what follows the relation between BUIFD <ref type="figure" target="#fig_0">(Fig. 1(c)</ref>) and the Bayesian solution Eq. <ref type="bibr" target="#b8">(8)</ref>. We first note that a Gaussian prior does not perfectly model real images, and thus, we expect that the real-image BUIFD network ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>) deviates from the Fusion Net ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), from which it is inspired, to adapt to real images. However, as addressed in Sec. III-D, the relation between BUIFD and the Bayesian framework is strongly pertinent.</p><p>First, the product fusion Eq. 12 explicitly creates the same components as in the Bayesian equation Eq. (8). This product fusion weighs noisy input and learned prior based on SNR, as in the Bayesian fusion. The fusion layers are only 3 convolutional layers with no non-linearities, to ensure that mostly an additive fusion of our Bayesian terms takes place, with local smoothing, and the relation with the Bayesian solution is preserved as much as possible.</p><p>Second, we do not predict an image prior in the sense of a pixel intensity probability distribution, but only the expected mean of that unknown distribution. In the literature, priors are often probability distributions of image gradients, but our definition is quite distinctive. Our prior is, per pixel, the expected value of the distribution out of which the pixel's intensity was sampled. Even with noise-free images, one cannot exactly know that distribution (nor its mean), per pixel, to assess how much this definition is still respected in the BUIFD network with real images. However, all other Bayesian components are consistent, and the empirical results as well. Our improvement of 3.30d B at unseen noise level 70 in the theoretical experiment is paralleled by an improvement of about 3d B at noise level 75 in the real image BSD68 experiment.</p><p>We hope our methodology motivates future work to analyze deep network optimality on theoretical experiments that are designed such that an optimal solution is known, and that it motivates deep network design inspired from Bayesian solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fusion Net Experimental Setup</head><p>The networks are trained (and tested) with data generated synthetically according to the theoretical assumption of a Gaussian image prior as defined in Sec. III-A. The training data is composed of over 200k patches of size 40 × 40 pixels. Image pixel intensities for the training data are drawn at random from N (127, 25 2 ), following the Gaussian image prior assumption, and all values are normalized to [0, 1] before the training through division by 255 and clipping of all values outside the interval to the interval's closer bound when noise is added. For the testing data, 256 images of size 256 × 256 pixels are used, and they are created with the same procedure as that of the training data.</p><p>We train the networks for 50 epochs with mini-batches of size 128. We use the Adam optimizer <ref type="bibr" target="#b22">[22]</ref> with an initial learning rate of 0.001 that is decayed by a factor of 10 every 30 epochs, the remaining parameters being set to the default values. The weight α in Eq. (10) is set to 0.1. We train the networks with multiple levels of noise. The standard deviation of the additive Gaussian noise is chosen uniformly at random within the interval <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">25]</ref> during the training. At the end of every epoch, the noise components are re-sampled, following the same procedure, but not the ground-truth images. For the testing phase, the networks are evaluated on test images where the added noise is also Gaussian, with a given standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion Net Evaluation</head><p>PSNR results of DnCNN, our Fusion Net, as well as the optimal upper bound are presented in <ref type="table" target="#tab_0">Table I</ref>. The optimal upper bound denoising performance is that of the optimal mathematical solution in Eq. <ref type="bibr" target="#b8">(8)</ref>. We can see that both the DnCNN and the Fusion Net perform similarly on the training noise levels (left half of the table), and very close to optimal. To validate that the results are indeed statistically similar, we analyze the distribution of PSNR values across the test This shows that the Fusion Net, despite the modeling that mimics optimal denoising fusion and the additional training information to learn SNR values, performs similarly to the DnCNN. The latter has therefore enough capacity and learns an optimal denoising. This, however, only holds for the noise levels seen during training by the networks, shown in the left half of <ref type="table" target="#tab_0">Table I</ref>. The confidence in the null hypothesis decreases with increasing test noise levels. With a significance level above 0.053, the null hypothesis would even be rejected for noise level 25. The evaluation results on noise levels larger than 25, which are not trained on by any of the networks, are reported in the right half of <ref type="table" target="#tab_0">Table I</ref>. For these larger noise levels, the null hypothesis is very clearly rejected as there is a growing performance gap between DnCNN and our Fusion Net. The p-value quickly drops to zero when there is a PSNR gap, since variances are very small in our results. The Fusion Net generalizes better to unseen noise levels, even performing close to optimal up to noise level 60. The further we increase the noise level, the larger is the performance gap between the Fusion Net and the DnCNN. Although both networks perform well for the training noise levels, the Fusion Net learns a more general model and clearly outperforms on unseen noise levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Real-Image Experimental Setup</head><p>We use the referenced implementation by the authors of DnCNN and the same datasets. <ref type="bibr" target="#b0">1</ref> As mentioned in Sec. III-D, the architecture of our prior-predicting network is identical to 1 https://github.com/SaoYan/DnCNN-PyTorch   <ref type="figure">CNN)</ref> is plotted with a dotted blue curve, and the overall loss for the fusion methods (the sum of the former two losses) is plotted with a dotted green curve. Note the abrupt small improvement in loss reduction at epoch 30, which is when the learning rate is exponentially decayed. We can see that the different learned function converge by the end of training (logs shown for the methods with upper training noise level 55).</p><p>that of DnCNN. <ref type="bibr" target="#b1">2</ref> All the network details are available in <ref type="bibr" target="#b60">[59]</ref> and we omit the repetition. The same network depth and feature layers are thus used in the prior-predicting network (18 main blocks) in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>. The noise level network is a shallower one consisting of 5 blocks similar to the ones used in the prior predictor. Each block is a convolution followed by a batch normalization and a ReLU, and we append to the noise level predictor a convolution followed by an application of the logistic sigmoid function to obtain the normalized f N (·, ·) ∈ [0, 1]. The noise level values are thus mapped during the training to the range [0, 1] by dividing by the largest training noise level. The three convolution layers approximating the final fusion have 16 channels. Both the BUIFD and the DnCNN networks are trained with the same training parameters and optimization settings, similar to Sec. IV-A except for the patch size. For completeness, we provide all the details of the training hyper-parameters. We use the Adam optimizer <ref type="bibr" target="#b22">[22]</ref> with an initial learning rate of 0.001 that is decayed by a factor of 10 every 30 epochs, the remaining optimizer parameters being set to the default values. The networks are trained for 50 epochs each, and the progress of the different losses can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>. We use a patch size of 50 × 50 with a stride of 10 on the training images. The training mini-batch size is set to 128 patches per mini-batch. The added noise is drawn from a Gaussian distribution of given standard deviation, based on the noise level. This standard deviation is sampled uniformly at random  <ref type="table" target="#tab_0">Color IMAGE DENOISING, SIMILAR TO TABLE II, ON THE CBSD68 STANDARD TEST SET. BOLD INDICATES THE  BEST BLIND RESULT, FOR EACH RANGE OF TRAINING NOISE LEVELS, AND THAT BEST RESULT IS SELECTED BEFORE ROUNDING</ref> from a specified range (details in Sec. IV-D), and is the same for all pixels in a given training patch. We use the training hyper-parameters of DnCNN, for training it and for training BUIFD, the hyper-parameters are not tweaked for BUIFD. The noise level predictor is jointly trained within BUIFD, so both network branches always see the same training data (with the same simulated noise distributions) as each other in the experiments of Sec IV-D. We use the 400 Berkeley images <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b44">[43]</ref> for grayscale training and the 432 color Berkeley images for color training, as in <ref type="bibr" target="#b60">[59]</ref>. The same architectures are retained for grayscale and color networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Real-Image Evaluation</head><p>Grayscale denoising evaluation is carried out over the standard Berkeley 68 image test set (BSD68) <ref type="bibr" target="#b42">[41]</ref> taken from <ref type="bibr" target="#b33">[32]</ref>. <ref type="table" target="#tab_0">Table II</ref> reports the results of our fusion approach and of the state-of-the-art blind DnCNN, when they are both trained with noise levels up to 55 or up to 75. Note that for our fusion approach that is trained up to noise level 55, we map the maximum network prediction of 1, during training, to 55 and not to the maximum test noise level, for a more fair comparison. The results of the blind version of BM3D as well as those of the non-blind BM3D, which is given the correct test noise level at inference time, are also reported for reference. We restrict all noisy test images to the range [0, 255], as having negative intensities, or values exceeding 255, is not a configuration encountered in practice. <ref type="figure" target="#fig_2">Fig. 3</ref> shows our intermediate feature results, the prior and the noise level values, along with denoising results. The denoised image is created by fusing the noisy input image with the network-derived prior and the noise level values. The fusion is carried out by the product fusion step and the three convolution layers. As in practical scenarios, the denoised outputs are clipped to [0, 255], as are the noisy input images. Our results better remove the noise compared with those of DnCNN over low frequency regions, and details are better reconstructed over the high-frequency content. We note that, at high noise levels, there is a smudging effect most visible around low-frequency regions ( <ref type="figure" target="#fig_2">Fig. 3 (k)</ref> and (l)), which creates blurry and noisy edges. These are created by both networks, but are more salient in our result (k) as it is less noisy than (l). The higher the noise level and standard deviation of the Gaussian noise, the larger the number of averaged samples needs to be such that the statistical mean converges to zero. This makes the local mean of the noise across small patches vary around zero from region to region, randomly, and causes the smudging-like or wave-like effect (notice over low-frequency regions how almost all these artifacts have a curve shape, rather than a linear one, which is modeled by the various different mean values around them).</p><p>As seen in <ref type="table" target="#tab_0">Table II</ref>, our fusion approach improves the PSNR at every single noise level starting from 15−20, which includes seen levels for both training ranges. Comparing DnCNN 75 and BUIFD 75 , which are trained on all noise levels, we also note with our approach an improvement of up to 0.7d B and an average improvement of 0.36d B. We outperform even the non-blind version of BM3D by an average of 0.25d B with our version trained on all noise levels and we perform just as well as the non-blind BM3D when training only up to level 55. Comparing the results of DnCNN 55 and of BUIFD <ref type="bibr" target="#b56">55</ref> in <ref type="table" target="#tab_0">Table II</ref>, for unseen noise levels in the range <ref type="bibr" target="#b56">(55,</ref><ref type="bibr">75]</ref>, we see that the generalization of the fusion approach to unseen noise levels indeed applies to real images. The improvement of multiple PSNR points for level 75 is consistent with that obtained in our synthetic experiment in <ref type="table" target="#tab_0">Table I.</ref> The results in <ref type="table" target="#tab_0">Table III</ref> illustrate denoising images with spatially-varying noise level, without re-training the networks. Noise is added across an image with a level that increases linearly with rows. For the non-blind BM3D, we input the average noise level as a guide. The BUIFD network can handle spatially-varying noise, which neither the prior nor the noise level predicting network branches are trained on. It outperforms DnCNN on all noise setups, whether the networks are trained on the full range or only up to level 55.</p><p>For color image denoising, we use the standard color version of BSD68 (CBSD68) for testing. Noise is simulated and added to each test image before running it through a denoising method. PSNR results are reported in <ref type="table" target="#tab_0">Table IV</ref>. The high inter-channel correlation between the RGB color channels <ref type="bibr" target="#b13">[13]</ref> allows all methods to perform significantly better in terms of denoising PSNR on color images compared with grayscale images. We note that this advantage of having multiple correlated channels as in color imaging is not always available, for instance with single-wavelength imaging <ref type="bibr" target="#b30">[29]</ref>. We hypothesize that this correlation also enables the networks to implicitly learn the noise level prediction. High correlation implies that the network sees multiple approximately equal data samples with different noise instances drawn from the same distribution. Thus, it more easily learns an estimate of the noise variance. Each of the two networks therefore performs more or less the same when trained up to noise level 55 and when trained up to noise levels 75. Our fusion approach, however, consistently outperforms CDnCNN on every single noise level for both training noise ranges. Our average improvement over CDnCNN is about 0.1d B. We also note that the networks outperform, on average, even the nonblind CBM3D by about 0.5d B for CDnCNN and 0.6d B for our CBUIFD. Sample image denoising results for grayscale and color images are illustrated in <ref type="figure" target="#fig_3">Fig. 4, 5</ref> and <ref type="figure" target="#fig_4">Fig. 6, 7</ref> respectively, for the non-blind BM3D and the blind networks DnCNN and BUIFD trained on the full range of noise levels. The main trade-off seen between the results of BM3D and those of DnCNN is on detail reconstruction. The non-blind BM3D achieves good PSNR reconstruction but at the expense of blurring the results. This causes a loss of details (visible on the large rock in <ref type="figure">Fig. 4</ref>, and the zoom-in insert in <ref type="figure" target="#fig_3">Fig. 5</ref> and of edge sharpness (visible on the borders of the lake in the zoom-in insert in <ref type="figure">Fig. 4</ref>). The DnCNN results suffer less of a blurring problem, but the noise-removal is not optimal in certain areas such as smooth surfaces (visible on the inner area of the lake in the zoom-in insert in <ref type="figure">Fig. 4</ref>). Our approach achieves a good performance in terms of this trade-off. BUIFD achieves good PSNR results, with significantly less blurring than the non-blind BM3D (see <ref type="figure" target="#fig_3">Fig. 5</ref> for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extended Benchmark Comparisons</head><p>We present more denoising experimental tests on different benchmark datasets, and compare the results of different denoising approaches on these datasets. We report blind denoising results for noise levels 10 to 80 (with a step size of 10) on the BSD68 dataset, Set5, Set14, Sun_Hays80, Urban100, and Manga109 datasets. Set5 and Set14 are made up of, respectively, 5 and 14 traditionally-used images for testing image processing algorithms. Most of their images are smaller than 512×512. The Sun_Hays80 dataset is made up of the high-resolution version of the 80 images presented in <ref type="bibr" target="#b47">[46]</ref>, with sizes smaller than 1024 × 1024. The Urban100 dataset is a collection of 100 high resolution images taken from Flickr using urban keywords <ref type="bibr" target="#b20">[20]</ref>. The Manga109 dataset is constituted of 109 professional artist drawings <ref type="bibr" target="#b34">[33]</ref>, of size 827 × 1170. We present in <ref type="table" target="#tab_4">Table V</ref> the denoising results of the blind non-learning methods BM3D, EPLL <ref type="bibr" target="#b65">[64]</ref>, KSVD <ref type="bibr" target="#b0">[1]</ref>, and WNNM <ref type="bibr" target="#b17">[17]</ref> that were developed for Gaussian denoising and are given, to enforce the blind setting, the default noise level set by the non-blind BM3D (set to <ref type="bibr" target="#b26">25)</ref>, and the learningbased methods DnCNN <ref type="bibr" target="#b60">[59]</ref> and BUIFD, on denoising the luminance of the images with added Gaussian noise levels ranging from 10 to 80. We also evaluate another learning-based method with the same training hyper-parameters as those of DnCNN, namely, the MemNet architecture <ref type="bibr" target="#b48">[47]</ref>, and extend our fusion technique to that architecture and call it BUIFD(M). It is constructed following <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, with the exception that the MemNet architecture replaces that of DnCNN for the prior-predicting CNN. All the learning-based methods in this section are trained up to noise level 55. <ref type="table" target="#tab_4">Table V</ref> shows the PSNR and SSIM metrics for each method, and we highlight in bold the best-PSNR and best-SSIM method between DnCNN and BUIFD, and between MemNet and BUIFD(M). A sample visual result is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, taken from Set14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We define a theoretical framework under which we derive an optimal denoising solution that we call fusion denoising. We integrate it into a deep learning architecture and compare with the optimal mathematical solution and with the stateof-the-art blind universal denoiser DnCNN. Our synthetic experimental results show that our Fusion Net generalizes far better to higher unseen noise levels.</p><p>We learn a data-dependent fusion function to adapt our fusion denoising network to real images. Our blind universal image fusion denoising network BUIFD improves the stateof-the-art real image denoising performance both on training noise levels and on unseen noise levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(a) Schematic of the DnCNN residual learning approach for denoising. The network predicts the noise in an image. (b) Our Fusion Net that explicitly learns the SNR function for optimal fusion of the noisy image with the learned prior, following Eq. (8). (c) Our real-image fusion denoiser, BUIFD, where fusion is carried out with a pixel-wise product stage followed by three convolution layers for learning a general fusion function (Sec. III-D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Training losses of the different learning-based methods. Per epoch, we plot with a full black curve the overall loss (i.e. reconstruction loss) of the base methods DnCNN and MemNet, in (a) and (b) respectively. The same reconstruction loss with our fusion method is plotted with a dotted red curve, the noise-level loss computed on the corresponding intermediate output (i.e. the output of the noise level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Left to right: original and noisy images, prior and noise level predictions of BUIFD, our fused denoising result and the DnCNN denoised image. Our denoising result is created by fusing the noisy image, the prior and the noise level values, for instance (e) isF((b), (c), (d)). All the networks are trained on noise levels in [0, 55]. Whether the noise level is seen<ref type="bibr" target="#b26">(25)</ref>, or not seen (75), during training, our denoised results show better noise removal (sky in (e-f), window, wall and arms in (k-l)). We show the PSNR in d B and the SSIM<ref type="bibr" target="#b51">[50]</ref> between parentheses for the different results. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Grayscale image denoising example from BSD68. All networks are trained on all noise levels [0, 75] and we test on noise level 45. Non-blind BM3D results are very smoothed, and details are lost. DnCNN preserves more details, but at the expense of PSNR. Our blind approach preserves details and outperforms the non-blind BM3D in terms of PSNR. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Color image denoising example from CBSD68. All networks are trained on the full range of noise levels [0, 75] and we test on noise level 25. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Color image denoising example from CBSD68. All networks are trained on the full range of noise levels [0, 75] and we test on noise level 45. Best viewed on screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Sample visual result from Set14, with PSNR(d B)/SSIM values. The top row shows non-blind results with the traditional methods KSVD, BM3D, EPLL and WNNM, as the noise level is 25, which the default set when the noise level is unknown. And the bottom row shows the results with the different learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>SET PSNR (d B) RESULTS FOR THE NOISE STANDARD DEVIATIONS GIVEN IN THE TOP ROW. THE NETWORKS ARE TRAINED ON NOISE LEVELS RANDOMLY CHOSEN IN</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PSNR</head><label>II</label><figDesc>(d B)/SSIM COMPARISONS OF grayscale IMAGE DENOISING ON THE BSD68 STANDARD TEST SET. WE COMPARE THE NON-BLIND BM3D, THE BLIND BM3D, DNCNN, AND OUR BUIFD. DNCNN σ OR BUIFD σ INDICATES THAT THE NETWORK SEES NOISE LEVELS only UP TO σ DURING THE TRAINING. BOLD INDICATES THE BEST BLIND RESULT, FOR EACH RANGE OF TRAINING NOISE LEVELS, AND THAT BEST RESULT IS SELECTED BEFORE ROUNDING. NOTE: SMALL DEVIATIONS IN REPORTED PSNR VALUES COMPARED WITH THE LITERATURE, NOTABLY ON HIGHER NOISE LEVELS, ARE DUE TO CLIPPING NOISY INPUTS (AND OUTPUTS) TO [0, 255], AS A PRACTICAL CONSIDERATION</figDesc><table /><note>set. A two-sided T-test (independent two-sample T-test) is used to evaluate the null hypothesis that the PSNR results of both networks have similar expected values. This test is chosen as we have the exact same sample sizes defined by the test dataset, and the variances of PSNR results are very similar. The T-test results are given in the bottom row of Table I, and the null hypothesis holds for all configurations in the left half of the table (for a 0.05 significance level, i.e., a p-value ≥ 0.05).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III WE</head><label>III</label><figDesc>EVALUATE PSNR VALUES, WITH SPATIALLY-VARYING NOISE LEVEL, ON THE BSD68 TEST SET. THE NOISE LEVEL INCREASES LINEARLY WITHIN</figDesc><table /><note>THE IMAGE OVER THE RANGE [σ c − 10, σ c + 10]. THE NON-BLIND BM3D IS GIVEN THE CENTRAL NOISE LEVEL σ c</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PSNR</head><label>IV</label><figDesc></figDesc><table /><note>(d B)/SSIM COMPARISONS OF</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>PSNR/SSIM EVALUATION OF THE Blind BM3D, EPLL, KSVD, WNNM, DNCNN, BUIFD, MEMNET, AND BUIFD(M). BOLD INDICATES THE BEST BLIND DENOISING RESULT IN TERMS OF PSNR OR SSIM BETWEEN EACH PAIR OF LEARNING METHODS, FOR EACH GAUSSIAN NOISE LEVEL. WE CLIP NOISY IMAGES TO [0, 255], AS A PRACTICAL CONSIDERATION</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>)</head><label></label><figDesc>Fig. 4. Grayscale image denoising example from BSD68. All networks are trained on all noise levels [0, 75] and we test on noise level 25. Nonblind BM3D loses edge details due to blur smoothing. The network results are sharper, with the better PSNR being that of BUIFD 75 . Best viewed on screen.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is available at: https://github.com/majedelhelou/BUIFD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K -SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Cvf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
		<title level="m">Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="11036" to="11045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlocal image and movie denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="139" />
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with BM3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2392" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeruIPS</title>
		<meeting>NeruIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1256" to="1272" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3D collaborative filtering with grouping constraint in luminance-chrominance space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2007-09" />
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="1259" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mobile robotic painting of texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Correlation-based deblurring leveraging multispectral chromatic aberration in color and near-infrared joint acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sadeghipoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>IEEE Int. Conf. Image ess. (ICIP)</meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1402" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="273" to="307" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep burst denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="538" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved BM3D image denoising using SSIM-optimized Wiener filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>El-Sakka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Image Video Process</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<ptr target="http://arxiv.org/abs/1412.6980" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyperlaplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1033" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An unbiased risk estimator for image denoising in the presence of mixed Poisson-Gaussian noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Montagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Olivo-Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1255" to="1268" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3587" to="3596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Universal denoising networks : A novel CNN architecture for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3204" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When image denoising meets high-level vision tasks: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Joint Conf</title>
		<meeting>27th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="842" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detach and adapt: Learning cross-domain disentangled deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8867" to="8876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A genetically encoded single-wavelength sensor for imaging cytosolic and cell surface ATP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Lobas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">711</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Noise parameter mismatch in variance stabilization, with an application to Poisson-Gaussian noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Makitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5348" to="5359" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class-aware fully convolutional Gaussian and Poisson denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5707" to="5722" />
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fields of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="229" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D, Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2774" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The curvelet transform for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="670" to="684" />
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting the potential of standard convolutional autoencoders for image restoration by evolutionary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4778" to="4787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Super-resolution from Internet-scale scene matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Photogr. (ICCP)</title>
		<meeting>IEEE Int. Conf. Comput. Photogr. (ICCP)</meeting>
		<imprint>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MemNet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning for image denoising: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05052</idno>
		<ptr target="http://arxiv.org/abs/1810.05052" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Blob reconstruction using unilateral second order Gaussian kernels with application to high-ISO long-exposure image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lopez-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Baets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4817" to="4825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">What makes a good model of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Patch group based nonlocal self-similarity prior learning for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">DualGAN: Unsupervised dual learning for Image-to-Image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Neural style transfer: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04058</idno>
		<ptr target="http://arxiv.org/abs/1705.04058" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf</title>
		<meeting>28th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint geometrical and statistical alignment for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1859" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning deep CNN denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a fast and flexible solution for CNN-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4608" to="4622" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning a single convolutional super-resolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
