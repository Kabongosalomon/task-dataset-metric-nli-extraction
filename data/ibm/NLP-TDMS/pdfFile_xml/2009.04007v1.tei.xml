<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh</surname></persName>
							<email>sachan.devendra@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachan</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
							<email>manzilzaheer@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study bidirectional LSTM network for the task of text classification using both supervised and semi-supervised approaches. Several prior works have suggested that either complex pretraining schemes using unsupervised methods such as language modeling <ref type="bibr" target="#b2">(Dai and Le 2015;</ref><ref type="bibr" target="#b23">Miyato, Dai, and Goodfellow 2016)</ref> or complicated models (Johnson and Zhang 2017) are necessary to achieve a high classification accuracy. However, we develop a training strategy that allows even a simple BiLSTM model, when trained with cross-entropy loss, to achieve competitive results compared with more complex approaches. Furthermore, in addition to cross-entropy loss, by using a combination of entropy minimization, adversarial, and virtual adversarial losses for both labeled and unlabeled data, we report state-of-the-art results for text classification task on several benchmark datasets. In particular, on the ACL-IMDB sentiment analysis and AG-News topic classification datasets, our method outperforms current approaches by a substantial margin. We also show the generality of the mixed objective function by improving the performance on relation extraction task. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is an important problem in natural language processing (NLP). The task is to assign a document to one or more predefined categories. It has a wide range of applications such as sentiment analysis <ref type="bibr" target="#b25">(Pang and Lee 2008)</ref>, topic categorization <ref type="bibr" target="#b17">(Lewis et al. 2004)</ref>, and email filtering <ref type="bibr" target="#b31">(Sahami et al. 1998)</ref>. Early machine learning approaches for text classification were based on the extraction of bag-of-words features followed by a supervised classifier such as naïve Bayes <ref type="bibr" target="#b19">(McCallum and Nigam 1998)</ref> or a linear SVM <ref type="bibr" target="#b8">(Joachims 1998)</ref>. Later, better word representations were introduced, such as latent semantic analysis <ref type="bibr" target="#b3">(Deerwester et al. 1990</ref>), skipgram <ref type="bibr" target="#b20">(Mikolov et al. 2013)</ref>, and fastText <ref type="bibr" target="#b13">(Joulin et al. 2017)</ref>, which improved classification accuracy. Recently, recurrent and convolutional neural network <ref type="bibr" target="#b14">(Kim 2014</ref>) models were introduced to utilize the word order and grammatical structure. Many complex variations Copyright © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://github.com/DevSinghSachan/ssl text classification of these models have been proposed to improve the text classification accuracy, e.g. training one-hot CNN (JZ15a; Johnson and Zhang 2015a) or one-hot bidirectional LSTM (BiL-STM) network with dynamic max-pooling (JZ16; <ref type="bibr" target="#b11">Johnson and Zhang 2016)</ref>. Current state-of-the-art approaches for text classification involve using pretrained LSTMs (DL15; <ref type="bibr" target="#b2">Dai and Le 2015)</ref> or complex computationally intensive models (JZ17; Johnson and . DL15 argued that randomly initialized LSTMs are difficult to optimize and can lead to worse performance than linear models. Therefore, to improve the performance, they proposed pretraining the LSTM with either a language model or a sequence auto-encoder. However, pretraining or using complicated models can be very time consuming, which is a major disadvantage and may not be always feasible. In this paper, we consider a BiLSTM classifier model similar to the one proposed by DL15 for text classification. For this simple BiLSTM model with pretrained embeddings, we propose a training strategy that can achieve accuracy competitive with the previous purely supervised models, but without the extra pretraining step. We also perform ablation studies to understand aspects of the proposed training strategy that result in an improvement.</p><p>Pretraining approaches often use extra unlabeled data in addition to the labeled data. We explore the applicability of such semi-supervised learning (SSL) in our training framework, where there is no prior pretraining step. In this regard, we propose a mixed objective function for SSL that can utilize both labeled and unlabeled data to obtain further improvement in classification. To summarize, our contributions are as follows:</p><p>• We show that with proper model training, using a maximum likelihood objective with a simple one-layer BiL-STM model ( §2) can produce competitive accuracies, • We propose a mixed objective function that can be applied to text classification tasks ( §2), • On seven benchmark text classification tasks, we achieve new state-of-the-art results despite having a much simpler model, minimal model tuning, and fewer parameters ( §4), • We extend our proposed mixed objective function to relation extraction task, where we achieve better F1 score on SemEval-2010 and TACRED datasets, again with a simple model and minimal model tuning ( §6).  </p><formula xml:id="formula_0">L S T M F L S T M F L S T M F L S T M B L S T M B L S T M B Max</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we will describe the model architecture, our training strategy, and our proposed mixed objective function. For mathematical notation, we will use bold lowercase to denote vectors, bold uppercase to denote matrices, and lowercase to denote scalars and individual words in a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>The classification model consists of an embedding layer, a bidirectional long short-term memory (BiLSTM) encoder <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b33">Schuster and Paliwal 1997)</ref>, a max-pooling layer, a linear (fullyconnected) layer, and finally a softmax layer (see <ref type="figure" target="#fig_0">Figure 1</ref>). First, the sequence of words (w 1 , . . . , w T ) contained in a document are passed through an embedding layer that contains a lookup table which maps them to dense vectors</p><formula xml:id="formula_1">(v 1 , . . . , v T , v t ∈ R d ).</formula><p>Next, the forward and backward LSTM of the BiLSTM encoder processes these word vectors in the forward (left to right) and backward (right to left) directions respectively, updating corresponding hidden states</p><formula xml:id="formula_2">at each time-step − → h t = − −−−− → LSTM F ( −−→ h t−1 , v t ), ← − h t = ← −−−− − LSTM B ( ←−− h t−1 , v t ).</formula><p>Next, these hidden state outputs from the forward LSTM ( − → h t ) and backward LSTM ( ← − h t ) are concatenated at every time-step to enable encoding of information from past and future contexts respectively</p><formula xml:id="formula_3">h t = [ − → h t || ← − h t ], t ∈ [1, T ], h t ∈ R n .</formula><p>These concatenated hidden states are next fed to the pooling layer that computes the maximum value over time to obtain the feature representation of the input sequence (h ∈ R n )</p><formula xml:id="formula_4">h = max t h t , t ∈ [1, T ], ∀ ∈ {1, . . . , n}.</formula><p>This max-pooling mechanism constrains the model to capture the most useful features produced by the BiLSTM encoder. Next, the linear layer applies an affine transformation to the feature vector to produce logits (d)</p><formula xml:id="formula_5">d = Wh + b, d ∈ R K ,</formula><p>where K is the number of classes, W is the weight matrix, and b is the bias. Next, these logits are normalized using the softmax function to give our estimated class probabilities as</p><formula xml:id="formula_6">p(y = k|x; θ) = exp(d k ) K j=1 exp(d j ) , ∀k ∈ {1, . . . , K},</formula><p>where (x, y) is a training example and θ denotes the model parameters. For model training, we use supervised and unsupervised loss functions, which are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Training</head><p>Let there be m labeled examples in the training set that are denoted as {(x (1) , y (1) ), . . . , (x (m ) , y (m ) )}, where x (i) represents a document's word sequence, y (i) represents class label such that y (i) ∈ {1, 2, . . . , K}. For supervised training of the classification model, we make use of two methodologies: maximum likelihood estimation and adversarial training, which are described next.</p><p>Maximum Likelihood (ML) This is the most widely used method to learn the parameters of neural network models from observed data for text classification task. Here, we minimize the average cross-entropy loss between the estimated class probability and the ground truth class label for all training examples</p><formula xml:id="formula_7">L ML (θ) = −1 m m i=1 K k=1 1(y (i) = k) log p(y (i) = k|x (i) ; θ),</formula><p>where 1(; ) is an indicator function.</p><p>Adversarial Training (AT) Adversarial examples are created from inputs by small perturbations to mislead the machine learning algorithm. The objective of adversarial training is to construct and give as input adversarial examples during model training procedure to make the model more robust to adversarial noise and thereby improving its generalization ability (Goodfellow, Shlens, and Szegedy 2014).</p><p>In this work, we make adversarial perturbations to the input word embeddings ( <ref type="bibr" target="#b23">Miyato, Dai, and Goodfellow 2016)</ref>. These perturbations (r at ) are estimated by linearizing the supervised cross-entropy loss around the input word embeddings. Specifically, to get the adversarial embedding (v * ) corresponding to v, we use the L 2 norm of the training loss gradient (g) that is computed by backpropagation using the current model parameters (θ)</p><formula xml:id="formula_8">v = [v 1 , . . . , v T ]) (MDG16,</formula><formula xml:id="formula_9">r at = g/ g 2 , where g = −∇ v log p(y = k|v;θ) v * = v + r adv</formula><p>where, k is the correct class label, is a hyperparameter that controls the magnitude of the perturbation. We apply adversarial loss to only the labeled data. It is defined as</p><formula xml:id="formula_10">L AT (θ) = −1 m m i=1 K k=1</formula><p>1(y (i) = k) log p(y (i) = k|v * (i) ;θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Training</head><p>In this paper, in addition to supervised training, we also experiment with two unsupervised methodologies: entropy minimization and virtual adversarial training. These loss functions when incorporated into the objective function act as effective regularizers during model training. To describe them, we assume that there exists an additional m u unlabeled examples in the dataset {x (1) , . . . , x (mu) }.</p><p>Entropy Minimization (EM) In addition to supervised cross-entropy loss, we also minimize the conditional entropy of the estimated class probabilities <ref type="bibr" target="#b5">(Grandvalet and Bengio 2004;</ref><ref type="bibr" target="#b22">Miyato et al. 2018)</ref>. This can also be interpreted as a special case of the missing label problem where the probability p(y (i) = k|x (i) ; θ) signifies a soft assignment of the i th example to label k (i.e. soft clustering). Entropy minimization loss is applied in an unsupervised manner to both the labeled and unlabeled data.</p><formula xml:id="formula_11">L EM (θ) = −1 m m i=1 K k=1 p(y (i) = k|x (i) ) log p(y (i) = k|x (i) ),</formula><p>where m = m + m u and dependence on θ is suppressed.</p><p>Virtual Adversarial Training (VAT) As opposed to minimizing the cross-entropy loss of the adversarial examples in AT, in VAT, we minimize the KL divergence between p(v) and p(v * ), where v * = v+r vat . The motivation of using KL divergence as an additional loss term in the objective function is that it tends to make the loss surface smooth at the current example <ref type="bibr" target="#b22">(Miyato et al. 2018)</ref>. Also, computing the VAT loss doesn't require class labels, so it can be applied to unlabeled data as well. In this work, we follow the approach proposed by MDG16 that makes use of the second-order Taylor expansion of distance followed by power iteration method to approximate the virtual adversarial perturbation. First, an i.i.d. random unit vector is sampled for every example from the Normal distribution (d (i) ∼ N (0, I) ∈ R d ) and then adversarial perturbation computed as ξd (i) is added to the word embeddings, where ξ is a hyperparameter</p><formula xml:id="formula_12">v (i) = v (i) + ξd (i) .</formula><p>Next, the gradient is estimated from the KL divergence as:</p><formula xml:id="formula_13">g = ∇ v D KL (p(. | v (i) ;θ) p(. | v (i) ;θ)).</formula><p>Virtual adversarial perturbation (r vadv ) is generated using the L 2 norm of the gradient and added to the word embeddings</p><formula xml:id="formula_14">r (i) vat = g/ g 2 v * (i) = v (i) + r (i) vat .</formula><p>Lastly, virtual adversarial loss can be computed from both the labeled and unlabeled data as:</p><formula xml:id="formula_15">L VAT (θ) = 1 m m i=1 D KL (p(. | v (i) ; θ) p(. | v * (i) ; θ)), where m = m + m u .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Objective Function</head><p>Our proposed mixed objective function combines the above described supervised and unsupervised loss functions using λ ML , λ AT , λ EM , and λ VAT as hyperparameters</p><formula xml:id="formula_16">L MIXED = λ ML L ML + λ AT L AT + λ EM L EM + λ VAT L VAT .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy</head><p>We note that there are three main considerations to be taken into account when training the text classifier. First, the knowledge of the entire sequence is essential for good classification performance. Thus, commonly used practice of truncated backpropagation through time (Werbos 1988) is a key limiting factor. One should perform gradient update for the entire text sequence. To prevent out of memory issues that can result from longer sequences, we propose to use dynamic batch size that consists of fixed number of total words per mini-batch. Second, not only we need to use pretrained word embeddings, but we need to finetune them for the specific task. Lastly, we should use a larger vocabulary size and not limit to only high-frequency words. This is because rare or tail words are often strong indicators of the class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset BSize Vocab</head><p>ACL-IMDB 3,000 80,000 5 Elec 2,000 40,000 2 AG-News 2,000 75,000 1 DBpedia 7,500 50,000 1 RCV1 2,000 100,000 2 IMDB 15,000 150,000 5 Arxiv 8,000 100,000 1   3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description</head><p>In this work, we experiment with seven datasets that are summarized in <ref type="table">Table 1</ref>. ACL-IMDB <ref type="bibr" target="#b18">(Maas et al. 2011)</ref> and Elec (JZ15a) datasets are widely used for binary sentiment classification of movie reviews and Amazon product reviews respectively while AG-News, DBpedia <ref type="bibr" target="#b38">(Zhang, Zhao, and LeCun 2015)</ref>, and RCV1 <ref type="bibr" target="#b17">(Lewis et al. 2004</ref>) are for topic classification of news articles, Wikipedia, and Reuters corpus respectively. For the RCV1 dataset, we perform multiclass topic classification based on the second-level topics and construct its training, dev, and test splits in accordance with JZ15a. To show that our proposed method also scales to larger datasets and categories, we also experiment with large-scale datasets of IMDB reviews and Arxiv abstracts that are used for fine-grained sentiment-and topic classification respectively <ref type="bibr" target="#b30">(Sachan, Zaheer, and Salakhutdinov 2018)</ref>. We preprocess all the datasets by converting the text to lowercase and treat all punctuations as separate tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>All of our models were implemented in PyTorch framework <ref type="bibr" target="#b26">(Paszke et al. 2017</ref>) and were trained on a single GPU. Our experimental setup is common for all the datasets unless specified otherwise. We use 300D pretrained vectors to initialize the embedding layer. We learn embeddings for the ACL-IMDB, RCV1, IMDB, and Arxiv datasets using word2vec <ref type="bibr" target="#b20">(Mikolov et al. 2013</ref>) and use fastText pretrained embeddings 2 <ref type="bibr" target="#b21">(Mikolov et al. 2018</ref>) for all the other datasets. We use one-layer BiLSTM of size 512D. For regularization, we apply dropout (p drop = 0.5) to word embeddings and to LSTM's hidden states. We also use the word dropout strategy in which we randomly set a word to be "UNK" with a probability p w = 0.1. For training, we employ SGD using Adam optimizer (Kingma and Ba 2014) (learning rate = 10 −3 , β 1 = 0, β 2 = 0.98, adam = 10 −8 ) with an exponential learning rate decay scheme. We perform gradient clipping by having a maximum L 2 norm of 1. For training, we backpropagate through time over entire sequence, i.e. we did not truncate sequence. This differs from DL15 where they perform truncated backpropagation through time for 400 time-steps from the end of a sequence. For semi-supervised training, we experiment with all the objective functions described in §2. For L MIXED , we include all the constituent terms with λ ML , λ AT , λ EM , λ VAT set to 1 and ξ = 0.1. We want to emphasize that in contrast with MDG16, we do not perform embedding layer normalization during AT or VAT objectives, as by including it, we noticed a drop in accuracy during our initial experiments. We select the hyperparameters such as dynamic batch size, vocabulary size, and adversarial perturbation ( ) by cross-validation on the development set. We mention these dataset-specific hyperparameters in <ref type="table" target="#tab_2">Table 2</ref>. For supervised experiments (L ML ), we perform training till 20 epochs and for semi-supervised experiments (L MIXED ), training is done till 50 epochs. For ACL-IMDB, Elec, RCV1, IMDB, and Arxiv datasets, we use training and test set as unlabeled data, while for AG-News and DBpedia datasets as their test sets are small, we use only the training set as unlabeled data. During training, we keep the batch size of the unlabeled data the same as that of the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we report the classification accuracy on the test set and perform ablation studies for both supervised and semi-supervised training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Likelihood Training</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we present the error rates of our method and the previous best-published models when training is done using only the maximum likelihood objective (L ML ). We observe that our model that consists of one-layer BiLSTM and pretrained embedding weights achieves a very competitive performance on all the datasets compared with the more complex approaches such as one-hot LSTM (JZ16) or pyramidal CNN (JZ17). Specifically, for ACL-IMDB, AG-News, IMDB, and Arxiv datasets, we report much better results than earlier methods. Thus, our proposed model and training strategy enjoy the following advantages: (a) it is very easy to implement using current deep learning frameworks; (b) it requires much less training time and GPU memory compared with other complicated models; (c) it entirely avoids complex initialization strategies such as pretraining the LSTM weights using a language model; (d) Our results can serve as strong baselines when developing more advanced taskspecific models.</p><p>To know the importance of various components in the model and training regimen, we perform ablation studies using the ACL-IMDB dataset (see <ref type="table" target="#tab_6">Table 5</ref>). We verify that good performance of our model mostly results from finetuning the pretrained embeddings, using a larger vocabulary size, and using a carefully preprocessed dataset. We also see that excluding word dropout, smaller-sized LSTM, and lowering the batch size causes a slight drop in performance while using static pretrained or randomly initialized embeddings or smaller vocabulary size can cause a large drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Training</head><p>For our next set of experiments, we perform training using L MIXED objective whose results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We observe that the mixed objective improves over maximum likelihood objective and achieves state-of-the-art results on all the seven datasets. Specifically, on the widely used ACL-IMDB dataset, there is a substantial reduction of 26.9% in relative error compared with the previous best-published model of JZ16, which was substantially more complex as they use one-hot encodings of words along with a lot of L U λML λAT λEM λVAT ACL-IMDB  <ref type="table">Table 6</ref>: Error rates (%) for ablation study on the importance of hyperparameters when the BiLSTM model is trained using L MIXED objective; L = was labeled data used? U = was unlabeled data used?</p><p>additional features such as multi-view region embeddings from CNNs and LSTMs. We also want to highlight that, although the model of MDG16 also experiments with adversarial and virtual adversarial training, our approach performs much better compared with them due to our improved training strategy and the use of L EM objective. Similarly, for the benchmark AG-News dataset, we observe relative error reduction of 26.6% compared with previous state-of-the-art model of JZ17 who use a very deep pyramidal-CNN along with region embeddings. Even on the Elec, DBPedia, and RCV1 datasets, our results present significant improvements over the previous best semi-supervised results. L MIXED objective also scales well to the dataset sizes, as on the large datasets of IMDB and Arxiv, it outperforms the above mentioned previous approaches by a substantial margin. We note here that the approach of Howard and Ruder <ref type="formula">(2018)</ref> is not directly comparable with our results as they use a three-layer LSTM model. We discuss the effect of model size in §5. Next, we perform ablation studies when the model is trained using L MIXED on the ACL-IMDB dataset and analyze the contributions of the different component terms present in the objective (see <ref type="table">Table 6</ref>). First, we observe that when the model is trained using L MIXED objective on both the labeled and unlabeled data, the accuracy on ACL-IMDB drastically improves by 33% compared with using only the L ML objective. Second, we also observe that when trained only on labeled data the inclusion of L AT and L VAT can also significantly improve the performance. However, L EM alone doesn't lead to any significant gains. Furthermore, when L MIXED is trained with only labeled data, we see 12% relative increase in accuracy. Finally, when we add unlabeled data to both L VAT and L EM , we see consistent improvements, thus suggesting that these objective functions complement each other and together improve the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis Effect on Word Embeddings</head><p>To understand the effect on word embeddings due to training using L ML and L MIXED objectives, we show the top-10 closest words for the query word "good" based on their cosine similarity in <ref type="table">Table 7</ref>, where the word embeddings were  <ref type="table">Table 7</ref>: Top-10 nearest neighbors according to cosine similarity (shown in parentheses) for the word "good" computed in the embedding space. "word2vec" refers to the static embeddings i.e. not finetuned during training.</p><p>extracted from the models trained on ACL-IMDB dataset. We see that for static embeddings, the closest words have a mix of both positive ('great', 'decent', 'nice') and negative sentiments ('bad', 'but'). This can be understood as they are syntactically similar adjectives. When these embeddings are finetuned using the L ML objective, the network learns more meaningful representations and accommodates more positive sentiment words close to the query word 'good'. Moreover, when trained using the L MIXED objective, we see that those words that have a very high correlation with the class label (positive sentiment class in this case) are clustered together in the embedding space. Our hypothesis is that this factor also contributes to an increase in the overall classification accuracy.  <ref type="figure" target="#fig_1">Figure 2a</ref> and <ref type="figure" target="#fig_1">Figure 2b</ref> show the moving average training loss and test error respectively versus the number of epochs on the ACL-IMDB dataset with the L ML , L AT , L VAT , L EM , and L MIXED objectives. We can see that L ML begins to overfit after 5 epochs, L AT overfits after 10 epochs while L MIXED , L EM , and L VAT don't overfit much and thus achieve better generalization than the L ML and L AT objectives (see in <ref type="figure" target="#fig_1">Figure 2b)</ref>. Moreover, as L MIXED and L VAT objectives can use unlabeled data, their training loss decays gradually. Thus, L MIXED objective while being very effective in performance is also a very robust model regularizer. On the other hand, from <ref type="figure" target="#fig_1">Figure 2b</ref>, we can see that L VAT , L EM , and L MIXED take a long time to converge compared with L ML and L AT and are thus quite slow to train. In our experiments, one epoch of L MIXED takes around 20m on GeForce GTX 1080 GPU and it requires roughly 45 epochs to converge. This is considerably slower than L ML objective where each epoch takes around 3m and the overall convergence time is thus 15m for 5 epochs. In this setup, we first analyze the test error on ACL-IMDB dataset by feeding the model trained with different objective functions ( §2) with an increasing number of training examples (learning curve; see <ref type="figure" target="#fig_2">Figure 3a</ref>). We observe that all the objective functions converge to lower error rates when training data is increased. We also see that mixed objective model is always optimal (achieves lower test error rate) for any setting of the number of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Regularization Effect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying Data Size</head><p>Next, we analyze the test error on ACL-IMDB dataset by varying the amount of unlabeled data. For this experiment, we use additional 50,000 reviews provided with the ACL-IMDB dataset and its 25,000 reviews from test set as unlabeled data. We evaluate the performance of each objective function by linearly increasing the amounts of unlabeled data (see <ref type="figure" target="#fig_2">Figure 3b</ref>). Initially, increasing the amount of unlabeled data tends to improve the performance of L VAT , L EM , and L MIXED . However, we observe that their performance saturates once 25,000 unlabeled examples are available. Furthermore, as the amount of unlabeled data increases, the performance tends to degrade sharply. As ACL-IMDB training set also consists of 25,000 examples, from this observation, it can be assumed that to obtain the best performance using L MIXED , the size of unlabeled and labeled dataset should be roughly the same. We also note that as L ML and L AT are supervised approaches, their performance remains unaffected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying Model Size</head><p>We note that prior work in the supervised text classification task has used smaller-sized LSTMs i.e. models with hidden state sizes at most 512 units because larger models didn't give accuracy gains (DL15, JZ16). This is also consistent with our observation, as we find that that supervised approaches (L ML ) do not benefit much from increasing the model size. However, when using additional loss functions such as the mixed objective, accuracy scales much better with model size (see <ref type="figure" target="#fig_3">Figure 4a</ref>). Further, we also observe accuracy gains for all methods upon increasing the number Virtual Adversarial Training (LVAT)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative</head><p>The plot seemed to be interesting, but this film is a great dissapointment. Bad actors, a camera moving like in the hands of an amateur. If there was C-movies, this would be a perfect example. A plus for a nice DVD cover though and a great looking female actor. <ref type="table">Table 8</ref>: Examples from ACL-IMDB dataset for sentiment classification task that are correctly classified by the method indicated directly above it and incorrectly classified by all the other methods. of layers in the model (see <ref type="figure" target="#fig_3">Figure 4b</ref>). Specifically, the error rate of L MIXED objective improves to 4.15% when using a three-layer deep model. This suggests that larger-sized semisupervised methods can lead to the development of more accurate models for text classification task. However, a fourlayer model hurts the L MIXED objective's performance due to the training instability of L EM method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect on Prediction Probabilities</head><p>To study the behavior of different methods, we plot a histogram of the prediction probabilities for both the correct <ref type="figure" target="#fig_4">(Figure 5a</ref>) and incorrect <ref type="figure" target="#fig_4">(Figure 5b)</ref> predictions. We observe that for correct predictions all the methods especially L EM and L MIXED have very sharp and confident distribution of class probabilities. However, for incorrect predictions only L EM has sharp peaks while L VAT , L AT , and L ML encourage the model to learn a smoother distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Approach vs Mixed Objective</head><p>To understand if the above objectives have complementary strengths, we combine their predicted probabilities with a linear interpolation strategy. Given the output probability for a class k as p(y = k|x), the interpolated probability p I (y = k|x) is calculated as: where α i ∈ [0, 1] and is chosen based on grid search. This simple interpolation technique results in an improved error rate of 5.2%. However, the error rate of our proposed mixed objective function is substantially lower (4.3%) thus highlighting the importance of performing joint training of the model based on different objective functions.</p><formula xml:id="formula_17">p I = α ML p ML + α AT p AT + α VAT p VAT + α EM p EM | α i = 1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example Predictions</head><p>In <ref type="table">Table 8</ref>, we show some example movie reviews from the test set of ACL-IMDB dataset that are correctly classified by the highlighted method and incorrectly by all the remaining methods. We observe that methods such as L MIXED , L EM , and L VAT that are based on unsupervised training are able to correctly classify difficult instances in which the overall sentiment is determined by the entire sentence structure.</p><p>This illustrates the ability of these methods to learn complex long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relation Extraction</head><p>To evaluate if the mixed objective function can generalize to other tasks we also perform experiments on relation extraction (RE) task. In this, the objective is to identify if a predefined semantic "relation category" exists (or doesn't exist) between a pair of subject and object entities present in text. This task also presents specific challenges: the linguistics coverage problem due to the lack of all possible training examples of a relation class and longer text span between the entities in a sentence.</p><p>For the RE task, we use a position-aware attention model ) that consists of a word embedding, position embedding, LSTM, attention layer, linear layer, and softmax layer. We augment the word embeddings by concatenating them with POS tag and NER category embeddings which is then fed to the LSTM to get hidden states for each word. The position embeddings for a word is derived based on the relative distance of the current word from the subject and object entities. Next, the attention layer computes the final sentence representation by focusing on both the hidden states and position embeddings. Finally, sentence representation is fed to the linear layer followed by a softmax layer for relation classification.</p><p>We perform experiments using our proposed mixed objective function on two RE datasets: TACRED and SemEval-2010 Task 8 whose statistics are shown in <ref type="table" target="#tab_11">Table 9</ref>. The POS and NER tags are computed using the Stanford CoreNLP toolkit. 3 Following standard convention, we report the micro-averaged F 1 score on TACRED and official macro-averaged F 1 score on SemEval datasets. We performed only a small number of experiments to search for the hyper-parameter values of dropout, embedding size, hidden layer size, and learning rate on the development set, all other parameters remained the same as the positional attention model of <ref type="bibr" target="#b37">Zhang et al. (2017)</ref>. <ref type="bibr">4</ref> Our results in <ref type="table" target="#tab_12">Table 10</ref> show that when trained with mixed objective function, our model performs quite well, producing better results than all previously reported models despite the lack of complex taskspecific hyper-parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Neural Network Methods. Neural network models for NLP have yielded impressive results on several benchmark tasks <ref type="bibr" target="#b1">(Collobert et al. 2011;</ref><ref type="bibr" target="#b27">Peters et al. 2018)</ref>. To learn document features for text classification task, several methods have been proposed- <ref type="bibr" target="#b14">Kim (2014)</ref>      <ref type="bibr" target="#b32">Santos, Xiang, and Zhou (2015)</ref>; b <ref type="bibr" target="#b35">Xu et al. (2015)</ref>; c <ref type="bibr" target="#b37">Zhang et al. (2017)</ref>. For fair comparison, we only report results from models that don't use ensembling or ranking-based approaches.</p><p>first learn sentence representations followed by combining them to learn document features. To do this, <ref type="bibr" target="#b34">Tang, Qin, and Liu (2015)</ref> first apply a CNN or LSTM followed by a gated RNN while <ref type="bibr" target="#b36">Yang et al. (2016)</ref> learn the sentence and document features in a hierarchical manner using a self-attention mechanism.</p><p>Semi-Supervised Learning. SSL approaches can be broadly categorized into three types: multi-view, data augmentation, and transfer learning. First, under multi-view learning, the objective is to use multiple views of both the labeled and unlabeled data to train the model. These multiple views can be obtained either from raw text <ref type="bibr" target="#b0">(Blum and Mitchell 1998)</ref> or from the features (JZ15b). Second, under data augmentation, as the name implies, involves pseudoaugmenting either the features or the labels. For text classification, <ref type="bibr" target="#b24">Nigam et al. (2000)</ref> performed semi-supervised training using naïve Bayes and expectation-maximization algorithms and demonstrated substantial improvements in performance. MDG16 compute embedding perturbations using adversarial and virtual adversarial approaches to improve model training. Third, under transfer learning, the approach of initializing the task-specific model weights by pretrained weights from an auxiliary task is a widely used strategy that has shown to improve the performance in tasks such as text classification <ref type="bibr" target="#b2">(Dai and Le 2015;</ref><ref type="bibr" target="#b7">Howard and Ruder 2018)</ref>, question-answering <ref type="bibr" target="#b4">(Devlin et al. 2018)</ref>, and machine translation <ref type="bibr" target="#b29">(Ramachandran, Liu, and Le 2017;</ref><ref type="bibr" target="#b28">Qi et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We show that a simple BiLSTM model using maximum likelihood training can result in a competitive performance on text classification tasks without the need for an additional pretraining step. Also, in addition to maximum likelihood, using a combination of entropy minimization, adversarial, and virtual adversarial training, we report state-of-the-art results on several text classification datasets. This mixed objective function also generalizes well to other tasks such as relation extraction where it outperforms current best models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Text classification model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Training loss vs. epochs on ACL-IMDB; (b) Test error vs. epochs on ACL-IMDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Test Error on ACL-IMDB vs. (a) number of training examples (b) increasing number of unlabeled examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Test error on ACL-IMDB vs. (a) hidden layer size of LSTMs in the BiLSTM with one-hidden layer; (b) number of BiLSTM layers where each LSTM has size of 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>)Figure 5 :</head><label>5</label><figDesc>Histogram of prediction probabilities on ACL-IMDB test set for the case of (a) correct predictions; (b) incorrect predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>uses 1D CNNs, Lai et al. (2015) uses a simple bidirectional recurrent CNN with max-pooling, Zhou et al. (2016) applies 2D max-pooling on top of BiLSTMs, Zhou et al. (2015) investigates a joint CNN-LSTM model, and JZ15a, JZ16, JZ17 apply CNNs, LSTMs, and pyramidal CNNs respectively to one-hot encoding of word sequences. An alternative approach is to 3 https://stanfordnlp.github.io/CoreNLP/ 4 https://github.com/yuhaozhang/tacred-relation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="7">ACL-IMDB Elec AG-News DBpedia RCV1 IMDB Arxiv</cell></row><row><cell>Linear Model (TFIDF + SVM)</cell><cell cols="2">9.51 9.16</cell><cell>7.64</cell><cell>1.31</cell><cell>10.68</cell><cell>40.00</cell><cell>34.81</cell></row><row><cell>Vanilla LSTM [Dai and Le (2015)]</cell><cell>10.00</cell><cell>-</cell><cell>-</cell><cell>13.64</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DocVec [Le and Mikolov (2014)]</cell><cell cols="2">11.40 8.95</cell><cell>8.00</cell><cell>2.34</cell><cell>-</cell><cell>44.10</cell><cell>37.40</cell></row><row><cell>FastText [Joulin et al. (2017)]</cell><cell>11.34</cell><cell>-</cell><cell>7.50</cell><cell>1.40</cell><cell>-</cell><cell>42.13</cell><cell>33.23</cell></row><row><cell>CNN [Kim (2014)]</cell><cell cols="2">9.17 8.03</cell><cell>5.92</cell><cell>0.98</cell><cell>10.44</cell><cell>49.53</cell><cell>34.21</cell></row><row><cell>oh-CNN [Johnson and Zhang (2015b; 2017)]</cell><cell cols="2">7.67 7.14</cell><cell>6.88</cell><cell>0.88</cell><cell>9.17</cell><cell>38.15</cell><cell>35.89</cell></row><row><cell>char-CNN [Zhang, Zhao, and LeCun (2015)]</cell><cell>-</cell><cell>-</cell><cell>9.51</cell><cell>1.55</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LML [Our Method]</cell><cell cols="2">6.43 7.40</cell><cell>5.62</cell><cell>0.91</cell><cell>7.78</cell><cell>35.64</cell><cell>31.76</cell></row></table><note>Dataset-specific hyperparameters; BSize = number of tokens in a minibatch; Vocab = number of words present in vocabulary; = adversarial perturbation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Error rates (%) when the model is trained using L ML and comparison with previous best supervised methods.</figDesc><table><row><cell>Model</cell><cell cols="7">ACL-IMDB Elec AG-News DBpedia RCV1 IMDB Arxiv</cell></row><row><cell>SA-LSTM [Dai and Le (2015)]</cell><cell>7.24</cell><cell>-</cell><cell>-</cell><cell>1.19</cell><cell>7.40</cell><cell>-</cell><cell>-</cell></row><row><cell>LSTM [Miyato, Dai, and Goodfellow (2016)]</cell><cell cols="2">5.91 5.40</cell><cell>6.78</cell><cell>0.76</cell><cell>6.68</cell><cell>35.85</cell><cell>30.97</cell></row><row><cell>oh-LSTM [Johnson and Zhang (2016; 2017)]</cell><cell cols="2">5.94 5.55</cell><cell>6.57</cell><cell>0.84</cell><cell>7.15</cell><cell>37.56</cell><cell>31.17</cell></row><row><cell>ULMFit [Howard and Ruder (2018)]</cell><cell>4.60</cell><cell>-</cell><cell>5.01</cell><cell>0.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LMIXED [Our Method]</cell><cell cols="2">4.32 5.24</cell><cell>4.95</cell><cell>0.70</cell><cell>6.23</cell><cell>34.04</cell><cell>29.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Error rates (%) when the model is trained using L MIXED and comparison with previous best semi-supervised methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Error rates (%) of variations on the BiLSTM model trained using L ML on the ACL-IMDB dataset. Unlisted values are identical to those of the first row; N = number of BiLSTM layers; H = LSTM hidden size.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>NegativeThis movie is so over-the-top as to be a borderline comedy. Laws of physics are broken. Things explode for no good reason. Great movie to sit down with a six-pack and enjoy. Do not -I repeat DO NOT see this movie sober. You will die horrible death!Entropy Minimization Training (LEM)Positive Coming from Oz I probably shouldn't say it but I find a lot of the local movies lacking that cohesive flow with a weak storyline. This comedy lacks in nothing. Great story, no overacting, no melodrama, just brilliant comedy as we know Oz can do it. Do yourself a favour and laugh till you drop.</figDesc><table><row><cell>Sentiment Text</cell></row><row><cell>Mixed Objective Training (LMIXED)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Summary statistics for RE datasets; % Neg = percentage of examples with class label of "no relation" between entities; K = number of classes including the no relation class; = average length of a sentence in the dataset.</figDesc><table><row><cell></cell><cell></cell><cell>TACRED</cell><cell></cell><cell cols="3">SemEval-2010</cell></row><row><cell>Model</cell><cell>P</cell><cell>R</cell><cell>F 1</cell><cell>P</cell><cell>R</cell><cell>F 1</cell></row><row><cell>CNN-PE a</cell><cell cols="6">70.3 54.2 61.2 82.1 83.1 82.5</cell></row><row><cell cols="4">SDP-LSTM b 66.3 52.7 58.7</cell><cell>-</cell><cell>-</cell><cell>83.7</cell></row><row><cell>PA-LSTM c</cell><cell cols="3">65.7 64.5 65.1</cell><cell>-</cell><cell>-</cell><cell>82.7</cell></row><row><cell>Our Method</cell><cell cols="6">66.4 67.3 66.8 83.5 84.8 84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Model performance on TACRED and SemEval datasets; P = Precision; R = Recall; a</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">These embeddings were trained on data containing 600B tokens from Common Crawl (crawl-300d-2M.vec).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>In COLT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>JAIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
	<note>Explaining and harnessing adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal language model finetuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised and semi-supervised text categorization using lstm for region embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Rcv1: A new benchmark collection for text categorization research. JMLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Learning for Text Categorization</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Found. Trends Inf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Retr</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS-W</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When and why are pre-trained word embeddings useful for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Investigating the working of text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A bayesian approach to filtering junk E-mail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Learning for Text Categorization</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<editor>EMNLP. Werbos, P. J</editor>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
	<note>Generalization of backpropagation with application to a recurrent gas market model. Neural networks</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A C-LSTM neural network for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text classification improved by integrating bidirectional lstm with two-dimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
