<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adapting Object Detectors with Conditional Domain Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adapting Object Detectors with Conditional Domain Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world object detectors are often challenged by the domain gaps between different datasets. In this work, we present the Conditional Domain Normalization (CDN) to bridge the domain gap. CDN is designed to encode different domain inputs into a shared latent space, where the features from different domains carry the same domain attribute. To achieve this, we first disentangle the domain-specific attribute out of the semantic features from one domain via a domain embedding module, which learns a domain-vector to characterize the corresponding domain attribute information. Then this domain-vector is used to encode the features from another domain through a conditional normalization, resulting in different domains' features carrying the same domain attribute. We incorporate CDN into various convolution stages of an object detector to adaptively address the domain shifts of different level's representation. In contrast to existing adaptation works that conduct domain confusion learning on semantic features to remove domain-specific factors, CDN aligns different domain distributions by modulating the semantic features of one domain conditioned on the learned domain-vector of another domain. Extensive experiments show that CDN outperforms existing methods remarkably on both real-to-real and synthetic-to-real adaptation benchmarks, including 2D image detection and 3D point cloud detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>data to real data, as it can save the amount of cost and time. However, current objector detectors trained with synthetic data can rarely generalize on real data due to a significant domain distribution gap <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Adversarial domain adaptation emerges as a hopeful method to learn transferable representations across domains. It has achieved noticeable progress in various machine learning tasks, from image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>, semantic segmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48]</ref>, object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref> to reinforcement learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>. According to Ben-David's theory <ref type="bibr" target="#b0">[1]</ref>, the empirical risk on the target domain is bounded by the source domain risk and the H domain divergence. Adversarial adaptation dedicates to learn domain invariant representation to reduce the H divergence, which eventually decreases the upper bound of the empirical error on the target domain.</p><p>However, existing adversarial adaptation methods still suffer from several problems. First, previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref> directly feed semantic features into a domain discriminator to conduct domain confusion learning. But the semantic features contain both image contents and domain attribute information. It's difficult to make the discriminator only focusing on removing domain-specific information without inducing undesirable influence on the images contents. Second, existing adversarial adaptation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b38">39]</ref> use domain confusion learning at one or few convolution stages to handle the distribution mismatch, which ignores the differences of domain shifts at various representation levels. For example, the first few convolution layers' features mainly convey low-level information of local patterns, while the higher convolution layers' features include more abstract global patterns with semantics <ref type="bibr" target="#b43">[44]</ref>. Such differences born within deep convolution neural networks naturally exhibit different types of domain shift at various convolution stages.</p><p>Motivated by this, we propose the Conditional Domain Normalization (CDN) to embed different domain inputs into a shared latent space, where the features of all different domains inputs carry the same domain attribute information. Specifically, CDN utilizes a domain embedding module to learn a domain-vector to characterize the domain attribute information, through disentangling the domain attribute out of the semantic features of domain inputs. We use this domain-vector to encode the semantic features of another domain's inputs via a conditional normalization. Thus different domain features carry the same domain attributes information. We adopt CDN in various convolution stages to address different types of domain shift adaptively. The experiment on both real-to-real and synthetic-to-real adaptation benchmarks demonstrate that our method outperforms the-state-of-the-art adaptation methods. To summarize, our contributions are three folds: <ref type="bibr" target="#b0">(1)</ref> We propose the Conditional Domain Normalization to bridge the domain distribution gap, through embedding different domain inputs into a shared latent space, where the features from different domains carry the same domain attribute. (2) CDN achieves state-of-the-art unsupervised domain adaptation performance on both real-to-real and synthetic-to-real benchmarks, including 2D image and 3D point cloud detection tasks. And we conduct both quantitative and qualitative comparisons to analyze the features learned by CDN. <ref type="formula" target="#formula_3">(3)</ref> We construct a large-scale synthetic-to-real driving benchmark for 2D object detection, including a variety of public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Object Detection is the center topic in computer vision, which is crucial for many real-world applications, such as autonomous driving. In 2D detection, following the pioneering work of RCNN <ref type="bibr" target="#b10">[11]</ref>, a number of object detection frameworks based on convolutional networks have been developed like Fast R-CNN <ref type="bibr" target="#b9">[10]</ref>, Faster R-CNN <ref type="bibr" target="#b32">[33]</ref>, and Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>, which significantly push forward the state of the art. In 3D detection, spanning from detecting 3d objects from 2d images <ref type="bibr" target="#b2">[3]</ref>, to directly generate 3D box from point cloud <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, abundant works has been successfully explored. All these 2D and 3D objectors have achieved remarkable success on one or few specific public datasets. However, even top-grade object detectors still face significant challenges when deployed in realworld settings. The difficulties usually arise from the changes in environmental conditions. Domain Adaptation generalizes a model across different domains, and it has been extensively explored in various tasks, spanning from image classification <ref type="bibr">[2, 41, 24, 27, ?]</ref>, semantic segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref> to reinforcement learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref>. For 2D detection, domain confusion learning via a domain discriminator has achieved noticeable progress in cross-domain detection. <ref type="bibr" target="#b3">[4]</ref> incorporated a gradient reversal layer <ref type="bibr" target="#b7">[8]</ref> into a Faster R-CNN model. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref> adopt domain confusion learning on both global and local levels to align source and target distributions. In contrast to existing methods conducting domain confusion learning directly on semantic features, we explicitly disentangle the domain attribute out of semantic features. And this domain attribute is used to encode other domains' features, thus different domain inputs share the same domain attribute in the feature space. For 3D detection, only a few works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b16">17]</ref> has been explored to adapt object detectors across different point cloud dataset. Different from existing works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b16">17]</ref> are specifically designed for point cloud data, our proposed CDN is a general adaptation framework that adapts both 2D image and 3D point cloud object detector through the conditional domain normalization. Conditional Normalization is a technique to modulate the neural activation using a transformation that depends on external data. It has been successfully used in the generative models and style transfer, like conditional batch normalization <ref type="bibr" target="#b5">[6]</ref>, adaptive instance normalization (AdaIN) <ref type="bibr" target="#b15">[16]</ref> and spatial adaptive batch normalization <ref type="bibr" target="#b24">[25]</ref>. <ref type="bibr" target="#b15">[16]</ref> proposes AdaIN to control the global style of the synthesized image. <ref type="bibr" target="#b41">[42]</ref> modulates the features conditioned on semantic masks for image super-resolution. <ref type="bibr" target="#b24">[25]</ref> adopts a spatially-varying transformation, making it suitable for image synthesis from semantic masks. Inspired by these works, we propose Conditional Domain Normalization (CDN) to modulate one domain's inputs condition on another domain's attributes information. But our method exhibits significant difference with style transfer works: Style transfer works mod-ify a content image conditioned on another style image, which is a conditional instance normalization by nature; but CDN modulates one domain's features conditioned on the domain embedding learned from another domains' inputs (a group of images), which is like a domain-to-domain translation. Hence we use different types of conditional normalization to achieve different goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We first introduce the general unsupervised domain adaptation approach in section 3.1. Then we present the proposed Conditional Domain Normalization (CDN) in section 3.2. Last we adapt object detectors with the CDN in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Adversarial Adaptation Framework</head><p>Given source images and labels {(</p><formula xml:id="formula_0">x S i , y S i )} N S i=1 drawn from P s , and target images {x T i } N T i=1 from target domain P t ,</formula><p>the goal of unsupervised domain adaptation is to find a function f : x → y that minimize the empirical error on target data. For object detection task, the f can be decomposed as f = G(·; θ g ) • H(·; θ h ), where G(·; θ g ) represents a feature extractor network and H(·; θ h ) denotes a bounding box head network. The adversarial domain adaptation introduces a discriminator network D(·; θ d ) that tries to determine the domain labels of feature maps generated by G(·; θ g ).</p><formula xml:id="formula_1">min θg,θ h L det = L cls (G(x; θ g ), H(x; θ h )) + L reg (G(x; θ g ), H(x; θ h )) min θ d max θg L adv = E x∼Ps [log(D(G(x; θ g ); θ d ))] + E x∼Pt [log(1 − D(G(x; θ g ); θ d )]<label>(1)</label></formula><p>As illustrated in Eq.1, G(·; θ g ) and H(·; θ h ) are jointly trained to minimize the detection loss L det by supervised training on the labeled source domain. At the same time, the backbone G(·; θ g ) is optimized to maximize the probability of D(·; θ d ) to make mistakes. Through this two-player min-max game, the final G(·; θ g ) will converge to extract features that are indistinguishable for D(·; θ d ), thus domain invariant representations are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Domain Normalization</head><p>Conditional Domain Normalization is designed to embed source and target domain inputs into a shared latent space, where the semantic features from different domains carry the same domain attribute information. Formally, let v s ∈ R N ×C×H×W and v t ∈ R N ×C×H×W represent feature maps of source and target inputs, respectively. C is the channel dimension and N denotes the mini-batch size. We first learn a domain embedding vector e s domain ∈ R 1×C×1 to characterize the domain attribute of source inputs. It is accomplished by a domain embedding network F d (·; W ) parameterized by two fully-connect layers with ReLU non-linearity δ as And v s avg ∈ R N ×C×1 represents the channel-wise statistics of source feature v s generated by global average pooling</p><formula xml:id="formula_2">e s domain = F d (v s avg ; W ) = δ(W 2 δ(W 1 v s avg )).<label>(2)</label></formula><formula xml:id="formula_3">v s avg = 1 HW H h=1 W w=1 v s (h, w).<label>(3)</label></formula><p>To embed both source and target domain inputs into a shared latent space, where source and target features carry the same domain attributes while preserving individual image contents. We encode the target features v t with the source domain embedding via an affine transformation aŝ</p><formula xml:id="formula_4">v t = F(e s domain ; W γ , b γ ) · v t − µ t σ t + F(e s domain ; W β , b β ),<label>(4)</label></formula><p>where µ t and σ t denote the mean and variance of target feature v t . The affine parameters are learned by function F (·; W γ , b γ ) and F (·; W β , b β ) conditioned on the source domain embedding vector e s domain ,</p><formula xml:id="formula_5">F (e s domain ; W γ , b γ ) = W γ e s domain + b γ , F (e s domain ; W β , b β ) = W β e s domain + b β .<label>(5)</label></formula><p>For the target feature mean µ t ∈ R 1×C×1 and variance σ t ∈ R 1×C×1 , we calculate it with a standard batch normalization <ref type="bibr" target="#b18">[19]</ref> </p><formula xml:id="formula_6">µ t c = 1 N HW N n=1 H h=1 W w=1 v t nchw , σ t c = 1 N HW N n=1 H h=1 W w=1 (v t nchw − µ t c ) 2 + ,<label>(6)</label></formula><p>where µ t c and σ t c denotes c-th channel of µ t and σ t . Finally, we have a discriminator to supervise the encoding process of domain attribute as</p><formula xml:id="formula_7">min θ d max θg L adv = E[log(D(F d (v s )); θ d )] + E[log(1 − D(F d (v t ); θ d ))],<label>(7)</label></formula><p>where v s and v t are generated by G(·; θ g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Faster R-CNN network incorporates with CDN. The CDN is adopted in both backbone network and bounding box head network to adaptively address the domain shift at different representation levels.</p><p>Discussion CDN exhibits a significant difference compared with existing adversarial adaptation works. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, previous methods conduct domain confusion learning directly on semantic features to remove domain-specific factors. However, the semantic features contain both domain attribute and image contents. It is not easy to enforce the domain discriminator only regularizing the domain-specific factors without inducing any undesirable influence on image contents. In contrast, we disentangle the domain attribute out of the semantic features via conditional domain normalization. And this domain attribute is used to encode other domains' features, thus different domain features carry the same domain attribute information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adapting Detector with Conditional Domain Normalization</head><p>Convolution neural network's (CNN) success in pattern recognition has been largely attributed to its great capability of learning hierarchical representations <ref type="bibr" target="#b43">[44]</ref>. More specifically, the first few layers of CNN focus on low-level features of local pattern, while higher layers capture semantic representations. Given this observation, CNN based object detectors naturally exhibit different types of domain shift at various levels' representations. Hence we incorporate CDN into different convolution stages in object detectors to address the domain mismatch adaptively, as shown in <ref type="figure">Fig.2</ref>.</p><p>Coincident to our analysis, some recent works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref> empirically demonstrate that global and local region alignments have different influences on detection performance. For easy comparison, we refer to the CDN located at the backbone network as global alignment, and CDN in the bounding box head networks as local or instance alignment.</p><p>As shown in <ref type="figure">Fig. 2</ref>, taking faster-RCNN model <ref type="bibr" target="#b32">[33]</ref> with ResNet <ref type="bibr" target="#b12">[13]</ref> backbone as an example, we incorporate CDN in the last residual block at each stage. Thus the global alignment loss can be computed by</p><formula xml:id="formula_8">L global adv = L l=1 E[log(D l (F l d (v s l ); θ l d )] + E[log(1 − D l (F l d (v t l ); θ l d ))],<label>(8)</label></formula><p>where v s l and v t l denote l-th layer's source feature and the encoded target feature, and D l represents the corresponding domain discriminator parameterized by θ l d . As for bounding box head network, we adopt CDN on the fixed-size region of interest (ROI) features generated by ROI pooling <ref type="bibr" target="#b32">[33]</ref>. Because the original ROIs are often noisy and the quantity of source and target ROIs are not equal, we randomly select min(N S roi , N T roi ) ROIs from each domain. N S roi and N T roi represent the quantity of source and target ROIs after non-maximum suppression (NMS). Hence we have instance alignment regularization for ROI features as</p><formula xml:id="formula_9">L instance adv = E[log(D roi (F roi d (v s roi ); θ roi d )] + E[log(1 − D roi (F roi d (v t roi ); θ roi d ))]. (9)</formula><p>The overall training objective is to minimize the detection loss L det (of the labeled source domain) that consists of a classification loss L cls and a regression loss L reg , and min-max a adversarial loss L adv of discriminator network</p><formula xml:id="formula_10">min θ d max θg L adv = λL global adv + L instance adv min θg,θ h L det = L cls (G(x; θ g ), H(x; θ h )) + L reg (G(x; θ g ), H(x; θ h )),<label>(10)</label></formula><p>where λ is a weight to balance the global and local alignment regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate CDN on various real-to-real (KITTI to Cityscapes) and syntheticto-real (Virtual KITTI/Synscapes/SIM10K to BDD100K, PreSIL to KITTI) adaptation benchmarks. We also report results on cross-weather adaptation, Cityscapes to Foggy Cityscapes. Mean average precision (mAP) with an intersectionover-union (IOU) threshold of 0.5 is reported for 2D detection experiments. We use Source and Target to represent the results of supervised training on source and target domain, respectively. For 3D point cloud object detection, PointR-CNN <ref type="bibr" target="#b37">[38]</ref> with backbone of PointNet++ <ref type="bibr" target="#b29">[30]</ref> is adopted as our baseline model. Following standard metric on KITTI benchmark <ref type="bibr" target="#b37">[38]</ref>, we use Average Precision(AP) with IOU threshold 0.7 for car and 0.5 for pedestrian/cyclist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Cityscapes <ref type="bibr" target="#b4">[5]</ref> is a European traffic scene dataset, which contains 2, 975 images for training and 500 images for testing. Foggy Cityscapes <ref type="bibr" target="#b34">[35]</ref> derives from Cityscapes with a fog simulation proposed by <ref type="bibr" target="#b34">[35]</ref>. It also includes 2, 975 images for training, 500 images for testing.</p><p>KITTI <ref type="bibr" target="#b8">[9]</ref> contains 21, 260 images collected from different urban scenes, which includes 2D RGB images and 3D point cloud data. Virtual KITTI <ref type="bibr" target="#b6">[7]</ref> is derived from KITTI with a real-to-virtual cloning technique proposed by <ref type="bibr" target="#b6">[7]</ref>. It has the same number of images and categories as KITTI.</p><p>Synscapes <ref type="bibr" target="#b42">[43]</ref> is a synthetic dataset of street scene, which consists of 25, 000 images created with a photo-realistic rendering technique. SIM10K <ref type="bibr" target="#b20">[21]</ref> is a street view dataset generated from the realistic computer game Grand Theft Auto V (GTA-V). It has 10, 000 training images and the same categories as in Cityscapes.</p><p>PreSIL <ref type="bibr" target="#b16">[17]</ref> is synthetic point cloud dataset derived from GTA-V, which consists of 50, 000 frames of high-definition images and point clouds.</p><p>BDD100K <ref type="bibr" target="#b44">[45]</ref> is a large-scale dataset (contains 100k images) that covers diverse driving scenes. It is a good representative of real data in the wild.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We train the Faster R-CNN [33] model for 12 epochs on all experiments. The model is optimized by SGD with multi-step learning rate decay. SGD uses the learning rate of 0.00625 multiplied by the batchsize, and momentum of 0.9. We adopt CDN layer in all convolution stages, including the backbone and bounding box network. All experiments use sync BN <ref type="bibr" target="#b25">[26]</ref> with a batchsize of 32. λ is set as 0.4 by default in all experiments. On Cityscapes to Foggy Cityscapes adaptation, we follow <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref> to prepare the train/test split, and use an image shorter side of 512 pixels. On synthetic-to-real adaptation, for a fair comparison, we randomly select 7000 images for training and 3000 for testing, for all synthetic datasets and BDD100K dataset. For 3D point cloud detection, we use PointRCNN <ref type="bibr" target="#b37">[38]</ref> model with same setting as <ref type="bibr" target="#b37">[38]</ref>. We incorporated the CDN layer in the pointwise feature generation stage (global alignment) and 3D ROIs proposal stage (instance alignment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Cityscapes to Foggy Cityscapes</head><p>We compare CDN with the state-of-the-art methods in <ref type="table">Table 1</ref>. Following <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>, we also report results using Faster R-CNN model with VGG16 backbone. As shown in <ref type="table">Table 1</ref>, CDN outperforms previous state-of-the-art methods by a large margin of 1.8% mAP. The results demonstrate the effectiveness of CDN on reducing domain gaps. A detailed comparison of different CDN settings can be found at the ablation study 7. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>  <ref type="table">Table 1</ref>: Cityscapes to Foggy Cityscapes adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on KITTI to Cityscapes</head><p>Different camera settings may influence the detector performance in real-world applications. We conduct the cross-camera adaptation on KITTI to Cityscapes. <ref type="table" target="#tab_2">Table 2</ref> shows the adaptation results on car category produced by Faster R-CNN with VGG16. Global and Instance represent global and local alignment respectively. The results demonstrate that CDN achieves 1.7% mAP improvements over the state-of-the-art methods. We can also find that instance feature alignment contributes to a larger performance boost than global counterpart, which is consistent with previous discovery <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on SIM10K to Cityscapes</head><p>Following the setting of <ref type="bibr" target="#b33">[34]</ref>, we evaluate the detection performance on car on SIM10K-to-Cityscapes benchmark. The results in <ref type="table" target="#tab_3">Table 3</ref> demonstrate CDN constantly performs better than the baseline methods. CDN with both global and instance alignment achieves 49.3% mAP on validation set of Cityscapes, which outperforms the previous state-of-the-art method by 1.6% mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on Synthetic to Real Data</head><p>To thoroughly evaluate the performance of the state-of-the-art methods on synthetic to real adaptation, we construct a large-scale synthetic-to-real adaptation benchmark on various public synthetic datasets, including Virtual KITTI, Synscapes and SIM10K. "All" represents using the combination of 3 synthetic datasets. Compared with SIM10K-to-Cityscapes, the proposed benchmark is more challenging in terms of much larger image diversity in both real and synthetic domains. We compare CDN with the state-of-the-art method SWDA <ref type="bibr" target="#b33">[34]</ref> in <ref type="table" target="#tab_4">Table 4</ref>. CDN consistently outperforms SWDA under different backbones, which achieves average 2.2% mAP and 2.1% mAP improvements on Faster-R18 and Faster-R50 respectively. Using the same adaptation method, the detection performance strongly depends on the quality of synthetic data. For instance, the adaptation performance of SIM10K is much better than Virtual KITTI. Some example predictions produced by our method are visualized in <ref type="figure" target="#fig_1">Fig. 3</ref>.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Adaptation on 3D Point Cloud Detection</head><p>We evaluate CDN on adapting 3D object detector from synthetic point cloud (PreSIL) to real point cloud data (KITTI). PointRCNN <ref type="bibr" target="#b37">[38]</ref> with backbone of PointNet++ <ref type="bibr" target="#b29">[30]</ref> is adopted as our baseline model. Following standard metric on KITTI benchmark <ref type="bibr" target="#b37">[38]</ref>, we use Average Precision(AP) with IOU threshold 0.7 for car and 0.5 for pedestrian / cyclist. <ref type="table" target="#tab_6">Table 5</ref> shows that CDN constantly outperforms the state-of-the-art method PointDAN <ref type="bibr" target="#b30">[31]</ref> across all categories, with an average improvement of 1.9% AP. We notice that instance alignment contributes to a larger performance boost than global alignment. It can be attributed by the fact that point cloud data spread over a huge 3D space but most information is stored in the local foreground points (see <ref type="figure">Fig. 4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visualize and Analyze the Feature Maps</head><p>Despite the general efficiency on various benchmarks, we are also interested in the underlying principle of CDN. We interpret the learned domain embedding via appending a decoder network after the backbone to reconstruct the RGB images from the feature maps. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, the top row shows the original inputs from Foggy Cityscapes, SIM10K and Synscapes (left to right), and the bottom row shows the reconstructed images from the corresponding features encoded with the domain embedding of another domain. The reconstructed images carry the same domain style of another domain, suggesting the learned domain embedding captures the domain attribute information and CDN can effectively transform the domain style of different domains. Furthermore, we compute Frchet Inception Distance (FID) <ref type="bibr" target="#b13">[14]</ref> to quantitatively investigate the difference between source and target features. FID has been a popular metric to evaluate the style similarity between two groups of images in GANs. Lower FID score indicates a smaller style difference. For easy comparison, we normalize the FID score to [0, 1] by dividing the maximum score. As shown in <ref type="table">Table 6</ref>, the feature learned with CDN achieves significantly smaller FID score compared with feature learned on source domain only, suggesting CDN effectively reduces the domain gap in the feature space. Obviously, supervised joint training on source and target data gets the smallest FID score, which is verified by the best detection performance achieved by joint training. As shown in <ref type="figure">Fig. 6</ref>, synthetic-to-real has larger FID score than real-to-real dataset, since the former owns larger domain gaps.  <ref type="table">Table 6</ref>: FID score and mAP. <ref type="figure">Fig. 6</ref>: FID scores on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis on Domain Discrepancy</head><p>We adopt symmetric KullbackLeibler divergence to investigate the discrepancy between source and target domain in feature space. To simplify the analysis, we assume source and target features are drawn from the multivariate normal distribution. The divergence is calculated with the Res5-3 features and plotted in log scale. <ref type="figure">Fig. 7 (a)</ref> and (c) show that the domain divergence continues decreasing during training, indicating the Conditional Domain Normalization keeps reducing domain shift in feature space. Benefiting from the reduction of domain divergence, the adaptation performance on the target domain keeps increasing. Comparing with SWDA, CDN achieves lower domain discrepancy and higher adaptation performance. Except for the quantitative measure of domain divergence, we also visualize the t-SNE plot of instance features extracted by a Faster R-CNN incorporated with CDN. <ref type="figure">Fig. 7 (b)(d)</ref> shows the t-SNE plot of instance features extracted by a Faster R-CNN model incorporated with CDN. The same category features from two domains group in tight clusters, suggesting source and target domain distributions are well aligned in feature space. Besides, features of different categories own clear decision boundaries, indicating discriminative features are learned by our method. These two factors contribute to the detection performance on target domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation Study</head><p>For the ablation study, we use a Faster R-CNN model with ResNet-18 on SIM10K to BDD100K adaptation benchmark, and a Faster R-CNN model with VGG16 on Cityscapes-to-Foggy Cityscapes adaptation benchmark. G and I denote adopting CDN in the backbone and bounding box head network, respectively.  Adopting CDN at different convolution stages. <ref type="figure" target="#fig_5">Fig. 8(a)</ref> compares the results of Faster R-CNN models adopting CDN at different convolution stages.</p><p>We follow <ref type="bibr" target="#b12">[13]</ref> to divide ResNet into 5 stages. Bbox head denotes the bounding box head network. From left to right, adding more CDN layers keeps boosting the adaptation performance on both benchmarks, benefiting from adaptive distribution alignments across different levels' representation. It suggests that adopting CDN in each convolution stage is a better choice than only aligning domain distributions at one or two specific convolution stages.</p><p>Comparing with existing domain adaptation frameworks adopting CDN. <ref type="figure" target="#fig_5">Fig. 8(b)</ref> shows the results of adopting CDN layer in existing adaptation methods like SWDA <ref type="bibr" target="#b33">[34]</ref> and SCDA <ref type="bibr" target="#b46">[47]</ref>. Directly adopting CDN in SWDA and SCDA can bring average 1.3% mAP improvements on two adaptation benchmarks, suggesting CDN is more effective to address domain shifts than traditional domain confusion learning. It can be attributed to that CDN disentangle the domain-specific factors out of the semantic features via learning a domainvector. Leveraging the domain-vector to align the different domain distributions can be more efficient.</p><p>Compare domain embedding with semantic features. In Eq. 7, we can either use semantic features (v s ,v t ) or domain embedding (F d (v s ), F d (v t )) as inputs of discriminator. <ref type="figure" target="#fig_5">Fig. 8</ref>(c) compares the adaptation performance of using semantic features with using domain embedding. Although semantic features can improve the performance over baseline, domain embedding consistently achieves better results than directly using semantic features. Suggesting the learned domain embedding well captures the domain attribute information, and it is free from some undesirable regularization on specific image contents.</p><p>Value of λ In Eq. 10, we use λ controls the balance between global and local regularization. <ref type="figure" target="#fig_6">Fig. 9 (left)</ref> shows the influence on adaptation performance by different λ. Because object detectors naturally focus more on local regions, we can see stronger instance regularization largely contributes to detection performance. In our experiments, λ between 0.4 and 0.5 gives the best performance. Scale of target domain dataset <ref type="figure" target="#fig_6">Fig. 9</ref> middle/right quantitatively investigate the relation between real data detection performance and percentage of synthetic data used for training. "All" means to use the combination of 3 differ-ent synthetic datasets. The larger synthetic dataset provides better adaptation performance, on both 2D image and 3D point cloud detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We present the Conditional Domain Normalization (CDN) to adapt object detectors across different domains. CDN aims to embed different domain inputs into a shared latent space, where the features from different domains carry the same domain attribute. Extensive experiments demonstrate the effectiveness of CDN on adapting object detectors, including 2D image and 3D point cloud detection tasks. And both quantitative and qualitative comparisons are conducted to analyze the features learned by our method. the learned domain embedding via reconstructing the RGB images from the features. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, we first built a decoder network Decoder(·; θ dec ) upon the backbone network G(·; θ * g ) of fixed weights. The parameters of the backbone network are obtained in the adaptation training (see Eq. 1). The decoder network mostly mirrors the backbone network, with all pooling layers replaced by nearest up-sampling and all normalization layers removed. The decoder network is trained to reconstruct the RGB images from the features extracted by the backbone, arg min</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Interpret the Domain Embedding</head><formula xml:id="formula_11">θ dec L = ||Decoder(G(x; θ * g ); θ dec ) − x|| 2 .<label>(11)</label></formula><p>For contrast analysis, only single domain images are used to train the decoder network, i.e. the decoder for Cityscapes experiment is trained on Foggy Cityscapes images, the decoder for SIM10K experiment is trained on SIM10K images. After we got a trained decoder network, we use it to reconstruct the RGB image from features encoded with the domain embedding. characteristics, suggesting that both Cityscapes and Foggy Cityscapes inputs are embedded into a shared latent space, where their features carry the same domain attribute. Given the domain gap bridged, the object detector supervised trained on Cityscapes also works on Foggy Cityscapes. <ref type="figure" target="#fig_0">Fig. 11 and 13</ref> show the reconstructed results from synthetic data's features encoded with domain embedding of real data (BDD100K), which are learned in SIM10K-to-BDD100K and Synscapes-to-BDD10K adaptation experiments, respectively (see <ref type="bibr">Section 5.4)</ref>. Without the domain embedding of real data, the reconstructed images (middle row of <ref type="figure" target="#fig_0">Fig 11 and 13</ref>) still exhibit characteristic of CG (computer graphic), that look identical to the original images. When the same features of original inputs are encoded with the domain embedding of real data, the reconstructed images (bottom row of <ref type="figure" target="#fig_0">Fig 11 and 13)</ref> obviously becomes more realistic. For example, the color of the sky, the texture of the road and objects in the reconstructed images look similar to the real images. It proves that the learned domain embedding well captures the domain attribute information of real data, and it can be used to effectively translate the synthetic images towards real images.</p><p>A.2 Visualize the Feature Maps Despite the general efficiency on various benchmarks, we are also interested in the underlying principle of CDN. We first visualize the features of different domain images. As shown in <ref type="figure" target="#fig_0">Fig. 14, we</ref> can not easily distinguish the domain label from feature maps alone, suggesting the features from synthetic and real domain carry the same domain attribute. Besides, the same category objects across synthetic and real domain share similar activation patterns and contours, indicating the our method well preserves the feature semantics. <ref type="figure" target="#fig_0">Fig. 15</ref> shows 3D point cloud detection results on the KITTI dataset.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Qualitative Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>(Left) Traditional domain adversarial approach. (Right) Conditional Domain Normalization (CDN). The green and blue cubes represent the feature maps of domain A and domain B respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Example results on Foggy Cityscapes/Synscapes/SIM10K/BDD100K (from top to bottom). The results are produced by a Faster R-CNN model incorporated with CDN. The class and score predictions are at the top left corner of the bounding box. Zoom in to visualize the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Top row: Original inputs from Foggy Cityscapes, SIM10K and Synscapes (left to right); Bottom row: Reconstructed images from features encoded with the learned domain embedding of another domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) City-to-Foggy (b) City-to-Foggy (c) SIM-to-BDD (d) SIM-to-BDD Fig. 7: (a)(c): Divergence and adaptation performance. (b)(d): t-SNE plot of instance features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>(a) Adopt CDN at different convolution stages of ResNet; (b) Adopt CDN in existing adaptation frameworks; (c) Domain embedding vs. semantic features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Left: mAP vs. Value of λ; Middle: mAP vs. Percentage (%) of synthetic image data; Right: AP vs. Percentage (%) of synthetic point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Top row: Original inputs of Foggy Cityscapes; Middle row: Reconstructed results from features of original inputs; Bottom row: Reconstructed results from features encoded with the domain embedding of Cityscapes. Conditional domain normalization disentangles the domain-specific attribute out of the semantic features from one domain via a learning a domain embedding to characterize the domain attribute information. In this section, we interpret Original Original's feature +Domain Embedding Top row: Original inputs of SIM10K; Middle row: Reconstructed results from features of original inputs; Bottom row: Reconstructed results from features encoded with the domain embedding of BDD100K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 showsFig. 12 :</head><label>1012</label><figDesc>the effect of domain embedding learned in Cityscapes to Foggy Cityscapes adaptation experiments (Section 5.1). The top row shows the inputs of Foggy Cityscapes; the middle row shows the reconstructed results from features of Foggy Cityscapes inputs; the bottom row is reconstructed results from Foggy Cityscapes features encoded with the domain embedding learned on Cityscapes. With the help of the domain embedding learned on Cityscapes, the reconstructed results from Foggy Cityscapes features no longer exhibit foggy Interpreting the learned domain embedding with a decoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Top row: Original inputs of Synscapes; Middle row: Reconstructed results from features of original inputs; Bottom row: Reconstructed results from features encoded with the domain embedding of BDD100K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 :</head><label>14</label><figDesc>Res5-3 features learned by Faster R-CNN with CDN. Left two images are from synthetic data (SIM10K) and the right two images are from real data (Cityscapes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 :</head><label>15</label><figDesc>Example results of PointRCNN model with Conditional Domain Normalization (with AP of moderate level of 19.0). For each example, the upper part is the image and the lower part is the corresponding point cloud. The detected objects are shown with red 3D bounding boxes. The green bounding boxes represent the ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, our method exhibits good generalization capability under foggy weather conditions.</figDesc><table><row><cell>Method</cell><cell cols="3">Person Rider Car Truck Bus Train Motorcycle Bicycle mAP</cell></row><row><cell>Source</cell><cell>29.3 31.9 43.5 15.8 27.4 9.0</cell><cell>20.3</cell><cell>29.9 26.1</cell></row><row><cell cols="2">DA-Faster [4] 25.0 31.0 40.5 22.1 35.3 20.2</cell><cell>20.0</cell><cell>27.1 27.9</cell></row><row><cell>DT [18]</cell><cell>25.4 39.3 42.4 24.9 40.4 23.1</cell><cell>25.9</cell><cell>30.4 31.5</cell></row><row><cell>SCDA [47]</cell><cell>33.5 38.0 48.5 26.5 39.0 23.3</cell><cell>28.0</cell><cell>33.6 33.8</cell></row><row><cell cols="2">DDMRL [22] 30.8 40.5 44.3 27.2 38.4 34.5</cell><cell>28.4</cell><cell>32.2 34.6</cell></row><row><cell>SWDA [34]</cell><cell>30.3 42.5 44.6 24.5 36.7 31.6</cell><cell>30.2</cell><cell>35.8 34.8</cell></row><row><cell>CDN (ours)</cell><cell>35.8 45.7 50.9 30.1 42.5 29.8</cell><cell>30.8</cell><cell>36.5 36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>KITTI to Cityscapes.</figDesc><table><row><cell>Method</cell><cell>Global Instance mAP(%)</cell></row><row><cell>Source only</cell><cell>34.3</cell></row><row><cell>DA-Faster[4]</cell><cell>38.3</cell></row><row><cell>SWDA [34]</cell><cell>47.7</cell></row><row><cell>SCDA [47]</cell><cell>44.1</cell></row><row><cell></cell><cell>41.2</cell></row><row><cell>CDN</cell><cell>45.8</cell></row><row><cell></cell><cell>49.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>SIM10K to Cityscapes.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="3">Virtual KITTI Synscapes SIM10K All</cell></row><row><cell></cell><cell>Source</cell><cell>9.8</cell><cell>24.5</cell><cell>37.7 38.2</cell></row><row><cell>Faster-R18</cell><cell>SWDA[34] CDN</cell><cell>15.6 17.5</cell><cell>27.0 29.1</cell><cell>40.2 41.3 42.7 43.6</cell></row><row><cell></cell><cell>Target</cell><cell></cell><cell>70.5</cell><cell></cell></row><row><cell></cell><cell>Source</cell><cell>13.9</cell><cell>29.1</cell><cell>41.6 42.8</cell></row><row><cell>Faster-R50</cell><cell>SWDA[34] CDN</cell><cell>19.7 21.8</cell><cell>31.5 33.4</cell><cell>42.9 44.3 45.3 47.2</cell></row><row><cell></cell><cell>Target</cell><cell></cell><cell>75.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Adaptation from different synthetic data to real data. mAP on car is re- ported on BDD100K validation. The results of supervised training on BDD100K are highlighted in gray.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Adapting from synthetic (PreSIL) to real (KITTI) pint cloud. AP of moderate level on KITTI test is reported.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3722" to="3731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3339" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Precise synthetic image and lidar (presil) dataset for autonomous vehicle perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hurl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2522" to="2529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12627" to="12637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rosaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01983</idno>
		<title level="m">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diversify and match: A domain adaptive representation learning paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12456" to="12465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<title level="m">Compound domain adaptation in an open world</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07291</idno>
		<title level="m">Semantic image synthesis with spatially-adaptive normalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1806.09755</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointdan: A multi-scale 3d domain adaption network for point cloud representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7190" to="7201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Strong-weak distribution alignment for adaptive object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-018-1072-8</idno>
		<ptr target="https://doi.org/10.1007/s11263-018-1072-8" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation for vehicle detection from bird&apos;s eye view lidar point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abobakr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahvandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3752" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05427</idno>
		<title level="m">Domain adaptation for structured output via discriminative representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wrenninge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08705</idno>
		<title level="m">Synscapes: A photorealistic synthetic dataset for street scene parsing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<title level="m">Understanding neural networks through deep visualization</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<title level="m">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A lidar point cloud generator: from a virtual world to autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Seshia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="458" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adapting object detectors via selective cross-domain alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
