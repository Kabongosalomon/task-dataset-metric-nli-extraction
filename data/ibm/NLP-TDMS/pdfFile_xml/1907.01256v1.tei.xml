<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yo</forename><forename type="middle">Joong</forename><surname>Choe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyeon</forename><surname>Ham</surname></persName>
							<email>jiyeon.ham@kakaobrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyubyong</forename><surname>Park</surname></persName>
							<email>kyubyong.park@kakaobrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeoil</forename><surname>Yoon</surname></persName>
							<email>yeoil.yoon@kakaobrain.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kakao</forename><surname>Brain</surname></persName>
						</author>
						<title level="a" type="main">A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grammatical error correction can be viewed as a low-resource sequence-to-sequence task, because publicly available parallel corpora are limited. To tackle this challenge, we first generate erroneous versions of large unannotated corpora using a realistic noising function. The resulting parallel corpora are subsequently used to pre-train Transformer models. Then, by sequentially applying transfer learning, we adapt these models to the domain and style of the test set. Combined with a context-aware neural spellchecker, our system achieves competitive results in both restricted and low resource tracks in ACL 2019 BEA Shared Task. We release all of our code and materials for reproducibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammatical error correction (GEC) is the task of correcting various grammatical errors in text, as illustrated by the following example:</p><p>[Travel → Travelling] by bus is [exspensive → expensive], [bored → boring] and annoying.</p><p>While the dominant approach following the <ref type="bibr">CoNLL-2014</ref><ref type="bibr">Shared Task (Ng et al., 2014</ref> has been different adaptations of phrase-based and statistical machine translation (PBSMT) models <ref type="bibr">(Junczys-Dowmunt and Grundkiewicz, 2016)</ref>, more recent work on GEC increasingly adopted partial <ref type="bibr">(Grundkiewicz and Junczys-Dowmunt, 2018)</ref> or exclusive <ref type="bibr">(Junczys-Dowmunt et al., 2018;</ref><ref type="bibr">Chollampatt and Ng, 2018a)</ref> use of deep sequence-to-sequence (seq2seq) architectures <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Cho et al., 2014)</ref>, which showed immense success in neural machine translation (NMT) <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr">Gehring et al., 2017;</ref><ref type="bibr">Vaswani et al., 2017)</ref>.</p><p>In GEC, unlike NMT between major languages, there are not enough publicly available corpora (GEC's hundreds of thousands to NMT's tens of millions). This motivates the use of pre-training and transfer learning, which has shown to be highly effective in many natural language processing (NLP) scenarios in which there is not enough annotated data, notably in low-resource machine translation (MT) <ref type="bibr">(Lample et al., 2018b;</ref><ref type="bibr">Ruder, 2019)</ref>. As a result, recent GEC systems also include pre-training on various auxiliary tasks, such as language modeling (LM) <ref type="bibr">(Junczys-Dowmunt et al., 2018)</ref>, text revision <ref type="bibr">(Lichtarge et al., 2018)</ref>, and denoising <ref type="bibr">(Zhao et al., 2019)</ref>.</p><p>In this paper, we introduce a neural GEC system that combines the power of pre-training and transfer learning. Our contributions are summarized as follows:</p><p>• We pre-train our model for the denoising task using a novel noising function, which gives us a parallel corpus that includes realistic grammatical errors;</p><p>• We leverage the idea of sequential transfer learning <ref type="bibr">(Ruder, 2019)</ref>, thereby effectively adapting our pre-trained model to the domain as well as the writing and annotation styles suitable for our final task.</p><p>• We introduce a context-aware neural spellchecker, which improves upon an off-the-shelf spellchecker by incorporating context into spellchecking using a pre-trained neural language model (LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background 2.1 Transformers</head><p>Transformers <ref type="bibr">(Vaswani et al., 2017)</ref> are powerful deep seq2seq architectures that rely heavily on the arXiv:1907.01256v1 [cs.CL] 2 Jul 2019 attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr">Luong et al., 2015)</ref>. Both the encoder and the decoder of a Transformer are stacks of Transformer blocks, each of which consists of a multi-head self-attention layer followed by a position-wise feed-forward layer, along with residual connection <ref type="bibr">(He et al., 2016)</ref> and layer normalization <ref type="bibr">(Ba et al., 2016)</ref>. Each decoder block also attends <ref type="bibr">(Luong et al., 2015)</ref> to the encoder outputs, in between its self-attention and feed-forward layers. Each input token embedding in a Transformer is combined with a positional embedding that encodes where the token appeared in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Copy-Augmented Transformers</head><p>Copy-augmented Transformers <ref type="bibr">(Zhao et al., 2019)</ref> are a class of Transformers that also incorporate an attention-based copying mechanism <ref type="bibr">(Gu et al., 2016;</ref><ref type="bibr">See et al., 2017;</ref><ref type="bibr">Jia and Liang, 2016)</ref> in the decoder. For each output token y t at output position t, the output probability distribution of a copy-augmented Transformer is a mixture of the decoder's generative distribution p gen and a copy distribution p copy , which is defined as an encoderdecoder attention layer that assigns a distribution over tokens appearing in the source sentence. By defining a mixture weight parameter α copy t per each decoding step, the output distribution can be compactly represented as follows: p(y t ) = (1 − α copy t ) · p gen (y t ) + α copy t · p copy (y t )</p><p>(1)</p><p>The mixture weight balances between how likely it is for the model to simply copy a source token, rather than generating a possibly different token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Denoising Autoencoders</head><p>Denoising autoencoders (DAEs) <ref type="bibr">(Vincent et al., 2008)</ref> are a class of neural networks that learns to reconstruct the original input given its noisy version. Given an input x and a (stochastic) noising function x →x, the encoder-decoder model of a DAE minimizes the reconstruction loss:</p><p>L(x, dec(enc(x)))</p><p>where L is some loss function.</p><p>Within the NLP domain, DAEs have been for pre-training in seq2seq tasks that can be cast as a denoising task. For example, in GEC, pre-trained DAEs have been used for correcting erroneous sentences <ref type="bibr">(Xie et al., 2018;</ref><ref type="bibr">Zhao et al., 2019</ref>). Another example is low-resource machine translation (MT) <ref type="bibr">(Lample et al., 2018b)</ref>, pre-trained DAEs were used to convert word-by-word translations into natural sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Many recent neural GEC models <ref type="bibr">(Junczys-Dowmunt et al., 2018;</ref><ref type="bibr">Lichtarge et al., 2018;</ref><ref type="bibr">Zhao et al., 2019)</ref> made use of the Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> architecture and saw results nearly as good as or better than convolutional <ref type="bibr">(Chollampatt and Ng, 2018a,b)</ref> and <ref type="bibr">recurrent (Grundkiewicz and Junczys-Dowmunt, 2018;</ref><ref type="bibr">Ge et al., 2018a)</ref> architectures. <ref type="bibr">Recently, Zhao et al. (2019)</ref> further incorporated a copying mechanism <ref type="bibr">(Gu et al., 2016;</ref><ref type="bibr">See et al., 2017;</ref><ref type="bibr">Jia and Liang, 2016)</ref> to the Transformer, highlighting the fact that most (from 83% in Lang-8 to 97% in CoNLL-2013) of the target tokens are exact copies of the corresponding source tokens.</p><p>Several prior results, both early <ref type="bibr" target="#b2">(Brockett et al., 2006;</ref><ref type="bibr">Felice and Yuan, 2014)</ref> and recent <ref type="bibr">(Ge et al., 2018a;</ref><ref type="bibr">Xie et al., 2018;</ref><ref type="bibr">Zhao et al., 2019)</ref>, introduced different strategies for generating erroneous text that can in turn be used for model (pre-)training. One major direction is to introduce an additional "back-translation" model <ref type="bibr">(Ge et al., 2018a;</ref><ref type="bibr">Xie et al., 2018)</ref>, inspired by its success in <ref type="bibr">NMT (Sennrich et al., 2016a)</ref>, and let this model learn to generate erroneous sentences from correct ones. While these back-translation models can learn naturally occurring grammatical errors from the parallel corpora in reverse, they also require relatively large amounts of parallel corpora, which are not readily available in low resource scenarios. The other direction, which can avoid these issues, is to incorporate a pre-defined noising function, which can generate pre-training data for a denoising task <ref type="bibr">(Zhao et al., 2019)</ref>. Compared to <ref type="bibr">(Zhao et al., 2019)</ref>, our work introduces a noising function that generates more realistic grammatical errors.</p><p>When pre-training a seq2seq model on an auxiliary denoising task, the choice of the noising function is important. For instance, in low-resource <ref type="bibr">MT, Lample et al. (2018a,b)</ref> made use of a noising function that randomly insert/replace/remove tokens or mix up nearby words at uniform probabilities. They showed that this approach is effective in translating naive word-by-word translations into correct ones, both because the coverage of word-to-word dictionaries can be limited and because word order is frequently swapped between languages (e.g., going from SVO to SOV).</p><p>In <ref type="bibr">GEC, Zhao et al. (2019)</ref> used a similar noising function to generate a pre-training dataset. However, we find that this noising function is less realistic in GEC than in low-resource MT. For example, randomly mixing up nearby words can be less effective for GEC than for low-resource MT, because word order errors occur less frequently than other major error categories, such as missing punctuations and noun numbers. Also, replacing a word to any random word in the vocabulary is a less realistic scenario than only replacing it with its associated common error categories, such as prepositions, noun numbers and verb tenses.</p><p>To generate realistic pre-training data, we introduce a novel noising function that captures indomain grammatical errors commonly made by human writers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Constructing Noising Scenarios</head><p>We introduce two kinds of noising scenarios, using a token-based approach and a type-based approach.</p><p>In the token-based approach, we make use of extracted human edits from annotated GEC corpora, using automated error annotation toolkits such as ERRANT <ref type="bibr" target="#b5">(Bryant et al., 2017)</ref>. We first take a subset of the training set, preferably one that contains in-domain sentences with high-quality annotations, and using an error annotation toolkit, we collect all edits that occurred in the parallel corpus as well as how often each edit was made. We then take edits that occur in for at least k times, where k is a pre-defined threshold (we fix k = 4 in our experiments), in order to prevent overfitting to this (possibly small) subset. These extracted edits include errors commonly made by human writers, including missing punctuations (e.g., adding a comma), preposition errors (e.g., of → at), and verb tenses (e.g., has → have). As a result, we obtain an automatically constructed dictionary of common edits made by human annotators on the in-domain training set. Then, we can define a realistic noising scenario by randomly applying these human edits, in reverse, to a grammatically correct sentence.</p><p>In the type-based approach, we also make use of a priori knowledge and construct a noising scenario based on token types, including prepositions, nouns, and verbs. For each token type, we define a noising scenario based on commonly made errors associated with that token type, but without changing the type of the original token. In particular, we replace prepositions with other prepositions, nouns with their singular/plural version, and verbs with one of their inflected versions. This introduces another set of realistic noising scenarios, thereby increasing the coverage of the resulting noising function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating Pre-training Data</head><p>Our goal is to come up with an error function that introduces grammatical errors that are commonly made by human writers in a specific setting (in this case, personal essays written by English students). Given sets of realistic noising scenarios, we can generate large amounts of erroneous sentences from high-quality English corpora, such as the Project Gutenberg corpus <ref type="bibr">(Lahiri, 2014)</ref> and <ref type="bibr">Wikipedia (Merity et al., 2016)</ref>.</p><p>We first check if a token exists in the dictionary of token edits. If it does, a token-based error is generated with the probability of 0.9. Specifically, the token is replaced by one of the associated edits with the probabilities proportional to the frequency of each edit. For example, the token for may be replaced with during, in, four, and also for (coming from a noop edit).</p><p>If a token is not processed through the tokenbased scenario, we then examine if it belongs to one of the pre-defined token types: in our case, we use prepositions, nouns, and verbs. If the token belongs to one such type, we then apply the corresponding noising scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sequential Transfer Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transferring Pre-trained DAE Weights</head><p>As discussed in <ref type="bibr">(Zhao et al., 2019)</ref>, an important benefit of pre-training a DAE is that it provides good initial values for both the encoder and the decoder weights in the seq2seq model. Given a pre-trained DAE, we initialize our seq2seq GEC model using the learned weights of the DAE and train on all available parallel training corpora with smaller learning rates. This model transfer <ref type="bibr">approach (Wang and Zheng, 2015)</ref> can be viewed as a (relatively simple) version of sequential transfer learning <ref type="bibr">(Ruder, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adaptation by Fine-tuning</head><p>As noted in <ref type="bibr">(Junczys-Dowmunt et al., 2018)</ref>, the distribution of grammatical errors occurring in text can differ across the domain and content of text. For example, a Wikipedia article introducing a historical event may involve more rare words than a personal essay would. The distribution can also be affected significantly by the writer's style and proficiency, as well as the annotator's preferred style of writing (e.g., British vs. American styles, synonymous word choices, and Oxford commas).</p><p>In this work, given that the primary source of evaluation are personal essays at various levels of English proficiency -in particular the W&amp;I+LOCNESS dataset (Yannakoudakis et al., 2018) -we adapt our trained models to such characteristics of the test set by fine-tuning the model only on the training portion of W&amp;I, which largely matches the domain of the development and test sets. 2 Similar to our training step in §5.1, we use (even) smaller learning rates. Overall, this sequential transfer learning framework can also be viewed as an alternative to oversampling indomain data sources, as proposed in <ref type="bibr">(Junczys-Dowmunt et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A Context-Aware Neural Spellchecker</head><p>Many recent GEC systems include an off-theshelf spellchecker, such as the open-source package enchant <ref type="bibr">(Sakaguchi et al., 2017;</ref><ref type="bibr">Junczys-Dowmunt et al., 2018)</ref> and Microsoft's Bing spellchecker <ref type="bibr">(Ge et al., 2018a,b)</ref>. While the idea of incorporating context into spellchecking has been repeatedly discussed in the literature <ref type="bibr">(Flor and Futagi, 2012;</ref><ref type="bibr" target="#b7">Chollampatt and Ng, 2017)</ref>, popular open-sourced spellcheckers such as hunspell primarily operate at the word level. This fundamentally limits their capacity, because it is often difficult to find which word is intended for without context. For example, given the input sentence This is an esay about my favorite sport.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Public? # Sent. # Annot. hunspell invariably suggests easy as its top candidate for esay, which should actually be corrected as essay.</p><p>Our spellchecker incorporates context to hunspell using a pre-trained neural language model (LM). Specifically, we re-rank the top candidates suggested by hunspell through feeding each, along with the context, to the neural LM and scoring them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>Throughout our experiments, we use fairseq 3 (Ott et al., 2019), a publicly available sequenceto-sequence modeling toolkit based on PyTorch <ref type="bibr">(Paszke et al., 2017)</ref>. Specifically, we take fairseq-0.6.1 and add our own implementations of a copy-augmented transformer model as well as several GEC-specific auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets &amp; Setups</head><p>In <ref type="table">Table 1</ref>, we summarize all relevant data sources, their sizes, whether they are public, and the number of annotators.</p><p>For pre-training, we use the Gutenberg dataset (Lahiri, 2014), the Tatoeba 4 dataset, and the WikiText-103 dataset <ref type="bibr">(Merity et al., 2016)</ref>. We learned through initial experiments that the quality of pre-training data is crucial to the final model's performance, because our DAE model assumes §4 that these unannotated corpora contain little grammatical errors. Our choice of corpora is based on both the quality and diversity of text: Guten-   berg contains clean novel writings with minimal grammatical errors, Tatoeba contains colloquial sentences used as sample sentences in dictionaries, and WikiText-103 contains "Good" and "Featured" articles from Wikipedia. Our final pretraining data is a collection of 45M (perturbed, correct) sentence pairs based on these datasets, with our noising approach ( §4) applied multiple times to each dataset to approximately balance data from each source (1x Gutenberg, 12x Tatoeba, and 5x WikiText-103). Our default setup is the "Restricted Track" scenario ( §7.5) for the BEA 2019 Shared Task, where we use four data sources: the FCE dataset <ref type="bibr" target="#b3">(Bryant et al., 2019)</ref>, the Lang-8 dataset 5 <ref type="bibr">(Mizumoto et al., 2011;</ref><ref type="bibr">Tajiri et al., 2012)</ref>, the NUCLE (v3.3) dataset <ref type="bibr">(Dahlmeier et al., 2013)</ref>, and the newly released Write &amp; Improve and LOCNESS (W&amp;I+L) datasets <ref type="bibr">(Yannakoudakis et al., 2018)</ref>. <ref type="bibr">6</ref> For the "Low Resource Track" ( §7.6), we use a 3:1 traintest random split of the W&amp;I+L development set, keeping the proportions of proficiency levels the same. In both tracks, we report our final results on the W&amp;I+L test set, which contains 5 annotations. Further, because the W&amp;I+L dataset is relatively new, we also include results on the CoNLL-2014 (Ng et al., 2014) dataset, with and without using the W&amp;I+L dataset during training ( §7.7). In Table 2, we summarize which datasets were used in each setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Pre-processing</head><p>As part of pre-processing, we first fix minor tokenization issues in the dataset using regular expressions. We use spaCy v1.9 (Honnibal and Montani, 2017) to make tokenization consistent with the final evaluation module (ERRANT).</p><p>This tokenized input is then fed to our contextaware neural spellchecker ( §6). For the neural LM, we use a gated convolutional neural network language model <ref type="bibr">(Dauphin et al., 2017)</ref> pre-trained on <ref type="bibr">WikiText-103 (Merity et al., 2016)</ref>.</p><p>During spellchecking, we also found it beneficial to fix casing errors within our context-aware spellchecking process. To fix case errors, we extract a list of words used in the capital form much more than their lower-case version (more than 99 times) in <ref type="bibr">WikiText-103 (Merity et al., 2016)</ref>. We then include a capitalized version of the word as a candidate in the LM re-scoring process if it appears in its capitalized form is in the extracted list of common capital words.</p><p>Before feeding spellchecked text into our seq2seq model, we apply byte-pair encoding (BPE) (Sennrich et al., 2016b) using Sentence-Piece <ref type="bibr">(Kudo and Richardson, 2018)</ref>. We first train a SentencePiece model with 32K vocabulary size on the original Gutenberg corpus, and apply this model to all input text to the model. This allows us to avoid &lt;unk&gt; tokens in most training and validation sets, including the W&amp;I+L development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Model &amp; Training Details</head><p>Throughout our experiments, we use two variants of the Transformer model: the "vanilla" Transformer <ref type="bibr">(Vaswani et al., 2017)</ref> and the copyaugmented Transformer <ref type="bibr">(Zhao et al., 2019)</ref>. We use two configurations for the vanilla Transformer: a base model with 6 blocks of 512-2048 units with 8 attention heads, and a large model with 6 blocks of 1024-4096 units with 16 attention heads and pre-attention layer normalization. We only use the large model for Restricted Track ( §7.5) and for the CoNLL-2014 comparison ( §7.7). For the copy-augmented Transformer, we follow the default configuration from <ref type="bibr">(Zhao et al., 2019)</ref>: 6 blocks of 512-4096 units with 8 attention heads, along with an 8-head copy attention layer. For each model configuration, we train two independent models using different seeds.</p><p>Our model training is a three-stage process, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>: DAE pre-training, training, and fine-tuning, except in Low Resource Track where there is no fine-tuning data (see <ref type="table" target="#tab_2">Table 2</ref>). At each step, we train a model until its ERRANT score on the development set reaches convergence, and use the learned weights as initial values for the next step. In all training steps, we used the Adam (Kingma and <ref type="bibr">Ba, 2015)</ref> optimizer.</p><p>Our final model is an ensemble among the different model configurations and seeds. Among the six (four for Low Resource Track) best models, we greedily search for the best combination, starting with the best-performing single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Post-processing</head><p>Our post-processing phase involves three steps. First, we find any &lt;unk&gt; tokens found in the original input text, and using ERRANT, we remove any edits associated with the token. Next, since many of the model's corrections can still be unnatural, if not incorrect, we re-rank candidate corrections within each sentence using a pretrained neural LM <ref type="bibr">(Dauphin et al., 2017)</ref>. Specifically, we remove any combination of up to 7 edits per sentence, and choose the combination that yields the highest LM score. Finally, we noticed that, as in many previous results, our neural system performs well on some error categories (e.g., M:PUNCT) but poorly on others (e.g., R:OTHER). Because ERRANT provides a finegrained analysis of model performance based on error types, we found it beneficial to remove edits belonging to certain categories in which the model performs too poorly. Given our final model, we randomly remove all edits from a subset of (at most N ) categories for repeated steps, and choose to remove the subset of error categories that gave the highest score on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Restricted Track Results</head><p>In <ref type="table" target="#tab_4">Table 3</ref>, we summarize our results on Restricted Track. The results illustrate that each step in our approach substantially improves upon the previous model, both on the W&amp;I+L development and test sets. We highlight that our pre-training step with realistic human errors already gets us at a 54.82 F 0.5 score on span-based correction in ER-RANT for the test set, even though we only indirectly used the W&amp;I training set for error extraction and no other parallel corpora. This suggests that pre-training on a denoising task with realistic and common errors can already lead to a decent GEC system. Our final ensemble model is a combination of five independent models -one base model, two large models, and two copy-augmented modelsachieving 69.06 F 0.5 score on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Low Resource Track Results</head><p>In <ref type="table" target="#tab_5">Table 4</ref>, we summarize our results on Low Resource Track. Similar to Restricted Track, each step in our approach improves upon the previous model significantly, and despite the lack of parallel data (3K for training, 1K for validation), our pretraining step already gets us at 51.71 F 0.5 score on the test set. Compared to Restricted Track, the only difference in pre-training is that the reverse dictionary for the noising function was constructed using much fewer parallel data (3K), but we see that this amount of parallel data is already enough to get within 3 points of our pre-trained model in Restricted Track.</p><p>Our final model is an ensemble of two independent models -one base model and one copy model -achieving 61.47 F 0.5 score on the test set.   were used for the training step and is excluded during evaluation. Pre-processing and post-processing are included in the first step and last steps, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">CoNLL-2014 Results</head><p>In  <ref type="bibr">, 2018)</ref>, and convolutional seq2seq models with quality estimation <ref type="bibr">(Chollampatt and Ng, 2018b)</ref>.</p><p>The results show that our approach is competitive with some of the recent state-of-the-art results that achieve around 56 MaxMatch (M 2 ) scores and further achieves 60+ M 2 score when the W&amp;I+L dataset is used. This illustrates that our approach can also achieve a "near humanlevel performance" (Grundkiewicz and Junczys-Dowmunt, 2018). We also note that the 60.33 M 2 score was obtained by the final ensemble model 7 See Appendix F for a step-by-step training progress. 8 http://nlpprogress.com/english/ grammatical_error_correction.html. from §7.5, which includes a fine-tuning step to the W&amp;I model. This suggests that "overfitting" to the W&amp;I dataset does not necessarily imply a reduced performance on an external dataset such as CoNLL-2014.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Error Analysis</head><p>Here, we give an analysis of our model's performance on some of the major ERRANT error categories on the W&amp;I test set. Detailed information is available in Tabel 10. We observe that our model performs well on syntax relevant error types, i.e., subject-verb agreement (VERB:SVA) (84.09 F 0.5 ), noun numbers (NOUN:NUM) (72.19), and prepositions (PREP) (64.27), all of which are included as part of our type-based error generation in the pre-training data ( §4.2). Our model also achieves 77.26 on spelling errors (SPELL) and 75.83 on orthographic errors (ORTH), both of which are improvements made mostly by our context-aware neural spellchecker. Our model also achieves 77.86 on punctuation errors (PUNCT), which happen to be the most common error category in the    W&amp;I+L dataset. This may be due to both our use of extracted errors from the W&amp;I dataset during pre-training and our fine-tuning step. Finally, we find it challenging to match human annotators' "naturalness" edits, such as VERB (26.76), NOUN (41.67), and OTHER (36.53). This is possibly due to the variability in annotation styles and a lack of large training data with multiple human annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Effect of Realistic Error Generation</head><p>To see how effective our realistic error based pre-training is, we compare it with (Zhao et al., 2019)'s method. According to them, random insertion, deletion, and substitution occur with the probability of 0.1 at every word, and words are reordered with a certain probability. As seen in <ref type="table" target="#tab_9">Table 6</ref> and 7, our pre-training method outperforms the random based one in both Restricted and Low Resource Tracks by 22.57 and 19.70, respectively. And it remains true for each step of the following transfer learning. The performance gap, however, decreases to 5.3 after training and to 3.2 after finetuning in Restricted Track. On the other hand, the gap in Low Resource Track slightly increases to 20.54 after training. This leads to the conclusion that our pre-training functions as proxy for training, for our generated errors resemble the human errors in the training data more than the random errors do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Effect of Context-Aware Spellchecking</head><p>We further investigate the effects of incorporating context and fixing casing errors to the off-the-shelf hunspell, which we consider as a baseline. We test three spellchecker variants: hunspell, hunspell using a neural LM, and our final spellchecker model. On the original W&amp;I+L test set, our LM-based approach improves upon the ERRANT F0.5 score by 5.07 points, and fixing casing issues further improves this score by 4.02 points. As a result, we obtain 32.69 F0.5 score just by applying our context-aware spellchecker model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion &amp; Future Work</head><p>We introduced a neural GEC system that leverages pre-training using realistic errors, sequential transfer learning, and context-aware spellchecking with a neural LM. Our system achieved competitive results on the newly released W&amp;I+L dataset in both standard and low-resource settings.  There are several interesting future directions following our work. One is to extend sentencelevel GEC systems to multi-sentence contexts, for example by including the previous sentence, to better cope with complex semantic errors such as collocation. Because the W&amp;I+L dataset is also a collection of (multi-)paragraph essays, adding multi-sentence contexts can improve these GEC systems. Also, to better understand the role of several components existing in modern GEC systems, it is important to examine which components are more necessary than others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Copy-Augmented Transformers: Formal Derivation</head><p>Copy-augmented Transformers <ref type="bibr">(Zhao et al., 2019)</ref> incorporate an attention-based copying mechanism <ref type="bibr">(Gu et al., 2016;</ref><ref type="bibr">See et al., 2017;</ref><ref type="bibr">Jia and Liang, 2016)</ref> in the decoder of Transformers. For each output token y t at output position t, given source token sequence x = (x 1 , . . . , x T ), the output probability distribution over token vocabulary V is defined as:</p><formula xml:id="formula_1">H enc = enc(x) (3) h dec t = dec (y 1:t−1 ; H enc ) (4) p gen (y t | y 1:t−1 ; x) = softmax W gen h dec t<label>(5)</label></formula><p>where enc denotes the encoder that maps the source token sequence x to a sequence of hidden vectors H enc ∈ R d×T , dec denotes the decoder that takes output tokens at previous time steps along with encoded embeddings and produces a hidden vector h dec t ∈ R d , and W gen ∈ R |V |×d is a learnable linear output layer that maps the hidden vector to a pre-softmax output probabilities ("logits"). We denote the resulting distribution as the (token) generative distribution, denoted as p gen .</p><p>A copy attention layer can be defined as an additional (possibly multi-head) attention layer between the encoder outputs and the final-layer hidden vector at the current decoding step. The attention layer yields two outcomes, the layer output o t and the corresponding attention scores s t :</p><formula xml:id="formula_2">s t = softmax (h dec t ) T H enc √ d (6) o t = H enc s t<label>(7)</label></formula><p>The copy distribution is then defined as the attention scores in (6) themselves 9 : p copy (y t | y 1:t−1 ; x) = s t (8)</p><p>The final output of a copy-augmented Transformer as a mixture of both generative and copy distributions. The mixture weight 10 α copy t is defined at each decoding step as follows:</p><formula xml:id="formula_3">α copy t = sigmoid (w alpha ) T o t (9) p(y t ) = (1 − α copy t ) · p gen (y t ) + α copy t · p copy (y t )<label>(10)</label></formula><p>where w alpha ∈ R d is a learnable linear output layer. (For simplicity, we omit the dependencies of all probabilities in (10) on both y 1:t−1 and x.)</p><p>The mixture weight balances between how likely it is for the model to simply copy a source token, rather than generating a possibly different token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Exploratory Data Analysis</head><p>B.1 Data Sizes <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the number of available parallel corpora (counting multiple annotations) across data sources. Note that the vertical axis is capped at 100K for a better visual comparison among other sources. For the Lang-8 dataset, we count all available (ranging from 1 to 8) annotations for each of 1.04M original sentences. Also note that we only use the subset of Lang-8 whose source and target sentences are different, leaving only 575K sentences instead of 1.11M. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the distribution of sentence lengths and the number of edits per sentence across different data sources. <ref type="table" target="#tab_16">Table 9</ref> includes our permutation test 11 results on the number of edits per sentence, normalized 9 In practice, this involves adding up the copy scores defined for each source token into a |V |-dimensional vector, using commands such as scatter add() in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Sentence Length vs. Number of Edits</head><p>10 When computing the mixture weight α copy t , Zhao et al. (2019) applies a linear layer to H encs t, wherest are the attention scores in (6) before taking softmax. Our formulation gives essentially the same copying mechanism, while being more compatible to standard Transformer implementations. <ref type="bibr">11</ref> We used the off-the-shelf mlxtend package to run permutation tests. See http://rasbt.github.io/ mlxtend/user_guide/evaluate/permutation_ test/. by sentence length (i.e., number of word-level tokens), between training data sources. Using an approximate permutation test with 10k simulations and a significant level of α = 0.05, we find that there is a statistical difference in the normalized edit count per sentence between the W&amp;I training set and each of FCE, NUCLE, and Lang-8. This serves as a preliminary experiment showing how the distribution of grammatical errors can be significantly different across different sources -even when they belong to a roughly similar domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Noising Algorithm</head><p>Algorithms 1 and 2 detail our noising scenarios.  : Sentence length versus the number of edits made in each sentence, across all training data sources for the Restricted Track. The horizontal axis is capped at 100 words (less than 0.02% of all sentences contain more than 100 words). The vertical axis is capped at 40 edits (less than 0.02% of all sentences contain more than 30 edits).  from a different source than the actual test set (CoNLL-2014) -despite the fact that both datasets have similar domains (personal essays from English students), they can still have many other different characteristics, including the writer's English proficiency and annotation styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results on error categories</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Training Details</head><p>Our model training is a three-stage process: DAE pre-training, training, and fine-tuning, except in Low Resource Track where there is no fine-tuning data. At each step, we train a model until its ERRANT score on the development set reaches convergence, and use the learned weights as ini-   <ref type="table">Table 11</ref>: Training progress on CoNLL-2014. No W&amp;I+Locness datasets were used in these results. 'b' and 'c' refer to the base and copy configurations of the Transformer, respectively. Evaluation is done using the MaxMatch (M 2 ) scorer. Pre-processing &amp; postprocessing are included before the first step and after the last step, respectively. tial values for the next step. For pre-training, we used a learning rate of 5 · 10 −4 for the base and copy-augmented Transformers and 10 −3 for the large Transformer. For training, we reset the optimizer and set the learning rate to 10 −4 . For finetuning (if available), we again reset the optimizer and set the learning rate to 5 · 10 −5 . In all training steps, we used the Adam (Kingma and <ref type="bibr">Ba, 2015)</ref> optimizer with the inverse square-root schedule and a warmup learning rate of 10 −7 , along with a dropout rate of 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Further Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Effect of Copying Mechanisms &amp; Ensembles</head><p>One of our contributions is to highlight the benefit of ensembling multiple models with diverse characteristics. As shown in <ref type="table" target="#tab_4">Table 3</ref>, the final ensemble step involving different types of models was crucial for our model's performance, improving the test score by over 6 F 0.5 points. We first no-  ticed that the copy-augmented Transformer learns to be more conservative -i.e., higher precision but lower recall given similar overall scores -in its edits than the vanilla Transformer, presumably because the model includes an inductive bias that favors copying (i.e., not editing) the input token via its copy attention scores. <ref type="table" target="#tab_2">Table 12</ref> shows this phenomenon for Restricted Track. Given multiple models with diverse characteristics, the choice of models for ensemble can translate to controlling how conservative we want our final model to be. For example, combining one vanilla model with multiple independent copyaugmented models will result in a more conservative model. This could serve as an alternative to other methods that control the precision-recall ratio, such as the edit-weighted loss <ref type="bibr">(Junczys-Dowmunt et al., 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overall pipeline for our approach. Gray shaded box includes the training steps for a seq2seq model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Data size per source for all Restricted Track training data. Number includes multiple annotations for Lang-8. Vertical axis is capped at 100K for a better visual comparison among the smaller sources. The three FCE splits(train, dev, test)  are collectively used for training, and the three W&amp;I+L splits correspond to three English proficiency levels ("A", "B", "C"). After duplicate removal, only 575K of the Lang-8 parallel corpus are actually used for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure 3: Sentence length versus the number of edits made in each sentence, across all training data sources for the Restricted Track. The horizontal axis is capped at 100 words (less than 0.02% of all sentences contain more than 100 words). The vertical axis is capped at 40 edits (less than 0.02% of all sentences contain more than 30 edits).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Datasets used for each set of results. For the W&amp;I+L development set, Dev-3K and Dev-1K respectively indicate a 3:1 train-test random split of the development set, such that the original proportions of English proficiency (A, B, C, N) are kept the same in each split. SeeTable 1for more information about each dataset.</figDesc><table><row><cell cols="5">Sequential Transfer Learning Using (copy) Transformers</cell></row><row><cell>Pre-processing • Fix tokenization errors • Spellcheck • BPE segmentation</cell><cell>Pre-training (DAE) • Error extraction • Perturbation</cell><cell>Training</cell><cell>Fine-tuning (optional)</cell><cell>Post-processing • &lt;unk&gt; edit removal • Re-rank • Error type control</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>DAE Pre-train 48.58 24.92 40.82 +21.39 58.33 44.20 54.82 +22.13 + Train 54.30 28.67 46.07 + 5.25 66.05 50.72 62.28 + 7.46 + Fine-tune 54.34 32.15 47.75 + 1.68 66.02 53.41 63.05 + 0.77 + Ensemble (5) 63.54 31.48 52.79 + 5.04 76.19 50.25 69.06 + 6.01</figDesc><table><row><cell>Steps</cell><cell>P</cell><cell>W&amp;I+L Dev R F0.5</cell><cell>∆</cell><cell>P</cell><cell>W&amp;I+L Test R F0.5</cell><cell>∆</cell></row><row><cell>Spellcheck</cell><cell>59.28</cell><cell>5.27 19.43</cell><cell>n/a</cell><cell cols="2">68.77 10.55 32.69</cell><cell>n/a</cell></row><row><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ACL 2019 BEA Workshop Restricted Track results. For each training step, we only list results from the model configuration that achieved the best F 0.5 test set score. All evaluation is done using ERRANT's span-based correction scorer. Pre-processing and post-processing are included in the first step and last steps, respectively.</figDesc><table><row><cell>Steps</cell><cell>P</cell><cell>W&amp;I+L Dev-1K R F0.5</cell><cell>∆</cell><cell>P</cell><cell>W&amp;I+L Test R F0.5</cell><cell>∆</cell></row><row><cell>Spellcheck</cell><cell>61.88</cell><cell>5.29 19.72</cell><cell>n/a</cell><cell cols="2">68.77 10.55 32.69</cell><cell>n/a</cell></row><row><cell cols="7">+ DAE Pre-train 46.26 19.84 36.53 +16.81 57.14 37.46 51.71 +19.02</cell></row><row><cell>+ Train</cell><cell cols="6">47.97 30.91 43.20 + 6.67 58.60 47.47 55.98 + 4.27</cell></row><row><cell>+ Ensemble (4)</cell><cell cols="6">58.89 26.68 47.02 + 5.75 69.69 41.76 61.47 + 5.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>ACL 2019 BEA Workshop Low Resource Track results. For each training step, we only list results from the model configuration that achieved the best F 0.5 test set score. All evaluation is done using ERRANT's span- based correction scorer. Note that 3K examples from the W&amp;I+Locness development set ("W&amp;I+L Dev-3K")</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>, we show the performance of our ap- proach on the CoNLL-2014 (Ng et al., 2014) dataset, with and without the newly released W&amp;I+L dataset.7 We also list some of the state- of-the-art 8 results prior to the shared task: copy- augmented Transformers pre-trained on random error denoising (Zhao et al., 2019), Transform- ers pre-trained on Wikipedia revisions and round- trip translations (Lichtarge et al., 2019), hybrid statistical and neural machine translation systems (Junczys-Dowmunt et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on CoNLL-2014 as point of comparison. "W&amp;I+L" indicates whether the approach made use of the (newly released) W&amp;I+L dataset. Evaluation is done using the MaxMatch (M 2 ) scorer, rather than ERRANT. Pre-processing &amp; post-processing are included before the first step and after the last step, respectively. See §7.7 for details and references.</figDesc><table><row><cell>Step</cell><cell cols="2">Ours Random</cell><cell>∆</cell></row><row><cell>DAE</cell><cell>54.82</cell><cell>32.25</cell><cell>+22.57</cell></row><row><cell>+ Train</cell><cell>62.28</cell><cell>57.00</cell><cell>+ 5.28</cell></row><row><cell cols="2">+ Fine-tune 63.05</cell><cell>60.22</cell><cell>+ 2.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of realistic and random error generation on Restricted Track. ∆ means the difference between Ours and Random.</figDesc><table><row><cell>Step</cell><cell cols="2">Ours Random</cell><cell>∆</cell></row><row><cell>DAE</cell><cell>51.71</cell><cell>32.01</cell><cell>+19.70</cell></row><row><cell cols="2">+ Train 55.98</cell><cell>35.44</cell><cell>+20.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Comparison of realistic and random error gen- eration on Low Resource Track. ∆ means the differ- ence between Ours and Random.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>LM 65.14 8.85 28.67 +5.07 Ours 68.77 10.55 32.69 +4.02</figDesc><table><row><cell>Spellchecker</cell><cell>P</cell><cell>W&amp;I+L Test R F0.5</cell><cell>∆</cell></row><row><cell>Hunspell</cell><cell cols="2">53.59 7.29 23.60</cell><cell>n/a</cell></row><row><cell>Hunspell +</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Effect of incorporating context into a standard spellchecker.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1243-1252. JMLR. org.</figDesc><table><row><cell>Li. 2016. Incorporating copying mechanism in Papers), pages 1073-1083, Vancouver, Canada. As-Sebastian Ruder. 2019. Neural Transfer Learning for Natural Language Processing. Ph.D. thesis, NA-TIONAL UNIVERSITY OF IRELAND, GALWAY. Keisuke Sakaguchi, Matt Post, and Benjamin Jonas Gehring, Roman Grundkiewicz and Marcin Junczys-Dowmunt. 2018. Near human-level performance in grammat-ical error correction with hybrid machine transla-tion. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), volume 2, pages 284-290. Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Van Durme. 2017. Grammatical error correction with neural reinforcement learning. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pages 366-372. Abigail See, Peter J. Liu, and Christopher D. Man-ning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long</cell><cell>Shamil Chollampatt and Hwee Tou Ng. 2018a. A multilayer convolutional encoder-decoder neu-ral network for grammatical error correction. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, Phrase-based &amp; neural unsupervised machine trans-February 2-7, 2018, pages 5755-5762. Shibamouli Lahiri. 2014. Complexity of Word Collo-cation Networks: A Preliminary Structural Analysis. In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 619-628, New Orleans, Louisiana. Associa-tion for Computational Linguistics. 96-105, Gothenburg, Sweden. Association for Com-putational Linguistics. Guillaume Lample, Alexis Conneau, Ludovic De-noyer, and Marc'Aurelio Ranzato. 2018a. Unsu-Helen Yannakoudakis, Øistein E Andersen, Ardeshir Geranpayeh, Ted Briscoe, and Diane Nicholls. 2018. Developing an automated writing placement system for esl learners. Applied Measurement in Education, 31(3):251-267. pervised machine translation using monolingual cor-pora only. In International Conference on Learning Representations. Guillaume Lample, Myle Ott, Alexis Conneau, Lu-dovic Denoyer, and Marc'Aurelio Ranzato. 2018b. Wei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and Jingming Liu. 2019. Improving grammatical error correction via pre-training a copy-augmented archi-tecture with unlabeled data. In NAACL.</cell></row><row><cell>sequence-to-sequence learning. sociation for Computational Linguistics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1631-1640, Berlin, Germany. Asso-Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Improving neural machine translation models with monolingual data. In Proceedings ciation for Computational Linguistics. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings in deep residual net-works. In European conference on computer vision, of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86-96, Berlin, Germany. Association for Computational Linguistics. pages 630-645. Springer. Matthew Honnibal and Ines Montani. 2017. spacy 2: Natural language understanding with bloom embed-Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation of rare words with subword units. In ACL.</cell><cell>lation. In Proceedings of the 2018 Conference on Shamil Chollampatt and Hwee Tou Ng. 2018b. Neu-ral quality estimation of grammatical error correc-tion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2528-2539. Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner Empirical Methods in Natural Language Processing, pages 5039-5049, Brussels, Belgium. Association for Computational Linguistics. Jared Lichtarge, Chris Alberti, Shankar Kumar, Noam Shazeer, Niki Parmar, and Simon Tong. 2019. Cor-pora generation for grammatical error correction. arXiv preprint arXiv:1904.05780. english: The nus corpus of learner english. In Proceedings of the eighth workshop on innovative use of NLP for building educational applications, pages 22-31. Jared Lichtarge, Christopher Alberti, Shankar Kumar, Noam Shazeer, and Niki Parmar. 2018. Weakly su-pervised grammatical error correction using iterative</cell></row><row><cell>dings, convolutional neural networks and incremen-Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. tal parsing. To appear. Robin Jia and Percy Liang. 2016. Data recombina-Sequence to sequence learning with neural net-works. In NIPS. tion for neural semantic parsing. In Proceedings Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12-22, Berlin, Germany. Association for Computational Linguistics. sumoto. 2012. Tense and aspect error correction for esl learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</cell><cell>Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In Proceedings of the decoding. arXiv preprint arXiv:1811.01710. Thang Luong, Hieu Pham, and Christopher D. Man-ning. 2015. Effective approaches to attention-based 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 933-941, International Convention Centre, Sydney, Australia. PMLR. Mariano Felice and Zheng Yuan. 2014. Generat-neural machine translation. In EMNLP. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.</cell></row><row><cell>Marcin Junczys-Dowmunt and Roman Grundkiewicz. 2016. Phrase-based machine translation is state-pages 198-202. Association for Computational Lin-guistics. of-the-art for automatic grammatical error correc-tion. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1546-1556, Austin, Texas. Association for Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez, and Ł ukasz Kaiser. 2017. Attention is all you need. In NIPS. Computational Linguistics. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Marcin Junczys-Dowmunt, Roman Grundkiewicz, Pierre-Antoine Manzagol. 2008. Extracting and Shubha Guha, and Kenneth Heafield. 2018. Ap-composing robust features with denoising autoen-proaching neural grammatical error correction as coders. In Proceedings of the 25th international a low-resource machine translation task. In conference on Machine learning, pages 1096-1103. Proceedings of the 2018 Conference of the ACM.</cell><cell>ing artificial errors for grammatical error correction. In Proceedings of the Student Research Workshop Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-gata, and Yuji Matsumoto. 2011. Mining revision at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116-126, Gothenburg, Sweden. Association for Computational Linguistics. Michael Flor and Yoko Futagi. 2012. log of language learning sns for automated japanese error correction of second language learners. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 147-155. On us-Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian ing context for automatic correction of non-word Hadiwinoto, Raymond Hendy Susanto, and Christo-misspellings in student essays. In Proceedings pher Bryant. 2014. The conll-2014 shared task on of the seventh workshop on building educational grammatical error correction. In Proceedings of the applications Using NLP, pages 105-115. Associa-Eighteenth Conference on Computational Natural tion for Computational Linguistics. Language Learning: Shared Task, pages 1-14.</cell></row><row><cell>North American Chapter of the Association for Computational Linguistics: Dong Wang and Thomas Fang Zheng. 2015. Transfer Human Language learning for speech and language processing. Technologies, Volume 1 (Long Papers), volume 1, In 2015 Asia-Pacific Signal and Information pages 595-606. Processing Association Annual Summit and</cell><cell>Tao Ge, Furu Wei, and Ming Zhou. 2018a. Fluency Myle Ott, Sergey Edunov, Alexei Baevski, Angela boost learning and inference for neural grammatical Fan, Sam Gross, Nathan Ng, David Grangier, and error correction. In Proceedings of the 56th Annual Michael Auli. 2019. fairseq: A fast, extensi-Meeting of the Association for Computational ble toolkit for sequence modeling. arXiv preprint</cell></row><row><cell>Diederik P Kingma and Jimmy Ba. 2015. Adam: A Conference (APSIPA), pages 1225-1237. IEEE.</cell><cell>Linguistics (Volume 1: Long Papers), volume 1, arXiv:1904.01038.</cell></row><row><cell>method for stochastic optimization. ICLR.</cell><cell>pages 1055-1065.</cell></row><row><cell>Ziang Xie, Guillaume Genthial, Stanley Xie, Andrew</cell><cell>Adam Paszke, Sam Gross, Soumith Chintala, Gre-</cell></row><row><cell>Taku Kudo and John Richardson. 2018. Sentencepiece: Ng, and Dan Jurafsky. 2018. Noising and denois-</cell><cell>Tao Ge, Furu Wei, and Ming Zhou. 2018b. Reaching gory Chanan, Edward Yang, Zachary DeVito, Zem-</cell></row><row><cell>A simple and language independent subword tok-ing natural language: Diverse backtranslation for</cell><cell>human-level performance in automatic grammatical ing Lin, Alban Desmaison, Luca Antiga, and Adam</cell></row><row><cell>enizer and detokenizer for neural text processing. grammar correction. In Proceedings of the 2018</cell><cell>error correction: An empirical study. arXiv preprint Lerer. 2017. Automatic differentiation in pytorch.</cell></row><row><cell>arXiv preprint arXiv:1808.06226. Conference of the North American Chapter of the</cell><cell>arXiv:1807.01270. In NIPS-W.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10</head><label>10</label><figDesc>shows the result on error categories.InTable 11, we include the training progress for our result forCoNLL-2014.   A noticeable difference between this result and our results for Restricted Track and Low Resource Track is that adaptation via fine-tuning is not necessarily effective here. We hypothesize that this is mostly due to the fact that the training subset to which we fine-tune our model (NUCLE) comes Pseudocode for generating noisy sentences function CHANGE TYPE(word, prob) preposition set = [∅, for, to, at, · · · ] if random[0, 1] &gt; prob then return word else if word in preposition set then random choose one from(preposition set) else if word is Noun then change number(word) else if word is Verb then change form(word)</figDesc><table><row><cell>E CoNLL-2014 Full Results</cell></row><row><cell>(Without Using W&amp;I+L)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Comparing the average number of edits per sentence, normalized by sentence length, between the W&amp;I training set and other available training data sources for the Restricted Track. "vs. W&amp;I" refers to the result of an approximate permutation test (10k rounds) against that in the W&amp;I training set. Under the significance level of α = 0.05, the number for FCE, NUCLE, and Lang-8 are all significantly different from that for the W&amp;I training set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Results on error types.</figDesc><table><row><cell>Steps</cell><cell>P</cell><cell>CoNLL-2014 R F0.5</cell></row><row><cell>Spellcheck</cell><cell cols="2">54.75 5.75 20.25</cell></row><row><cell>+ Pre-train (b)</cell><cell cols="2">54.76 15.09 35.89</cell></row><row><cell>+ Train (b)</cell><cell cols="2">60.43 34.22 52.40</cell></row><row><cell>+ Fine-tune (b)</cell><cell cols="2">60.81 33.32 52.20</cell></row><row><cell>+ Pre-train (c)</cell><cell cols="2">65.81 24.17 48.95</cell></row><row><cell>+ Train (c)</cell><cell cols="2">61.38 30.97 51.30</cell></row><row><cell>+ Fine-tune (c)</cell><cell cols="2">60.82 32.50 51.79</cell></row><row><cell cols="3">+ Ensemble (b+c) 71.11 32.56 57.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>Single-model ERRANT scores for Restricted Track, using a large Transformer and a copyaugmented Transformer.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Pre-training a Denoising Autoencoder on Realistic Grammatical ErrorsGiven the relative lack of parallel corpora for the GEC task, it is important to define a realistic pretraining task, from which the learned knowledge can transfer to an improved performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is analogous to the NUCLE dataset matching "perfectly" with the CoNLL dataset, as noted in(Junczys- Dowmunt et al., 2018).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/pytorch/fairseq 4 https://tatoeba.org/eng/downloads</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">As in previous results, we remove all duplicates but take multiple annotations (if available) the Lang-8 dataset, leaving only 575K parallel examples. 6 See Appendix B for an exploratory data analysis.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correcting esl errors using phrasal smt techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Andersen, and Ted Briscoe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Øistein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shared Task on Grammatical Error Correction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 14th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic annotation and evaluation of error types for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward John</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Connecting the dots: Towards human-level grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 12th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
