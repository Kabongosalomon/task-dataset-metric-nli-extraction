<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Locating Objects Without Bounding Boxes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ribera</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video and Image Processing Laboratory (VIPER)</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Güera</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video and Image Processing Laboratory (VIPER)</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video and Image Processing Laboratory (VIPER)</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Video and Image Processing Laboratory (VIPER)</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Locating Objects Without Bounding Boxes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in convolutional neural networks (CNN) have achieved remarkable results in locating objects in images. In these networks, the training procedure usually requires providing bounding boxes or the maximum number of expected objects. In this paper, we address the task of estimating object locations without annotated bounding boxes which are typically hand-drawn and time consuming to label. We propose a loss function that can be used in any fully convolutional network (FCN) to estimate object locations. This loss function is a modification of the average Hausdorff distance between two unordered sets of points. The proposed method has no notion of bounding boxes, region proposals, or sliding windows. We evaluate our method with three datasets designed to locate people's heads, pupil centers and plant centers. We outperform state-of-the-art generic object detectors and methods fine-tuned for pupil tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Locating objects in images is an important task in computer vision. A common approach in object detection is to obtain bounding boxes around the objects of interest. In this paper, we are not interested in obtaining bounding boxes. Instead, we define the object localization task as obtaining a single 2D coordinate corresponding to the location of each object. The location of an object can be any key point we are interested in, such as its center. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of localized objects in images. Differently from other keypoint detection problems, we do not know in advance the number of keypoints in the image. To also make the method as generic as possible we do not assume any physical constraint between the points, unlike in cases such as pose estimation. This definition of object localization is more appropriate for applications where objects are very small, or substantially overlap (see the overlapping plants in <ref type="figure" target="#fig_0">Figure 1</ref>). In these cases, bounding boxes may not be provided by the dataset or they may be infeasible to groundtruth.</p><p>Bounding-box annotation is tedious, time-consuming and expensive <ref type="bibr" target="#b36">[37]</ref>. For example, annotating ImageNet <ref type="bibr" target="#b42">[43]</ref>  required 42 seconds per bounding box when crowdsourcing on Amazon's Mechanical Turk using a technique specifically developed for efficient bounding box annotation <ref type="bibr" target="#b49">[50]</ref>. In <ref type="bibr" target="#b5">[6]</ref>, Bell et al. introduce a new dataset for material recognition and segmentation. By collecting click location labels in this dataset instead of a full per-pixel segmentation, they reduce the annotation costs an order of magnitude.</p><p>In this paper, we propose a modification of the average Hausdorff distance as a loss function of a CNN to estimate the location of objects. Our method does not require the use of bounding boxes in the training stage, and does not require to know the maximum number of objects when designing the network architecture. For simplicity, we describe our method only for a single class of objects, although it can trivially be extended to multiple object classes. Our method is object-agnostic, thus the discussion in this paper does not include any information about the object characteristics. Our approach maps input images to a set of coordinates, and we validate it with diverse types of objects. We evaluate our method with three datasets. One dataset contains images acquired from a surveillance camera in a shopping mall, and we locate the heads of people. The second dataset contains images of human eyes, and we locate the center of the pupil. The third dataset contains aerial images of a crop field taken arXiv:1806.07564v2 [cs.CV] 3 Apr 2019 from an Unmanned Aerial Vehicle (UAV), and we locate the centers of highly occluded plants.</p><p>Our approach to object localization via keypoint detection is not a universal drop-in replacement for bounding box detection, specially for those tasks that inherently require bounding boxes, such as automated cropping. Also, a limitation of this approach is that bounding box labeling incorporates some sense of scale, while keypoints do not.</p><p>The contributions of our work are:</p><p>• We propose a loss function for object localization, which we name weighted Hausdorff distance (WHD), that overcomes the limitations of pixelwise losses such as L 2 and the Hausdorff distances.</p><p>• We develop a method to estimate the location and number of objects in an image, without any notion of bounding boxes or region proposals.</p><p>• We formulate the object localization problem as the minimization of distances between points, independently of the model used in the estimation. This allows to use any fully convolutional network architectural design.</p><p>• We outperform state-of-the-art generic object detectors and achieve comparable results with crowd counting methods without any domain-specific knowledge, data augmentation, or transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generic object detectors. Recent advances in deep learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> have increased the accuracy of localization tasks such as object or keypoint detection. By generic object detectors, we mean methods that can be trained to detect any object type or types, such as Faster-RCNN <ref type="bibr" target="#b14">[15]</ref>, Single Shot MultiBox Detector (SSD) <ref type="bibr" target="#b30">[31]</ref>, or YOLO <ref type="bibr" target="#b39">[40]</ref>. In Fast R-CNN, candidate regions or proposals are generated by classical methods such as selective search <ref type="bibr" target="#b58">[59]</ref>. Although activations of the network are shared between region proposals, the system cannot be trained end-to-end. Region Proposal Networks (RPNs) in object detectors such as Faster R-CNN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref> allow for end-to-end training of models. Mask R-CNN <ref type="bibr" target="#b17">[18]</ref> extends Faster R-CNN by adding a branch for predicting an object mask but it runs in parallel with the existing branch for bounding box recognition. Mask R-CNN can estimate human pose keypoints by generating a segmentation mask with a single class indicating the presence of the keypoint. The loss function in Mask R-CNN is used location by location, making the keypoint detection highly sensitive to alignment of the segmentation mask. SDD provides fixed-sized bounding boxes and scores indicating the presence of an object in the boxes. The described methods either require groundtruthed bounding boxes to train the CNNs or require to set the maximum number of objects in the image being analyzed. In <ref type="bibr" target="#b18">[19]</ref>, it is observed that generic object detectors such as Faster R-CNN and SSD perform very poorly for small objects.</p><p>Counting and locating objects. Counting the number of objects in an image is not a trivial task. In <ref type="bibr" target="#b27">[28]</ref>, Lempitsky et al. estimate a density function whose integral corresponds to the object count. In <ref type="bibr" target="#b46">[47]</ref>, Shao et al. proposed two methods for locating objects. One method first counts and then locates, and the other first locates and then counts.</p><p>Locating and counting people is necessary for many applications such as crowd monitoring in surveillance systems, surveys for new businesses, and emergency management <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b59">60]</ref>. There are multiple studies in the literature, where people in videos of crowds are detected and tracked <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. These detection methods often use bounding boxes around each human as ground truth. Acquiring bounding boxes for each person in a crowd can be labor intensive and imprecise under conditions where lots of people overlap, such as sports events or rush-hour agglomerations in public transport stations. More modern approaches avoid the need of bounding boxes by estimating a density map whose integral yields the total crowd count. In approaches that involve a density map, the label of the density map is constructed from the labels of the people's heads. This is typically done by centering Gaussian kernels at the location of each head. Zhang et al. <ref type="bibr" target="#b61">[62]</ref> estimate the density image using a multi-column CNN that learns features at different scales. In <ref type="bibr" target="#b43">[44]</ref>, Sam et al. use multiple independent CNNs to predict the density map at different crowd densities. An additional CNN classifies the density of the crowd scene and relays the input image to the appropriate CNN. Huang et al. <ref type="bibr" target="#b19">[20]</ref> propose to incorporate information about the body part structure to the conventional density map to reformulate the crowd counting as a multi-task problem. Other works such as Zhang et al. <ref type="bibr" target="#b60">[61]</ref> use additional information such as the groundtruthed perspective map.</p><p>Methods for pupil tracking and precision agriculture are usually domain-specific. In pupil tracking, the center of the pupil must be resolved in images obtained in real-world illumination conditions <ref type="bibr" target="#b12">[13]</ref>. A wide range of applications, from commercial applications such as video games <ref type="bibr" target="#b51">[52]</ref>, driving <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b16">17]</ref> or microsurgery <ref type="bibr" target="#b13">[14]</ref> rely on accurate pupil tracking. In remote precision agriculture, it is critical to locate the center of plants in a crop field. Agronomists use plant traits such as plant spacing to predict future crop yield <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>, and plant scientists to breed new plant varieties <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, Aich et al. count wheat plants by first segmenting plant regions and then counting the number of plants in each segmented patch.</p><p>Hausdorff distance. The Hausdorff distance can be used to measure the distance between two sets of points <ref type="bibr" target="#b4">[5]</ref>. Modifications of the Hausdorff distance <ref type="bibr" target="#b9">[10]</ref> have been used for various multiple tasks, including character recog-nition <ref type="bibr" target="#b32">[33]</ref>, face recognition <ref type="bibr" target="#b22">[23]</ref> and scene matching <ref type="bibr" target="#b22">[23]</ref>. Schutze et al. <ref type="bibr" target="#b45">[46]</ref> use the average Hausdorff distance to evaluate solutions in multi-objective optimization problems. In <ref type="bibr" target="#b23">[24]</ref>, Elkhiyari et al. compare features extracted by a CNN according to multiple variants of the Hausdorff distance for the task of face recognition. In <ref type="bibr" target="#b10">[11]</ref>, Fan et al. use the Chamfer and Earth Mover's distance, along with a new neural network architecture, for 3D object reconstruction by estimating the location of a fixed number of points. The Hausdorff distance is also a common metric to evaluate the quality of segmentation boundaries in the medical imaging community <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">55</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Average Hausdorff Distance</head><p>Our work is based on the Hausdorff distance which we briefly review in this section. Consider two unordered nonempty sets of points X and Y and a distance metric d(x, y) between two points x ∈ X and y ∈ Y . The function d(·, ·) could be any metric. In our case we use the Euclidean distance. The sets X and Y may have different number of points. Let Ω ⊂ R 2 be the space of all possible points. In its general form, the Hausdorff distance between X ⊂ Ω and Y ⊂ Ω is defined as</p><formula xml:id="formula_0">d H (X, Y ) = max sup x∈X inf y∈Y d(x, y), sup y∈Y inf x∈X d(x, y) .</formula><p>(1) When considering a discretized and bounded Ω, such as all the possible pixel coordinates in an image, the suprema and infima are achievable and become maxima and minima, respectively. This bounds the Hausdorff distance as</p><formula xml:id="formula_1">d(X, Y ) ≤ d max = max x∈Ω,y∈Ω d(x, y),<label>(2)</label></formula><p>which corresponds to the diagonal of the image when using the Euclidean distance. As shown in <ref type="bibr" target="#b4">[5]</ref>, the Hausdorff distance is a metric. Thus ∀X, Y, Z ⊂ Ω we have the following properties:</p><formula xml:id="formula_2">d H (X, Y ) ≥ 0 (3a) d H (X, Y ) = 0 ⇐⇒ X = Y (3b) d H (X, Y ) = d H (Y, X) (3c) d H (X, Y ) ≤ d H (X, Z) + d H (Z, Y )<label>(3d)</label></formula><p>Equation (3b) follows from X and Y being closed, because in our task the pixel coordinate space Ω is discretized. These properties are very desirable when designing a function to measure how similar X and Y are <ref type="bibr" target="#b3">[4]</ref>.</p><p>A shortcoming of the Hausdorff function is its high sensitivity to outliers <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b53">54]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example for two finite sets of points with one outlier. To avoid this, the average Hausdorff distance is more commonly used:</p><formula xml:id="formula_3">d AH (X, Y ) = 1 |X| x∈X min y∈Y d(x, y) + 1 |Y | y∈Y min x∈X d(x, y),<label>(4)</label></formula><p>where |X| and |Y | are the number of points in X and Y , respectively. Note that properties (3a), (3b) and (3c) are still true, but (3d) is not. Also, the average Hausdorff distance is differentiable with respect to any point in X or Y .</p><p>Let Y contain the ground truth pixel coordinates, and X be our estimation. Ideally, we would like to use d AH (X, Y ) as the loss function during the training of our convolutional neural network (CNN). We find two limitations when incorporating the average Hausdorff distance as a loss function. First, CNNs with linear layers implicitly determine the estimated number of points |X| as the size of the last layer. This is a drawback because the actual number of points depends on the content of the image itself. Second, FCNs such as U-Net <ref type="bibr" target="#b41">[42]</ref> can indicate the presence of an object center with a higher activation in the output layer, but they do not return the pixel coordinates. In order to learn with backpropagation, the loss function must be differentiable with respect to the network output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Weighted Hausdorff Distance</head><p>To overcome these two limitations, we modify the average Hausdorff distance as follows:</p><formula xml:id="formula_4">d WH (p, Y ) = 1 S + x∈Ω p x min y∈Y d(x, y)+ 1 |Y | y∈Y M α x∈Ω [ p x d(x, y) + (1 − p x )d max ] ,<label>(5)</label></formula><formula xml:id="formula_5">where S = x∈Ω p x ,<label>(6)</label></formula><formula xml:id="formula_6">M α a∈A [f (a)] = 1 |A| a∈A f α (a) 1 α ,<label>(7)</label></formula><p>is the generalized mean, and is set to 10 −6 . We call d WH (p, Y ) the weighted Hausdorff distance (WHD). p x ∈ [0, 1] is the single-valued output of the network at pixel coordinate x. The last activation of the network can be bounded between zero and one by using a sigmoid nonlinearity. Note that p does not need to be normalized, i.e., x∈Ω p x = 1 is not necessary. Note that the generalized mean M α [·] corresponds to the minimum function when α = −∞. We justify the modifications applied to Equation (4) to obtain Equation <ref type="formula" target="#formula_4">(5)</ref> as follows:</p><p>1. The in the denominator of the first term provides numerical stability when p x ≈ 0 ∀x ∈ Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>When p x = {0, 1}, α = −∞, and = 0, the weighted Hausdorff distance becomes the average Hausdorff distance. We can interpret this as the network indicating with complete certainty where the object centers are. As</p><formula xml:id="formula_7">d WH (p, Y ) ≥ 0, the global minimum (d WH (p, Y ) = 0) corresponds to p x = 1 if x ∈ Y and 0 otherwise.</formula><p>3. In the first term, we multiply by p x to penalize high activations in areas of the image where there is no ground truth point y nearby. In other words, the loss function penalizes estimated points that should not be there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>In the second term, by using the expression</p><formula xml:id="formula_8">f (·) := p x d(x, y) + (1 − p x )d max we enforce that (a) If p x0 ≈ 1, then f (·) ≈ d(x 0 , y)</formula><p>. This means the point x 0 will contribute to the loss as in the AHD (Equation <ref type="formula" target="#formula_3">(4)</ref>).</p><formula xml:id="formula_9">(b) If p x0 ≈ 0, x 0 = y, then f (·) ≈ d max .</formula><p>Then, if α = −∞, the point x 0 will not contribute to the loss because the "minimum" M x∈Ω [ · ] will ignore x 0 . If another point x 1 closer to y with p x1 &gt; 0 exists, x 1 will be "selected" instead by</p><formula xml:id="formula_10">M [ · ]. Otherwise M x∈Ω [ · ]</formula><p>will be high. This means that low activations around ground truth points will be penalized.</p><p>Note that f (·) is not the only expression that would enforce these two constraints (f | px=1 = d(x, y) and f | px=0 = d max ). We chose a linear function because of its simplicity and numerical stability.</p><p>Both terms in the WHD are necessary. If the first term is removed, then the trivial solution is p x = 1 ∀x ∈ Ω. If the second term is removed, then the trivial solution is p x = 0 ∀x ∈ Ω. These two cases hold for any value of α and the proof can be found in the appendix. Ideally, the parameter α → −∞ so that M α (·) = || · || −∞ becomes the minimum operator <ref type="bibr" target="#b25">[26]</ref>. However, this would make the second term flat with respect to the output of the network. For a given y, changes in p x0 in a point x 0 that is far from y would be ignored by M −∞ (·), if there is another point x 1 with high activation and closer to y. In practice, this makes training difficult because the minimum is not a smooth function with respect to its inputs. Thus, we approximate the minimum with the generalized mean M α (·), with α &lt; 0. The more negative α is, the more similar to the AHD the WHD becomes, at the expense of becoming less smooth. In our experiments, α = −1. There is no need to use M α (·) in the first term because p x is not inside the minimum, thus the term is already differentiable with respect to p.</p><p>If the input image needs to be resized to be fed into the network, we can normalize the WHD to account for this distortion. Denote the original image size as (S </p><formula xml:id="formula_11">o /S (1) r 0 0 S (2) o /S (2) r .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Advantage Over Pixelwise Losses</head><p>A naive alternative is to use a one-hot map as label, defined as l x = 1 for x ∈ Y and l x = 0 otherwise, and then use a pixelwise loss such as the Mean Squared Error (MSE) or the L 2 norm, where L 2 (l, p) = ∀x∈Ω |p x − l x | 2 ∝ MSE(l, x). The issue with pixelwise losses is that they are not informative of how close two points x ∈ Ω and y ∈ Y are unless x = y. In other words, it is flat for the vast majority of the pixels, making training unfeasible. This issue is locally mitigated in <ref type="bibr" target="#b57">[58]</ref> by using the MSE loss with Gaussians centered at each x ∈ Y . By contrast, the WHD in Equation <ref type="formula" target="#formula_4">(5)</ref> will decrease the closer x is to y, making the loss function informative outside of the global minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CNN Architecture And Location Estimation</head><p>In this section, we describe the architecture of the fully convolutional network (FCN) we use, and how we estimate the final object locations. We want to emphasize that the network design is not a meaningful contribution of this work, thus we have not made any attempt to optimize it. Our main contribution is the use of the weighted Hausdorff distance as the loss function. We adopt the U-Net architecture <ref type="bibr" target="#b41">[42]</ref> and modify it minimally for this task. Networks similar to U-Net have been proven to be capable of accurately mapping the input image into an output image, when trained in a conditional adversarial network setting <ref type="bibr" target="#b21">[22]</ref> or when using a carefully tuned loss function <ref type="bibr" target="#b41">[42]</ref>. <ref type="figure">Figure 3</ref> shows the <ref type="figure">Figure 3</ref>. The FCN architecture used for object localization, minimally adapted from the U-Net <ref type="bibr" target="#b41">[42]</ref> architecture. We add a small fully-connected layer that combines the deepest features and the estimated probability map to regress the number of points.</p><p>hourglass design of U-Net. The residuals connections between each layer in the encoder and its symmetric layer in the decoder are not shown for simplicity.</p><p>This FCN has two well differentiated blocks. The first block follows the typical architecture of a CNN. It consists of the repeated application of two 3 × 3 convolutions (with padding 1), each followed by a batch normalization operation and a Rectified Linear Unit (ReLU). After the ReLU, we apply a 2 × 2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels, starting with 64 channels and using 512 channels for the last 5 layers.</p><p>The second block consists of repeated applications of the following elements: a bilinear upsampling, a concatenation with the feature map from the downsampling block, and two 3 × 3 convolutions, each followed by a batch normalization and a ReLU. The final layer is a convolution layer that maps to the single-channel output of the network, p.</p><p>To estimate the number of objects in the image, we add a branch that combines the information from the deepest level features and also from the estimated probability map. This branch combines both features (the 1 × 1 × 512 feature vector and the 256 × 256 probability map) into a hidden layer, and uses the 128-dimensional feature vector to output a single number. We then apply a ReLU to ensure the output is positive, and round it to the closest integer to obtain our final estimate of the number of objects,Ĉ.</p><p>Although we use this particular network architecture, any other architecture could be used. The only requirement is that the output images of the network must be of the same size as the input image. The choice of a FCN arises from the natural interpretation of its output as the weights (p x ) in the WHD (Equation <ref type="formula" target="#formula_4">(5)</ref>). In previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>, variants of the average Haussdorf distance were successfully used with non-FCN networks that estimate the point set directly. However, in those cases the size of the estimated set is fixed by the size of the last layer. To locate an unknown number of objects, the network must be able to estimate a variable number of object locations. Thus, we could envision the WHD also being used in non-FCN networks as long as the output of the network is used as p in Equation <ref type="bibr" target="#b4">(5)</ref>.</p><p>The training loss we use to train the network is a combination of Equation (5) and a smooth L 1 loss for the regression of the object count. The final training loss is</p><formula xml:id="formula_13">L(p, Y ) = d W H (p, Y ) + L reg (C −Ĉ(p)),<label>(9)</label></formula><p>where Y is the set containing the ground truth coordinates of the objects in the image, p is the output of the network, C = |Y |, andĈ(p) is the estimated number of objects. L reg (·) is the regression term, for which we use the smooth L 1 or Huber loss <ref type="bibr" target="#b20">[21]</ref>, defined as</p><formula xml:id="formula_14">L reg (x) = 0.5x 2 , for|x| &lt; 1 |x| − 0.5, for|x| ≥ 1<label>(10)</label></formula><p>This loss is robust to outliers when the regression error is high, and at the same time is differentiable at the origin. The network outputs a saliency map p indicating with p x ∈ [0, 1] the confidence that there is an object at pixel x. <ref type="figure" target="#fig_3">Figure 4</ref> shows p in the second row. During evaluation, our ultimate goal is to obtainŶ , i. e., the estimate of all object locations. In order to convert p toŶ , we threshold p to obtain the pixels T = {x ∈ Ω | p x &gt; τ }. We can use three different methods to decide which τ to use:</p><p>1. Use a constant τ for all images.</p><p>2. Use Otsu thresholding <ref type="bibr" target="#b35">[36]</ref> to find an adaptive τ different for every image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Use a Beta mixture model-based thresholding (BMM).</head><p>This method fits a mixture of two Beta distributions to the values of p using the algorithm described in <ref type="bibr" target="#b44">[45]</ref>, and then takes the mean value of the distribution with highest mean as τ . <ref type="figure" target="#fig_3">Figure 4</ref> shows in the third row an example of the result of thresholding the saliency map p. Then, we fit a Gaussian mixture model to the points T . This is done using the expectation maximization (EM) <ref type="bibr" target="#b33">[34]</ref> algorithm and the estimated number of plantsĈ. The means of the fitted Gaussians are considered the final estimateŶ . The third row of <ref type="figure" target="#fig_3">Figure 4</ref> shows the estimated object locations with red crosses. Note that even if the map produced by the FCN is of good quality, i.e., there is a cluster on each object location, EM may not yield the correct object locations if |Ĉ − C| &gt; 0.5. An example can be observed in the first column of <ref type="figure" target="#fig_3">Figure 4</ref>, where a single head is erroneously estimated as two heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>We evaluate our method with three datasets. The first dataset consists of 2,000 images acquired from a surveillance camera in a shopping mall.</p><p>It contains annotated locations of the heads of the crowd.</p><p>This dataset is publicly available at http://personal.ie.cuhk.edu.hk/˜ccloy/ downloads_mall_dataset.html <ref type="bibr" target="#b31">[32]</ref>. 80%, 10% and 10% of the images were randomly assigned to the training, validation, and testing datasets, respectively.</p><p>The second dataset is presented in <ref type="bibr" target="#b12">[13]</ref> with the roman letter V and publicly available at http://www.ti.uni-tuebingen.de/ Pupil-detection.1827.0.html.</p><p>It contains 2,135 images with a single eye, and the goal is to detect the center of the pupil. It was also randomly split into training, validation and testing datasets as 80/10/10 %, respectively.</p><p>The third dataset consists of aerial images of a crop field taken from a UAV flying at an altitude of 40 m. The images were stitched together to generate a 6, 000 × 12, 000 orthoimage of 0.75 cm/pixel resolution shown in <ref type="figure" target="#fig_4">Figure 5</ref>. The location of the center of all plants in this image was groundtruthed, resulting in a total of 15,208 unique plant centers. This mosaic image was split, and the left 80% area was used for training, the middle 10% for validation, and the right 10% for testing. Within each region, random image crops were generated. These random crops have a uniformly distributed height and width between 100 and 600 pixels. We extracted 50,000 random image crops in the training region, 5, 000 in the validation region, and 5, 000 in the testing region. Note that some of these crops may highly overlap. We are making the third dataset publicly available at https://engineering.purdue.edu/ sorghum/dataset-plant-centers-2016. We believe this dataset will be valuable for the community, as it poses a challenge due to the high occlusion between plants.</p><p>All the images were resized to 256 × 256 because that is the minimum size our architecture allows. The groundtruthed object locations were also scaled accordingly. As for data augmentation, we only use random horizontal flip. For the plant dataset, we also flipped the images vertically. We set α = −1 in Equation <ref type="bibr" target="#b6">(7)</ref>. We have also experimented with α = −2 with no apparent improvement, but we did not attempt to find an optimal value. We retrain the network for every dataset, i.e., we do not use pretrained weights. For the mall and plant dataset, we used a batch size of 32 and Adam optimizer <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref> with a learning rate of 10 −4 and momentum of 0.9. For the pupil dataset, we reduced the size of the network by removing the five central layers, we used a batch size of 64, and stochastic gradient descent with a learning rate of 10 −3 and momentum of 0.9. At the end of each epoch, we evaluate the average Haussdorf distance (AHD) in Equation (4) over the validation set, and select the epoch with lowest AHD on validation.</p><p>As metrics, we report Precision, Recall, F-score, AHD, Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percent Error (MAPE):</p><formula xml:id="formula_15">MAE = 1 N N i=1 |e i |, RMSE = 1 N N i=1 e i 2<label>(11)</label></formula><formula xml:id="formula_16">MAPE = 100 1 N N i=1 Ci =0 e i C i<label>(12)</label></formula><p>where e i =Ĉ i −C i , N is the number of images, C i is the true object count in the i-th image, andĈ i is our estimate.</p><p>A true positive is counted if an estimated location is at most at distance r from a ground truth point. A false positive is counted if an estimated location does not have any ground truth point at a distance at most r. A false negative is counted if a true location does have any estimated location at a distance at most r. Precision is the proportion of our estimated points that are close enough to a true point. Recall is the proportion of the true points that we are able to detect. The F-score is the harmonic mean of precision and recall. Note that one can achieve a precision and recall of 100% even if we estimate more than one object location per ground truth point. This would not be an ideal localization. To take this into account, we also report metrics (MAE, RMSE and MAPE) that indicate if the number of objects is incorrect. The AHD can be interpreted as the average location error in pixels. <ref type="figure">Figure 8</ref> shows the F-score as a function of r. Note that r is only an evaluation parameter. It is not needed during training or testing. MAE, RMSE, and MAPE are shown in <ref type="table">Table 1</ref>. Note that we are using the same architecture for all tasks, except for the pupil dataset, where we removed intermediate layers. Also, in the case of the pupil detection, we know that there is always one object in the image. Thus, regression is not necessary and we can remove the regression term in Equation <ref type="formula" target="#formula_13">(9)</ref> and fixĈ i = C i = 1 ∀i.</p><p>A naive alternative approach to object localization would be to use generic object detectors such as Faster R-CNN <ref type="bibr" target="#b40">[41]</ref>. One can train these detectors by constructing bounding boxes with fixed size centered at each labeled point. Then the center of each bounding box can be taken as the estimated location. We used bounding boxes of size 20 × 20 (the approximate average head and pupil size) and anchor sizes of 16 × 16 and 32 × 32. Note that these parameters may be suboptimal even though they were selected to match the type of object. The threshold we used for the softmax scores was 0.5 and for the intersection over union it was 0.4, because they minimize the AHD over the validation set. We used the VGG-16 architecture <ref type="bibr" target="#b48">[49]</ref> and trained it using stochastic gradient descent with learning rate of 10 −3 and momentum of 0.9. For the pupil dataset, we always selected the bounding box with the highest score. We experimentally observed that Faster R-CNN struggles with detecting very small objects that are very close to each other. <ref type="table">Tables 2-4</ref> show the results of Faster R-CNN results on the mall, pupil, and plant datasets. Note that the mall and plant datasets, with many small and highly overlapping objects, are the most challenging for Faster R-CNN. This behaviour is consistent with the observations in <ref type="bibr" target="#b18">[19]</ref>, where, all generic object detectors perform very poorly and Faster R-CNN yields a mean Average Precision (mAP) of 5% in the best case.</p><p>We also experimented using mean shift <ref type="bibr" target="#b8">[9]</ref> instead of Gaussian mixtures (GM) to detect the local maxima. However, mean shift is prone to detect multiple local maxima, and GMs are more robust against outliers. In our experiments, we observed that precision and recall were substantially worse than using GM. More importantly, using Mean  Shift slowed down validation an order of magnitude. The average time for the Mean Shift algorithm to run on one of our images was 12 seconds, while fitting GM using expectation maximization took around 0.5 seconds, when using the scikit-learn implementations <ref type="bibr" target="#b37">[38]</ref>.</p><p>We also investigated the effect of the parameter τ , and the three methods to select it presented in Section 5. One may think that this parameter could be a trade-off between some metrics, and that it should be cross-validated. In practice, we observed that τ does not balance precision and recall, thus a precision-recall curve is not meaningful. Instead, we plot the F-score as a function of r in <ref type="figure">Figure 8</ref>. Also, cross-validating τ would imply fixing an "optimal" value for all images. <ref type="figure" target="#fig_5">Figure 6</ref> shows that we can do better with adaptive thresholding methods (Otsu or BMM). Note that BMM thresholding (dashed lines) always outperforms Otsu (solid lines), and most of fixed τ . To justify the appropriateness of the BMM method, note that in <ref type="figure" target="#fig_3">Figure 4</ref> most of the values in the estimated map are very high or very low. This makes a Beta distribution a better fit than a Normal distribution (as used in Otsu's method) to model p x . <ref type="figure" target="#fig_6">Figure 7</ref> shows the fitted BMM and a kernel density estimation of the values of τ adaptively selected by the BMM method. <ref type="figure">Figure 8</ref>. F-score as a function of r, the maximum distance between a true and an estimated object location to consider it correct or incorrect. A higher r makes correctly locating an object easier. <ref type="table">Table 1</ref>. Results of our method for object localization, using r = 5. Metrics are defined in Equations (4), (11)- <ref type="bibr" target="#b11">(12)</ref>. Regression metrics for the pupil dataset are not shown because there is always a single pupil (Ĉ = C = 1). <ref type="figure">Figure 8</ref> shows the F-score for other r values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Mall dataset Lastly, as our method locates and counts objects simultaneously, it could be used as a counting technique. We also evaluated our technique in the task of crowd counting using the ShanghaiTech Part B dataset presented in <ref type="bibr" target="#b61">[62]</ref>, and achieve a MAE of 19.9. Even though we do not outperform state of the art methods that are specifically fine-tuned for crowd counting <ref type="bibr" target="#b28">[29]</ref>, we can achieve comparable results with our generic method. We expect future improvements such as architectural changes or using transfer learning to further increase the performance.</p><p>A PyTorch implementation of the weighted Hausdorff distance loss and trained models are available at https://github.com/javiribera/ locating-objects-without-bboxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a loss function for the task of locating objects in images that does not need bounding boxes. This loss function is a modification of the average Hausdorff distance (AHD), which measures the similarity between two <ref type="table">Table 2</ref>. Head location results using the mall dataset, using r = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Faster-RCNN Ours unordered sets of points. To make the AHD differentiable with respect to the network output, we have considered the certainty of the network when estimating an object location. The output of the network is a saliency map of object locations and the estimated number of objects. Our method is not restricted to a maximum number of objects in the image, does not require bounding boxes, and does not use region proposals or sliding windows. This approach can be used in tasks where bounding boxes are not available, or the small size of objects makes the labeling of bounding boxes impractical. We have evaluated our approach with three different datasets, and outperform generic object detectors and task-specific techniques. Future work will include developing a multi-class object location estimator in a single network, and evaluating more modern CNN architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Object localization with human heads, eye pupils and plant centers. (Bottom) Heat map and estimations as crosses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of two different configurations of point sets X = {x1, ..., x5} (solid dots) and Y = {y1, ..., y4} (dashed dots). Despite the clear difference in the distances between points, their Hausdorff distance are equal because the worst outlier is the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r ). In Equation (5), we compute distances in the original pixel space by replacing d(x, y) with d(Sx, Sy), where x, y ∈ Ω and S = S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>First row: Input image. Second row: Output of the network (p in the text) overlaid onto the input image. This can be considered a saliency map of object locations. Third row: The estimated object locations are marked with a red cross.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>An orthorectified image of a crop field with 15,208 plants. The red region was used for training, the region in green for validation, and the region in blue for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Effect on the F-score of the threshold τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Beta mixture model fitted on the values of px, and the thresholds τ used by the BMM method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Pupil detection results, using r = 5. Precision and recall are equal because there is only one estimated and one true object. Plant location results using the plant dataset, using r = 5.</figDesc><table><row><cell cols="2">Precision 81.1%</cell><cell cols="2">95.2 %</cell></row><row><cell>Recall</cell><cell>76.7%</cell><cell cols="2">96.2 %</cell></row><row><cell>F-score</cell><cell>78.8 %</cell><cell cols="2">95.7 %</cell></row><row><cell>AHD</cell><cell>7.6 px</cell><cell cols="2">4.5 px</cell></row><row><cell>MAE</cell><cell>4.7</cell><cell>1.4</cell><cell></cell></row><row><cell>RMSE</cell><cell>5.6</cell><cell>1.8</cell><cell></cell></row><row><cell>MAPE</cell><cell>14.8%</cell><cell cols="2">4.4 %</cell></row><row><cell>Method</cell><cell cols="3">Precision Recall AHD</cell></row><row><cell>Swirski [53]</cell><cell>77 %</cell><cell>77 %</cell><cell>-</cell></row><row><cell>ExCuSe [13]</cell><cell>77 %</cell><cell>77 %</cell><cell>-</cell></row><row><cell cols="2">Faster-RCNN 99.5 %</cell><cell cols="2">99.5 % 2.7 px</cell></row><row><cell>Ours</cell><cell>99.5 %</cell><cell cols="2">99.5 % 2.5 px</cell></row><row><cell>Metric</cell><cell cols="3">Faster-RCNN Ours</cell></row><row><cell cols="2">Precision 86.6 %</cell><cell cols="2">88.1 %</cell></row><row><cell>Recall</cell><cell>78.3 %</cell><cell cols="2">89.2 %</cell></row><row><cell>F-score</cell><cell>82.2 %</cell><cell cols="2">88.6 %</cell></row><row><cell>AHD</cell><cell>9.0 px</cell><cell cols="2">7.1 px</cell></row><row><cell>MAE</cell><cell>9.4</cell><cell>1.9</cell><cell></cell></row><row><cell>RMSE</cell><cell>13.4</cell><cell>2.7</cell><cell></cell></row><row><cell>MAPE</cell><cell>17.7 %</cell><cell cols="2">4.2 %</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was funded by the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award Number DE-AR0000593. The views and opinions of the authors expressed herein do not necessarily reflect those of the U.S. Government or any agency thereof. We thank Professor Ayman Habib for the orthophotos used in this paper. Contact information: Edward J. Delp, ace@ecn.purdue.edu</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Ablation Of Terms</head><p>In Section 4, we made the following claim:</p><p>Claim. Both terms of the Weighted Hausdorff Distance (WHD) are necessary. If the first term is removed, then p x = 1 ∀x ∈ Ω is the solution that minimizes the WHD. If the second term is removed, then the trivial solution is p x = 0 ∀x ∈ Ω.</p><p>Proof. If the first term is removed and p x = 1 ∀x ∈ Ω, then Equation (5) reduces to</p><p>From the definition in Equation <ref type="formula">(2)</ref>, ∀x, y ∈ Ω,</p><p>For any p x ∈ [0, 1] and α &lt; 0, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepwheat: Estimating phenotypic traits from crop images with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Obsyannikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Josuttes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strueby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duddu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pozniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirtliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision<address><addrLine>Stateline, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Peopletracking-by-detection and people-detection-bytracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Field high-throughput phenotyping: the new crop breeding frontier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Araus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cairns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Plant Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="61" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An efficiently computable metric for comparing polygonal shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Arkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The topology of the ρ-Hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lucchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J B</forename><surname>Wets</surname></persName>
		</author>
		<editor>Annali di Matematica Pura ed Applicata</editor>
		<imprint>
			<date type="published" when="1991-12" />
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="303" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database (supplemental material)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online multiperson trackingby-detection from a single, uncalibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1820" to="1833" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Row spacing and weed control timing affect yield of aerobic rice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Field Crops Research</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="2001-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A modified Hausdorff distance for object matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Dubuisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1994-10" />
			<biblScope unit="page" from="566" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2463" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Row spacing, plant density, and hybrid effects on corn grain yield and moisture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Farnham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Agronomy Journal</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1049" to="1053" />
			<date type="published" when="2001-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ExCuSe: Robust pupil detection in realworld scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosenstiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Analysis of Images and Patterns</title>
		<meeting>the International Conference on Computer Analysis of Images and Patterns<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="39" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-intrusive practitioner pupil detection for unmodified microscope oculars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herkommer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="36" to="44" />
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic facial analysis: From bayesian filtering to recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1548" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
	</analytic>
	<monogr>
		<title level="j">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Body structure aware deep crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1049" to="1059" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>Honolulu</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Spatially eigen-weighted Hausdorff distances for human face recognition. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-08" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1827" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Age invariant face recognition using convolutional neural networks and set distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Khiyari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="174" to="185" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for Learning Representations</title>
		<meeting>the International Conference for Learning Representations<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Banach spaces L p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kubrusly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essentials of Measure Theory</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning: A unified deep learning framework for automatic prostate mr segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Amsterdam</publisher>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowd counting and profiling: Methodology and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="347" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An approach to word image matching based on weighted Hausdorff distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition</title>
		<meeting>International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2001-09" />
			<biblScope unit="page" from="921" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Utilization of a high-throughput shoot imaging system to examine the dynamic phenotypic responses of a C 4 cereal crop plant to nitrogen and water deficiency over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Neilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Blomstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Gleadow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Botany</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1817" to="1832" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">We don&apos;t need no bounding-boxes: Training object class detectors using only human verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4031" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hybrid parameter estimation algorithm for beta mixtures and applications to methylation state classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schröder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms for Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Using the averaged Hausdorff distance as a performance measure in evolutionary multiobjective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schutze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Esquivel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="504" to="522" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02326</idno>
		<title level="m">Learning to point and count</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Crowdsourcing annotations for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>WS-12-08:40- 46</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence Human Computation Workshop</title>
		<meeting>the Association for the Advancement of Artificial Intelligence Human Computation Workshop<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-biomass sorghum yield estimate with aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Searcy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">53523</biblScope>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Gazing at Games: An Introduction to Eye Tracking Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sundstedt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>San Rafael, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust realtime pupil tracking in highly off-axis images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Świrski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research and Applications</title>
		<meeting>the Symposium on Eye Tracking Research and Applications<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Metrics for evaluating 3D medical image segmentation: Analysis, selection, and tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hynynen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02382</idno>
		<title level="m">Deep learning convolutional networks for multiphoton microscopy vasculature segmentation</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Crop yield and planting density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H M</forename><surname>Thornley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Botany</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="259" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A review of maize hybrids&apos; dependence on high plant populations and its implications for crop yield stability. Field Crops Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tokatlidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Koutroubas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-08" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatiotemporal modeling for crowd counting in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="5151" to="5159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Crossscene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Deep Learning for Medical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Academic Press</publisher>
			<pubPlace>London, United Kingdom</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
