<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Non-Negative Factorization approach to node pooling in Graph Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer</publisher>
				<availability status="unknown"><p>Copyright Springer</p>
				</availability>
				<date type="published" when="2019">2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
							<email>bacciu@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Largo B. Pontecorvo</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<addrLine>3 -Pisa</addrLine>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Sotto</surname></persName>
							<email>l.disotto@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Largo B. Pontecorvo</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<addrLine>3 -Pisa</addrLine>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Largo B. Pontecorvo</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<addrLine>3 -Pisa</addrLine>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Sotto</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica Largo B. Pontecorvo</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<addrLine>3 -Pisa</addrLine>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Non-Negative Factorization approach to node pooling in Graph Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">the Proceedings of the 18th International Conference of the Italian Association for Artificial Intelligence (AIIA 2019)</title>
						<imprint>
							<publisher>Springer</publisher>
							<date type="published" when="2019">2019</date>
						</imprint>
					</monogr>
					<note>A Non-Negative Factorization approach to node pooling in Graph Convolutional Neural Networks</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph Convolutional Neural Networks · Differentiable Graph Pooling · Non-Negative Matrix Factorization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper discusses a pooling mechanism to induce subsampling in graph structured data and introduces it as a component of a graph convolutional neural network. The pooling mechanism builds on the Non-Negative Matrix Factorization (NMF) of a matrix representing node adjacency and node similarity as adaptively obtained through the vertices embedding learned by the model. Such mechanism is applied to obtain an incrementally coarser graph where nodes are adaptively pooled into communities based on the outcomes of the non-negative factorization. The empirical analysis on graph classification benchmarks shows how such coarsening process yields significant improvements in the predictive performance of the model with respect to its non-pooled counterpart.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays many real-world phenomena are modeled as interacting objects possibly living into high-dimensional manifolds with added topological structure. Examples can be found in genomics with protein-protein interaction networks, fake news discovery in social networks, functional networks in neuroscience. Graphs are the natural mathematical model for such data with underlying non-Euclidean nature. Current Euclidean Convolutional Neural Networks have built their success leveraging on the statistical properties of stationarity, locality and compositionality of flat domains. Rendering convolutional neural networks able also to learn over non-Euclidean domains is not that straightforward in that is required a re-designing of the computational model for adaptively learning graph embeddings. Over flat domains, i.e. grid-like structures, convolutional filters are compactly supported because of the grid regularity and the availability of consistent node ordering across different samples. This makes it possible to learn arXiv:1909.03287v1 [cs.LG] 7 Sep 2019 2 D. Bacciu and L. Di Sotto filters of fixed size and independent of the input signal dimension leveraging, to this end, weight sharing techniques. Furthermore, a set of symmetric functions is also applied for sub-sampling purposes to fully exploit the multi-scale nature of the grids. The same does not apply to domains with highly varying topologies where learnt filters (non-Toeplitz operators) may be too representative of the considered domain, since they highly depend on the eigen-basis of the filter operator and they may thus fail to model sharp changes in the graph signal. State-of-the-art Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> try to overcome the above difficulties with convolutions based on k-order Chebyshev polynomials, introducing the interesting duality of implicitly learning the graph spectrum by simply acting on the spatial representation. GCNs efficiently avoid the computational burden of performing a spectral decomposition of the graph, yielding to learned filters that are independent of the number of nodes in the graph. When considering graph classification tasks, we lack a principled multi-resolution operator providing coarser and more abstract representations of the input data as we go deeper in the network. Standard approaches to graph pooling employ symmetric functions such as max, summation or average along features axes of the graph embeddings. In <ref type="bibr" target="#b28">[29]</ref>, it is given an account of the discriminative power of these different coarsening operators. In the present work, we introduce a simple pooling operator for graphs that builds on the Non-Negative Matrix Factorization (NMF) methods to leverage on the community structure underlying graph structured data to induce subsampling, or equivalently, a multiscale view of the input graph in order to capture long-range interactions as we go deeper in Graph Convolutional Networks (GCNs). That would be of practical interest especially in the context of graph classification or regression tasks where the whole graph is fed into downstream learning systems as a single signature vector. Such mechanism is thus applied to incrementally obtain coarser graphs where nodes are pooled into communities based on the soft assignments output of the NMF of the graph adjacency matrix and Gram matrix of learned graph embeddings. Results on graph classification tasks show how jointly using such a coarsening operator with GCNs translate into improved predictive performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In the following we introduce some basic notation used throughout the paper, then we briefly introduce the necessary background to understand state-of-theart Graph Convolutional Neural Networks (GCNs). We mainly refer to spectral graph theory as introduced in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic notation</head><p>A graph G is a tuple G = (V, E), where V is the set of vertices of the graph and E is the set of edges connecting vertices, i.e. E ⊆ V × V. Let N (i) be the set of neighbours of a node i ∈ V. And let A ∈ IR n×n , with n = |V|, be the adjacency matrix such that</p><formula xml:id="formula_0">A i,j = a i,j &gt; 0 if (i, j) ∈ E 0 otherwise.</formula><p>Note that in the above formulation we consider undirected graphs, i.e. such that (i, j) ∈ E and (j, i) ∈ E. Thus, matrix A is such that A = A T . In the present work, without loss of generality, we generalize to undirected graphs</p><p>We also indicate with X ∈ IR n×d as the matrix of the n signals x i ∈ IR d associated to each node i ∈ V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolution via polynomial filters</head><p>Spectral construction. A first approach to representation learning on graphs is to explicitly learn the graph spectrum. In matrix notation, we can express the generalized convolution over graphs as follows <ref type="bibr" target="#b6">[7]</ref> </p><formula xml:id="formula_1">LX = U ΛU T X<label>(1)</label></formula><p>where L is the combinatorial graph Laplacian, L = D − A, with D the degree matrix such that D ii = j a ij , where U ∈ IR n×k is an orthonormal basis generalizing the Fourier basis, and where Λ is a diagonal matrix being the spectral representation of the filter <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. Matrices U and Λ are the solution to the generalized eigenvalue problem LU = U Λ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. With such an approach there are multiple problems: (a) the eigendecomposition in (1), and its application (filtering), require non-trivial computational time; (b) the corresponding filters are non-localized <ref type="bibr" target="#b10">[11]</ref>; (c) filter size is O(n), hence introducing a direct link between the parameters and the n nodes in the graph (no weight sharing).</p><p>Spatial construction. In <ref type="bibr" target="#b10">[11]</ref>, it is proposed an alternative approach to explicit learning of the graph spectrum, by showing how it can be learned implicitly through a polynomial expansion of the diagonal operator Λ. Formally,</p><formula xml:id="formula_2">g θ (Λ) = K−1 k=0 θ k Λ k<label>(2)</label></formula><p>where θ ∈ IR K is the vector of polynomial coefficients. In <ref type="bibr" target="#b10">[11]</ref> is pointed out that spectral filters represented as K-order polynomials are exactly K-localized and that weight sharing is thus made possible, since filters have size O(K). Graph CNN (GCNN), also known as ChebNet <ref type="bibr" target="#b10">[11]</ref>, exploited the previous observation by employing Chebyshev polynomials for approximating filtering operation <ref type="bibr" target="#b0">(1)</ref>. Chebyshev polynomials are recursively defined using the recurrence relation</p><formula xml:id="formula_3">T j (λ) = 2λT j−1 (λ) − T j−2 (λ);</formula><p>T 0 (λ) = 1;</p><formula xml:id="formula_4">T 1 (λ) = λ.</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">D. Bacciu and L. Di Sotto</head><p>Also, polynomials recursively generated by (3) form an orthonormal basis in [−1, 1] <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. A filter can thus be represented as a polynomial of the form</p><formula xml:id="formula_5">g θ (L) = K−1 k=0 θ k U T k (Λ)U T = K−1 k=0 θ k T k (L),<label>(4)</label></formula><p>whereL = 2Λ/λ max − I n andΛ = 2Λ/λ max − I n indicate a rescaling of the Laplacian eigenvalues to [−1, 1]. The filtering operation in (1) can be rewritten, for one-dimensional input graph signals, asx = g θ (L)x ∈ IR n , where the k-th polynomialx k = T k (L)x can be computed using the recurrence relation in (3) now defined asx = 2Lx k−1 −x k−2 withx 0 = x andx 1 =Lx. More generally, taking into account multi-dimensionality of input data, we have a convolutional layer as followsX</p><formula xml:id="formula_6">= σ K−1 k=0 T k (∆) XΘ k<label>(5)</label></formula><p>with σ a non-linear activation, and Θ ∈ IR din×dout the matrix of learnable parameters, with d in number of input features and d out number of neurons. A widely used convolutional layer over graphs are GCNs by <ref type="bibr" target="#b16">[17]</ref> that are layers of the form of (5) with K = 2, namelŷ</p><formula xml:id="formula_7">X = ReLU Â XΘ .<label>(6)</label></formula><p>The Θ term, the matrix of polynomial coefficients to be learned, stems from (5) by imposing Θ 0 = −Θ 1 , and withÂ = A + I, and non-linearity being the ReLU function <ref type="bibr" target="#b16">[17]</ref>. Thus, the main idea is to generate a representation for a node i ∈ V by aggregating its own features x i ∈ IR d and its neighbors features x j ∈ IR d , where j ∈ N (i). Note that, apart from the formulation meant to highlight the symmetry with convolutions on image data, the GCN model is not substantially different from the contextual approach to graph processing put forward by <ref type="bibr" target="#b21">[22]</ref> a decade before GCN, and recently extended to a probabilistic formulation <ref type="bibr" target="#b2">[3]</ref> by leveraging an hidden tree Markov model <ref type="bibr" target="#b0">[1]</ref> with relaxed causality assumptions and a fingerprinting approach to structure embedding <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Node Pooling in Graph CNNs</head><p>A first attempt to formalize graph pooling can be found in <ref type="bibr" target="#b8">[9]</ref>, a simple framework for multiresolution clustering of a graph is given based on a naive agglomerative method. There are some recent works proposing pooling mechanisms for graph coarsening in Deep GCNs, in <ref type="bibr" target="#b9">[10]</ref> a subset of the nodes are dropped based on a learnable projection vector where at each layer only the top-k interesting nodes are retained. In <ref type="bibr" target="#b14">[15]</ref>, it is employed a rough node sampling and a differentiable approach through a LSTM model for learning aggregated node embeddings, though it may render difficult satisfying invariance with respect to node ordering. Interestingly, in <ref type="bibr" target="#b4">[5]</ref> it is applied a simple and well known method from Graph Theory for node decimation based on the largest eigenvector u max of the graph Laplacian matrix. They further employ a more sophisticated procedure to reduce Laplacian matrix using the sparsified Kron reduction. Another relevant differentiable approach is that put forward by DiffPool <ref type="bibr" target="#b31">[31]</ref>, where the model learns soft assignments to pool similar activating patterns into the same cluster, though the idea of learning hiearchical soft-clustering of graphs via adjacency matrix decomposition using a symmetric variant of NMF can be dated back to <ref type="bibr" target="#b32">[32]</ref>. In DiffPool, the learned soft assignment matrix is applied as a linear reduction operator on the adjacency matrix and the input signal matrix, and the coarsened graph is thus further convolved with GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NMFPool: node pooling by Non-Negative Matrix Factorization</head><p>In the following section we introduce our model, NMFPool, a principled Pooling operator enabling deep graph CNNs develop multi-resolution representations of input graphs. NMFPool leverages community structure underlying graphs to pool similar nodes to progressively gain coarser views of a graph. To that end we take inspiration from <ref type="bibr" target="#b32">[32]</ref> in which latent community structure of graph data is made explicit via adjacency matrix decomposition using Symmetric NMF (SNMF). NMFPool is grounded on that idea, building, instead, on a general non-symmetrical NMF of the adjacency matrix without constraining solutions to be stochastic. Before going further into details of our approach, we first introduce the formal definition of the NMF problem, then we give an intuitive interpretation of its solutions to clarify why NMF would help solve the graph pooling problem on graphs. At the end we will show how to use product factors of NMF as linear operators to aggregate topology and content information associated to graphs. NMF is a popular technique for extracting salient features in data by extracting a latent space representation of the original information.</p><p>Throughout the paper we refer to the original idea of NMF <ref type="bibr" target="#b18">[19]</ref> though it has been extensively studied in numerical linear algebra in the last years by many authors and for a variety of applications. Formally, the NMF problem can be stated as follows:</p><formula xml:id="formula_8">Definition 1. Given a non-negative matrix A ∈ IR n×m + , find non-negative ma- trix factors W ∈ IR n×k + and H ∈ IR k×m + , with k &lt; min(m, n), such that A ≈ W H<label>(7)</label></formula><p>If we see matrix A as having m multivariate objects column-stacked, the straightforward interpretation of (7) is as follows</p><formula xml:id="formula_9">a j ≈ W h j ,<label>(8)</label></formula><p>with a j and h j corresponding to j-th columns of A and H. The approximation (8) entails that each multi-variate object is a linear combination of columns of W weighted by coefficients in h j . Thus W is referred to as the basis matrix or equivalently the cluster centroids matrix if we intend to interpret NMF as a clustering method. Matrix H can be seen, instead, as a low-dimensional representation of the input data making thus NMF also useful for dimensionality reduction. Latent representation, in the clustering perspective, may indicate whether a sample object belongs to a cluster. For example, we could constrain each data-point to belong to a single cluster at a time: namely, each data-point is assigned to the closest cluster x j ≈ u j . We generally look for non-trivial encodings to explain community evolution in graphs. Thus, the problem could be relaxed to a soft-clustering problem in that each data-point can belong to k overlapping clusters <ref type="bibr" target="#b27">[28]</ref>. Formulation <ref type="bibr" target="#b6">(7)</ref> requires to define a metric to measure the quality of the approximation, and Kullback-Leibler (KL-) divergence or the more common Frobenius norm (F-norm) are common choices. Many techniques from numerical linear algebra can be used to minimize problem <ref type="formula" target="#formula_8">(7)</ref> whatever the cost function we use, although its inherently non-convex nature does not give any guarantee on global minimum <ref type="bibr" target="#b12">[13]</ref>. In <ref type="bibr" target="#b18">[19]</ref> were first proposed multiplicative and additive update rules that ensure monotone descrease under KL-or F-norm. Thus, our proposed solution can be summarized into two main steps. First, we encode the input adjacency matrix to learn soft-assignments of nodes, and that could accomplished via exact NMF of the adjacency matrix. Second, we apply soft-assignments as linear operators to coarse adjacency matrix and node embeddings. To this end, we refer to algebraic operations seen in <ref type="bibr" target="#b32">[32]</ref> for decomposing adjacency matrices and we extend it using equations widely used for graph coarsening <ref type="bibr" target="#b31">[31]</ref>, for they take into account embedding matrix reduction and nodes connectivity strength. For a complete picture, consider NMFPool layers interleaved with at least + 1 stacked Graph Convolutions (GCs) as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, where the graph convolutions are computed according to <ref type="bibr" target="#b5">(6)</ref>. Then, let Z (i) ∈ IR ni×d be the output of i-th GC, namely the convolved node embeddings at layer i-th, defined as</p><formula xml:id="formula_10">Z (i) = ReLU A (i) Z (i−1) Θ (i)<label>(9)</label></formula><p>with adjacency matrix A (i) ∈ IR ni×ni , with n i number of nodes at previous layer, and Θ (i) ∈ IR d×d matrix of weights. Observe that we are assuming, without loss of generality, each GC layer (9) as having the same number of neurons. Observe also that Z (0) = X ∈ IR n×d , namely the initial node labels, and the initial adjacency matrix is set to A (0) =D −1/2ÂD−1/2 , i.e. the normalized adjacency matrix withÂ = A + I, A ∈ IR n×n , andD is a diagonal matrix of node degrees <ref type="bibr" target="#b16">[17]</ref>. The i-th NMFPool layer solves the problem in <ref type="bibr" target="#b6">(7)</ref>, i.e. the decomposition of the symmetric and positive A (i) , by minimizing the following loss</p><formula xml:id="formula_11">||A (i) − W (i) H (i) || F<label>(10)</label></formula><p>GC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; fc &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; Pool &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; GC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; GC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; Pool &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; , and k i number of overlapping communities to pool the n i nodes into, and . F the Frobenius norm. Observe that k i 's are hyper-parameters to control graph coarsening scale. The algorithm to minimize (10) depends on the underlying NMF implementation. Then NMFPool applies the encoding H (i) to coarsen graph topology and its content as follows</p><formula xml:id="formula_12">Z (i+1) = H (i)T Z (i) ∈ IR ki×d<label>(11)</label></formula><formula xml:id="formula_13">A (i+1) = H (i)T A (i) H (i) ∈ IR ki×ki .<label>(12)</label></formula><p>A graphical interpretation of the inner workings of the NMFPool layer is provided in <ref type="figure" target="#fig_2">Figure 2</ref>, highlighting the interpretation of pooling as a matrix decomposition operator. It is crucial to point out that NMFPool layers are independent of the number of nodes in the graph, which is essential to deal with graphs with varying topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We assess the effectiveness of using the exact NMF of the adjacency matrix A as a pooling mechanism in graph convolutional neural networks. To this end, we consider five popular graph classification benchmarks and we further compare the performance of our approach, referred to as NMFPool in the following, with that of DiffPool, with the goal of showing how a simple and general method may easily compare to differentiable and parameterized pooling operators such as DiffPool. Results were gathered on graph classification tasks for solving biological problems on the ENZYMES ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>), NCI1 <ref type="bibr" target="#b26">[27]</ref>, PROTEINS ( <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>), and D&amp;D ( <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b25">[26]</ref>) datasets and the scientific collaboration dataset COLLAB <ref type="bibr" target="#b29">[30]</ref>. In <ref type="table" target="#tab_0">Table 1</ref> are summarized statistics on benchmark datasets.</p><p>In our experiments, the baseline graph convolution is the vanilla layer in <ref type="bibr" target="#b5">(6)</ref>. For both models, we employed the interleaving of pooling and convolutional &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;  &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; W &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; H &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; n &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; NMFPool Layer &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; pooled graph: &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; input graph: &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " ( n u l l ) " &gt; ( n u l l ) &lt; / l a t e x i t &gt;  layers depicted in the architecture in <ref type="figure" target="#fig_0">Figure 1</ref>, varying the number of poolingconvolution layer pairs to assess the effect of network depth on task performance. Note that the number of layers in the convolutional architecture influences the context spreading across the nodes in the graph. Implementation of NMFPool and Diffpool is based on the Pytorch Geometric library <ref type="bibr" target="#b13">[14]</ref>, complemented by the NMF implementation available in the Scikit library. Models configurations were run on a multi-core architecture equipped with 4 NUMA nodes each with 18 cores (Intel(R) Xeon(R) Gold 6140M @ 2.30GHz) capable of running 2 threads each for a total of 144 processing units available. We had access also to 4 Tesla V100 GPUs accelerators.</p><p>Model selection was performed for exploring a variety of configurations using stratified 3-fold cross validation. Following standard practice in graph convolution neural networks, learning rate was set with an initial value of 0.1 and then decreased by a factor of 0.1 whenever validation error did not show any improvement after 10 epochs wait. The number of neurons is the same for each graph convolutional layer and it has been selected in {16, 32, 64, 128} as part of the cross-validation procedure. When applying the pooling operator both NMFPool and Diffpool require to define the number of communities k, similarly to how the pooling operator on images requires the definition of the pooling windows size (and stride). Here, following the idea indicated in the original DiffPool paper <ref type="bibr" target="#b31">[31]</ref>, we choose different k for each dataset as a fraction of the average number of nodes in the samples. Thus during cross-validation we intended to study how NMFPool and DiffPool behave as a function of the cluster sizes k i at each layer. To this end, pooling size has been selected from the set {k 1 , k 2 }. In particular, for models with a single pooling layer, we tested both sizes k 1 and k 2 . Instead, for deeper architectures, we restricted to use the largest k i for the first layer, following up in decreasing order of k i . <ref type="table">Table 2</ref> summarizes the number of clusters used for the first and second pooling layer in the architectures considered in this empirical assessment. <ref type="table">Table 2</ref>: k 1 is computed using formula k 1 = n avg · p with p varying in [21% − 25%], and n avg average number of nodes (see <ref type="table" target="#tab_0">Table 1</ref>). Then k 2 = k 1 /2.</p><p>Fractions are chosen depending on the size of task at hand and to previous empirical observation. Except for the D&amp;D dataset where p = 5%, 1%, being the bigger dataset we needed a good compromise between abstraction capability and computational time. The outcome of the empirical assessment is summarized in <ref type="table" target="#tab_2">Table 3</ref>, where it is reported the mean classification accuracy of the different models averaged on the dataset folds. <ref type="table" target="#tab_2">Table 3</ref> reports results for a vanilla GCN (no pooling) and a varying number of graph convolution layers: results show how at most two layers are sufficient to guarantee good performances, while three layers are only required for the COLLAB dataset and a single layer network obtains the best performance on the NCI1 dataset. In the experiments we thus decided to employ at most three GCN layers, namely at most two NMF and DiffPool pooling layers. It is still evident how adding more convolutional and pooling layers does not always result into better performances. The analysis of the results for NMFPool shows how the addition of the simple NMF pooling allows a consistent increase of the classification accuracy with respect to the non-pooled model for all the benchmark datasets. Note how a single pooling layer is sufficient, on most datasets, to obtain the best results, confirming the fact that pooling allows to effectively fasten the process of context spreading between the nodes. When compared to DiffPool, our approach achieves accuracies which are only marginally lower than DiffPool on few datasets. This despite the fact that DiffPool employs a solution performing an task-specific parameterized decomposition of the graph, while our solution simply looks for quasi-symmetrical product matrices by knowing nothing of the underlying task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduced a pooling mechanism based on the NMF of the adjacency matrix of the graph, discussing how this approach can be used to yield a hierarchical soft-clustering of the nodes and to induce a coarsening of the graph structure.</p><p>We have empirically assessed our NMPool approach with the task-specific adaptive pooling mechanism put forward by the DiffPool model on a number of state-of-the-art graph classification benchmarks. We argue that our approach can yield to potentially more general and scalable pooling mechanisms than DiffPool, allowing to choose weather the pooling mechanism has to consider the node embeddings computed by the model and the task-related information when performing the decomposition (as in DiffPool), but also allowing to directly decompose the graph structure a-priori with no knowledge of the node embeddings adaptively computed by the convolutional layer. This latter aspect, in particular, allows to pre-compute the graph decomposition and results in a multiresolution representation of the graph structure which does not change with the particular task at hand. Future works will consider the use of symmetric and optimized NMF variants to increase prediction performances. It also would be of particular interest to improve the quality and quantity of information NMFPool retains into the encoding matrix. NMFPool could evolve out of its general purpose form, for example, making it a generative end-to-end differentiable layer using probabilistic approaches. See <ref type="bibr" target="#b7">[8]</ref> for an attempt to solve NMF using probabilistic models. We could refer to the popular probabilistic generative model of the Variational Auto-Encoders (VAEs) <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref> possibly extended to graphs <ref type="bibr" target="#b17">[18]</ref>. The underlying hierarchical structure of graph data may also be taken into account by imposing latent encoding to match priors referring to hyperbolic spaces <ref type="bibr" target="#b19">[20]</ref>. Interestingly, latent matrix encoding may not be forced to match overimposed priors, for they could make the model too biased over particular graph geometries. Instead, such priors could be directly learned from relational data using adversarial approaches <ref type="bibr" target="#b20">[21]</ref> extended also to graph auto-encoders <ref type="bibr" target="#b22">[23]</ref>. Another interesting feature would be to make NMFPool independent of hyper-parameter k.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>High level architecture of a 3-layers GCN interleaved with 2 NMF Pooling layers. with W (i) ∈ IR ni×ki + and H (i) ∈ IR ki×ni +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>The NMFPool layer. Orange circles represent nodes of input graph, and solid lines the edges. Dashed lines are the predicted edges in between nodes pooled together. Colored dashed circles represent discovered communities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 -</head><label>1</label><figDesc>(0.023) 0.625 (0.014) 0.713 (0.019) 0.681 (0.045) 0.671 (0.007) 2-GCs 0.228 (0.023) 0.620 (0.057) 0.720 (0.034) 0.704 (0.048) 0.678 (0.007) 3-GCs 0.182 (0.022) 0.628 (0.031) 0.688 (0.024) 0.692 (0.032) 0.681 (0.002) 1-NMFPool 0.241 (0.039) 0.662 (0.026) 0.721 (0.031) 0.760 (0.015) 0.650 (0.004) 2-NMFPool 0.175 (0.023) 0.655 (0.013) 0.724 (0.020) 0.753 (0.010) 0.658 (0.002) DiffPool 0.259 (0.069) 0.661 (0.017) 0.743 (0.011) 0.770 (0.007) 0.659 (0.005) 2-DiffPool 0.239 (0.064) 0.632 (0.017) 0.744 (0.026) 0.761 (0.003) 0.667 (0.022)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics on benchmark datasets.</figDesc><table><row><cell cols="5">Dataset Graphs Classes Nodes (avg) Edges (avg)</cell></row><row><cell>COLLAB</cell><cell>5000</cell><cell>3</cell><cell>74.49</cell><cell>2457.78</cell></row><row><cell>D&amp;D</cell><cell>1178</cell><cell>2</cell><cell>284.32</cell><cell>715.66</cell></row><row><cell>ENZYMES</cell><cell>600</cell><cell>6</cell><cell>32.63</cell><cell>62.14</cell></row><row><cell>NCI1</cell><cell>4110</cell><cell>2</cell><cell>29.87</cell><cell>32.30</cell></row><row><cell cols="2">PROTEINS 1113</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean and standard deviation (in brackets) of graph classification accuracies on the different benchmarks, for the vanilla GCN with convolutional layers ( -GC), for NMFPool and DiffPool with p pooling layers and p + 1 convolutional layers (i.e.</figDesc><table /><note>p1 -NMFPool and p2 -DiffPool, respectively).</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the Italian Ministry of Education, University, and Research (MIUR) under project SIR 2014 LIST-IT (grant n. RBSI14STDE).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional generative mapping for treestructured data -part II: Topographic projection model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learning Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="247" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative kernels for tree-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4932" to="4946" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Contextual graph Markov model: A deep and generative approach to graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, Stockholmsmssan</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic</title>
		<meeting>the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
	<note>NIPS&apos;01</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional ARMA filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno>abs/1901.01343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schonauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1611.08097</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast bayesian non-negative matrix factorisation and tri-factorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno>09-12-2016 Through 09-12-2016</idno>
	</analytic>
	<monogr>
		<title level="m">nIPS 2016 : Advances in Approximate Bayesian Inference Workshop ; Conference date</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<title level="m">Towards Sparse Hierarchical Graph Classifiers</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>abs/1606.09375</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive computation of the Symmetric Nonnegative Matrix Factorization (NMF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Menchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Romani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01321</idno>
		<imprint>
			<date type="published" when="2019-03" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno>abs/1903.02428</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-Encoding Variational Bayes</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational Graph Auto-Encoders</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Leen, T.K., Dietterich, T.G., Tresp, V.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Le Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Whye Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06033</idno>
		<title level="m">Hierarchical Representations with Poincar\&apos;e Variational Auto-Encoders</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1701.04722</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarially regularized graph autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1802.04407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Xing, E.P., Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning. Machine Learning Research<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="22" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: Updates and major new developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schomburg</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh081</idno>
		<ptr target="https://doi.org/10.1093/nar/gkh081" />
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="431" to="434" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-007-0103-5</idno>
		<ptr target="https://doi.org/10.1007/s10115-007-0103-5" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Borhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<title level="m">Machine Learning Refined: Foundations, Algorithms, and Applications</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>1st edn.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2783258.2783417</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783417" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4804" to="4814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Soft clustering on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/2948-soft-clustering-on-graphs.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weiss, Y., Schölkopf, B., Platt, J.C.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1553" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
