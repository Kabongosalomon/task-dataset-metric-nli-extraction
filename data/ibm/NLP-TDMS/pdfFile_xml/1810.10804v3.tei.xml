<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
							<email>vladimir.nekrasov@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<email>hao.chen01@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<email>chunhua.shen@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Neural Architecture Search of Compact Semantic Segmentation Models via Auxiliary Cells</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used-e.g., dilated convolutions-to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with stateof-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/ drsleep/nas-segm-pytorch</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For years, the design of neural network architectures was thought to be solely a duty of a human expert -it was * Equal contribution. her responsibility to specify which type of architecture to use, how many layers should there be, how many channels should convolutional layers have and etc. This is no longer the case as the automated neural architecture search -a way of predicting the neural network structure via a non-human expert (an algorithm) -is fast-growing. Potentially, this may well mean that instead of manually adapting a single stateof-the-art architecture for a new task at hand, the algorithm would discover a set of best-suited and high-performing architectures on given data.</p><p>Few decades ago, such an algorithm was based on evolutionary programming strategies where best seen so far architectures underwent mutations and their most promising off-springs were bound to continue evolving <ref type="bibr" target="#b1">[2]</ref>. Now, we have reached the stage where a secondary neural network, oftentimes called controller, replaces a human in the loop, by iteratively searching among possible architecture candidates and maximising the expected score on the held-out set <ref type="bibr" target="#b49">[50]</ref>. While there is a lack of theoretical work behind this latter approach, several promising empirical breakthroughs have already been achieved <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>At this point, it is important to emphasise the fact that such accomplishments required an excessive amount of computational resources-more than 20, 000 GPU-days for the work of Zoph and Le <ref type="bibr" target="#b49">[50]</ref> and 2, 000 for Zoph et al. <ref type="bibr" target="#b50">[51]</ref>. Although a few works have reduced those to single digit numbers on image classification and language processing tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>, we consider more challenging dense perpixel tasks that produce an output for each pixel in the input image and for which no efficient training regimes have been previously presented. Although here we concentrate only on semantic image segmentation, our proposed methodology can immediately be applied to other per-pixel prediction tasks, such as depth estimation and pose estimation. In our experiments, we demonstrate the transferability of the discovered segmentation architecture to the latter problems. Notably, all of them play an important role in computer vision and robotic applications and so far have been relying on manually designed accurate low-latency models for realworld scenarios.</p><p>The focus of our work is to automatically discover compact high-performing fully convolutional architectures, able to run in real-time on a low-computational budget, for example, on the Jetson platform. To this end, we are explicitly looking for structures that not only improve the performance on the held-out set, but also facilitate the optimisation during the training stage. Concretely, we consider the encoder-decoder type of a fully-convolutional network <ref type="bibr" target="#b24">[25]</ref>, where encoder is represented by a pre-trained image classifier, and the decoder structure is emitted by the controller network. The controller generates the connectivity structure between encoder and decoder, as well as the sequence of operations (that form the so-called cell) to be applied on each connected path. The same cell structure is used to form an auxiliary classifier, the goal of which is to provide intermediate supervision and to implicitly over-parameterise the model. Over-parameterisation is believed to be the primary reason behind the successes of deep learning models, and a few theoretical works have already addressed it in simplified cases <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>. Along with empirical results, this is the primary motivation behind the described approach.</p><p>Last, but not least, we devise a search strategy that permits to find high-performing architectures within a small number of days using only few GPUs. Concretely, we pursue two goals here: i.) To prevent 'bad' architectures from being trained for long; and ii.) To achieve a solid performance estimate as soon as possible.</p><p>To tackle the first goal, we divide the training process during the search into two stages. During the first stage, we fix the encoder's weights and pre-compute its outputs, while only training the decoder part. For the second stage, we train the whole model end-to-end. We validate the performance after the first stage and terminate the training of non-promising architectures. For the second goal, we employ Polyak averaging <ref type="bibr" target="#b30">[31]</ref> and knowledge distillation <ref type="bibr" target="#b12">[13]</ref> to speed-up convergence.</p><p>To summarise, our contributions in this work are to propose an efficient neural architecture search strategy for dense-per-pixel tasks that (i.) allows to sample compact high-performing architectures, and (ii.) can be used in realtime on low-computing platforms, such as JetsonTX2. In particular, the above points are made possible by:</p><p>• Devising a progressive strategy able to eliminate poor candidates early in the training; • Developing a training schedule for semantic segmentation able to provide solid results quickly via the means of knowledge distillation and Polyak averaging; • Searching for an over-parameterised auxiliary cell that provides better training and is obsolete during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditionally, architecture search methods have been relying upon evolutionary strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, where a population of networks (oftentimes together with their weights) is continuously mutated, and less promising networks are being discarded. Modern neuro-evolutionary approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref> rely on the same principles and benefit from available computational resources, that allow them to achieve impressive results. Bayesian optimisation methods estimating the probability density of objective function have long been used for hyper-parameter search <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>. Scaling up Bayesian methods for architecture search is an ongoing work, and few kernel-based approaches have already shown solid performance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Most recently, neural architecture search (NAS) strategies based on reinforcement learning (RL) have attained state-of-the-art results on the tasks of image classification and natural language processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. Relying on enormous computational resources, these algorithms comprise a separate neural network, the so-called 'controller', that emits an architecture design and receives a scalar reward after the emitted architecture is trained on the task of interest. Notably, thousand of iterations and GPU-days are needed for convergence. Rather than searching for the whole network structure from scratch, these methods tend to look for cells-repeatable motifs that can be stacked multiple times in a feedforward fashion.</p><p>Several solutions for making NAS methods more efficient have been recently proposed. In particular, Pham et al. <ref type="bibr" target="#b29">[30]</ref> unroll the computational graph of all possible architectures and allow sharing the weights among different architectures. This dramatically reduces the number of resources needed for convergence. In a similar vein of research, Liu et al. <ref type="bibr" target="#b22">[23]</ref> exploit a progressive strategy where the network complexity is gradually increased, while the ranking network is trained in parallel to predict the performance of a new architecture. A few methods have been built around continuous relaxation of the search problem. Particularly Luo et al. <ref type="bibr" target="#b25">[26]</ref> use an encoder to embed the architecture description into a latent space, and estimator to predict the performance of an architecture given its embedding. While these methods make the search process more efficient, they achieve so by sacrificing the expressiveness of the search space, and hence, may arrive to a sub-optimal solution.</p><p>In semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, up to now all the architectures have been manually designed, closely following the winner entries of image classification challenges. Two prominent directions have emerged over the last few years: the encoder-decoder type <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, where better features are learned at the expense of having a spatially coarse output mask; whereas other popular approach discards several down-sampling layers and relies on di-lated convolutions for keeping the receptive field size intact <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. Chen et al. <ref type="bibr" target="#b6">[7]</ref> have also shown that the combination of those two paradigms lead to even better results across different benchmarks. In terms of NAS in semantic segmentation, independently of us and in parallel to our work, a straightforward adaptation of image classification NAS methods was proposed by Chen et al. <ref type="bibr" target="#b4">[5]</ref>. In it they randomly search for a single segmentation cell design and achieve expressive results by using almost 400 GPUs over the range of 7 days. In contrast to that, our method first and foremost is able to find compact segmentation models only in a fraction of that time. Secondly, it differs significantly in terms of the search design and search methodology.</p><p>For the purposes of a clearer presentation of our ideas, we briefly review knowledge distillation, an approach proposed by Hinton et al. <ref type="bibr" target="#b12">[13]</ref> to successfully train a compact model using the outputs of a single (or an ensemble of) large network(s) pre-trained on the current task. In it, the logits of the pre-trained network are being used as an additional regulariser for the small network. In other words, the latter has to mimic the outputs of the former. Such a method was shown to provide a better learning signal for the small network. As a result of that, it has already found its way across multiple domains: computer vision <ref type="bibr" target="#b47">[48]</ref>, reinforcement learning <ref type="bibr" target="#b32">[33]</ref>, continuous learning <ref type="bibr" target="#b17">[18]</ref> -to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We start with the problem formulation, proceed with the definitions of an auxiliary cell and knowledge distillation loss, and conclude with the overall search strategy.</p><p>We primarily focus on two research questions: (i.) how to acquire a reliable estimate of the segmentation model performance as quickly as possible; and (ii.) how to improve the training process of the segmentation architecture through over-parameterisation, obsolete during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We consider dense prediction task T , for which we have multiple training tuples {(X i , y i )}, where both X i and y i are 3-dimensional tensors with equal spatial and arbitrary third dimensions. In this work, X i is a 3-channel RGB image, while y i is a C-channel one-hot segmentation mask with C being equal to the number of classes, which corresponds to semantic image segmentation. Furthermore, we rely on a mapping f : X → y with parameters θ, that is represented by a fully convolutional neural network. We assume that the network f can further be decomposed into two parts: e -representing encoder, and d -for decoder. We initialise encoder e with weights from a pre-trained classification network consisting of multiple down-sampling operations that reduce the spatial dimensions of the input. The decoder part, on the other hand, has access to several outputs of encoder with varying spatial and channel dimensions. The search goal is to choose which feature maps to use and what operations to apply on them. We next describe the decoder search space in full detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Search Space</head><p>We restrict our attention to the decoder part, as it is currently infeasible to perform a full segmentation network search from scratch.</p><p>As mentioned above, the decoder has access to multiple layers from the pre-trained encoder with varying dimensions. To keep sampled architectures compact and of approximately equal size, each encoder output undergoes a single 1×1 convolution with the same number of output channels. We rely on a recurrent neural network, the controller, to sequentially produce pairs of indices of which layers to use, and what operations to apply on them. In particular, this sequence of operations is combined to form a cell (see example in <ref type="figure" target="#fig_0">Fig. 1</ref>). The same cell but with different weights is applied to each layer inside the sampled pair, and the outputs of two cells are summed up. The resultant layer is added to the sampling pool. The number of times pairs of layers are sampled is controlled by a hyperparameter, which we set to 3 in our experiments, allowing the controller to recover such encoder-decoder architectures as FCN <ref type="bibr" target="#b24">[25]</ref>, or RefineNet <ref type="bibr" target="#b18">[19]</ref>. All non-sampled summation outputs are concatenated, before being fed into a single 1×1 convolution to reduce the number of channels followed by the final classification layer.</p><p>Each cell takes a single input with the controller first deciding which operation to use on that input. The controller then proceeds by sampling with replacement two locations out of two, i.e., of input and the result of the first operation, and two corresponding operations. The outputs of each operation are summed up, and all three layers (from each operation and the result of their summation) together with the initial two can be sampled on the next step. The number of times the locations are sampled inside the cell is controlled by another hyper-parameter, which we also set to 3 in our experiments in order to keep the number of all possible architectures to a feasible amount 1 . All existing non-sampled summation outputs inside the cell are summed up, and used as the cell output. In this case, we resort to sum as concatenation may lead to variable-sized outputs between different architectures.</p><p>Based on existing research in semantic segmentation, we consider 11 operations: In this example, the controller first samples two indices (block1 and block3), both of which pass through the corresponding cells, before being summed up to create block4. The controller then samples block2 and block3 that are merged into block5. Since block4 was not sampled, it is concatenated with block5 and fed into 1×1 convolution followed by the final classifier. The output of block4 is also passed through an auxiliary cell for intermediate supervision.</p><p>To emit the cell design, the controller starts by sampling the first operation applied on the cell input (op1), followed by sampling of two indices -index0, corresponding to the cell input, and index1 of the output layer after the first operation. Two operations -op2 and op0 -are applied on each index, respectively, and their summation serves as the cell output.</p><formula xml:id="formula_0">• conv 1 × 1, • conv 3 × 3, • separable conv 3 × 3, • separable conv 5 × 5,</formula><p>• global average pooling followed by upsampling and</p><formula xml:id="formula_1">conv 1 × 1, • conv 3 × 3 with dilation rate 3, • conv 3 × 3 with dilation rate 12, • separable conv 3 × 3 with dilation rate 3, • separable conv 5 × 5 with dilation rate 6, • skip-connection,</formula><p>• zero-operation that effectively nullifies the path.</p><p>An example of the search layout with 2 decoder blocks and 2 cell branches is depicted on <ref type="figure" target="#fig_0">Fig. 1. 2</ref> 2 Please refer to Appendix A for more details on the search space and the sampling procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Search Strategy</head><p>We randomly divide the training set into two disjoint sets -meta-train and meta-val. The meta-train subset is used to train the sampled architecture on the given task (i.e., semantic segmentation), whereas meta-val, on the other hand, is used to evaluate the trained architecture and provide the controller with a scalar, oftentimes called reward in the reinforcement learning literature. Given the sampled sequence, its logarithmic probabilities and the reward signal, the controller is optimised via proximal policy optimisation (PPO) <ref type="bibr" target="#b35">[36]</ref>. Hence, there are two training processes present: inner -optimisation of the sampled architecture on the given task, and outer -optimisation of the controller. We next concentrate on the inner loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Progressive Stages</head><p>We divide the inner training process into two stages. During the first stage, the encoder weights are fixed and its out-puts are pre-computed, while only decoder is being trained. This leads to a quick adaptation of the decoder weights and a reasonable estimate of the performance of the sampled architecture. We exploit a simple heuristic to decide whether to continue training the sampled architecture for the second stage, or not. Concretely, the current reward value is being compared with the running mean of rewards seen so far, and if it is higher, we continue training. Otherwise, with probability 1 − p we terminate the training process. The probability p is annealed throughout our search (starting from 0.9).</p><p>The motivation behind this is straightforward: the results of the first stage, while noisy, can still provide a reasonable estimate of the potential of the sampled architecture. At the very least, they would present a reliable signal that the sampled architecture is non-promising, while spending only few seconds on it. Such a simple approach encourages exploration during early stages of search akin to the -greedy strategy often used in the multi-armed bandit problem <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fast Training via Knowledge Distillation and</head><p>Weights' Averaging Semantic segmentation models are notable for requiring many iterations to converge. Partially, this is addressed by initialising the encoder part from a pre-trained classification network. Unfortunately, no such thing exists for decoder. Fortunately, though, we can explore several alternatives that provide faster convergence. Besides tailoring our optimisation hyper-parameters, we rely on two more tricks: firstly, we keep track of the running average of the parameters during each stage and apply them before the final validation <ref type="bibr" target="#b30">[31]</ref>. Secondly, we append an additional l 2 −loss term between the logits of the current architecture and a pre-trained teacher network. We can either pre-compute the teacher's outputs beforehand, or acquire them on-the-fly in case the teacher's computations are negligible.</p><p>The combination of both of these approaches allows us to receive a very reliable estimate of the performance of the semantic segmentation model as quickly as possible without a significant overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Intermediate Supervision via Auxiliary Cells</head><p>We further look for ways of easing optimisation during fast search, as well as during a longer training of semantic segmentation models. Thus, still aligning with the goal of having a compact but accurate model, we explicitly aim to find ways of performing steps that are beneficial during training and obsolete during evaluation.</p><p>One approach that we consider here is to append an auxiliary cell after each summation between pairs of main cells -the auxiliary cell is identical to the main cell and can either be conditioned to output ground truth directly, or to mimic the teacher's network predictions (or the combination of the above two). At the same time, it does not influence the output of the main classifier either during the training or testing and merely provides better gradients for the rest of the network. In the end, the reward per the sampled architecture will still be decided by the output of the main classifier. For simplicity, we only apply the segmentation loss on all auxiliary outputs.</p><p>The notion of intermediate supervision is not novel in neural networks, but to the best of our knowledge, prior works have merely been relying on a simple auxiliary classifier, and we are the first to tie up the design of decoder with the design of the auxiliary cell. We demonstrate the quantitative benefits of doing so in our ablation studies (Sect. 4.2).</p><p>Furthermore, our motivation behind searching for cells that may also serve as intermediate supervisors stems from ever-growing empirical (and theoretical under certain assumptions) evidence that deep networks benefit from overparameterisation during training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref>. While auxiliary cells provide an implicit notion of over-parameterisation, we could have explicitly increased the number of channels and then resorted to pruning. Nonetheless, pruning methods tend to result in unstructured networks often carrying no tangible benefits in terms of the runtime speed, whereas our solution simply permits omitting unused layers during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments on PASCAL VOC which is an established semantic segmentation benchmark that comprises 20 semantic classes (and background) and provides 1464 training images <ref type="bibr" target="#b8">[9]</ref>. For the search process, we extend those to more than 10000 by exploiting annotations from BSD <ref type="bibr" target="#b10">[11]</ref>. As commonly done, during search, we keep 10% of those images for validation of the sampled architectures that provides the controller with the reward signal. For the first stage, we pre-compute the encoder outputs on 4000 images and store them for faster processing.</p><p>The controller is a two-layer recurrent LSTM <ref type="bibr" target="#b13">[14]</ref> neural network with 100 hidden units. All the units are randomly initialised from a uniform distribution. We use PPO <ref type="bibr" target="#b35">[36]</ref> for optimisation with the learning rate of 0.0001.</p><p>The encoder part of our network is MobileNet-v2 <ref type="bibr" target="#b33">[34]</ref>, pretrained on MS COCO <ref type="bibr" target="#b21">[22]</ref> for semantic segmentation using the Light-Weight RefineNet decoder <ref type="bibr" target="#b27">[28]</ref>. We omit the last layers and consider four outputs from layers 2, 3, 6, 8 as inputs to decoder; 1×1 convolutional layers used for adaptation of the encoder outputs have 48 output channels during search and 64 during training. Decoder weights are randomly initialised using the Xavier scheme <ref type="bibr" target="#b9">[10]</ref>. To perform knowledge distillation, we use Light-Weight RefineNet-152 <ref type="bibr" target="#b27">[28]</ref>, and apply 2 −loss with the coefficient of 0.3 which was set using the grid search. The knowledge distillation outputs are pre-computed for the first stage and omitted during the second one in the interests of time. Polyak averaging is applied with the decay rates of 0.9 and 0.99, correspondingly. Batch normalisation statistics are updated during both stages.</p><p>All our search experiments are being conducted on two 1080Ti GPU cards, with the search process being terminated after 4 days. All runtime measurements are carried out on a single 1080Ti card, or on JetsonTX2, if mentioned otherwise. In particular, we perform the forward pass 100 times and report the mean result together with standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search Results</head><p>For the inner training of the sampled architectures, we devise a fast and stable training strategy: we exploit the Adam learning rule <ref type="bibr" target="#b15">[16]</ref> for the decoder part of the network, and SGD with momentum -for encoder. In particular, we use learning rates of 3e-3 and 1e-3, respectively. We pre-train each sampled architecture for 5 epochs on the  first stage, and for 1 on the second (in case the stopping criterion is not triggered). As the reward signal, we consider the geometric mean of three quantities: namely, i.) mean intersection-over-union (IoU), or Jaccard Index <ref type="bibr" target="#b8">[9]</ref>, primarily used across semantic segmentation benchmarks;</p><p>ii.) frequency-weighted IoU, that scales each class IoU by the number of pixels present in that class, and iii.) mean-pixel accuracy, that averages the number of correct pixels per each class. When computing, we do not include background class as it tends to skew the results due to a large number of pixels belonging to background. As mentioned above, we keep the running mean of rewards after the first stage to decide whether to continue training a sampled architecture.</p><p>We visualise the reward progress during both stages on <ref type="figure" target="#fig_1">Figure 2</ref>. As evident from it, the quality of the emitted architectures grows with time -it is even possible that more iterations would lead to better results, although we do not explore that to save the time spent. On the other hand, while random search has the potential of occasionally sampling decent architectures, it finds only a fraction of them in comparison to the RL-based controller.</p><p>Moreover, we evaluate the impact of the inclusion of Polyak averaging, auxiliary cells and knowledge distillation on each training stage. To this end, we randomly sample and train 140 architectures. We visualise the distributions of rewards on <ref type="figure" target="#fig_3">Fig. 3</ref>. All the tested settings significantly outperform baseline on both stages, and the highest rewards on the second stage are attained when using all of the components above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of Intermediate Supervision via Auxiliary Cells</head><p>After the search process is finished, we select 10 architectures discovered by the RL controller with highest rewards and proceed by carrying out additional ablation studies aimed to estimate the benefit of the proposed auxiliary scheme in case the architectures are allowed to train for longer.</p><p>In particular, we train each architecture for 20 epochs on BSD together with PASCAL VOC and 30 epochs on PAS-CAL VOC only. For simplicity, we omit Polyak averaging and knowledge distillation. Three distinct setups are being tested: concretely, we estimate whether intermediate supervision helps at all, and whether auxiliary cell is superior to a plain auxiliar classifier</p><p>The results of these ablation studies are given in <ref type="figure" target="#fig_4">Fig. 4</ref>. Auxiliary supervised architectures achieve significantly higher mean IoU, and, in particular, architectures with auxiliary cells attain best results in 8 out of 10 cases, reaching 3 absolute best values across all the setups and architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relation between search rewards and training performance</head><p>We further measure the correlation effect between rewards acquired during the search and mean IoU attained by same architectures trained for longer. To this end, we randomly sample 30 architectures out of those explored by the controller: for fair comparison, we sample 10 architectures with poor search performance (with rewards being less than 0.4), 10 with medium rewards (between 0.4 and 0.6), and 10 with high rewards (&gt; 0.6). We train each architecture on BSD+VOC and VOC as in Sect. 4.2, rank each according to its rewards, and mean IoU, and measure the Spearman's rank correlation coefficient. As visible in <ref type="figure" target="#fig_6">Fig. 5</ref>, there is a strong correlation between rewards after each stage, as well as between the final reward and mean IoU. This signals that our search process is able to reliably differentiate between poor-performing and well-performing architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Full Training Results</head><p>Finally, we choose 3 best performing architectures from Sect. 4.2 and train each on the full training set, augmented with annotations from MS COCO <ref type="bibr" target="#b21">[22]</ref>. The training setup is analogous to the aforementioned one with the first stage being trained for 30 epochs (on COCO+BSD+VOC), the second stage -for 50 (BSD+VOC), and the last one -for 100 (VOC only). After each stage, the learning rates are halved. Additionally, halfway through the last stage we freeze the batch norm statistics and divide the learning rate in half. We exploit intermediate supervision via auxiliary cells with coefficients of 0.3, 0.25, 0.2, 0.15 across the stages.  <ref type="figure">Figure 6</ref> -Automatically discovered decoder architecture (arch0). We visualise the connectivity structure between encoder and decoder (top), and the cell design (bottom). represents an element-wise summation operation applied to each branch scaled to the highest spatial resolution among them (via bilinear interpolation), while 'gap' stands for global average pooling.</p><p>Quantitative results are given in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="bibr" target="#b2">3</ref> The architectures discovered by our method achieve competitive performance in comparison to state-of-the-art compact models and even do so with a significantly lower number of floating point operations for same output resolution. At the same time, the found architectures can be run in real-time both on a generic GPU card and JetsonTX2. 4 Qualitatively <ref type="figure" target="#fig_8">(Fig. 7)</ref>, our model is able to better recognise similar and easily confused classes (e.g. horse -dog in row 3, and cat -dog in row 5), better segment foreground from background and avoid spurious predictions (rows 1,2,4,5).</p><p>We visualise 5 the structure of the highest performing architecture (arch0) on <ref type="figure">Fig. 6</ref>. With multiple branches encoding information of different scales, it resembles several prominent blocks in semantic segmentation, notably the ASPP module <ref type="bibr" target="#b6">[7]</ref>. Importantly, the cell found by our method differs in the way the receptive field size is controlled. Whereas ASPP solely relies on various dilation rates, here convolutions with different kernel sizes arranged in a cascaded manner allow more flexibility. Furthermore, this design is more computationally efficient and has higher expressiveness as intermediate features are easily re-used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Transferability to other Dense Output Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Pose Estimation</head><p>We further apply the found architectures on the task of pose estimation. In particular, the MPII <ref type="bibr">[</ref>  point <ref type="bibr" target="#b21">[22]</ref> datasets are used as our benchmark. MPII includes 25K images containing 40K people with 16 annotated body joints. The evaluation measure is PCKh <ref type="bibr" target="#b34">[35]</ref> with thresholds of 0.5 and 0.1. The COCO dataset comprises 200K images of 250K people with 17 body joints. Based on object keypoint similarity (OKS) <ref type="bibr" target="#b5">6</ref> , we report average precision (AP) and average recall (AR) over 10 different OKS thresholds.</p><p>Our quantitative results are in <ref type="table" target="#tab_2">Table 2</ref>. <ref type="bibr" target="#b6">7</ref> We follow the training protocol of Xiao et al. <ref type="bibr" target="#b44">[45]</ref> and do not tune our architectures. As can be seen from the results, the discovered architectures achieve competitive performance even in comparison to a more powerful ResNet-50-based model.   <ref type="bibr" target="#b44">[45]</ref> is used for all models. DeepLab-v3+ is our re-implementation based on the official code. <ref type="bibr" target="#b5">6</ref> http://cocodataset.org/#keypoints-eval 7 Additional qualitative and quantitative results are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Depth Estimation</head><p>Finally, we train the architectures on NYUDv2 [37] for depth prediction. Following previous work <ref type="bibr" target="#b26">[27]</ref>, we only use 25K training images with depth annotations from the Kinect sensor, and report validation results on 654 images in <ref type="table" target="#tab_4">Table 3</ref>. Among other compact real-time networks, we achieve significantly better results across all the metrics without any additional tricks. Note also that the work in <ref type="bibr" target="#b26">[27]</ref> trained the depth model jointly with semantic segmentation, thus using extra information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>There is little doubt that manual design of neural architectures is a tedious and difficult task to handle. It is even more complicated to come up with a design of compact and high-performing architecture on challenging dense prediction problems, such as semantic segmentation. In this work, we showcased a simple and reliable approach of searching for fully convolutional architectures within a reasonable amount of time and computational resources. Our method is based around over-parameterisation of small networks that allows them to converge to better solutions. We achieved competitive performance to manually designed state-of-theart compact architectures on PASCAL VOC, while searching only for 4 days on 2 GPU cards. Moreover, best found architectures also attained excellent results on other dense per-pixel tasks -pose estimation and depth prediction.</p><p>Our future goals include exploration of alternative ways of over-parameterisation and search space description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Search Space</head><p>Decoder connectivity structure Our fully-convolutional networks follow the encoderdecoder design paradigm. In particular, in place of the encoder we rely on an existing image classifier -here, MobileNet-v2 <ref type="bibr" target="#b33">[34]</ref>. The decoder has access to 4 layers from the encoder with varying dimensions. To form connections inside the decoder part, we i.) first sample a pair of indices out of 4 possible choices with replacement, ii.) apply the same set of operations (cell) on each sample index, iii.) sum up the outputs <ref type="figure" target="#fig_9">(Fig. 8)</ref>, and iv.) add the resultant layer into the sampling pool. In total, we repeat this process 3 times. Finally, all non-sampled summation outputs are concatenated, before being fed into a single 1 × 1 convolution to reduce the number of channels followed by the final classification layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell structure</head><p>The cell structure is similarly generated via sampling a set of operations and corresponding indices. Nevertheless, there are several notable differences:</p><p>1. The operation at each position can vary; 2. A single operation is applied to the input without any aggregation operator;</p><p>3. After that, two indices and two operations are being sampled with replacement, with the corresponding outputs being summed up (this is repeated 3 times);</p><p>4. The outputs of each operation along with their summation layer are added into the sampling pool.</p><p>An example of the cell structure with its complete search space is illustrated in <ref type="figure" target="#fig_10">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture description</head><p>We use a list of integers to encode the architecture found by the controller, corresponding to the output sequence of the RNN. Specifically, the list describes the connectivity structure and the cell configuration. corresponding layer in the sampling pool. The cell con-</p><formula xml:id="formula_2">figuration, [o 1 , [i 2 , i 3 , o 2 , o 3 ], [i 4 , i 5 , o 4 , o 5 ], [i 6 , i 7 , o 6 , o 7 ]</formula><p>, comprises the first operation o 1 followed by three cell branches with the operation o j applied on the index i j . We provide the description of operations in <ref type="table" target="#tab_6">Table 4</ref>, and visualise the discovered structures in <ref type="figure" target="#fig_0">Fig. 10 (arch0)</ref>, <ref type="figure" target="#fig_0">Fig. 11 (arch1)</ref>, and <ref type="figure" target="#fig_0">Fig. 12 (arch2</ref>   weights are updated using SGD with the momentum value of 0.9, whereas for the decoder part we rely on Adam <ref type="bibr" target="#b15">[16]</ref> with default parameters of β 1 =0.9, β 2 =0.99 and =0.001. We exploit the batch size of 64, evenly divided over two 1080Ti GPU cards. Each image in the batch is randomly scaled in the range of [0.5, 2.0], randomly mirrored, before being randomly cropped and padded to the size of 450×450. During training, in order to calculate the loss term, we upsample the logits to the size of the target mask. In addition to the results presented in the main text, we provide per-class intersection-over-union values across the models in <ref type="table" target="#tab_7">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CityScapes</head><p>We also evaluate whether the found decoder designs work well when coupled with other encoders -in particular, we consider ResNet-50 <ref type="bibr" target="#b11">[12]</ref> and use the CityScapes dataset with 2975 training and 500 validation images. Our training strategy is as follows: we start with the learning rates of 1e−2 and 5e−2 and anneal them using the 'poly' sched-ule <ref type="bibr" target="#b5">[6]</ref> for 500 epochs. For both parts of the network we rely on SGD with the momentum value of 0.9, and train with the batch size of 20. Each image in the batch is randomly scaled in the range of [0.5, 2.0], randomly mirrored, before being randomly cropped and padded to the size of 800×800. As for PASCAL VOC, we upsample the logits to the size of the target mask.</p><p>Validation results together with comparison to few other networks are given in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose estimation</head><p>For pose estimation, we crop the human instance with fixed aspect ratios, 1:1 for MPII <ref type="bibr" target="#b0">[1]</ref> and 3:4 for COCO <ref type="bibr" target="#b21">[22]</ref>. Following Xiao et al. <ref type="bibr" target="#b44">[45]</ref>, the bounding box is further resized such that the longer side is equal to 256. For MPII, ±25% scale, ±30 degree rotation and random flip are used for data augmentation. The scale and rotation factors for COCO are ±30% and ±40 degrees, respectively. We generate keypoint heatmaps of output stride 4 with Gaussian distribution with σ = 2. The MobileNet-v2 encoder is initialised from ImageNet. We use the Adam optimiser with the base learning rate of 1e−3, and reduce it by 10 after epochs 90 and 120. The training terminates at the epoch 140. We use the batch size of 128 evenly split between two 1080Ti GPU cards.</p><p>We provide detailed quantitative results on MPII in <ref type="table">Table 7</ref> and COCO in <ref type="table">Table 8</ref> along with a few qualitative examples on <ref type="figure" target="#fig_0">Fig. 13</ref>. The discovered architectures are able to infer correctly the location of the majority of keypoints (rows 1, 2, 4, 5) while failing on a more difficult input image along with other models (row 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth estimation</head><p>For depth estimation, we start training with the learning rates of 1e-3 and 7e-3 -for the encoder and the decoder, respectively. For both we use SGD with the momentum value of 0.9, and anneal the learning rates via the 'Poly' schedule: lr * (1 − epoch 400 ) 0.9 . The training is stopped after 300 epochs. We exploit the batch size of 32, evenly divided over two 1080Ti GPU cards. Each image in the batch is randomly scaled in the range of [0.5, 2.0], randomly mirrored, before being randomly cropped and padded to the size of 500×500. We upsample the logits to the size of the target mask and use the inverse Huber loss <ref type="bibr" target="#b16">[17]</ref> for optimisation, ignoring pixels with missing depth measurements.</p><p>We visualise qualitative results on the validation set in <ref type="figure">Fig.</ref> 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C: JetsonTX2 runtime</head><p>During our experiments we observed a significant difference between models' runtime on JetsonTX2 and 1080Ti. To better understand it, we additionally measured runtime <ref type="table" target="#tab_4">Model   bg  aero  bike  bird  boat bottle  bus  car  cat  chair  cow  table  dog  horse mbike person plant sheep sofa  train  tv  mean  DeepLab-v3 [</ref>   <ref type="figure" target="#fig_0">Figure 13</ref> -Inference results of our models (arch0, arch1, arch2) on validation set of MPII, along with that of DeepLab-v3+-MobileNet-v2 and ResNet-50 <ref type="bibr" target="#b44">[45]</ref>.</p><p>Image GT arch0 arch1 arch2 RF-LW <ref type="bibr" target="#b26">[27]</ref> Figure 14 -Our depth estimation qualitative results on NYUDv2, along with that of Joint Light-Weight RefineNet <ref type="bibr" target="#b26">[27]</ref>. Dark-blue pixels in ground truth are pixels with missing depth measurements.  <ref type="table">Table 7</ref> -Per-keypoint pose estimation results on the validation set of MPII.</p><p>Model AP AP 50 AP 75 AP m AP l AR DeepLab-v3+ <ref type="bibr" target="#b6">[7]</ref> 0.668 0.894 0.740 0.641 0.707 0.700 ResNet-50 <ref type="bibr" target="#b44">[45]</ref> 0.704 0.886 0.783 0.671 0.772 0.763 Ours (arch0) 0.658 0.894 0.730 0.631 0.701 0.691 Ours (arch1) 0.659 0.884 0.729 0.633 0.698 0.694 Ours (arch2) 0.659 0.890 0.729 0.631 0.700 0.693 <ref type="table">Table 8</ref> -Pose estimation results on the validation set of COCO2017. We report average precision (AP) and average recall (AR). AP 50 and AP 75 stand for average precision computed with the object keypoint similarity (OKS) values of 0.5 and 0.75, respectively, whereas APm and AP l are average precision metrics as measured at medium and large area ranges. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 -</head><label>1</label><figDesc>Example of the encoder-decoder auxiliary search layout. Controller RNN (bottom) first generates connections between encoder and decoder (top left), and then samples locations and operations to use inside the cell (top right). All the cells (including auxiliary cell) share the emitted design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 -</head><label>2</label><figDesc>Distribution of rewards per each training stage for reinforcement learning (RL) and random search (RS) strategies. Higher peaks correspond to higher density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 -</head><label>3</label><figDesc>Distribution of rewards during each training stage of the search process across setups with Polyak averaging (Polyak), intermediate supervision through auxiliary cells (AUX) and knowledge distillation (KD).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 -</head><label>4</label><figDesc>Ablation studies on the value of intermediate supervision (none), and the type of supervision (cell or clf ). Each tick on the x-axis corresponds to a different architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 -</head><label>5</label><figDesc>Correlation between rewards acquired during search stages (a) and mean IoU after full training (b) of 30 architectures on BSD+VOC/VOC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 -</head><label>7</label><figDesc>Inference results of our model (arch0) on validation set of PASCAL VOC, together with Light-Weight-RefineNet (RF-LW) and DeepLab-v3 (DL-v3). All the models rely on MobileNet-v2 as the encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 -</head><label>8</label><figDesc>Block structure of the decoder. The same cell operation is applied to two different layers specified by the connectivity configuration. If the two features have different size, the smaller one is scaled up via bilinear upsampling to match the larger one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 -</head><label>9</label><figDesc>For example, the following connectivity structure [[c 1 , c 2 ], [c 3 , c 4 ], [c 5 , c 6 ]] contains three pairs of digits, indicating the input index c k of a Example cell structure of the decoder. The digit at the upper left corner of each operator is the index of the intermediate features. The cell is designed to utilize these features by skip connections. Except the first operator, other operators can be connected from any previous outputs. The solid black lines indicate the used paths and dashed grey lines are other unused possible paths. The cell configuration to generate the above cell is [op1, [1, 0, op2, op3], [4, 3, op4, op5], [2, 0, op6, op7]].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 -</head><label>12</label><figDesc>arch2: [[[1, 3], [4, 3], [2, 2]], [5, [0, 0, 4, 1], [3, 2, 0, 1], [5, 6, 5, 0]]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>34] 0.94 0.873 0.416 0.849 0.647 0.753 0.937 0.86 0.904 0.391 0.893 0.564 0.847 0.892 0.831 0.844 0.578 0.859 0.525 0.852 0.677 0.759 RefineNet-LW [28] 0.942 0.895 0.594 0.872 0.761 0.669 0.912 0.85 0.876 0.383 0.801 0.605 0.804 0.886 0.835 0.854 0.603 0.843 0.479 0.834 0.703 0.762 Ours (arch0) 0.947 0.885 0.558 0.885 0.748 0.74 0.944 0.868 0.898 0.429 0.863 0.604 0.846 0.842 0.866 0.86 0.592 0.869 0.593 0.875 0.669 0.780 Ours (arch1) 0.944 0.888 0.615 0.866 0.781 0.733 0.933 0.865 0.894 0.394 0.828 0.603 0.833 0.848 0.854 0.855 0.568 0.829 0.555 0.85 0.662 0.771 Ours (arch2) 0.947 0.873 0.589 0.887 0.753 0.75 0.943 0.885 0.895 0.372 0.829 0.635 0.845 0.832 0.867 0.866 0.555 0.843 0.537 0.851 0.671 0.773</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 -</head><label>15</label><figDesc>Models' runtime on JetsonTX2 (a) and 1080Ti (b). We visualise mean together with standard deviation values over 100 passes of each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 -</head><label>1</label><figDesc>Results on validation set of PASCAL VOC after full training on COCO+BSD+VOC. All networks share the same backbone -MobileNet-v2. FLOPs and runtime are being measured on 512 × 512 inputs. For DeepLab-v3 we use official models provided by the authors.</figDesc><table><row><cell>1] and MS COCO Key-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 -</head><label>2</label><figDesc>Comparisons on MPII validation and COCO val2017. Flip test is used. For COCO, the same detector as in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 -</head><label>3</label><figDesc>Quantitative results on the validation set of NYUDv2. For RMSE, abs rel and sqr rel lower values are better, whereas for accuracy (δ) higher values are better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). Note that inside the cell only the final summation operator is displayed as intermediate summations would lead to identical structures.</figDesc><table><row><cell cols="2">Index Abbreviation</cell><cell>Description</cell></row><row><cell>0</cell><cell>conv1x1</cell><cell>conv 1×1</cell></row><row><cell>1</cell><cell>conv3x3</cell><cell>conv 3×3</cell></row><row><cell>2</cell><cell>sep3x3</cell><cell>separable conv 3×3</cell></row><row><cell>3</cell><cell>sep5x5</cell><cell>separable conv 5×5</cell></row><row><cell>4</cell><cell>gap</cell><cell>global average pooling fol-</cell></row><row><cell></cell><cell></cell><cell>lowed by upsampling and conv</cell></row><row><cell></cell><cell></cell><cell>1×1</cell></row><row><cell>5</cell><cell>conv3x3 rate 3</cell><cell>conv 3×3 with dilation rate 3</cell></row><row><cell>6</cell><cell cols="2">conv3x3 rate 12 conv 3×3 with dilation rate 12</cell></row><row><cell>7</cell><cell>sep3x3 rate 3</cell><cell>separable conv 3×3 with dila-</cell></row><row><cell></cell><cell></cell><cell>tion rate 3</cell></row><row><cell>8</cell><cell>sep5x5 rate 6</cell><cell>separable conv 5×5 with dila-</cell></row><row><cell></cell><cell></cell><cell>tion rate 6</cell></row><row><cell>9</cell><cell>skip</cell><cell>skip-connection</cell></row><row><cell>10</cell><cell>zero</cell><cell>zero-operation that effectively</cell></row><row><cell></cell><cell></cell><cell>nullifies the path</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 -</head><label>4</label><figDesc>Operation indices and abbreviations used to describe the cell configuration.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Decoder Structure</cell><cell></cell><cell></cell><cell>Cell Structure</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cell</cell><cell></cell><cell></cell><cell>conv3x3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>gap</cell></row><row><cell>block 0</cell><cell cols="2">block 1</cell><cell>block 2</cell><cell>block 3</cell><cell>cell</cell><cell></cell><cell></cell><cell>sep5x5 rate 6</cell><cell>sep5x5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">conv1x1 conv1x1</cell><cell>cell cell cell</cell><cell>concat conv1x1</cell><cell>x</cell><cell>conv3x3 rate 3 sep3x3</cell><cell>rate 6</cell><cell>y</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>y</cell><cell></cell><cell></cell><cell>sep5x5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">conv1x1</cell><cell>cell</cell><cell></cell><cell></cell><cell></cell><cell>rate 6</cell></row><row><cell cols="10">Figure 10 -arch0: [[[3, 3], [3, 2], [3, 0]], [8, [0, 0, 5, 2], [0, 2, 8, 8], [0, 5, 1, 4]]]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Decoder Structure</cell><cell></cell><cell></cell><cell>Cell Structure</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>conv3x3</cell><cell></cell></row><row><cell>block 1</cell><cell>block 2</cell><cell cols="2">conv1x1 block 3</cell><cell>cell cell</cell><cell>cell cell</cell><cell>x</cell><cell>sep3x3</cell><cell>rate 6 sep5x5 sep5x5</cell><cell>conv3x3 rate 12</cell></row><row><cell></cell><cell></cell><cell cols="2">conv1x1</cell><cell>cell</cell><cell>concat</cell><cell></cell><cell>sep3x3</cell><cell></cell><cell>y</cell></row><row><cell></cell><cell></cell><cell cols="2">conv1x1</cell><cell>cell</cell><cell>conv1x1</cell><cell>y</cell><cell>conv3x3</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>rate 12</cell><cell></cell></row><row><cell cols="10">Figure 11 -arch1: [[[2, 3], [3, 1], [4, 4]], [2, [1, 0, 3, 6], [0, 1, 2, 8], [2, 0, 6, 1]]]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 -</head><label>5</label><figDesc>Per-class intersection-over-union on the validation set of PASCAL VOC.</figDesc><table><row><cell>GT</cell><cell>arch0</cell><cell>arch1</cell><cell>arch2</cell><cell>DeepLab-v3+</cell><cell>ResNet-50 [45]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Taking into account symmetrical -thus, identical -architectures, we estimate the number of unique connections in the decoder part to be 120, and the number of unique cells ∼10 10 , leading to ∼10 12 , which is on-par with concurrent works.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Per-class measures are provided in Appendix B.<ref type="bibr" target="#b3">4</ref> Please refer to Appendix C on notes regarding the Jetson's runtime.<ref type="bibr" target="#b4">5</ref> Other architectures are visualised in Appendix A.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements VN, CS, IR's participation in this work were in part supported by ARC Centre of Excellence for Robotic Vision. CS was also supported by the GeoVision CRC Project. Correspondence should be addressed to CS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC</head><p>We start training with the learning rates of 1e-3 and 3e-3for the encoder and the decoder, respectively. The encoder</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An evolutionary algorithm that constructs recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Angeline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>abs/1809.04184</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the power of over-parametrization in neural networks with quadratic activation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Intell. &amp; Stat</title>
		<meeting>Int. Conf. Artificial Intell. &amp; Stat</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural architecture search with bayesian optimisation and optimal transport</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. 3D Vision</title>
		<meeting>Int. Conf. 3D Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RefineNet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural architecture optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf</title>
		<meeting>Advances in Neural Inf</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-time joint semantic segmentation and depth estimation using asymmetric annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<idno>abs/1809.04766</idno>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Light-weight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vis</title>
		<meeting>British Machine Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<title level="m">Policy distillation. Proc. Int. Conf. Learn. Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MODEC: multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Theoretical insights into the optimization landscape of overparameterized shallow neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CReaM: Condensed real-time models for depth prediction using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots &amp; Systems</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots &amp; Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A hypercube-based encoding for evolving large-scale neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>. D&amp;apos;ambrosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evolving neural network through augmenting topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<title level="m">Raiders of the lost architecture: Kernels for bayesian optimization in conditional parameter spaces. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">Taskonomy: Disentangling task transfer learning. Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Model Backbone Val mIoU</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Table 6 -Results on the validation set of CityScapes. of each discovered architecture together with Light-Weight RefineNet [28] varying the input resolution. As evident from Fig. 15, the models with a larger number of floating point operations (i.e., Arch0 and RF-LW) do not scale well with the input resolution. The effect is even more pronounced on JetsonTX2, as been independently confirmed by an NVIDIA</title>
		<imprint/>
	</monogr>
	<note>employer in a private conversation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
