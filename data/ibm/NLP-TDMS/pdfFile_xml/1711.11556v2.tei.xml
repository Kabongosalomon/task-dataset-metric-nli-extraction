<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
							<email>yuhua.chen@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
							<email>liwen@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ESAT/PSI</orgName>
								<orgName type="institution">VISICS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatialaware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.</p><p>• real distribution orientation: To deal with the dis-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the exciting vision of autonomous driving, semantic segmentation of urban scenes, as a key module, has gained increasing attention from both academia and industry. However, collecting and labeling training data for semantic segmentation task is a laborious and expensive process, as it requires per-pixel annotation. This issue becomes even more severe with the surge of deep learning techniques, which usually require a large amount of training data. Therefore, it becomes much desired to exploit low cost ways to acquire data for semantic segmentation.</p><p>One way that becomes recently prevalent is to collect photo-realistic synthetic data from video games, where pixel-level annotation can be automatically generated at a much lower cost. For example, Richter etal. <ref type="bibr" target="#b31">[32]</ref> constructed a large scale synthetic urban scene dataset for semantic segmentation from the GTAV game. While the cost of acquiring training data and annotation is largely reduced, synthetic data still suffers from a considerable domain difference from the real data, which usually leads to a significant performance drop when applying the segmentation model to real world urban scenes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>The main reasons are two-fold. First, from the perspective of representation, since the model is trained on synthetic images, the convolutional filters tend to overfit to synthetic style images, making them incompetent to extract informative features for real images. Second, from the distribution perspective, synthetic and real data suffers a considerable distribution mismatch, which makes the model biased to synthetic domain.</p><p>To overcome such problems, we propose Reality Oriented ADaptation Networks(ROAD-Net) for semantic segmentation of urban scenes by learning from synthetic data. We address the above two issues respectively by a target guided distillation module for real style orientation, and a spatial-aware adaptation module for real distribution orientation, which are described respectively as follows,</p><p>• real style orientation: To prevent the segmentation model from overfitting to synthetic images, we propose to use the target real images to imitate a pretrained real style model. This can be achieved using the model distillation strategy by enforcing the output from segmentation model similar with the output of a pretrained model. On one hand, this encourages convolutional filters to fit to the real images through the distillation task. On the other hand, it also enforces the segmentation network to preserve good discriminative for real images by approaching the semantic output from the pretrained model. We refer to it as target guided distillation. <ref type="figure">Figure 1</ref>. Illustration of our Reality Oriented Adaptation networks(ROAD-Net) for semantic segmentation of urban scenes. Our network is built upon conventional semantic segmentation networks, and incorporates a target guided distillation module for real style orientation, and a spatial-aware adaptation module for real distribution orientation.</p><p>tribution mismatch for semantic segmentation, several recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref> applied domain adaptation methods on pixel-level features. However, this is generally a challenging task partially due to the large visual variance in urban scene. For example, the objects in the central region are usually much smaller, compared to objects in the outer region. Aligning distributions directly under such large variance presents significant difficulty. We therefore aim to ease the domain adaptation task by exploiting the intrinsic geometry information presented in urban scene. We propose a spatial-aware adaptation method to effectively align the two domains while considering the difference in spatial distribution. In particular, we divide the scene image into different spatial regions, and align the source and target domain samples from the same spatial region respectively. In this way, we align features from two domains with similar spatial properties.</p><p>The two modules above can be easily integrated with existing state-of-the-art semantic segmentation networks to boost their generalizability when adapting from synthetic to real urban scenes. We conduct extensive experiments by using the GTAV dataset and the Cityscapes datset. Our proposed method achieves a new state-of-the-art of 39.4% mean IoU. To further validate the effectiveness, we additionally evaluate our methods using SYNTHIA dataset and the Cityscapes dataset, in which the proposed method outperforms other competing methods as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Semantic Segmentation: Semantic segmentation is a highly active field since decades ago with large amount of methods proposed, we briefly review some of the works with a focus on CNN-based methods. Traditional works in semantic segmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref> are typically based on manually designed image features. With the recent surge of deep learning <ref type="bibr" target="#b18">[19]</ref>, learned representation demonstrated its power in many computer vision tasks. Pioneered by <ref type="bibr" target="#b24">[25]</ref>, the power of CNN has been transferred to semantic segmentation and we have witnessed a rapid boost in semantic segmentation performance. Long etal. <ref type="bibr" target="#b24">[25]</ref> formulates semantic segmentation as a per-pixel classification problem. Following the line of FCN-based semantic segmentation. Dilated convolution is proposed by <ref type="bibr" target="#b46">[47]</ref> to enlarge the receptive field. DeepLab <ref type="bibr" target="#b0">[1]</ref> incorporates Conditional random filed(CRF) with CNN to reason about spatial relationship. Recently, Zhao etal. <ref type="bibr" target="#b49">[50]</ref> proposed to use Pyramid Pooling Module to encode the global and local context, which achieved state-of-the-arts results on multiple datasets.</p><p>Domain Adaptation: A basic assumption in conventional machine learning is that the training and test data are sampled independently from an identical distribution, or i.i.d assumption in short. However, this does not always hold in real world scenarios, which often leads to a significant performance drop on the test data when applying the trained model. Domain adaptation aims to alleviate the impact of such distribution mismatch such that the generalization ability of the learned model can be improved on the target domain. In computer vision, domain adaptation has been widely studied as an image classification problem in computer vision <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b22">23]</ref>. Conventional methods include asymmetric metric learning <ref type="bibr" target="#b19">[20]</ref>, subspace interpolation <ref type="bibr" target="#b11">[12]</ref>, geodesic flow kernel <ref type="bibr" target="#b9">[10]</ref>, subspace alignment <ref type="bibr" target="#b6">[7]</ref>, covariance matrix alignment <ref type="bibr" target="#b39">[40]</ref>, etc. Recent works aim to improve the domain adaptability of deep neural networks, including <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Different from those works, our work aims to solve the semantic segmentation task, which is more challenging due to the large variance in pixel-level features.</p><p>Domain Adaptation for Semantic Segmentation: So far most works in domain adaptation focus on the task of image classification. Not until recently has the community pay attention to domain shift problem in semantic segmentation. This line of research is pioneered by <ref type="bibr" target="#b16">[17]</ref>, where they use adversarial training to align the feature from both domain. Another approach is done by <ref type="bibr" target="#b48">[49]</ref>, where they use a curriculum learning style approach to solve the domain shift. There are also some concurrent works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43]</ref> concerning similar problem in different ways.</p><p>Learning Using Synthetic Data: There are also a few works proposed to learn from synthetic data <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. In <ref type="bibr" target="#b40">[41]</ref>, generic object detectors are trained from synthetic images, while in <ref type="bibr" target="#b43">[44]</ref> virtual images are used to improve pedestrian detections in real environment. More recently, <ref type="bibr" target="#b2">[3]</ref> extends Faster R-CNN to learn a domaininvariant detector using synthetic data or data from another domain.</p><p>Other Related Works : From the methodology, our work is inspired by the model distillation <ref type="bibr" target="#b15">[16]</ref>, which was proposed for network compression. The following up works employed a similar strategy to perform knowledge transfer <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref>, and cross-modality supervision transfer <ref type="bibr" target="#b12">[13]</ref>. Our work, on the other hand, using the distillation strategy for learning the real style convolutional filters for semantic segmentation. Our work is also inspired by the previous works on scene understanding <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref>, in which they also exploited the intrinsic spatial layout prior of scene images for different tasks in various ways. We share a similar philosophy with those work, but aim to solve the distribution alignment between two domains for the urban scene semantic segmentation, which results in a totally different methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reality Oriented Adaptation Networks</head><p>In this section, we present our Reality Oriented Adaptation networks(ROAD-Net) for urban scene semantic segmentation. Two new modules are proposed in our ROAD-Net model, the target guided distillation and spatial-aware adaptation, which will be introduced respectively in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target Guided Distillation</head><p>When learning a model from synthetic urban scenes, one major issue is that synthetic data usually exhibits a clear visual difference with real images. As a result, the learned model is biased towards the synthetic data, which leads to an unsatisfactory segmentation performance on real urban scene images.</p><p>From the perspective of representation learning, this is largely because that the convolutional filters in the semantic segmentation models overfit to the synthetic image style. Consequently, when taking a real image as input, the feature representation generated using the learned model might not be sufficiently informative for semantic segmentation, since the convolutional filters tend to extract information that look discriminative in synthetic style only. To cope with this issue, we therefore propose to use the target real images to <ref type="bibr">Figure 2</ref>. Illustration of target guided distillation. By feeding the target real images into both the segmentation model, and a pretrained model, the segmentation model is encourage to imitate the pretrained model to learn real style convolutional filters. guide the segmentation model to learn robust convolutional filters for the real urban scenes.</p><p>Our method is motivated by the common practice in computer vision community of initializing network weights using a network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39]</ref> pretrained on large-scale image dataset such as ImageNet <ref type="bibr" target="#b5">[6]</ref>. Such strategy has also been widely exploited for learning semantic segmentation models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. Recall that the original ImageNet model is pretrained on real images, so we propose to employ a distillation loss to guide the semantic segmentation models to behave like the pretrained real style model.</p><p>We illustrate the target guided distillation process in Figure 2. The source synthetic urban scene images and corresponding annotation are used to learn the semantic segmentation model. At the same time, the unlabeled real urban scene images from target domain are used to imitate their corresponding feature map output from the pretrained Ima-geNet model. The pretrained model has the same structure as the backbone network used in the semantic segmentation model, and is frozen during this process.</p><p>Formally, for a real image, let us denote x i,j as the activation at the position (i, j) of the feature map from the semantic segmentation model, and also denote z i,j as an activation at the same location of the feature map from the pretrained model, then the loss for target guided distillation can be written as,</p><formula xml:id="formula_0">L dist = 1 N i,j x i,j − z i,j 2<label>(1)</label></formula><p>where N is the number of activations at the feature map, and · 2 is the Euclidean distance.</p><p>Discussion: There also exists other alternative approaches to prevent convolutional filters from overfitting to synthetic data. For instance, considering the semantic segmentation model is initialized with a pretrained ImageNet model which is learned from the real images, a possible way is to freeze a few convolutional layers (i.e., to set the learning rates for those layers to zero), such that convolutional filters will not be corrupted by synthetic data. As shown in our experiments(c.f. Section 4.3), this does work to some extent. However, it is still inferior to our target guided distillation approach. The main reason is that the ImageNet model is trained for image classification, and may not be optimal to the semantic segmentation of urban scenes. Freezing a few layers may help to prevent the model from overfitting to synthetic data, but also limits its capacity for the semantic segmentation task. As a comparison, in target guided distillation, all weights are allowed to be tuned, and convolutional filters are guided in a soft manner to imitate the ImageNet model when being trained for the segmentation task.</p><p>Another alternative approach is to use source data to perform the distillation task, which is also known as learning without forgetting <ref type="bibr" target="#b23">[24]</ref>. Though it is able to imitate the Im-ageNet pretrained model, we argue that it is not as effective as using the target real urban scene images, because the convolutional filters can be still corrupted due to taking solely synthetic images as input. Moreover, the deep neural networks often contain multiple layers, and the filters in higher layers are able to be trained to well imitate the ImageNet model even with corrupted low layers convolutional filters. We conduct an experimental comparison with all alternative baselines in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial-Aware Adaptation</head><p>Even with the target guided distillation, the feature presentation of two domains extracted by the semantic segmentation networks could still suffer from a distribution mismatch, due to the large domain difference between synthetic and real images. Thus, it is desirable to align the distributions of two domains with domain adaptation methods.</p><p>However, aligning pixel-level features between synthetic and real data for urban scene images is non-trivial. Conventional domain adaptation approaches are usually proposed for the image classification task. While similar methodology can be applied by taking each pixel-level feature as a training sample, it is still challenging to fully reduce the distribution mismatch, since the pixels vary significantly in appearance <ref type="bibr" target="#b16">[17]</ref> and scale <ref type="bibr" target="#b1">[2]</ref>.</p><p>To address this issue, we hence propose to leverage the intrinsic spatial structure in urban scene images to improve the domain distribution alignment between synthetic and real data. Our motivations are two-fold. Firstly, while the pixel-level features vary a lot, such variance also exhibits a nice spatial pattern. For example, in an urban scene image, the objects in the central region are usually small, while objects in the outer region are relatively larger. Second, the semantic categories also roughly follow a spatial layout. Usually road appears in the bottom part of an image, <ref type="figure">Figure 3</ref>. Illustration of spatial-aware adaptation. The source and target pixel-level features in the same spatial region are aligned(i.e., the regions marked with the same color), which eases the domain distribution adaptation task. and sky appears on the top part of an image. Therefore, it is beneficial to align pixel-level features with similar sizes and semantics based on their spatial location.</p><p>In particular, we divide each scene image into different spatial regions, and domain alignment is performed on the pixel-level features from the same spatial region. We illustrate this process in <ref type="figure">Figure 3</ref>. For each region, any traditional domain distribution alignment can be deployed. Formally, suppose there are m = 1, . . . , M regions. We denote R m as the set of indices contains in the m-th region, i.e., (u, v) ∈ R m means that a pixel-level activation x u,v locates at this region, and vice versa. Let us denote</p><formula xml:id="formula_1">X s m = {x s u,v |(u, v) ∈ R m } and X t m = {x t u,v |(u, v) ∈ R m }</formula><p>as all pixel-level activations locate at the m-th region for source and target domains respectively, then the loss for spatialaware adaptation can be written,</p><formula xml:id="formula_2">L spt = M m=1 L da (X s m , X t m )<label>(2)</label></formula><p>where L da is a domain adaptation loss which measures the domain difference between two sets of samples. We present an example of L da below. Domain Adversarial Training: Aligning two distributions has been widely studied in the literature. In this work, we deploy an H-divergence based loss used in the DANN model <ref type="bibr" target="#b7">[8]</ref>. Specifically, let us denote h : x → {0, 1} as a domain classifier, which is used to predict which domain an input pixel-level feature x comes from, where 0 denotes the source domain, and 1 denotes the target domain. Given a set of training data X = X s ∪ X t , the loss for training the domain classifier h can be written as:</p><formula xml:id="formula_3">L H (X s , X t ) = 1 |X | x∈X (h(x), d)<label>(3)</label></formula><p>where |X | is the number of samples in X , d ∈ {0, 1} is the domain label of x, (·, ·) is a conventional classification loss for which we use the cross-entropy loss.</p><p>Intuitively, training a domain classifier is to distinguish samples from two domains. To reduce the domain difference, we thus encourage the activation x to be domainindistinguishable. Considering each x is generated from a base network, denoted by F , we thus need to optimize F such that the domain classification loss L H is maximized. By jointly learning the domain classifier h and the base network F , we arrive at the following maxmin problem,</p><formula xml:id="formula_4">max F min h L H (X s , X t )<label>(4)</label></formula><p>The above maxmin problem can be optimized in an adversarial training manner. Hoffman et al. <ref type="bibr" target="#b16">[17]</ref> implemented it by switching domain label similarly as in the Generative Adversarial Networks (GAN) <ref type="bibr" target="#b10">[11]</ref>. We follow <ref type="bibr" target="#b7">[8]</ref> to insert a gradient reverse layer between F and h. Particularly, in back-propagation, the sign of gradients will be flipped when being passed through the gradient reverse layer, hence the classifier h is minimized while the network F maximized by directly using the conventional optimization methods like stochastic gradient descent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Overview</head><p>The aforementioned two modules can be integrated with a conventional semantic segmentation network, such as Di-latedNet <ref type="bibr" target="#b46">[47]</ref>, DeepLab <ref type="bibr" target="#b0">[1]</ref>, PSPNet <ref type="bibr" target="#b49">[50]</ref> etc. In particular, let us denote L seg as the segmentation loss, then our ROAD-Net model can be trained by minimizing a joint loss as follows,</p><formula xml:id="formula_5">L ROAD = L seg + λ 1 L dist + λ 2 L spt .<label>(5)</label></formula><p>where λ 1 and λ 2 are two trade-off parameters, which are set to 0.1 and 0.01 in our experiments. Note that the loss L spt in the above equation corresponds to L H in (4). Although we have removed the max operator in (4) from the above loss, it can be automatically achieved with the reverse gradient layer. We illustrate the pipeline of our proposed ROAD-net in <ref type="figure">Fig 1.</ref> During training, both synthetic and real images are fed into the network as input. The synthetic images and the annotation are used to train the segmentation task, while the real images are used to train the target guided distillation task. Both synthetic and real images are used for optimizing the spatial-aware adaptation loss. Note that, when training semantic segmentation model, due to the high resolution of input image, the input image is usually randomly cropped to fit the GPU memory. To perform the spatialaware adaptation, we therefore build a spatial-aware splitting layer, in which we recover the location for each activation in the original image coordinate, and split them into different domain classifiers according to the region it comes from. During the test phase, the two newly added modules can be removed, and one can perform semantic segmentation as the same as for the conventional semantic segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we present our experimental results on semantic segmentation of real urban scenes by learning from synthetic data. Experimental analysis and comparison with state-of-the-arts are also provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>We follow the classical unsupervised domain adaptation protocol where the supervision is assumed to be given in the source domain, and only unlabeled data is provided in the target domain. Following previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>, the experimental validation are conducted on the GTAV dataset <ref type="bibr" target="#b31">[32]</ref> and Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref>. We use GTAV dataset as our source domain, and we have access to the pixel-level annotation, and we use Cityscapes dataset as the target domain. Our goal is to learn a semantic segmentation model from synthetic data. We briefly introduce the datasets used in our experiment in below:</p><p>Cityscapes is a dataset focused on autonomous driving, which consists of 2, 975 images in training set, and 500 images for validation. The images have a fixed resolution of 2048 × 1024 pixels, and are captured by a video camera mounted in front of a car. 19 semantic categories are provided with pixel-level labels. In our experiment, we use the unlabeled images from the training set as the target domain to adapt our segmentation model, and the results are reported on the validation set.</p><p>GTAV is a dataset recently proposed for learning from synthetic data. It has 25, 000 photo-realistic images rendered by the gaming engine Grand Theft Auto V (GTAV). The resolution of images is around 2000 × 1000 pixels which is similar with Cityscapes, the semantic categories are also compatible between the two datasets.</p><p>In our experiments, we test the our proposed ROAD-Net with two semantic segmentation methods: DeepLab v2 <ref type="bibr" target="#b0">[1]</ref> and PSPNet <ref type="bibr" target="#b49">[50]</ref>. However, as discussed in the previous section, our method can be straightforwardly applied to other semantic segmentation methods as well.</p><p>The network is initialized using an ImageNet pre-trained weights, and the last convolutional layer is replaced with our new classification head to predict Cityscapes label. Similar with <ref type="bibr" target="#b0">[1]</ref>, a learning rate of 2.5 × 10 −4 is used, and learning rate policy of poly is used in our experiment. Each batch contains 10 sampled patches of size 321 × 321, of which 5 patches are from source domain, and the other 5 patches from target domain. The cross-entropy loss is used for supervising semantic segmentation in the source domain as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b0">1]</ref>. used as the base model, respectively. We report different variants of our proposed ROAD-Net, where "dst" and "spt" in the head row refers to the target guided distillation module and the spatial-aware adaptation module, respectively. The "NonAdapt" refers to the vanilla model which is trained using the GTAV data only. The best results for each base model are denoted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>For a comprehensive study on our method, we include two variants of our method, using only the target guided distillation module (referred to as "dst") and using only the spatial-aware adaptation module (referred to as "spt"). The vanilla base network without using any module is also included as a baseline (referred to as "NonAdapt"). Two different semantic segmentation networks are used, DeepLab V2 and the PSP Net, where the former one uses VGG-16 as the backbone network, and the latter one uses ResNet-101.</p><p>The results of all variants of our methods based on two different models are summarized in Talbe 1. Taking the results using DeepLab model as an example, the "NonAdapt" baseline gives 21.9% mean IoU, which is similar to those reported in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>, where VGG-16 was also used as the backbone network. By using our proposed target guided distillation module and spatial-aware adaptation module respectively, we obtain a mean IoU of 31.1% and 32.6%, which improves the baseline by a large margin of 9.2% and 10.7%, respectively. This clearly demonstrates the effectiveness of the two modules for orienting the semantic segmentation model to learn reality. By combining two modules, our final ROAD-Net gains further improvements, achieving 35.9% in terms of mean IoU. Those observations can also be observed for the results using the PSP-Net. Moreover, using PSPNet, all methods are improved, and our ROAD-net finally achieves 39.4% mean IoU. Some randomly selected qualitative examples are shown in <ref type="figure" target="#fig_0">Fig. 5</ref>. We observe that the proposed method, especially the spatial aware module, improves the visual segmentation result notably, and produces predictions with much more reasonable spatial layout.</p><p>The proposed method does not achieve significant improvement on some categories, such as pole and traffic sign. A possible reason might be that those categories have very few pixels(e.g. in Cityscapes Val, only 1.8M pixels for traffic light, as a comparison, there are 345M pixels for road), <ref type="figure">Figure 4</ref>. Study on alternative methods to target guided distillation: "bs" refers to the basline NonAdapt method, "fr" refers to the frozen method, "sd" refers to the source distillation method, and "td" refers to the target guided distillation. which makes the results less stable compared to categories with more pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis on Real Style Orientation</head><p>To validate the effectiveness of our proposed target guided distillation module, we conduct experiments by comparing with the two alternative methods discussed in Section 3.1. In particular, the "frozen" method refers to freezing the first a few layers when training the segmentation model, and "source distillation" refers to replacing real images with synthetic images for distillation. For all methods, the DeepLab network is deployed due to its efficiency. All other experimental settings are identical with previous experiments.</p><p>The results of different methods are summarized in <ref type="figure">Fig 4</ref> where the "NonAdapt" baseline is also included for comparison. From the figure, we observe that all methods achieve performance gain when compared with the "Non-Adapt" baseline, which implies that it is the beneficial to prevent the model from overfitting the synthetic images. Directly freezing a few layers is similar to the "source distillation' method. Our target guided distillation method  achieves the best improvement, which demonstrates its effectiveness by using real images in distillation to prevent the models from over-fitting synthetic style images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis on Real Distribution Orientation</head><p>In our experiments, we divide the image into 3×3 spatial regions for our spatial-aware adaptation module. To study the effect of different partitions of regions on our method, we conduct additional experiments by using 1 × 1, 2 × 1, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison to State-of-the-arts</head><p>In this section, we compare our method with two recent methods on semantic segmentation of urban scenes. The first one is FCNs in the Wild <ref type="bibr" target="#b16">[17]</ref>, which also adopted an adversarial training strategy, and further deployed a category specific adaptation method to align pixels of two domains that are likely from the same category. The second one is the curriculum learning adaptation <ref type="bibr" target="#b48">[49]</ref>, in which a curriculum learning approach was used to progressively adapt to the target domain. In both works, the dilation model with VGG-16 backbone was used as the base network.</p><p>For a fair comparison, we compare the results of our ROAD-Net based on VGG-16 model with those two stateof-the-arts, which are summarized in the top part of <ref type="table" target="#tab_1">Table 2</ref>. The "NonAdapt" baselines for each work are also included for comparison. We observe that, the results from our "Non-Adapt" baseline is similar to those reported in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49]</ref>, despite that we use the DeepLab model while Dilation network is used their experiment. Moreover, by using the proposed two module for real style orientation and real distribution orientation, our ROAD-Net achieves a mean IoU of 35.9%, which outperforms the two state-of-the-art methods. Specifically, we achieves +7.0% and +8.8% improvement compared to <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b16">[17]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Additional Results on SYNTHIA</head><p>To further validate the effectiveness of the proposed method, we additionally perform an experiment using SYN-THIA <ref type="bibr" target="#b32">[33]</ref> as source domain and Cityscapes as target domain. SYNTHIA is a dataset with synthetic images of urban scenes, with pixel-wise annotations. The rendering are across a variety of environments and weather conditions. In our experiment, we use the SYNTHIA-RAND-CITYSCAPES subset, which contains 9,400 images compatible with the cityscapes classes. All experiment settings remain unchanged with the previous experiments. For fair comparison, VGG-16 is used as backbone model. The results of all methods are summarized in the bottom part of <ref type="table" target="#tab_1">Table 2</ref>. The "NonAdapt" baseline for each method is also included for comparison. We observe that our proposed approach outperforms the other methods by a large margin, which again demonstrates the effectiveness of our ROAD Net in cross-domain semantic segmentation of urban scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a new model Reality Oriented ADaptation Network(ROAD-Net) for semantic segmentation of urban scenes by learning from synthetic data. Two modules are proposed in our paper, target guided distillation and spatial-aware adaptation, where the former one aims to adapt the style from the real images by imitating the pretrained network, and the latter one is used to reduce the domain distribution mismatch with the help of layout information. Those two modules can be integrated with different semantic segmentation networks to improve their generalizability when applying to a new domain. We evaluate the proposed method on Cityscapes, using GTAV and STYNHIA as the source domain. The experiments on benchmark datasets have clearly demonstrated the effectiveness of our proposed ROAD-Net.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative semantic segmentation results on the Cityscapes: (a) original image, (b) ground truth annotation, (c) NonAdapt baseline, (d) our model using target guided distillation only, (e) our model using spatial aware adaptation only, (f) our final ROAD-Net model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Effects of different partitions in spatial-aware adaptation. H×W denotes that the image is divided H times along the vertical axis, and W times along the horizontal axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b15">16</ref>.0 56.6 9.2 17.3 13.5 13.6 9.8 74.9 6.7 54.3 41.9 2.9 45.0 3.<ref type="bibr" target="#b2">3</ref> 13.1 1.3 6.8 0.0 21.9 Ours 82.0 29.5 73.8 20.0 12.7 8.5 14.6 9.0 73.2 27.6 63.2 39.3 5.9 71.7 23.1 17.6 14.4 4.0 0.0 31.1 85.3 36.1 77.5 24.1 18.8 21.4 22.5 13.2 71.3 20.9 54.5 40.6 6.9 76.7 19.3 16.4 7.0 7.1 0.0 32.6 85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4 8.3 0.0 35.9 31.2 15.7 14.4 15.2 82.5 36.1 73.1 52.1 17.1 78.1 26.5 28.0 3.1 24.7 4.4 36.8 75.9 34.2 72.3 28.0 21.7 30.4 25.2 15.2 79.4 33.5 73.5 52.3 18.7 76.2 31.0 25.2 3.5 23.7 2.1 37.8 76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4 The segmentation results on the Cityscapes dataset by using the GTAV dataset as the source domain. DeepLab and PSPNet are</figDesc><table><row><cell>dst spt</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mean IoU</cell></row><row><cell cols="21">DeepLab 29.8 PSPNet NonAdapt NonAdapt 14.6 17.0 40.4 21.1 14.7 18.3 18.0 5.6 80.8 16.0 68.4 49.1 4.4 78.5 31.8 23.9 1.5 24.4 0.0 27.8 Ours 73.2 32.0 71.0 25.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>] 31.9 18.9 47.7 7.4 3.1 16.0 10.4 1.0 76.5 13.0 58.9 36.0 1.0 67.1 9.5 3.7 0.0 0.0 0.0 21.1 FCNs Wld [17] 70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0 3.5 0.0 27.1 NonAdapt [49] 18.1 6.8 64.1 7.3 8.7 21.0 14.9 16.8 45.9 2.4 64.4 41.6 17.5 55.3 8.4 5.0 6.9 4.3 13.8 22.3 Curriculum [49] 74.8 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 14.6 28.9 NonAdapt 29.8 16.0 56.6 9.2 17.3 13.5 13.6 9.8 74.9 6.7 54.3 41.9 2.9 45.0 3.3 13.1 1.3 6.8 0.0 21.9 Ours 85.4 31.2 78.6 27.9 22.2 21.9 23.7 11.4 80.7 29.3 68.9 48.5 14.1 78.0 19.1 23.8 9.4 8.3 0.0 35.9 25.8 10.3 15.6 77.6 -79.8 44.5 16.6 67.8 -14.5 -7.0 23.8 36.2 Comparison with state-of-the-arts methods for semantic segmentation on Cityscapes using synthetic datasets as the training data. Top: adapting from GTAV, Bottom: adapting from SYNTHIA. Results of state-of-the-art methods are from their papers. We use VGG-16 as the backbone network for fair comparison. The best results are denoted in bold.</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic light</cell><cell>traffic sign</cell><cell>vegetation</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorbike</cell><cell>bicycle</cell><cell>mean IoU</cell></row><row><cell cols="21">GTAV NonAdapt [17SYNTHIA NonAdapt [17] 6.4 17.7 29.7 1.2 0.0 15.1 0.0 7.2 30.3 -66.8 51.1 1.5 47.3 -FCNs Wld [17] 11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 -68.7 51.2 3.8 54.0 -NonAdapt [49] 5.6 11.2 59.6 0.8 0.5 21.5 8.0 5.3 72.4 -75.6 35.1 9.0 23.6 -Curriculum [49] 65.2 26.1 74.9 0.1 0.5 10.7 3.7 3.0 76.1 -70.6 47.1 8.2 43.2 -20.7 -0.7 13.1 29.0 3.9 -0.1 0.0 17.4 3.2 -0.2 0.6 20.2 4.5 -0.5 18.0 22.0 NonAdapt 4.7 11.6 62.3 10.7 0.0 22.8 4.3 15.3 68.0 -70.8 49.7 6.4 60.5 -11.8 -2.6 4.3 25.4</cell></row><row><cell>Ours</cell><cell cols="5">77.7 30.0 77.5 9.6 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">× 2 partitions, respectively. The DeepLab network is used in this experiment, while other experimental settings remain unchanged with previous experiments.The results by using different region partitions are shown inFig. 6. Among those, the case of 1 × 1 is the ordinary domain adaptation method without considering the spatial information, which is also equivalent to the global alignment method in<ref type="bibr" target="#b16">[17]</ref>. We observe that all other partition methods outperform this baseline, which demonstrate the benefit of exploiting spatial information when aligning the domain distribution for urban scenes. The 2 × 1 (i.e., a top region and a bottom region) gains improvement when compared to the baseline, largely because it uses the semantic prior(e.g., the sky is in the top region whereas the road is in the bottom region). 2 × 2 gets little further improvement over 2 × 1, mainly because that the urban scene is generally vertically symmetric, thus a further partitioning the regions along the vertical middle does not help too much. The 3 × 3 partition gives the best results, a possible explanation is that it contains a central region, which further copes with the size variance caused by the perspective transformation (e.g., the objects in the central region are smaller, while the objects outside the central are larger).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Wen Li is the corresponding author. This project is supported in part by Toyota Motor Europe. The authors gratefully thank NVidia Corporation for donating GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scale-aware alignment of hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptive Faster R-CNN for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Associative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haeusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frerix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">FCNs in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Domain generalization and adaptation using low rank exemplar SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When unsupervised domain adaptation meets tensor representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Maria</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panareda</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Synthetic to real adaptation with deep generative correlation alignment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06969</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferrable representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic texton forests for image categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07828</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05547</idno>
		<title level="m">Return of frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From virtual to reality: Fast adaptation of virtual object detectors to real domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10349</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="797" to="809" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Transferring deep object and scene representations for event recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hierarchical adaptive structural svm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic segmentation of urban scenes using dense depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09465</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
