<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NETTAILOR: Tuning the architecture, not just the weights</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NETTAILOR: Tuning the architecture, not just the weights</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Real-world applications of object recognition often require the solution of multiple tasks in a single platform. Under the standard paradigm of network fine-tuning, an entirely new CNN is learned per task, and the final network size is independent of task complexity. This is wasteful, since simple tasks require smaller networks than more complex tasks, and limits the number of tasks that can be solved simultaneously. To address these problems, we propose a transfer learning procedure, denoted NETTAILOR 1 , in which layers of a pre-trained CNN are used as universal blocks that can be combined with small task-specific layers to generate new networks. Besides minimizing classification error, the new network is trained to mimic the internal activations of a strong unconstrained CNN, and minimize its complexity by the combination of 1) a soft-attention mechanism over blocks and 2) complexity regularization constraints. In this way, NETTAILOR can adapt the network architecture, not just its weights, to the target task. Experiments show that networks adapted to simple tasks, such as character or traffic sign recognition, become significantly smaller than those adapted to hard tasks, such as fine-grained recognition. More importantly, due to the modular nature of the procedure, this reduction in network complexity is achieved without compromise of either parameter sharing across tasks, or classification accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world applications of machine learning for vision often involve the ability to solve multiple recognition tasks. For example, a robot should be able to decide if a door is open or closed, whether an object can be picked up or not, what is the expression on a person's face, among others. However, attempting to design a single recognizer for all tasks is often impractical, since datasets for different tasks are not always available at the same time, and state-of-the-art models use complemented by task-specific blocks that enable adaptation to new tasks. Given a new target task, we propose to search for the best architecture that combines any number of large pre-trained blocks and small task-specific blocks. While pre-trained blocks are responsible for the bulk of feature extraction, task-specific blocks are used to 1) build the final (classification) layer, 2) simplify or even replace pre-trained blocks when possible, or 3) adjust network activations to compensate for domain differences between the source and target tasks.</p><p>Evidence for the feasibility of this idea was recently provided in <ref type="bibr" target="#b49">[49]</ref>, where a pre-trained network is successfully adapted to multiple tasks without changing its parameters by adding a small number of residual adaptation layers. In this work, however, instead of merely adding layers, we seek to adapt the network architecture, to tailor the network to the complexity of the new task. Because the process is analogous to a tailor that adjusts a pre-made suit to fit a new customer, we denote the procedure NETTAILOR. The main idea is illustrated in <ref type="figure">Fig. 1</ref>. First, we augment a pre-trained CNN with low-complexity blocks that introduce skip connections throughout the network, and a softattention mechanism that controls the selection of which blocks to use. Then, we train the augmented CNN with a loss that penalizes both classification error and complexity. The complexity penalty favors the small task-specific blocks over the large pretrained ones, encouraging the minimum amount of computation required by the target task. Good classification performance is promoted with a combination of the cross-entropy loss and a variant of model distillation <ref type="bibr" target="#b20">[21]</ref>, which encourages the simplified CNN to match the performance of a classically fine-tuned CNN. This optimization eliminates blocks with a poor trade-off between complexity and impact on recognition performance.</p><p>In sum, NETTAILOR seeks an architecture that matches the performance of standard fine-tuning, but that is as small as possible and mostly composed of universal blocks shared by many tasks. This procedure has three important properties. First, it enables the deployment of networks of different complexity for different tasks. For example, in simpler recognition problems such as digit recognition (SVHN dataset), NETTAILOR removed 73.4% parameters, while in high-level tasks such as the recognition of everyday objects (Pascal VOC dataset) only 36.1% of the parameters are removed. Second, because the majority of the parameters required per task belong to shared pre-trained blocks, NETTAILOR solves more tasks with the same resources and allows task switching to be more efficient. On average, NETTAILOR only introduces 8% of new task-specific parameters per task, when compared to the size of the pre-trained network. Third, we show that pre-trained blocks can be discarded without a significant loss in performance, achieving accuracy similar to previous transfer learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>NETTAILOR is related to various CNN topics. Transfer learning: CNNs are routinely transferred by fine-tuning. NETTAILOR is a flexible transfer procedure that adjusts the network architecture (not just the weights) while keeping the majority of the parameters unchanged. Life-long learning &amp; learning without forgetting Intelligent systems integrate knowledge over time, leveraging what they know to solve new tasks. This ability is known as lifelong learning <ref type="bibr" target="#b62">[62]</ref> or never-ending learning <ref type="bibr" target="#b41">[42]</ref> and is usually incremental, i.e. with tasks learned sequentially. Fine-tuning has two main problems for lifelong learning. First, since the original weights are modified, the number of parameters increases linearly with the number of tasks. This is wasteful since low and mid-level features can be shared across very different image domains <ref type="bibr" target="#b58">[58]</ref>. Second, after fine-tuning, network performance can degrade substantially on the source task <ref type="bibr" target="#b15">[16]</ref>. This degradation is known as "catastrophic forgetting" and has been the subject of various recent works, which we categorize into two groups.</p><p>The first group forces the CNN to "remember" the source task when training on target data <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref>. This is done either by 1) preventing network responses for source classes from changing significantly on images of the new task <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>, 2) maintaining an "episodic memory" of images from previous tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">51]</ref>, 3) preventing the reconstruction of features crucial to the source task from changing <ref type="bibr" target="#b48">[48]</ref>, or 4) identifying and protecting weights critical for previous tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. The second group retains previous task knowledge by freezing the source network and adding a small number of parameters for adaptation to the new task. For example, progressive neural networks <ref type="bibr" target="#b55">[55]</ref> and dynamically expandable networks <ref type="bibr" target="#b69">[69]</ref> expand the original network by adding hidden units to each layer, and Rebuffi et al. <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b50">50]</ref> add small task-specific layers, denoted residual adapters, that adapt the activations of the source network to the target task. Finally, Mallya et al. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> identify a small set of source weights that can be pruned or retrained to improve performance on the target task.</p><p>NETTAILOR has similarities with the second group, since it freezes pre-trained layers. Also, similarly to some methods in the first group, NETTAILOR uses source activations of intermediate layers as guidance for the activations of the new network. The main difference is that prior techniques do not seek to adapt the network complexity to the task requirements, which results in wasted computation when target tasks are simpler than the source task. Multi-task learning Multi-task learning (MTL) aims to improve generalization by leveraging relations between tasks <ref type="bibr" target="#b5">[6]</ref>. MTL is widely used for problems like object detection, where sharing representations between object location and classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">52]</ref> or even segmentation <ref type="bibr" target="#b18">[19]</ref> has led to significant gains. Other examples of successful MTL are head orientation and facial attribute detection <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b47">47]</ref>, scene geometry, instance, and semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41]</ref>, among others. The main difference between MTL and transfer techniques is that MTL assumes that all tasks are performed on the same domain, usually all operating on the same image. This is not the case for transfer, where the target task belongs to a different domain, possibly very dissimilar from that of the source images. Domain adaptation Domain adaptation addresses the transfer of a task across two domains. When labels are available for both domains, this is usually done by fine-tuning. NETTAILOR addresses the problem that, depending on the gap between domains, there may be a need to adjust the architecture. This is, however, different from unsupervised domain adaptation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64]</ref>, where there are no labeled data for the target domain. Unlike general transfer techniques like NETTAILOR, unsupervised domain adaptation is designed to bridge the gap between two datasets with exactly the same classes, and to maximize performance on the target (unsupervised) dataset with no concern for source domain performance. Network compression Network compression aims to reduce the size of a neural network by removing weights. Early works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b17">18]</ref> derived near-optimal strategies to identify and remove weights of low impact on network performance. However, because these methods rely on second order derivatives of the loss function, they are impractical for deep networks. Recently, good results have been shown with simpler procedures, such as pruning weights of low magnitude <ref type="bibr" target="#b16">[17]</ref> or introducing sparsity constraints during training <ref type="bibr" target="#b74">[74]</ref>. These methods reduce model size considerably but do not improve the speed of inference, due to the irregular sparsity of pruned weights. Alternative approaches advocate for "structured sparsity" as a means to remove entire filters <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">43]</ref>. NETTAILOR adopts the standard training methodology of iterative pruning (pre-train, prune, re-train), but takes the concept of structured sparsity one step further, pruning entire layers instead of weights or filters. However, NETTAILOR is not a network compression procedure, as layer pruning is only feasible in transfer learning, specifically when the target task is simpler than the source. Existing compression methods could also be used to compress the pre-trained network, further reducing the complexity of networks fine-tuned with NETTAILOR. Distillation Model distillation algorithms seek to emulate a model with a simpler, smaller or faster one. In <ref type="bibr" target="#b3">[4]</ref>, a strong ensemble model is used to label a large unlabeled dataset, which is then used to train a simpler model that mimics the ensemble predictions. Similar ideas have been used to transfer knowledge between networks with different characteristics. For example, Ba et al. <ref type="bibr" target="#b1">[2]</ref> demonstrate that shallow networks can mimic deeper networks while using the same amount of parameters for stronger parallelization, <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b53">[53]</ref> replicate complex networks with significantly smaller or thinner ones, and <ref type="bibr" target="#b6">[7]</ref> transfers a previous network to a new deeper or wider network without retraining for a faster development workflow. The student teacher paradigm used by NETTAILOR is similar to that of FitNets <ref type="bibr" target="#b53">[53]</ref>, as teacher supervision is added both at the network output and internal activations. However, instead of training a new network from scratch, NETTAILOR adapts the architecture of a pre-trained network without changing most of its weights. Cascaded classifiers &amp; Adaptive inference graphs Cascaded classifiers <ref type="bibr" target="#b66">[66]</ref>, can also significantly accelerate inference, by quickly rejecting easy negatives. Recent works developed these ideas within a deep learning framework, both for classification <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b21">22]</ref> and detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b68">68]</ref>. By introducing early-exits, the network can classify images as soon as it reaches the desired degree of confidence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b61">61]</ref>, or anytime the decision output is expected <ref type="bibr" target="#b21">[22]</ref>. Closer to NETTAILOR is the work on adaptive inference graphs (AIG) <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b12">13]</ref>, which dynamically adjusts the network topology at test time conditioned on the image alone. Thus, similar to NETTAILOR, both cascades and AIG methods can select which parts of the network to evaluate for each image. However, these methods cannot effectively solve the multi-domain classification problem. When networks are trained independently, a different network is generated per task. Training networks jointly requires simultaneous access to all datasets. This drastically restricts the training of networks by different developers, for different tasks, at different times, since different developers 1) may not have access to each other's data, and 2) usually lack the resources and desire to train for tasks other than their own. NETTAILOR addresses this problem by reusing a set of universal blocks shared across datasets, allowing each developer to focus on the single task of interest. It reduces both inference times and space requirements without the need for joint training on all datasets. Neural architecture search Neural architecture search (NAS) is devoted to learning new network architectures in a data-driven manner. Typically, this is accomplished using reinforcement learning or evolutionary algorithms to update a model responsible for generating architectures so as to maximize performance <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b76">76]</ref>. Since the space of possible architectures is extremely large, NAS can be quite slow and recent developments focus on accelerating the search process <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. NETTAILOR can be seen as a differentiable NAS procedure, since the network architecture is optimized for a given task. However, unlike general NAS, we seek a solution that reuses a set of pre-trained blocks in order to address the storage and computing inefficiencies associated with multi-domain transfer learning problems. Curriculum learning Curriculum learning techniques use variations of back-propagation to improve learning effectiveness. This can be done by controlling the order in which examples are introduced <ref type="bibr" target="#b2">[3]</ref>. Other approaches use a teacher network to enhance the learning of a student network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. NETTAILOR uses a replica of the source network, fine-tuned on the target task, as a teacher for the learning of the simplified network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce NETTAILOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task transfer</head><p>A CNN implements a function</p><formula xml:id="formula_0">f(x)=(G L •G L−1 •···•G 1 )(x).<label>(1)</label></formula><p>by composing L computational blocks G l (x) consisting of simple operations, such as convolutions, spatial pooling, normalization among others. For object recognition, x is an image from a class y ∈ {1,...,C}, and f(x) ∈ [0,1] C models the posterior class probability P (y|x). While the blocks G l (x) differ with the CNN model, they are often large, both in terms of computation and storage. For example, under the ResNet model, each G l (x) is formed by two 3×3 convolutions, or in deeper versions a "bottleneck" block containing two 1×1 and one 3×3 convolutions <ref type="bibr" target="#b19">[20]</ref>.</p><p>Since CNN training requires a large dataset, such as ImageNet <ref type="bibr" target="#b8">[9]</ref>, Places <ref type="bibr" target="#b73">[73]</ref> or COCO <ref type="bibr" target="#b32">[33]</ref>, not available for most applications, CNNs are rarely learned from scratch. Instead, a CNN pre-trained on a large dataset is fine-tuned on a new task. In this case, the original task is denoted as the source and the new one as the target task. Fine-tuning adjusts the weights of the blocks of (1), while maintaining the network architecture. Hence, independently of the complexity of the new task, the computational and storage complexity remain large. This is undesirable for target tasks simpler than the source task, especially for applications that have computational or storage constraints, such as mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NetTailor</head><p>In order to avoid these problems, task transfer should ideally have two properties. First, rather than reusing entire networks, it should reuse network blocks. In particular, it should be possible to add or remove blocks to best adapt the architecture to the new task, not just its weights. This way, if the target task is much simpler than the source task, network size could decrease significantly. Second, new networks should reuse existing pre-trained blocks to the largest possible extent, in order to minimize the number of parameters to be learned. Reusing blocks is particularly crucial for memory constrained implementations (e.g., robotics or mobile devices), because it allows sharing of blocks across tasks. In this case, since only a fraction of (task-specific) parameters need to be switched and stored per task, both the costs of task switching and model storage remain low.</p><p>In this work, we introduce a new transfer technique, denoted NETTAILOR that aims to achieve these goals. The NETTAILOR procedure illustrated in <ref type="figure">Fig. 1</ref> can be summarized as follows.</p><p>1. Train the teacher network by fine-tuning a pre-trained network on the target task. 2. Define the student network by augmenting the pre-trained network with task-specific low-complexity proxy layers. 3. Train the task-specific parameters of the student network on the target task to mimic the internal activations of the teacher, while imposing complexity constraints that encourage the use of low-complexity proxy layers over high-complexity pre-trained blocks. 4. Prune layers with low impact on network performance. 5. Fine-tune the remaining task-specific parameters.</p><p>While we only experimented with teacher networks that are learned by fine-tuning (step 1), NETTAILOR could also be used <ref type="figure">Figure 2</ref>: Augmentation of pre-trained block G l at layer l with multiple proxy layers A l p . xi represents the network activation after layer i.</p><formula xml:id="formula_1">… −1 1 2 + −1 * BN Max Pool * BN Max Pool * BN Max Pool 1 2 −1 2 1 … [] +</formula><p>with any transfer technique that produces a teacher that preserves the architecture of the pre-trained network. We focused on finetuning due to its popularity and high performance for most tasks where a reasonably sized dataset is available for training <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b70">70]</ref>. Layer pruning (steps 4 and 5) is performed using operations common in the network compression literature <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b74">74]</ref>, and is briefly described in Section 3.5. We now discuss steps 2 and 3 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture of the student network</head><p>The main architectural component introduced in this work is the augmentation of the pre-trained network f(</p><formula xml:id="formula_2">x) = (G L • G L−1 •···•G 1 )(x)</formula><p>with the complexity-aware pooling block of <ref type="figure">Figure 2</ref>. Starting from the pre-trained model, each layer G l is augmented with a set of lean proxy layers {A l p (·)} l−1 p=1 that introduce a skip connection between layers p and l. As the name suggests, proxy layers aim to approximate and substitute the large pre-trained blocks G l (·) whenever possible. The output activation x l of layer l is then computed by pooling the output of the l th pre-trained block G l (·) and proxies A l p (·)</p><formula xml:id="formula_3">x l =α l l G l (x l−1 )+ l−1 p=1 α l p A l p (x p ),<label>(2)</label></formula><p>where {α l p } l p=1 ∈[0,1] are a set of scalars that enable or disable the different network paths.</p><p>Two steps are taken to reduce the number of task-specific parameters. The first is to use proxy layers of low-complexity. Specifically, A l p (·) is composed of 1) a spatial max-pooling block that converts activations from the spatial resolution of x p into that of x l , and 2) a 1×1 convolution (with batch normalization) that projects the input feature map x p into the desired number of channels for x l . Thus, in comparison to the standard ResNet block which contains two 3×3 convolutions, each proxy A l p (·) contains only 1 18 of the parameters and performs only <ref type="bibr">1 18</ref> of floating point operations. Second, proxy layers are forced to compete with each other to minimize the propagation of redundant information through the network. This is accomplished by introducing a set of auxiliary parameters a l p and computing α l p as the softmax across all paths merging into layer l α l p = e a l p k e a l k .</p><p>Finally, while the description above implies a dense set of low-complexity proxies, connecting the outputs of all layers i &lt; l to that of layer l, we found this to be often unnecessary (see Section 4.1). Therefore, we limit the number of proxies in <ref type="bibr" target="#b1">(2)</ref> to the closest k, and use <ref type="figure">Figure 1</ref> illustrates the initial student architecture for k =3.</p><formula xml:id="formula_5">x l =α l l G l (x l−1 )+ l−1 p=max(l−k,1) α l p A l p (x p ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Tailoring the student to the target task</head><p>The student network seeks a trade-off of two goals: low complexity and performance similar to the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Constraining student complexity</head><p>In the complexity-aware pooling block of <ref type="figure">Fig. 2</ref>, scalars {α l p } l p=1 act as a soft-attention mechanism that selects which blocks to use for the target task. Let B j i (·) denote the computational block associated with path</p><formula xml:id="formula_6">i → j, i.e. B j i (·) = G i (·) if i = j or B j i (·) = A j i (·) otherwise. Then, block B j i (·)</formula><p>can be removed if one of three conditions hold:</p><p>• Self-exclusion ( <ref type="figure" target="#fig_0">Fig. 3a)</ref>: path i→j is excluded, i.e. α j i =0; • Input exclusion ( <ref type="figure" target="#fig_0">Fig. 3b</ref>): all paths merging into node i are excluded, i.e. α i k =0,∀k ≤i; • Output exclusion ( <ref type="figure" target="#fig_0">Fig. 3c</ref>): all paths departing from node j are excluded, i.e. α k j =0,∀k &gt;j and α j+1 j+1 =0. Note that while self-exclusion only allows the removal of a single block, both input and output exclusion remove multiple blocks simultaneously. For example, if all paths merging into node i are excluded, then all blocks departing from this node have no viable input and can be removed. Similarly, if all paths departing from node j are excluded, then all blocks merging into this node will end up being ignored and can be removed as well.</p><p>To tailor the architecture to the target task, the set of scalars {α l p } l p=1 should enable high performance, but minimize the expected network complexity. Let R i,j self , R i inp and R j out denote the events associated with conditions 1, 2 and 3, respectively, and C j i the complexity of block B j i . Then, the expected complexity of block B j i is</p><formula xml:id="formula_7">E C j i =C j i 1−P (R i,j self ∪R i inp ∪R j out ) .<label>(5)</label></formula><p>Under the assumptions that events R i,j self , R i inp and R j out are disjoint, and events R i,k self are all independent, the probability of (5) is given by</p><formula xml:id="formula_8">P (R i,j self ∪R i inp ∪R j out )=P (R i,j self )+P (R i inp )+P (R j out ) (6) with P (R i inp )=P ∩ k≤i R k,i self = k≤i P (R k,i self )<label>(7)</label></formula><formula xml:id="formula_9">P (R j out )=P ∩ k&gt;j R j,k self ∩R j+1,j+1 self =P (R j+1,j+1 self )· k&gt;j P (R j,k self ).<label>(8)</label></formula><p>Finally, by modeling the probability of self-exclusion by</p><formula xml:id="formula_10">P (R i,j self )=r j i =1−α j i , then (5) becomes E C j i =C j i 1−r j i − k≤i r i k −r j+1 j+1 k&gt;j r k j ,<label>(9)</label></formula><p>and the expected network complexity</p><formula xml:id="formula_11">E[C]= i,j E C j i .<label>(10)</label></formula><p>Although the exclusion events may not be disjoint or independent, the minimization of (10) still provides the desired incentive towards the use of low-complexity proxies. Hence, we use (10) as a differentiable complexity penalty explicitly enforced during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Mimicking the teacher</head><p>The teacher network is obtained by fine-tuning a pre-trained network for the target task. To transfer this knowledge to the student network, the latter is encouraged to match the internal activations of the teacher, by adding an L 2 regularizer</p><formula xml:id="formula_12">Ω= l x t l −x l 2 ,<label>(11)</label></formula><p>where x t l is the activation of l th block of the teacher network, x l the corresponding activation of the student network given by <ref type="bibr" target="#b1">(2)</ref>, and the sum is carried over all internal blocks as well as network outputs (prior to the softmax).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Loss function</head><p>NETTAILOR optimizes all task-specific parameters of the student network end-to-end to meet three goals: 1) minimize classification loss on the target task, 2) minimize network complexity and 3) minimize the approximation error to the teacher network. Given a target dataset D ={x i ,y i } of images x i and labels y i , this is accomplished by minimizing the loss function</p><formula xml:id="formula_13">L= i L cls (f(x i ),y i )+γ 1 E[C]+γ 2 Ω,<label>(12)</label></formula><p>where f(·) denotes the output of the student network, L cls (f(x), y) is the cross-entropy loss between the network prediction f(x) and ground-truth label y, E[C] is the expected network complexity of (9) and <ref type="formula" target="#formula_0">(10)</ref>, Ω is the teacher approximation loss of (11), and γ 1 and γ 2 two hyper-parameters that control the importance of each term.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Pruning and fine-tuning</head><p>After training the student network, the magnitude of the scalars α j i reflects the importance of each block, with values close to zero indicating a low impact on network performance. Given this observation, we threshold the scalars α j i , and use the three exclusion conditions outlined above to remove all unnecessary blocks. In order to enable better control over the trade-off between performance and complexity, proxies and pre-trained blocks are removed using different pruning schemes. Since proxy layers are both small and crucial for the adaptation to the target task, we define a very low threshold θ (typically 0.05) and only remove proxies with α j i &lt;θ. As for pre-trained layers, we first rank their importance by the values of α i i , and remove the k least important blocks. Finally, in order to recover from the removal of network components, all remaining task-specific layers are fine-tuned to minimize the loss of (12) without complexity constraints (γ 1 =0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We conducted a series of experiments to evaluate the NETTAILOR procedure. Section 4.1 provides an in-depth analysis of the impact of important variables such as the complexity of the target task, the depth of the pre-trained network, the importance of the teacher and the number of skips in the student network. Then, to demonstrate the effectiveness of the proposed procedure, Section 4.2 compares NET-TAILOR to prior work on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis</head><p>We analyze NET-TAILOR using three classification datasets of varying characteristics: SVHN, VGG-Flowers and Pascal VOC 2012. SVHN <ref type="bibr" target="#b45">[45]</ref> is a large digit recognition dataset containing 100k images of street view house numbers. VGG-Flowers <ref type="bibr" target="#b46">[46]</ref> is a small fine-grained dataset composed by 8k images distributed across 102 flower species. PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> is a dataset for the detection of a small number (20) of common objects. While VOC was designed for object detection, we test our method on the classification task alone. We used ground-truth bounding boxes to crop all objects with a 20 % margin and re-sampled the dataset to avoid large class imbalances. We used standard training and test sets in all cases.</p><p>Training details We now describe the standard implementation of NETTAILOR which, unless otherwise specified, is used throughout our experiments. Global blocks are obtained by pre-training a large CNN model on ImageNet (ResNet34 in most of our experiments) and remain unchanged afterward in order to share them across tasks. For each target task, the teacher is trained by fine-tuning the pre-trained network. The student is assembled by augmenting the pre-trained blocks with three skip connections per layer, and all task-specific parameters (i.e. final classifier, proxy layers and scalars α) are trained to minimize the loss of (12) with γ 1 =0.3 (complexity constraints) and γ 2 = 10 (teacher loss). In the complexity constraints of (9), the complexity C j i is defined as the number of FLOPs of each block normalized by the total number of FLOPs of the pre-trained network. This definition makes pre-trained layers about 20 times more expensive than proxy layers. One critical detail is the initialization of the scalars α to initially favor pre-trained over task-specific blocks. This initialization provided a good starting point for learning (i.e. similar to the pre-trained network alone) and reduced overfitting. Specifically, we set the initial value of a i i to 2 for all i (i.e. pre-trained blocks), and a j i to −2 for all i =j (i.e. proxies). After training the student network, we remove all proxies with α j i &lt;0.05 and the k least important pre-trained blocks (as ranked by the values of α i i ). Finally, we fine-tune the remaining task-specific parameters to minimize the loss of (12) without complexity constraints γ 1 =0. The pruning and retraining steps are repeated multiple times with different values of k, and the leanest model that achieves a target accuracy within 0.5% of the teacher network is chosen as the final architecture. All hyper-parameter values were chosen based on early experiments and used for all three datasets, as they tend to provide a good trade-off between accuracy and network complexity. A study of some of these parameters is provided below. As usual with classification problems, we used stochastic gradient descent with momentum in all training steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of NETTAILOR on various datasets:</head><p>To study the impact of dataset complexity, we tuned the ResNet34 architecture using NETTAILOR and measured the maximum achievable reduction in network complexity that retains performance similar to fine-tuning. The results are shown in <ref type="figure" target="#fig_1">Fig. 4</ref> for three different datasets. We list the total number of layers,   <ref type="figure">Figure 6</ref>: Accuracy vs. complexity of models discovered by NETTAILOR with and without the teacher. Right-most dots represent unpruned networks and subsequent ones networks with increasing numbers of removed layers. parameters (global and task-specific) and FLOPs removed from the pre-trained network by NETTAILOR. We also display the final learned architecture for each task. <ref type="figure" target="#fig_1">Fig. 4</ref> shows that networks trained for simpler tasks, such as SVHN, are the most heavily pruned, with 9 out of 18 pre-trained blocks removed. This results in a drastic 73.4 % reduction in total parameters and a 45.8 % reduction in FLOPs. For simpler tasks, most residual blocks are unnecessary and fine-tuning likely converts them into transformations close to the identity, which can be replaced by low complexity proxies. NETTAILOR also obtains significant reductions for the more complex Flowers and VOC datasets. Overall, the results of <ref type="figure" target="#fig_1">Fig. 4</ref> show that, for many applications, large pre-trained networks can be significantly reduced, both in size and speed, without loss of performance. Furthermore, because the pre-trained blocks remain unchanged, only a small number of new parameters is introduced per task:  the output, this block architecture does not allow the use of proxies as defined in <ref type="figure">Fig. 2</ref>, since 1×1 convolutions in the high-dimensional space are still expensive. Instead, to keep the complexity of each proxy at about <ref type="bibr">1 20</ref> of the pre-trained block, we employ a bottleneck structure to the proxy as well, i.e. we employ two consecutive 1×1 convolutions with batch-norm. The first projects the input into a low-dimension space (4 times smaller than the bottleneck of the pre-trained block), and the second restores the input dimensionality.</p><p>The results depicted in <ref type="figure" target="#fig_2">Fig. 5</ref> show that NETTAILOR can produce architectures that achieve the performance of a larger CNN (e.g. ResNet50) with the same or fewer parameters as a smaller one (ResNet18). This is especially important for more complex problems, where network depth has a bigger impact on performance. For example, for the VOC dataset, NETTAILOR is able to reduce ResNet50 to only 11.7M parameters, only 0.5M more than ResNet18, but with much higher performance (83.2 % vs 79.6 % accuracy). Reduction in inference speed, however, was not as drastic for the VOC dataset, since NETTAILOR mostly removed high-level layers which contain most of the parameters but only account for a small number of operations. Teacher supervision: <ref type="figure">Fig. 6</ref> shows the advantage of using a fine-tuned network as the teacher. Each line in <ref type="figure">Fig. 6</ref> shows the performance achieved by the model after removing different numbers of blocks k (smaller values of k produce models of higher complexity). As can be seen, removing the teacher leads to significant loss in performance, regardless of the number of removed blocks, with the student network never achieving the same performance as fine-tuning. The exception to this trend is the SVHN dataset, which is a simple dataset with a large number of images. This indicates that the skip architecture of <ref type="figure">Fig. 1</ref> is prone to overfitting in smaller datasets, but teacher supervision provides an effective solution to this problem. Student architecture: We also compare different student architectures, by augmenting ResNet34 with 1, 3, 5 or a dense set of skip proxies. The results presented in <ref type="table" target="#tab_2">Table 1</ref> show that augmenting the student architecture with a dense set of proxies can be beneficial for simpler datasets like SVHN. This CUB <ref type="bibr" target="#b67">[67]</ref> Cars <ref type="bibr" target="#b24">[25]</ref> Flowers <ref type="bibr" target="#b46">[46]</ref> WikiArt <ref type="bibr" target="#b56">[56]</ref> Sketch <ref type="bibr">[</ref>   is because accurate digit classification depends largely on lowerlevel features that are directly bypassed into the classification layer by proxies that skip a large number of blocks. By contrast, dense skips are unnecessary for harder datasets, such as Flowers or VOC, with NETTAILOR removing most of the long reach proxies. Also, as shown in <ref type="table" target="#tab_2">Table 1</ref>, directly imposing a limit on the number of proxies per layer leads to more significant reductions in complexity for the same performance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to prior work</head><p>Finally, we compare NETTAILOR to prior transfer learning methods designed for the efficient classification of multiple domains. We follow two experimental protocols. The first protocol described in <ref type="bibr" target="#b37">[38]</ref> consists of five datasets: CUB <ref type="bibr" target="#b67">[67]</ref>, Stanford Cars <ref type="bibr" target="#b24">[25]</ref>, Oxford Flowers <ref type="bibr" target="#b46">[46]</ref>, WikiArt <ref type="bibr" target="#b56">[56]</ref> and Sketch <ref type="bibr" target="#b9">[10]</ref>. Following <ref type="bibr" target="#b37">[38]</ref>, we use the same train/test set splits, and apply the NETTAILOR procedure to the same backbone network, ResNet50, with an input size 224x224. The second protocol is the visual decathlon benchmark <ref type="bibr" target="#b49">[49]</ref> and consists of ten different datasets including ImageNet, Omniglot, German Traffic Signs, among others. We use the same train/validation/test sets provided in <ref type="bibr" target="#b49">[49]</ref> which contain images resized to a common resolution of 72 pixels. Similar to <ref type="bibr" target="#b50">[50]</ref>, we also use a wide residual network <ref type="bibr" target="#b71">[71]</ref> with 26 layers pre-trained on ImageNet. Results are reported using both top-1 accuracy and the "decathlon score" <ref type="bibr" target="#b49">[49]</ref> which pools all results in a single metric that accounts for the different difficulty of each task. <ref type="table" target="#tab_4">Table 2</ref> compares the results of NETTAILOR to several methods. Feature extraction computes features from a pre-trained network, which are then used to build a simple classifier. While feature extraction shares most weights across datasets, differences between the source and target domains cannot be corrected, thus achieving low performance. More refined methods, such as PackNet <ref type="bibr" target="#b38">[39]</ref> and Piggyback <ref type="bibr" target="#b37">[38]</ref>, try to selectively adjust the network weights in order to remember previous tasks, or freeze the backbone network and learn a small set of task-specific parameters (a set of masking weights in the case of Piggyback) that is used to bridge the gap between source and target tasks. All these methods ignore the fact that source and target datasets can differ in terms of difficulty, and thus the architecture itself should be adjusted to the target task, not just the weights. As seen in <ref type="table" target="#tab_4">Table 2</ref>, these methods are not competitive with NETTAILOR, which can significantly reduce the network complexity both in terms of model size and inference speed. NETTAILOR outperforms all approaches in all datasets, improving the classification accuracy of the second best method in four out of five datasets, while requiring an average of 46% fewer parameters and 22% fewer FLOPS.</p><p>Comparisons in the Visual Decathlon benchmark show similar findings. In addition to Piggyback <ref type="bibr" target="#b37">[38]</ref>, we also compared to Learning without Forgetting (LwF) <ref type="bibr" target="#b31">[32]</ref>, deep adaptation networks (DAN) <ref type="bibr" target="#b54">[54]</ref> and parallel Residual Adapters (ResAdapt) <ref type="bibr" target="#b50">[50]</ref>. LwF learns a new network per task that retains the responses of the original ImageNet model. Hence, similar to fine-tuning, the number of parameters in LwF also grows linearly with the number of tasks. Both ResAdapt and DAN address this problem by introducing a small amount of extra parameters that adapt the source network to the target task. This is accomplished by adjusting each layer's activations in the case of ResAdapt, or their parameters directly in the case of DAN. Although both methods can share large blocks across tasks, none try to adjust the model complexity to the target task. As shown in <ref type="table" target="#tab_5">Table 3</ref>, NETTAILOR outperforms ResAdapt by 1.57% across 10 datasets and achieves 332 points higher in the decathlon score. More importantly, NETTAILOR only uses 3.67×10 6 parameters (43% fewer than ResAdapt) and 0.61×10 9 FLOPs (36% fewer than ResAdapt) per task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduced a novel transfer learning approach, denoted NETTAILOR, which adapts the architecture of a pre-trained model to a target task. NETTAILOR uses the layers of the pre-trained CNN as universal blocks shared across tasks and combines them with small task-specific layers to generate a new network. Experiments have shown that NETTAILOR is capable of learning architectures of increasing complexity for increasingly harder tasks, while achieving performances similar to that of transfer techniques like fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Block removal criteria. (a) Self-exclusion. (b) Input exclusion. (c) Output exclusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Reduction of network complexity and final architecture after adapting ResNet34 to three datasets using NETTAILOR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy vs. complexity of models of increasing depth. Diamonds represent the fine-tuned model and crosses the model obtained with NETTAILOR. The lines connect fine-tuned models to their adapted counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Effect of initial student architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy and model complexity for prior transfer learning methods in five datasets. PackNet performance is sensitive to the order in which datasets are presented. → indicates the following order: CUB, Cars, Flowers, WikiArt and Sketch. ← indicates reversed order.</figDesc><table><row><cell></cell><cell cols="13">ImNet [9] Airc [37] C100 [26] DPed [44] DTD [8] GTSR [60] Flwr [46] Oglt [28] SVHN [45] UCF [59] Mean Score Avg Params Avg FLOPS</cell></row><row><cell>LwF [32, 49]</cell><cell>59.87</cell><cell>61.15</cell><cell>82.23</cell><cell>92.34</cell><cell>58.83</cell><cell>97.57</cell><cell>83.05</cell><cell>88.08</cell><cell>96.10</cell><cell>50.04</cell><cell>76.93 2515</cell><cell>5.86</cell><cell>0.87</cell></row><row><cell>Piggyback [38]</cell><cell>57.69</cell><cell>65.29</cell><cell>79.87</cell><cell>96.99</cell><cell>57.45</cell><cell>97.27</cell><cell>79.09</cell><cell>87.63</cell><cell>97.24</cell><cell>47.48</cell><cell>76.60 2838</cell><cell>6.04</cell><cell>0.87</cell></row><row><cell>DAN [54]</cell><cell>57.74</cell><cell>64.12</cell><cell>80.07</cell><cell>91.30</cell><cell>56.54</cell><cell>98.46</cell><cell>86.05</cell><cell>89.67</cell><cell>96.77</cell><cell>49.38</cell><cell>77.01 2851</cell><cell>6.54</cell><cell>0.97</cell></row><row><cell>ResAdapt [50]</cell><cell>60.32</cell><cell>64.21</cell><cell>81.91</cell><cell>94.73</cell><cell>58.83</cell><cell>99.38</cell><cell>84.68</cell><cell>89.21</cell><cell>96.54</cell><cell>50.94</cell><cell>78.07 3412</cell><cell>6.44</cell><cell>0.96</cell></row><row><cell>NETTAILOR</cell><cell>61.42</cell><cell>75.07</cell><cell>81.84</cell><cell>94.68</cell><cell>61.28</cell><cell>99.52</cell><cell>86.53</cell><cell>90.09</cell><cell>96.44</cell><cell>49.54</cell><cell>79.64 3744</cell><cell>3.67</cell><cell>0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Accuracy and model complexity of several methods on the visual decathlon challenge.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expert gate: Lifelong learning with a network of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning complexity-aware cascades for deep pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05641</idno>
		<title level="m">Net2net: Accelerating learning via knowledge transfer</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<idno>44:1-44:10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03643</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Learning to teach. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>National Academy of Sciences</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International IEEE Workshop on 3D Representation and Recognition (3dRR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting by incremental moment matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tambet</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00183</idno>
		<title level="m">Teacher-student curriculum learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<pubPlace>Bo Yang, Justin Betteridge, Andrew Carlson, B</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Never-ending learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kisiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An experimental study on pedestrian classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Munder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1863" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop (NeurIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Encoder based lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Rannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Incremental learning through deep adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John K Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Large-scale classification of fine-art paintings: Learning the right metric on the right feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00855</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRw)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A lifelong learning perspective for mobile robot control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Lifelong learning with dynamically expandable networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01547</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Less is more: Towards compact cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<title level="m">Learning transferable architectures for scalable image recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
