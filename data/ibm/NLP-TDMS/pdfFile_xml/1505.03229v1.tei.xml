<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">APAC: Augmented PAttern Classification with Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuro</forename><surname>Sato</surname></persName>
							<email>isato@d-itlab.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Denso IT Laboratory, Inc. Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Nishimura</surname></persName>
							<email>hnishimura@d-itlab.co.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Denso IT Laboratory, Inc. Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensuke</forename><surname>Yokoi</surname></persName>
							<email>kensukeyokoi@denso.co.jp</email>
							<affiliation key="aff2">
								<orgName type="department">DENSO CORPORATION Aichi</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">APAC: Augmented PAttern Classification with Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classification problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classifiers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classification, which is a way of classification using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classifiers. Even our multilayer perceptron model beats some of the convolutional models with recentlyinvented stochastic regularization techniques on the CIFAR-10 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Output of an ideal pattern classifier satisfies two properties. One is the invariance under replacement of a data point by another data point within the same class, and we refer this as to intra-class invariance. The other is the distinctiveness under replacement of a data point in one class by a data point in another class, and we refer this as to inter-class distinctiveness. Good classifiers more or less have these properties for untrained data.</p><p>For a given class, there exists a set of transformations that leave the class label unchanged. In case of visual object recognition of "apple", the class label stays the same under different lighting conditions, backgrounds, and poses, to name a few. One can expect that a classifier gains good intra-class invariance through learning dataset containing many images with these variations.</p><p>A classifier should also show inter-class distinctiveness to distinguish one class from the other. If one construct a training dataset containing green apple class and red apple class, lighting condition must be paid careful attention, because important color feature may be spoiled under some lighting condition. Appropriate types and ranges of variations depends on the problem setting.</p><p>For an image classifier to gain good intra-class invariance without compromising inter-class distinctiveness, there are largely two types of approaches. One approach is to embed some mechanisms in classifiers to give robustness against intra-class variations. One of the most successful classifiers would be Convolutional Neural Network (CNN) <ref type="bibr" target="#b10">[11]</ref>. CNN has two important building blocks: convolution and spatial pooling, which give robustness against global and small local shifts, respectively. These shifts are a typical form of intra-class variation.</p><p>Another approach is data augmentation, meaning that a given dataset is expanded by virtual means. A common way is to deform original data in many ways using prior knowledge on intra-class variation. Color processing and geometrical transformation (rotation, resizing, etc.) are typical operations used in visual recognition problems. Adding virtual data points amounts to making the points denser in the manifold that the class instances form. Strong regularization effects are expected through augmented data learning.</p><p>Augmented data learning is also beneficial in an engineering point of view. Dataset creation is a painstaking and costly part in product development. Data augmentation allows the use of prior knowledge on recognition targets, which engineers do have in most cases, and thus provides easy and cheep substitutes. Secondly, quality of virtual data can be easily evaluated by human perception. In case of visual recognition task, one can check virtual images whether they resemble real ones by eyes.</p><p>Many of state-of-the-art methods in generic object recognition problems use deep CNNs, trained on augmented datasets comprising original data and deformed data (see recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>). It has been pointed out that CNN models with many layers have great discriminative power; on the other hand, theoretical and methodological aspects of data augmentation are not fully revealed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Data augmentation plays an essential role in boosting performance of generic object recognition. Krizhevsky et al. used a few types of image processing, such as random cropping, horizontal reflection, and color processing, to create image patches for the ImageNet training <ref type="bibr" target="#b8">[9]</ref>. More recently, Wu et al. vastly expanded the ImageNet dataset with many types of image processing including color casting, vignetting, rotation, aspect ratio change, and lens distortion on top of standard cropping and flipping <ref type="bibr" target="#b21">[22]</ref>. Although these two works use different network architectures and computational hardware, it is still interesting to see the difference in the performances levels. The top-5 prediction error rate of the latter is 5.33%, while that of the former is 16.42%. Such a large gap could be an implicit evidence that richer data augmentation leads to better generalization.</p><p>Paulin et al. proposed a novel method for creating augmented datasets <ref type="bibr" target="#b14">[15]</ref>. It greedily selects types of transformations that maximize the classification performance. The algorithm requires heavy computational resources, thus the exhaustive pursuit is almost intractable when deep networks with a huge number of parameters are trained.</p><p>Handwritten character/digit recognition has been an important problem for both industrial applications and algorithm benchmarking for a quarter century <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. The problem is relatively simple in a sense that there is no degree of freedom in the background and that stroke can be easily modified. Elastic distortion is a commonly used data augmentation technique that has a good property in giving a large degrees of freedom in the stroke forms, while leaving the topological structure invariant. Indeed, data augmentation by elastic distortion is crucial in boosting classification performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In case of pedestrian detection, use of synthetic pedestrians in real background <ref type="bibr" target="#b13">[14]</ref> and synthetic occlusion <ref type="bibr" target="#b0">[1]</ref> have been proposed. Though these approaches give additional degrees of freedom in expanding training datasets, we omit such means in this work.</p><p>Data augmentation can be categorized into two: off-line and on-line. In this work, off-line data augmentation means to increase the number of data points by a fixed factor before the training starts. The same instance is repeatedly used in the training stage until convergence <ref type="bibr" target="#b16">[17]</ref>. On-line data augmentation means to increase the number of data points by creating new virtual samples at each iteration in the training stage (see representative works: <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>). There, random deformation parameters are sampled at each iteration, hence the classifier always "sees" new samples during the training. Cireşan et al. claims that on-line scheme greatly improves classification performance because learning a very large number of samples likely avoids over-fitting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. Our work is mostly inspired by their work, and is focused on the on-line deformation.</p><p>Very recently, an website article reported a method named as Test-Time Augmentation <ref type="bibr" target="#b3">[4]</ref>, where prediction is made by taking average of the output from many virtual samples, though the algorithm is not fully described.</p><p>Tangent Prop <ref type="bibr" target="#b15">[16]</ref> is a way to avoid over-fitting with implicit use of data augmentation. Virtual samples are used to compute the regularization term that is defined as the sum of tangent distances, each of which is the distance between an original sample and a slightly deformed one. It is expected that classifier's output is stable in the vicinity of original data points, but not necessarily so in other locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contribution</head><p>This paper proposes the optimal decision rule for a given data sample using classifiers trained with augmented data. We do not discuss methods of data deformation themselves. Throughout this paper we assume that training is done with data samples deformed in on-line fashion. That is, random deformation parameters are sampled at every iteration, and a deformed sample is used only once and discarded after a single use. Such training minimizes an expectation value of loss function over random deformation parameters. We claim that class decision must be made so as to minimize the same expectation value for a given test sample.</p><p>We show by experiments that the proposed decision rule give lower classification error rates than the conventional decision rule. APAC improves test error rate of CNN by 0.16% for MNIST and by 9.72% for CIFAR-10. To the best of our knowledge, the improved error rate for MNIST is the best among non-ensemble classifiers reported in the past.</p><p>Though we believe that the proposed decision rule is beneficial to any classification problem, in which augmented data learning is applied, image classification problems are mainly discussed in this paper because we have not conducted experiments in other fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">On-line data deformation</head><p>On-line data deformation learning can generate classifiers with strong intra-class invariance. Such learning generally consumes many iterations to reach a minimum of the objective function. A vast number of training instances are processed because the number of instances increases linearly as the number of iterations increases. In the on-line deformation scheme, the original data themselves are not trained explicitly -they are only trained probabilistically.</p><p>In this section we provide a formal definition of augmented data learning, which has been treated rather heuristically so far. Let us first define the data deformation function as u : R d → R d , where d is the dimension of the original data. <ref type="bibr" target="#b0">1</ref> The function u(x; Θ) takes a datum x ∈ R d and deformation-controlling parameters Θ = {θ 1 , · · · , θ K }, and returns a virtual sample. Each element of the set Θ is defined as a continuous random variable for convenience. Some are responsible for continuous deformation; e.g., θ 1 being scaling factor, θ 2 being horizontal shift, etc. The other are responsible for discrete deformation; e.g., if θ 3 ∈ [0, 1 2 ) horizontal side is flipped, and if θ 3 ∈ [ 1 2 , 1] no side-flipping is performed, where θ 3 ∼ U(0, 1). We use class label c in the superscript, Θ c , if deformation is done in a classdependent fashion. In this work, it is assumed that probability density functions of deformation parameters are given at the beginning and held fixed during training and testing. In the following, we consider two cases: 1) the way of deformation being same for all classes, and 2) the others. We use the cross entropy as the loss function as it is most widely used for Deep Learning with supervised setting. The cross entropy requires vector normalization in the output units, where we use the softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Class-indistinctive deformation learning</head><p>We first discuss the case 1). Let i ∈ {1, · · · , N } denote an index of original training data, c i ∈ {1, · · · , N c } denote the class index of i-th sample, W denote the set of all parameters to be optimized, and f ( · ; W ) : In the following, regularization terms are ignored for simplicity.</p><formula xml:id="formula_0">R d → R Nc</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-indistinctive deformation learning:</head><formula xml:id="formula_1">Given D = {(x i , c i )}, i = 1, · · · , N , find W such that W = arg min W J D (W ),<label>(1)</label></formula><p>where the objective function J D (W ) is defined as</p><formula xml:id="formula_2">J D (W ) = N i=1 E Θ [− ln (f ci (u (x i ; Θ) ; W ))] . (2)</formula><p>The expectation value is computed by marginalizing the cross entropy over deformation parameters that independently obey unconditional probability densities p k (θ k ), k = 1, · · · , K. By using appropriate random number generators, one can generate countlessly many virtual samples during training. By sufficiently reducing the objective function, the classifier outputs a value close to the target value for an arbitrarily deformed training image. That means, the classifier gains a high level of intra-class invariance with respect to the set of deformations applied, without compromising inter-class distinctiveness.</p><p>Deformation must be meaningful for all classes in classindistinctive deformation learning. It may be homography transformation or global color processing such as gamma correction, to name a few.</p><p>A truly intra-class-invariant classifier would be obtained, if the integrals E Θ [·] = · · · k dθ k p k (θ k )(·) could be analytically calculated. However, it is hard to integrate out in reality. Then one needs to convert the integral into a sum of infinitely many terms,</p><formula xml:id="formula_3">E Θ [ · ] = lim R→∞ 1 R Θ=Θ (1) ,··· ,Θ (R) ( · ).<label>(3)</label></formula><p>Here,</p><formula xml:id="formula_4">Θ ( ) = {θ ( ) 1 , · · · , θ ( ) K</formula><p>} is a set of deformation parameters at -th sampling, based on the unconditional probability density functions p k (·), k = 1, · · · , K. With this summation form, the objective function can be approximately minimized by widely-used mini-batch Stochastic Gradient Descent (SGD). Note that a batch optimization algorithm is no longer applicable in a strict sense because the number of terms are infinite. At each iteration in the optimization process, data indices and deformation parameters are randomly sampled to generate a mini-batch. The minibatch is discarded after a single use. The total number of terms in the objective function is determined when the training is terminated. It is clear from Eq. (2) that the original data samples are not explicitly fed into the network.</p><p>We believe that the original data should not be used for validation, as opposed to the statement made by Cireşan et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, where they claim that the original data can be used for validation. The original and deformed data have strong correlations in the feature space especially when deformation is moderate. Therefore, it is advised not to use the original training data to estimate the generalization performance.</p><p>In the experiment, we employ class-indistinctive deformation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Class-distinctive deformation learning</head><p>Next, we discuss the case 2), where the probability densities for deformation parameters depend on the classes. Although such scheme requires one to design deformation in a class-specific way, it is likely to give a stronger inter-class distinctiveness to the classifier. For example, it is not probably a good idea to cast color with strong red component to an image belonging to "green apple" class, when there is "red apple" class, for an obvious reason. But casting red color to an image belonging to, say, "grape" class may be reasonable. In the hand-written digit classification problem, Cireşan et al. have used different ranges of deformation parameters for certain classes <ref type="bibr" target="#b1">[2]</ref>: rotation and shearing applied to digit 1 and 7 are less stronger than other digits. Their work is another example of class-distinctive deformation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class-distinctive deformation learning:</head><formula xml:id="formula_5">Given D = {(x i , c i )}, i = 1, · · · , N , find W such that W = arg min W J D (W ),<label>(4)</label></formula><p>where the objective function J D (W ) is defined as</p><formula xml:id="formula_6">J D (W ) = N i=1 E Θ [− ln (f ci (u (x i ; Θ) ; W )) |c i ] . (5)</formula><p>For an arbitrary i-th empirical sample, expectation value is computed by marginalizing over deformation parameters:</p><formula xml:id="formula_7">E Θ [·|c i ] = · · · k dθ k p k (θ k |c i )(·).</formula><p>Here, the k-th deformation parameter obeys a class-conditional probability density p k (θ k |c i ). <ref type="bibr" target="#b1">2</ref> The optimization procedure is similar to that of the classindistinctive deformation learning, except that deformation parameters obey conditional probabilities. The integral can be rewritten by a sum of an infinite number of terms,</p><formula xml:id="formula_8">E Θ [ · |c] = lim R→∞ 1 R Θ=Θ c(1) ,··· ,Θ c(R) ( · ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">Θ c( ) = {θ c( ) 1 , · · · , θ c( ) K</formula><p>} is a set of deformation parameters at -th sampling, based on the PDFs: p k (θ k |c), k = 1, · · · , K, c = 1, · · · , N c . Some form of SGD can be used to minimize the objective function with a finite term approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Decision rule for augmented data learning</head><p>In this section we propose a new way of classification, APAC: Augmented PAttern Classification, and claim that it gives the optimal class decision for augmented data learning described in the previous section. It is shown that a single feedforward of a given test sample is no longer optimal when one minimizes the expectation value at the training stage. Cross entropy loss with softmax normalization is assumed in the following discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APAC for class-indistinctive deformation learning:</head><p>Given parameters W and data x, find c such that c = arg min c∈{1,··· ,Nc}</p><formula xml:id="formula_10">J {(x,c)} (W ).<label>(7)</label></formula><p>APAC for class-distinctive deformation learning: Given parameters W and data x, find c such that c = arg min c∈{1,··· ,Nc} Non-APAC (conventional) <ref type="figure" target="#fig_1">Figure 1</ref>. APAC, the proposed way of classification (above). Non-APAC, conventional way of classification (below).</p><formula xml:id="formula_11">J {(x,c)} (W ).<label>(8)</label></formula><p>It is obvious from Eq. (7, 8) that class decision making is an optimization process requiring minimization of the expectation values. The expectation value for a given data sample must be computed at test stage, as it is minimized through training stage (with some approximation). Note that the test sample itself is not fed into the classifier. In practice, finite-term relaxation must be made at test stage to estimate the expectation value:</p><formula xml:id="formula_12">E Θ [ · ] 1 M Θ=Θ (1) ,··· ,Θ (M ) ( · ) (9) E Θ [ · |c] 1 M Θ=Θ c(1) ,··· ,Θ c(M ) ( · )<label>(10)</label></formula><p>for the class-indistinctive case and class-distinctive case, respectively. This means, a finite number of sets of deformation parameters must be randomly sampled using the same probability density functions used in the training. APAC requires to average the logarithms of the softmax output, and then take the maximum argument to give optimal prediction. The process flow is depicted in <ref type="figure" target="#fig_1">Fig. 1</ref>. We emphasize that taking logarithm is an important step, otherwise an irrelevant quantity gets minimized at the test stage and classification performance likely degrades. APAC is equivalent to picking the maximum argument of the product of the softmax output, which is analogous to selecting the largest joint probability among individual class-probabilities of many virtual instances. For a sufficiently trained classifier, it is expected that generalization performance asymptotically reaches the highest as the number of terms, M , increases. The decision rule for class-distinctive deformation learning requires to generate plural sets of virtual samples for a given test image. Suppose one uses N d sets of deformations 3 at the training stage, then at the testing stage a data sample has to be deformed in N d M different ways. Then average of M logarithms of softmax output is computed for each class, using the corresponding deformation type. A maximum argument is then picked to predict a class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Experiments on image classification are carried out to evaluate generalization abilities of APAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Two datasets are used in the experiments. MNIST <ref type="bibr" target="#b9">[10]</ref>. This dataset contains images of handwritten digits with ground truths. It has 60K training and 10K testing samples. There are ten types of digits (0-9). The images are gray-scaled with 28 × 28 size. Background has no texture.</p><p>CIFAR-10 <ref type="bibr" target="#b7">[8]</ref>. This dataset is for benchmarking the coarse-grained generic object classification. It has 50K training and 10K testing samples. The labels are: plane, car, bird, cat, deer, dog, frog, horse, ship, and truck. The images are colored with 32 × 32 size. Foreground objects appear in different poses. Background differs in each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image deformation</head><p>Class-indistinctive deformation learning is carried out in all experiments. Details of deformation are given below. Some processed images are shown in <ref type="figure">Fig. 2</ref>.</p><p>Deformation on MNIST. We employed random <ref type="formula" target="#formula_1">(1)</ref>  (2) Elastic distortion. We followed Simard et al. <ref type="bibr" target="#b16">[17]</ref>, except for parameter setting. We used 6.0 standard deviation for the Gaussian filter, and 38.0 for α, the enlargement factor to the displacement fields.</p><p>(3) Line thickening/thinning. Morphological image dilation or erosion is adopted on interpolated images, with probabilities 1 4 and 1 4 , respectively. No line thickening/thinning is done with probability of 1 2 . Deformation on CIFAR-10. We used the ZCAwhitening <ref type="bibr" target="#b7">[8]</ref> followed by random (1) scaling, (2) shifting, (3) elastic distortion and side-flipping with probability of 1 2 . (1) Scaling. Image is magnified by a factor s, randomly picked from continuous uniform distribution, U(1.0, 2.0). Here, 1/s is the step size of image interpolation.</p><p>(2) Shifting. Random cropping is applied in following fashion. The x-component of the top-left corner of the interpolated patch is determined by sampling a value from U(0, S x (1 − 1/s)), where S x is the horizontal size of the original image. Shift along y-axis is determined in the same way but the sampling is done independently.</p><p>(3) Elastic distortion. We used 8.0 standard deviation for the Gaussian filter, and 40.0 for α.</p><p>A few comments on elastic distortion to CIFAR-10 are in order. Applying elastic distortion could be harmful for The ZCAwhitening part is skipped for visibility.</p><p>images of rigid objects such as plane or car, but could be beneficial for images of flexible objects such as cat or dog. Nevertheless, we applied elastic distortion to all classes in the same random manner based on two thoughts: 1) A class is not likely to be altered by elastic distortion even if the resultant image looks somehow unnatural, and 2) Breaking spatial correlation helps avoiding over-fitting. It is not our intention to state that elastic distortion is particularly important for generic object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Network architectures</head><p>We evaluated CNN and MLP for each of the datasets. In all networks, the ReLU activation function <ref type="bibr" target="#b8">[9]</ref> was used. We trained and evaluated a single model for each of the experiments; i.e., no ensembles of classifiers are used. We did not impose any stochasticity to the networks during training, such as dropout <ref type="bibr" target="#b6">[7]</ref> or dropconnect <ref type="bibr" target="#b20">[21]</ref>. Network architectures used in our experiments are presented in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>CNN models. For MNIST, we used the same numbers of layers and maps in each layer as in <ref type="bibr" target="#b2">[3]</ref>, but we used 5 × 5 convolutional kernels in all convolutional layers (we use symbol C 5 ), whereas different sizes were used in <ref type="bibr" target="#b2">[3]</ref>. For CIFAR-10, we just set the architecture by hand without any validation. A non-overlapping maximum pooling with g × g grid size (we use the symbol P g ) follows each convolution and activation. We use the symbols F and S for fully-connected and softmax, respectively, in the tables.</p><p>MLP models. Numbers of layers are determined by validations for both datasets; it turned out that 3 weight-layers were the best in both datasets. Numbers of hidden units for MNIST, 2500 and 2000, are the same as in <ref type="bibr" target="#b1">[2]</ref>. Numbers of units for CIFAR-10 are set by hand without any validation. Softmax normalization is applied to the output units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training details</head><p>Mini-batch SGD with momentum was used in every experiment. Initial values of learning rates are: 2 −4 for MNIST-CNN, 2 −5 for MNIST-MLP, 2 −8 for CIFAR-10- CNN, and 2 −8 for CIFAR-10-MLP. Learning rate is multiplied by 0.9993 after each epoch <ref type="bibr" target="#b3">4</ref> . The momentum rate is fixed to 0.9 during training. The mini-batch size is 100. Training data are randomly sampled with replacement, meaning that the same empirical sample can be sampled more than once in the same mini-batch, but deformation is done independently. Training is terminated at 15K epochs. We confirmed that 15K epochs gave sufficient convergences through validation. We added L 2 regularization terms with 5e-6 factor to the MNIST-MLP cost function and with 5e-7 factor to all the rest of the cost functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Classification performance</head><p>We first compare classification accuracies between the different decision rules. <ref type="table" target="#tab_2">Table 2</ref> shows test error rated produced by APAC and non-APAC. In the experiments, M , the number of virtual samples created from a given image at the testing stage (see Eq. (9)), is varied from 1(= 4 0 ) to 16,384(= 4 7 ). Our claim is to use as large M as possible to give class prediction, so the APAC results shown in <ref type="table" target="#tab_2">Table 2</ref> are those with M =16,384. In all experiments, APAC consistently gives superior accuracies compared to   non-APAC -prediction made by feedforwarding the original test samples-albeit they use the same weight trained with augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Performance on MNIST</head><p>We evaluate how classification accuracies change as M goes to a large value. Plot for M versus the test error rate is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The tendency that the classification accuracy raises as M increases for both networks is clearly observed. This is due to the fact that the expected loss J {(x,c)} (W ) is better estimated as M gets larger.</p><p>Our CNN model achieved 0.23% test error rate. To the best of our knowledge, this test error is the best when a single model is evaluated. We used no ensemble classifiers, such as model averaging or voting. (The best test error rate, 0.21%, was achieved by Wan et al. <ref type="bibr" target="#b20">[21]</ref>, where voting of five models was used.) Training was done only once in each of our experiments. All misclassified test samples are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. The top-2 prediction error rate is as low as 0.01%; i.e., there is only one misclassified sample out of 10K test samples, with our CNN model.  <ref type="figure">Figure 5</ref>. Test error rates of our CNN and MLP models on CIFAR-10. See <ref type="figure" target="#fig_3">Fig. 3</ref> for detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M (# virtual samples at test time)</head><p>Our single MLP model achieved 0.26% test error rate. To the best of our knowledge, this is the best record among MLP models reported previously. The best MLP error rate reported in the past was 0.35%, which was achieved by Cireşan et al. <ref type="bibr" target="#b1">[2]</ref>. They used a single MLP model that has around 12.1M free parameters and 5 weight layers, whereas our MLP model has around 7.0M parameters and 3 weight layers. Though our model is smaller in both the number of free parameters and the network depth, ours reaches significantly better classification performance. Our MLP model has, again, 0.01% top-2 prediction error rate on the test dataset; i.e., there is only one misclassified sample. Interestingly, the very same test sample (shown at the top-left position in <ref type="figure" target="#fig_4">Fig. 4)</ref> is misclassified by our CNN and MLP models, and all other 9,999 samples are correctly classified within two guesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Performance on CIFAR-10</head><p>Plot for M , the number of virtual samples generated at the testing stage, versus the test error rates is given in the <ref type="figure">Fig. 5</ref>. The tendency that generalization performance raises as the number of virtual samples increases is also observed. Generalization of non-APAC is significantly inferior to that of APAC for both architectures.</p><p>Our single CNN model results in 10.33% test error rate. This error rate is better than the multi-column CNN (11.21%) <ref type="bibr" target="#b2">[3]</ref> and the deep CNN reported by Krizhevsky et al. (11%) <ref type="bibr" target="#b8">[9]</ref>, and worse than the Bayesian optimization method (9.5%) <ref type="bibr" target="#b17">[18]</ref>, Probabilistic Maxout (9.39%) <ref type="bibr" target="#b18">[19]</ref>, Maxout (9.35%) <ref type="bibr" target="#b4">[5]</ref>, DropConnect (9.32%) <ref type="bibr" target="#b20">[21]</ref>, Networkin-Network (8.8%) <ref type="bibr" target="#b12">[13]</ref>, and Deeply-Supervised Nets (8.22%) <ref type="bibr" target="#b11">[12]</ref>. Our result is not close to those of the stateof-the-art methods. However, we believe that APAC can even improve the generalization abilities of these highperforming methods if augmented data learning is adopted. Our single MLP model yields 14.07% test error rate. This error rate is worse than the multi-column CNN (11.21%) <ref type="bibr" target="#b2">[3]</ref>, but better than the CNN with stochastic pooling method (15.13%) <ref type="bibr" target="#b23">[24]</ref> and the CNN with dropout in final hidden units (15.6%) <ref type="bibr" target="#b6">[7]</ref>. We are aware that fullyconnected neural networks are easy to over-fit when used for image classification tasks. But still, this experiment gives an evidence that a fully-connected network trained with augmented data and tested with APAC can outperform CNNs trained with recently invented regularization techniques and without augmented data <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis</head><p>All the experiments we conducted showed that APAC consistently gives better test error rate than non-APAC, a way of class prediction through single feedforwarding of original (non-deformed) data, when augmented data are learned. Let us illustrate how the class prediction gets altered between the two decision rules in the case of MNIST. <ref type="figure" target="#fig_5">Figure 6</ref> (a) shows a scatter plot of test data points of class-5 and class-9 in a 2D subspace of the linear output space, with x and y-axis corresponding to class-5 unit and class-9 unit. There, weights are obtained through the class-indistinctive deformation learning, and plotted data points do not involve image deformation. A test sample, whose image is superposed in the plot, would be misclassified to class-5 by non-APAC. We deform this test sample in 1,000 different ways, and plot these virtual data points in <ref type="figure" target="#fig_5">Fig. 6 (b)</ref>. One observation is that the virtual data points lie close to the original point. This is not so surprising because the original and the virtual images share many features in common, and the network is trained to be insensitive to the differences amongst these samples; namely, weak homography relation, elastic distortion, and line thickness. The other observation is that the majority (661 out of 1,000) of such virtual data points are in favor of the true class ('9'). Indeed, APAC predicts the true class from the 1,000 virtual samples. An important point is that there is a better chance of predicting the correct class by taking the product of softmax output of many virtual samples created from a given test sample, rather than by using the softmax output of the test sample.</p><p>One might wonder what happens if summation, instead of product, of softmax output of many virtual samples is taken at test stage. Just for the record, we list the results below. Test error rates produced by taking the maximum argument of the softmax sum with M =16,384 are: 0.24% for MNIST-CNN, 0.27% for MNIST-MLP, 10.42% for CIFAR-10-CNN, and 14.01% for CIFAR-10-MLP. Softmax product gives better performance in all cases except for the CIFAR-10-MLP. We do not have a clear explanation why one out of four experiments exhibits opposite result, but it is safer and more meaningful to use softmax product so as to maximize the joint probability among individual class-probabilities of many virtual instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Some remarks on augmented data learning</head><p>We make some remarks on how augmented data learning make difference in weights. <ref type="figure">Figure 7</ref> shows the trained weight maps in our MLP models. <ref type="bibr" target="#b4">5</ref> Trained weights for MNIST. The weight maps obtained through the augmented data learning have local-feature sensitive patterns (see <ref type="figure">Fig. 7 (a)</ref>). It has been argued that local feature extraction plays an important role in visual recognition. Combining local features in a certain way gives discriminative information about the entire object. CNN is one particular way to embody such strategy. But, MLP is not, in a sense that local-feature extractor is not built-in. Nevertheless, it is not impossible to give local-feature extraction ability to an MLP as <ref type="figure">Fig. 7 (a)</ref> indicates. On the contrary, the weight maps obtained through original data learning have only global patterns (see <ref type="figure">Fig. 7 (b)</ref>), implying that over-fitting to the training data takes place.</p><p>Trained weights for CIFAR-10. The weight maps obtained through the augmented data learning exhibit two functionalities (see <ref type="figure">Fig. 7</ref> (c)): the gray-scaled, local-edge extractor and spatially-spread, color differentiator. Similar findings have been pointed out by Krizhevsky et al. <ref type="bibr" target="#b8">[9]</ref>. The weight maps obtained through original data learning exhibit no such functionalities (see <ref type="figure">Fig. 7 (d)</ref>). With lacking spatial structure, the generalization is really poor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper address an issue of optimal decision rule for augmented data learning of neural networks. Online data deformation scheme in network training leads a minimization of the loss expectation marginalized over deformation-controlling parameters. It is expected that robustness against intra-class variation can be trained. Some sort of SGD can reach one of the local minima of such objective function with finite-term approximation. The claim is that class decision must be made through similar optimization process; i.e., the expectation value must be minimized for a given test sample. This demands that a given test sample must be augmented using the same deformation function used in the training, to compute the loss expectation for each class, if analytical integration is not feasible.</p><p>Our experimental results show that the proposed way of classification, APAC, gives far better generalization abilities than traditional classification rule, which requires a single feedforwarding of a given test sample. Our CNN model achieved the best test error rate (0.23%) among nonensemble classifiers on MNIST. Top-2 prediction using the model yields a test error rate of 0.01%. Through augmented data learning, MLP models acquire local-feature extraction functionality, which is a key of avoiding over-fitting. Indeed in the CIFAR-10 experiment, our MLP model using APAC outperforms some CNN models trained with recently-invented regularization techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&gt;0</head><label></label><figDesc>denote a function realized by a neural network with the softmax output units. Let f c be the c-th component of the output, then Nc c=1 f c = 1 and f c &gt; 0, ∀c ∈ {1, · · · , N c }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>homography transformation, (2) elastic distortion, and (3) line thickening/thinning. (Homography transformation. Image is projectively transformed by homography matrix H. The eight elements are assigned as Gaussian random variables: H 11 , H 22 ∼ N (1, 0.1 2 ), H 12 , H 13 , H 21 , H 23 , H 31 , H 32 ∼ N (0, 0.1 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>10 Figure 2 .</head><label>102</label><figDesc>Visualization of image deformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Test error rates of our CNN and MLP models on MNIST. Classification performance of APAC is plotted as a function of the number of virtual samples created at the test time. Non-APAC (prediction made by a single feedforwarding of an original test sample) results are also shown in the figure with texts. In both cases, the same weights are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>All MNIST test samples misclassified by our CNN model. In each figure, ground truth is printed at the top-left corner. The bar plot in each figure indicates softmax output of the 1st and 2nd predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of APAC prediction of a class-marginal sample. The violet and light blue points corresponds to the class-9 and class-5 test data points, respectively, of MNIST. The red points corresponds to the virtual data points created from a particular test sample. See the text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 Figure 7 .</head><label>107</label><figDesc>(a) augmented MNIST (b) non-augmented MNIST (c) augmented CIFAR-10 (d) non-augmented CIFAR-Visualization of randomly selected weight maps in the 1st weight layers of the MLP models trained with: (a) augmented MNIST, (b) non-augmented MNIST, (c) augmented CIFAR-10, and (d) non-augmented CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The network architectures. Top: MNIST-CNN model. Middle: CIFAR-10-CNN model. Bottom: MLP models. 30 210 2 8 2 4 2 2 2 1 2 1 2 1 2</figDesc><table><row><cell>layer</cell><cell>0</cell><cell>1</cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell></cell></row><row><cell># maps</cell><cell>1</cell><cell cols="2">20</cell><cell>20</cell><cell cols="4">40 40 150 10</cell><cell></cell></row><row><cell cols="7">map size 28 2 24 2 12 2 8 2 4 2</cell><cell>1 2</cell><cell>1 2</cell><cell></cell></row><row><cell cols="2">operation C5</cell><cell cols="2">P2</cell><cell>C5</cell><cell>P2</cell><cell>F</cell><cell>F</cell><cell>S</cell><cell></cell></row><row><cell>layer</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell># maps</cell><cell>3</cell><cell cols="8">64 64 128 128 256 256 128 10</cell></row><row><cell cols="8">map size 32 2 operation C3 P3 C3 P2 C3 P2 F</cell><cell cols="2">F S</cell></row><row><cell></cell><cell>layer</cell><cell></cell><cell>0</cell><cell>1</cell><cell></cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">MNIST</cell><cell>784</cell><cell cols="4">2500 2000 10</cell><cell></cell><cell></cell></row><row><cell cols="8">CIFAR-10 3072 4096 3072 10</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Summary of test error rates produced by our experiments. Finite-term approximation with M =16,384 is taken in the APAC results. Non-APAC means the conventional way of prediction, in which each original test sample is fed into the network.</figDesc><table><row><cell>Trained on</cell><cell></cell><cell cols="2">augmented data</cell><cell>original data</cell></row><row><cell>Tested by</cell><cell></cell><cell>APAC</cell><cell>non-APAC</cell><cell>non-APAC</cell></row><row><cell>MNIST</cell><cell>CNN</cell><cell>0.23%</cell><cell>0.39%</cell><cell>0.69%</cell></row><row><cell></cell><cell>MLP</cell><cell>0.26%</cell><cell>0.29%</cell><cell>1.49%</cell></row><row><cell cols="3">CIFAR-10 CNN 10.33%</cell><cell>20.05%</cell><cell>22.63%</cell></row><row><cell></cell><cell cols="2">MLP 14.07%</cell><cell>23.20%</cell><cell>55.96%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The data deformation function can be generalized to u : R d 0 → R d 1 with d 0 = d 1 , but we consider d 0 = d 1 case in this study for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">There may be a case where certain types of deformation are only applied to selected class(es). In such a case, a delta function is used as PDF to "turn-off" the deformation for other classes; i.e., p k (θ k |c) = δ(θ k ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">N d = Nc when each class has a unique deformation set, and N d &lt; Nc when two or more classes share the same type of deformations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Here, an "epoch" equals the number of iterations needed to process N virtual samples, where N is the number of original training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Here, a weight map means a row of weight matrix in the 1st weight layer, rearranged in the 2D form to visualize its spatial weighting pattern.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Partially occluded pedestrian classification using part-based classifiers and restricted boltzmann machine model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sagheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1065" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Classifying plankton with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<ptr target="http://benanne.github.io/2015/03/17/plankton.html.2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>ArXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gradient based learning applied to document recognition. Proceedings of IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno>ArXiv:1409.5185</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pedestrian detection using augmented training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fredriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4548" to="4553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transformation pursuit for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3646" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tangent Prop -a formalism for specifying selected invariances in an adaptive network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Victorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1992-12" />
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving deep neural networks with probabilistic maxout units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>ArXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep Image: Scaling up image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>ArXiv:1501.02876</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective training of a neural network character classifier for word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Yaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1996-12" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>ArXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
