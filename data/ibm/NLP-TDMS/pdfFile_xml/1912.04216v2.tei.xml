<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-Class Hinge Loss for Conditional GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
							<email>ilyak@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
							<email>czaja@umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-Class Hinge Loss for Conditional GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new algorithm to incorporate class conditional information into the critic of GANs via a multi-class generalization of the commonly used Hinge loss that is compatible with both supervised and semi-supervised settings. We study the compromise between training a state of the art generator and an accurate classifier simultaneously, and propose a way to use our algorithm to measure the degree to which a generator and critic are class conditional. We show the trade-off between a generator-critic pair respecting class conditioning inputs and generating the highest quality images. With our multi-hinge loss modification we are able to improve Inception Scores and Frechet Inception Distance on the Imagenet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Loss functions for training GANs</head><p>We rewrite the GAN optimization problem in a more generic form with minimums [12] that is amenable to discussion and programming: min D</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">[15]</ref> are an attractive approach to constructing generative models that mimic a target distribution, and have shown to be capable of learning to generate high-quality and diverse images directly from data <ref type="bibr" target="#b4">[6]</ref>. Conditional GANs (cGANs) <ref type="bibr" target="#b19">[21]</ref> are a type of GAN that use conditional information such as class labels to guide the training of the discriminator and the generator. Most frameworks of cGANs either augment a GAN by injecting (embedded) class information into the architecture of the real/fake discriminator <ref type="bibr" target="#b21">[23]</ref>, or by adding an auxiliary loss that is class based <ref type="bibr" target="#b24">[26]</ref>.</p><p>We describe an algorithm that uses both a projection discriminator and an auxiliary classifier with a loss that ensures generator updates are always class specific. Rather than training with a function that measures the information theoretic distance between the generative distribution and one target distribution, we generalize the successful hinge-loss <ref type="bibr" target="#b16">[18]</ref> that has become an essential ingredient of state of the art GANs <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b4">6]</ref> to the multi-class setting and use it to train a single generator-classifier pair <ref type="bibr" target="#b25">[27]</ref>. While the canonical hinge loss made generator updates according to a class agnostic margin learned by a real/fake discriminator <ref type="bibr" target="#b16">[18]</ref>, our multi-class hinge-loss GAN updates the generator according to many classification margins. With this modification, we are able to accelerate training compared to other GANs with auxiliary classifiers by performing only 1 D-step per G-step, and we improve Inception and Frechet Inception Distance Scores on Imagenet at 128 × 128 on a SAGAN baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background</head><p>A GAN <ref type="bibr" target="#b13">[15]</ref> is a framework to train a generative model that maps random vectors z ∈ Z into data example space x ∈ X concurrently with a discriminative network that evaluates its success by judging examples from the dataset and generator as real or fake. The GAN was originally formulated as the minimax game:</p><formula xml:id="formula_0">max D E x∼p d [log(D(x))] + E z∼pz [log(1 − D(G(z)))], min G E z∼pz [log(1 − D(G(z)))],<label>(1)</label></formula><p>where p d is the real data distribution consisting of examples x ∈ X , p z is the latent distribution over the latent space Z, G : Z → X is the generator neural network, and D : X → [0, 1] is the discriminator neural network. The GAN model transfers the success of the deep discriminative model D to the generative model G and succeeds in generating impressive samples G(z). To regain the minimax objective in equation <ref type="formula" target="#formula_0">(1)</ref> we set f (w) = log(1 + e −w ) and h(w) = −g(w) = −w − log(1 + e −w ) <ref type="bibr" target="#b10">[12]</ref>. This choice minimizes the Jensen-Shannon divergence between p d and p g <ref type="bibr" target="#b13">[15]</ref>, which denotes the model distribution implicitly defined by G(z), z ∼ p z . The minimax objective however was difficult to train <ref type="bibr" target="#b2">[4]</ref>, and the study of various ways to measure the divergence or distance between p d and p g has been a source of improved loss functions that make training more stable and samples G(z) of higher quality.</p><p>WGAN set f (w) = −w, h(w) = −g(w) = −w and clipped the weights of D to greatly improve the ease and quality of training and reduced the mode dropping problem of GANs; it has the interpretation of minimizing the Wasserstein-1 distance between p d and p g . Minimizing Wassertstein-1 distance was shown to be a special instance of minimizing the integral probability metric (IPM) between p d and p g <ref type="bibr" target="#b22">[24]</ref>, and inspired a mean and covariance feature matching IPM loss (McGAN) <ref type="bibr" target="#b22">[24]</ref> following the empirical successes of the Maximum Mean Discrepancy objective <ref type="bibr" target="#b15">[17]</ref> and feature matching <ref type="bibr" target="#b25">[27]</ref>.</p><p>The mean feature matching of McGAN has a geometric interpretation: the gradient updates of feature matching for the generator are normal to the separatating hyperplane learned by the discriminator <ref type="bibr" target="#b16">[18]</ref>. The SVM like hinge-loss choice of f (w) = max(0, 1 − w), g(w) = max(0, 1 + w) and h(w) = −w <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> has gradients similar to those of McGAN. When combined with spectral normalization of weights in D <ref type="bibr" target="#b20">[22]</ref>, the hinge loss greatly improves performance, and has become a mainstay in recent state of the art GANs <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b21">23]</ref>. In this work we generalize this hinge loss to a multi-class setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Supervised training for conditional GANs</head><p>Conditional GANs (cGANs) are a type of GAN that use conditional information <ref type="bibr" target="#b19">[21]</ref> in the discriminator and generator. G and D become functions of the pairs (z ∼ p z , y ∼ p d )</p><p>and (x, y) ∼ p d , where y is the conditional data, for example the class labels of an image. In a cGAN with a hinge loss, the discriminator would minimize L D in equation <ref type="formula" target="#formula_1">(3)</ref>, and the generator would minimize L G in equation <ref type="formula" target="#formula_1">(3)</ref>  <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b4">6]</ref>.</p><formula xml:id="formula_1">L D =E (x,y)∼p d [max(0, 1 − D(x, y))] +E z∼pz,y∼p d [max(0, 1 + D(G(z, y), y))] =L Dreal + L Dfake , L G = − E z∼pz,y∼p d [D(G(z, y), y)].<label>(3)</label></formula><p>We briefly review some work on using conditional information to train the discriminator of GANs, as well as uses of classifiers.</p><p>A projection discriminator <ref type="bibr" target="#b21">[23]</ref> is a type of conditional discriminator that adds the inner product between an intermediate feature and a class embedding to its final output, and proves highly effective when combined with spectral normalization in G <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b4">6]</ref>. Several GANs have used a classifier in addition to, or in place of, a discriminator to improve training. CatGAN <ref type="bibr" target="#b26">[28]</ref> replaces the discriminator with a K-class classifier trained with cross entropy loss that the generator tries to confuse. ACGAN <ref type="bibr" target="#b24">[26]</ref> uses an auxiliary classification network or extra classification layer appended to the discriminator, and adds the cross entropy loss from this network to the minimax GAN loss. Triple GAN <ref type="bibr" target="#b5">[7]</ref> trains a classifier in addition to a discriminator and updates it with a special minimax type loss.</p><p>Improved GAN <ref type="bibr" target="#b25">[27]</ref> originally proposed using a K + 1 classifier for semi-supervised learning (SSL) with feature matching loss, and others <ref type="bibr" target="#b23">[25]</ref> have a used a similar approach for SSL GANs as well. The single conditional critic architecture of D : (x, y) → R is swapped for the classifier architecture C : x → R K+1 , where there are K class labels and an extra label for fake images (the "+1") <ref type="bibr" target="#b25">[27]</ref>. The Improved GAN trains this classifier architecture in a semisupervised setting with log-likelihood loss, and trains the generator with a class agnostic mean feature matching loss.  <ref type="figure">Figure 2</ref>: 2a shows the projection discriminator architecture <ref type="bibr" target="#b21">[23]</ref> of our SAGAN. 2b shows how the projection discriminator could simultaneously be trained with an auxiliary classifier loss: we train MHSharedGAN this way. 2c Is an ACGAN <ref type="bibr" target="#b24">[26]</ref> architecture that also contains a projection discriminator: we train our ACGAN and MHGAN with this architecture.</p><p>BadGAN <ref type="bibr" target="#b7">[9]</ref> used the Improved GAN to achieve state of the art performance on semi-supervised learning classification and found the aim of having a low classification error on the K real classes is orthogonal to generating realistic examples. MarginGAN <ref type="bibr" target="#b11">[13]</ref> used a Triple GAN to train a high-quality classifier with a "bad" GAN <ref type="bibr" target="#b7">[9]</ref> by decreasing the margins of error of the cross entropy loss for generated images. There are few works which focus on improving the generator quality in a label limited setting. One approach has used two discriminators, one specializing on labeled and the other on unlabeled data <ref type="bibr" target="#b27">[29]</ref>, another has found that pre-labelling the unlabelled training data with a SSL classifier before GAN training to be a high performance method <ref type="bibr" target="#b18">[20]</ref>. Our proposed multi-hinge GAN (MHGAN) uses a classifier like ACGAN <ref type="bibr" target="#b24">[26]</ref> but instead of using a probabilistic cross entropy loss with the WGAN, we use a multi-hinge loss similar to that in the discriminator. We demonstrate that adding this loss to the current state of the art SAGAN architecture with projection discrimination improves image quality and diversity, and trains stably at only 1 discriminator step per generator step for both supervised and semi-supervised settings. We compare our class specific modification with cross entropy loss <ref type="bibr" target="#b24">[26]</ref> with projection discrimination, and with projection discrimination alone. Unlike cross-entropy, which has a basis in probabilistic quantities that are not present in WGANs, our multi-hinge loss is completely compatible with the WGAN formulation. We also find that the task of classification and discrimination should not share parameters, and show how to trade-off image diversity for quality in a shared parameter version MHSharedGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-hinge loss</head><p>We propose a multi-hinge loss that can be easily plugged into the popular and state of the art projection discriminator cGAN architecture <ref type="bibr" target="#b21">[23]</ref>. Our fully supervised formulation, motivated in Section 2.1, uses the auxiliary classifier setup seen in <ref type="figure">Figure 2c</ref>, but instead of using cross entropy as ACGAN <ref type="bibr" target="#b24">[26]</ref> does to train this classifier we generalize the binary hinge loss <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> to a multi-class hinge loss developed for SVMs <ref type="bibr" target="#b6">[8]</ref>, and we use it to train a spectrally normalized WGAN <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b30">32]</ref>. Unlike other ACGANs, our multi-hinge loss formulation only requires 1 Critic step per 1 Generator step greatly speeding up training, and uses a single classifier for all classes in the dataset.</p><p>We denote the classifier function as C : X → Y and let C k (x) denote the kth element of the vector output of C for an example x, which represents the affinity of class k for x. The Crammer-Singer multi-hinge loss that we propose as an auxiliary term is:</p><formula xml:id="formula_2">L Daux = E (x,y)∼p d [max(0, 1 − C y (x) + C ¬y (x))] L Gaux = E z∼pz y∼p d [max(0, 1 − C y (G(z, y)) + C ¬y (G(z, y)))]<label>(4)</label></formula><p>where C ¬y (x) is the classifier's highest affinity for any label that is "not y": C ¬y (x) = max k =y C k (x), y = 0, 1, . . . , K. We then train with the modified loss:</p><formula xml:id="formula_3">L MH,D =L Dreal + L Dfake + L Daux , L MH,G =L G + λL Gaux .<label>(5)</label></formula><p>The advantage of a conditional WGAN trained with the auxiliary terms in equation <ref type="formula" target="#formula_2">(4)</ref> is the main result of this work. In the following sections we discuss the motivation and advantage of this training procedure. We also provide an SSL formulation in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation &amp; Intuition</head><p>A class conditional discriminator should obviously not output "real" when it is conditioned on the wrong class. That is for a pair (x, y) ∼ p d we expect if our discriminator loss is minimized then so is the quantity: 1 − D(x, y) + D(x, k), k = y. This quantity is positive for all k so long as the output of the discriminator conditioned on the correct label is larger by at least one than the discriminator conditioned on the rest of the labels. To explicitly enforce this The class margins (dotted lines) are enforced for generated samples (unfilled dots, colored by their conditioning) by updating the generator with respect to class margins. Green samples are classified incorrectly and the generator update gradient is shown with pointed arrows. Blue generated samples are classified correctly but within the margin, and the gradient is shown. Red generated samples are classified correctly outside of the margin and the MHinge loss has no gradient for these samples.</p><p>margin, we could minimize the expectation:</p><formula xml:id="formula_4">E (x,y)∼p d [max(0, 1 − D(x, y) + max k =y D(x, k))]<label>(6)</label></formula><p>This form of a hinge-loss has been used by <ref type="bibr" target="#b6">[8]</ref> in their formulation of efficient multi-class kernel SVMs. The ReLU max(0, ·) leads equation <ref type="formula" target="#formula_4">(6)</ref> to ignore cases where the correct decision is made with a margin more than 1. We design equation <ref type="formula" target="#formula_2">(4)</ref> to enforce the margins in equation <ref type="bibr" target="#b4">(6)</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the effect this loss has on generator training. Note that a projection discriminator cGAN implicitly has a classifier in it. The output of D(x, k) projects the penultimate features onto an embedding for class k. Similarly the vector output of a typical classifier is a matrix multiply of the penultimate features with a features × class matrix. A projection discriminator D(x, k) could be turned into a classifier C(x) by using the entire matrix of class embeddings to output an affinity for every class, as shown in <ref type="figure">Figure 2b</ref>. Creating a classifier this way doesn't increase the parameter count at all, and only increases computation in one layer of D by a constant factor (the number of classes) which we find is completely negligible for the large models typically trained for 64 × 64 images or greater. However, we find that this sharing of parameters between the classification task in equation <ref type="bibr" target="#b4">(6)</ref> and the discrimination task around which the adversarial training centers is disadvantageous. As we will discuss, it is instead preferable to add an auxiliary classifier via an extra final fully connected layer, the additional memory cost of a features × class matrix proves to be negligible in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-supervised learning</head><p>When additional unlabeled data x ∼ p u is available, we find that learning with projection discrimination is not stable, that is bypassing the projection discriminator when a label is not available does not lead to successful training. For semi-supervised settings we modify the training procedure in equation <ref type="formula" target="#formula_3">(5)</ref> by using pseudo-labels <ref type="bibr" target="#b18">[20]</ref>. For the discriminator we add the term:</p><formula xml:id="formula_5">L Dunlab = E x∼pu [max(0, 1 − D(x, y C (x)))] (7)</formula><p>where y C (x) = arg max k∈Y C k (x), that is we depend on the classifier that we co-train with the discriminator. The loss for the generator is left unchanged. Overall for the semi-supervised setting we train with the losses:</p><formula xml:id="formula_6">L MH,SSL,D = L Dreal + L Dunlab 2 + L Dfake + L Daux , L MH,SSL,G =L MH,G .<label>(8)</label></formula><p>A similar loss has previously been used with a WGAN cotrained with a cross entropy auxiliary classifier <ref type="bibr" target="#b18">[20]</ref> L AC,SSL,</p><formula xml:id="formula_7">D = L Dreal + L Dunlab 2 + L Dfake + E (x,y)∼p d [log C y (x)], L AC,SSL,G =L G + λE z∼pz y∼p d [log C y (G(z, y))].<label>(9)</label></formula><p>However we find that the consistency of hinge functions throughout the loss terms leads to more successful training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>As our baseline, we use a spectrally normalized <ref type="bibr" target="#b20">[22]</ref> SAGAN <ref type="bibr" target="#b30">[32]</ref> architecture. We use this baseline from the publicly available tfgan implementation <ref type="bibr" target="#b1">[3]</ref>, and execute experiments on single v2-8 and v3-8 TPUs available on Google TFRC. This baseline was chosen for its exceptional performance <ref type="bibr" target="#b1">[3]</ref>. On top of this baseline we implement a second baseline, ACGAN, and our MHGAN (we found ACGAN trained without projection discrimination to not be competitive). The only architectural changes to SAGAN for both of these networks is that a single dense classification layer is added to the penultimate features. For these networks conditional information is given to G using class conditional BatchNorm <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b8">10]</ref> and to D with projection discrimination <ref type="bibr" target="#b21">[23]</ref>. A spectral norm is applied to both D and G during training <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b21">23]</ref>. We train our SAGAN baseline with the hinge loss <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref> in equation <ref type="bibr" target="#b1">(3)</ref>. We train our proposed MHGAN with equation <ref type="bibr" target="#b3">(5)</ref>. To better see the advantage of our multi-hinge loss formulation we train an ACGAN baseline with the loss: <ref type="figure">Figure 4</ref>: Evaluation metrics for Imagenet-128. Inception Score and Frechet Inception Distance during training, and a comparison of Intra-class Frechet Inception distance for our best MHGAN model and the best baseline SAGAN we trained. In 4c there are 622 classes whose intra-fid improve (each point below the red line is a class for which FID improves) and there are 46 classes in the lower right cluster.</p><formula xml:id="formula_8">L AC,D =L Dreal + L Dfake + E (x,y)∼p d [log C y (x)], L AC,G =L G + λE z∼pz y∼p d [log C y (G(z, y))].<label>(10)</label></formula><formula xml:id="formula_9">(a) (b) (c)</formula><p>We also evaluate our multi-hinge formulation for semisupervised settings and similarly train a MHGAN-SSL model with equation <ref type="formula" target="#formula_6">(8)</ref>, and compare it with an ACGAN-SSL model trained with equation (9) (SAGAN was not able to train stably without an auxiliary classifier). For SAGAN, MHGAN, and MHGAN-SSL we optimize with size 1024 batches, learning rates of 1e − 4 and 4e − 4 for G and D, and 1 D step per G step. For ACGAN and ACGAN-SSL, we found training to be unsuccessful with only 1 D step per G step, so we train with 2 D steps per G step (training with more than 2 D steps did not improve results). For ACGAN-SSL we also used a learning rate of 5e − 4 for D and a z dimension of 120 instead of 128 in our other experiments <ref type="bibr" target="#b18">[20]</ref>. The generator's auxiliary classifier loss weight is fixed at λ = 0.1 for all experiments. We use 64 channels and limit most of our experiments to 1,000,000 iterations. We use the Inception Score (IS) <ref type="bibr" target="#b25">[27]</ref> and Frechet Inception Distance (FID) for quantitative evaluation of our generative models. During training we compute these scores with 10 groups of 1024 randomly generated samples using the official tensorflow implementations <ref type="bibr" target="#b1">[3,</ref><ref type="bibr">2]</ref>, and for the final numbers in <ref type="table">Table 1</ref> we use 50k samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully supervised image generation</head><p>In <ref type="figure">Figure 4</ref> and <ref type="table">Table 1</ref> we present fully supervised results on the Imagenet dataset <ref type="bibr" target="#b9">[11]</ref>. Previous work has noted a multitude of GAN algorithms that train well on datasets of limited complexity and resolution but may not provide an indication that they can scale <ref type="bibr" target="#b17">[19]</ref>. Thus we choose the largest and most diverse image data set commonly used to evaluate our GANs. Imagenet contains 1.3M training images and 50k validation images, each corresponding to one of 1k object classes. We resize the images to 128 × 128 for our experiments. On a single v3-8 TPU MHGAN and SAGAN complete 10k steps every 2 hours, and ACGAN completes 10k G-steps every 3.3 hours. Thus 1M MHGAN iterations takes 8.3 days. <ref type="table">Table 1</ref> shows that the auxiliary MHGAN loss added to SAGAN trains a better GAN according to Inception score and FID. The classifier increases the fidelity of the samples generated by the GAN without sacrificing diversity as shown in <ref type="figure">Figure 4c</ref>, where we plot the intra-class FID of SAGAN versus MHGAN for the best models we trained by overall IS. The mean class FID improves by 3.5%, and the class FID is lowered for 622 classes, and for 46 classes in the lower right cluster improves by an average of 50%. For the point below the cluster, mode collapse is prevented by MHGAN for the tench class. Some of these points in the cluster are shown in <ref type="figure" target="#fig_4">Figure 5</ref>, where we randomly sample images from classes where the intra-FID score decreased (improved) from the SAGAN baseline to our MHGAN. We see in some of these cases that SAGAN is showing distortions or early signs of mode collapse, despite overall IS being at a high. We also show the opposite relation in <ref type="figure">Figure 6</ref>, where we randomly sample the classes for which FID increases (worsens) the most from SAGAN to MHGAN. In <ref type="figure">Figure 6</ref> we see that MH-GAN doesn't show a worrying level of decreased diversity or mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Measuring the conditioning of the discriminator and generator</head><p>We attribute our model's success to the fact that it gradually incorporates class specific information into both the discriminator and the generator networks than just projection discrimination alone. When we have a classifier as in MHGAN and ACGAN it is straightforward to calculate the validation accuracy and self accuracy. Validation accuracy is the accuracy of the classifier on real validation data, the whole standard validation partition of the dataset is used. This measures how good the classifier learned is; if it starts to overfit the training data then we expect validation accuracy to decay. For self accuracy we test if arg max k∈Y C k <ref type="figure">(G(z, y)</ref>)   <ref type="figure">Figure 6</ref>: Classes for which Intra-FID gets worse for our proposed MHGAN. We show random samples from the 5 classes that worsened the most in <ref type="figure">Figure 4c</ref> by points. We did not observe any instances of mode collapse in MHGAN. equals y. This measures how G incorporates the label information into its output, as measured by the concurrently trained C. This measures the self consistency of the GAN. For the baseline SAGAN network with a projection discriminator, it is also possible to perform classification using the method mentioned in Section 2.1. That is our "classification"</p><p>for an example x is arg max k∈Y D(x, k), where each k is used as input to the projection discriminator layer that was trained.</p><p>Conditional information in C&amp;D. We find that our suspicion from Section 2.1 is confirmed for the baseline projection discriminator model: for (x, y) ∼ p d it is not the case that  <ref type="bibr" target="#b1">[3]</ref>.</p><p>D(x, y) &gt; D(x, k), k = y with high probability. In fact, we find that for all projection discriminators in our trained models that D(x, y) &gt; D(x, k), k = y uniformly and randomly. However, the motivation behind equation <ref type="formula" target="#formula_2">(4)</ref> was to create a better GAN by training the discriminator to incorporate as much conditional information in the dataset. Though we find that the projection discriminator layer cannot be made useful for classification, we can still incorporate more conditional information into the discriminator network through the embeddings it produces. In <ref type="figure">Figure 7a</ref> we plot validation and self accuracy for the models with a classifier (these metrics oscillate randomly near "1/ num classes" throughout training for SAGAN).</p><p>We see that validation performance (top-1 accuracy on Imagenet) is about the same for both models, converging around 50% and peaking at 50.32% for ACGAN and 52.66% for MHGAN. This is not competitive with purpose built classifiers which have different architectures and are typically much deeper than the discriminator of our GANs. Meanwhile classification accuracy on the training set hovers around the level of 98%. Self accuracy converges to 90% in both, and for obvious reasons goes to 100% when the generator experiences collapse (it is a curious detail that the SOA top-1 classification accuracy on Imagenet has also been converging to 90% <ref type="bibr" target="#b29">[31]</ref>). We leave it to future work to control self accuracy to be at the same level as validation accuracy, and for both to increase even more gradually, since this could prolong training and IS and FID improvements further. Intuitively there is a trade-off between fidelity (generated images looking like the right class) and diversity. In <ref type="figure">Figure 7a</ref> we see that the absence of a logarithm, and the hinge function which limits learning on examples that are correct by a margin, leads to the more gradual incorporation of class specific information into the generator for MHGAN.  <ref type="figure">Figure 7</ref>: Validation accuracy and self accuracy track IS and FID improvements and reach their own plateaus during training. Discriminator accuracy is very noisy during training, and behaves differently than classification accuracy, suggesting that the two tasks can be separately optimized.</p><p>In <ref type="figure">Figure 7b</ref> we plot the discriminator's accuracy on the real validation set (the percentage of examples for which D(x, y) &gt; 0). This is a very noisy metric during training, so we also plot the 100k itr moving average with the heavier line. It is interesting that this metric declines to around 50% as training progresses, since the same discriminator accuracy on the training set is stable at 91% on the training set for MHGAN, and 98% for SAGAN. Intuitively this indicates that D is memorizing the training set <ref type="bibr" target="#b4">[6]</ref>. Yet since the classification accuracy remains stable or declines only slightly, this shows a disconnect between the classification and discrimination tasks.</p><p>The importance of separating classification and discrimination tasks. Earlier we mentioned that for all three models that we compare, the projection discriminator never incorporates class specific information; that the discriminator output has the highest affinity for the correct class about "1/ num classes" of the time. We observe that it is disadvantageous to try and train the projection discriminator to have the property D(x, y) &gt; D(x, k), k = y with high probability. Both MH-GAN and ACGAN use an extra fully connected layer as an auxiliary classifier during training. Having this extra layer and training with equation <ref type="formula" target="#formula_3">(5)</ref> improves both quality and diversity as shown in <ref type="figure">Figure 4</ref>. It is also possible to follow the approach mentioned in Section 2.1 and share parameters between the projection discriminator and the classifier, and train with equation <ref type="bibr" target="#b3">(5)</ref>. We call this MHSharedGAN and illustrate this strategy in <ref type="figure">Figure 2b</ref>.</p><p>Training from scratch this way does not lead to satisfactory performance, but introducing this loss in the middle of SAGAN training produces interesting results (introducing it after 1M steps is also unsuccessful). After just 5k iterations after transitioning from training with equation <ref type="formula">(</ref>  unlike with MHGAN, there is a trade-off between quality and diversity being made when parameters are shared. This is illustrated in <ref type="figure" target="#fig_7">Figure 8</ref>, where we see the diversity of the generator drops drastically. Though the layer is spectrally normalized during training with 1 step of the power iteration method, during this second phase of training the spectrum of the projection discriminator (rank v.s. value plot of eigenvalues) changes from a sigmoid shape to a steep exponential shape, indicating that the projection has collapsed to a just a few dimensions.</p><p>In developing MHGAN we also experimented with K + 1 <ref type="bibr" target="#b25">[27]</ref> formulations of the loss where the discrimination and classification tasks are unified. At low resolutions (48 × 48 and below) we observed that such models are able to train from scratch and obtain competitive IS and FID scores, but that fidelity came at the cost of diversity there too. Our experiments show that classification should be left as an auxiliary task to discrimination, and not combined with it by sharing parameters too closely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semi-supervised image generation</head><p>To demonstrate our multi-hinge loss in a semi-supervised setting we use the partially labeled Imagenet data set with a random selection of 10% of the samples from each class retaining their label publicly available on TFDS <ref type="bibr" target="#b0">[1]</ref>. We perform experiments on this 128 × 128 sized dataset. We keep the same architecture choices described in Section 3 and train our MHGAN-SSL with equation <ref type="bibr" target="#b6">(8)</ref> and compare to ACGAN-SSL trained with equation <ref type="bibr" target="#b7">(9)</ref>. MHGAN-SSL trains faster than ACGAN-SSL but reaches a similar level of performance, with (IS, FID) scores of (32.40, 26.12) for MHGAN-SSL and <ref type="bibr">(32.38, 26.12)</ref> for the ACGAN-SSL baseline. Both networks have their validation accuracy go to 40%, and self accuracy above 80%. The speed of of MHGAN-SSL is an advantage over the ACGAN-SSL we train, which has a similar loss formulation to S 2 GAN-CO which achieves (IS, FID) scores of (37.2, 17.7) using a larger architecture than we train <ref type="bibr" target="#b18">[20]</ref>. We leave it to future work to train the network in S 2 GAN-CO with the MHGAN-SSL loss. However it has been shown that co-training with pseudo-labels is not as competitive as pre-labeling the unlabeled data with a separate classifier and then using a fully supervised ACGAN like loss, this approach S 2 GAN achieves (IS, FID) scores of (73.4, 8.9) <ref type="bibr" target="#b18">[20]</ref>. We also leave it to future work to use this pre-labeling strategy with MHGAN to improve on SSL GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>MHGAN is a powerful addition to projection discrimination and improves training with negligible additional computational cost. <ref type="table">Table 1</ref> and <ref type="figure">Figure 4</ref> show that the multi-hinge loss improves both the quality and diversity of generated images on Imagenet-128. We also show how classification and discrimination tasks should not be integrated too closely, and show how the multi-hinge loss can be used to trade diversity for image quality in MHSharedGAN. MHGAN is able to perform well in both fully supervised and semisupervised settings, and learns a relatively accurate classifier concurrently with a high quality generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Appendix: Source Code</head><p>Our MHGAN and baseline implementations are based on the Self-Attention GAN <ref type="bibr" target="#b30">[32]</ref> available on github <ref type="bibr" target="#b1">[3]</ref>. Our code is available at https://github.com/ ilyakava/gan. <ref type="figure">Figure 9</ref> shows how labelled and unlabelled data flow through our ACAN and MHGAN networks during semisupervised learning. In <ref type="figure">Figure 9a</ref> is a projection discrimination network with an auxiliary classifier added. <ref type="figure">Figure 9b</ref> shows that for unlabeled data, the role of y is substituted with the pseudolabel y C(x) , and the classification loss is not trained. <ref type="figure" target="#fig_0">Figure 10</ref> shows that the best FID over time was more quickly achieved by MHGAN-SSL for 1 million iterations, though the performance is not as impressive as in the fully supervised case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Appendix: Additional details on our SSL GANs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix: Imagenet-64</head><p>We also evaluated MHGAN on a smaller scale dataset of Imagenet at resolution 64 × 64. The conclusions from these experiments are the same but more muted compared to Imagenet at 128 × 128. <ref type="figure" target="#fig_0">Figures 11a and 11b</ref> show that MHGAN improves the FID and IS scores over SAGAN and ACGAN baselines, and in table 2 are FID and IS numbers calculated over 50k images, and the Intra-FID comparison of SAGAN versus MHGAN which shows that diversity stayed the same between the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">MHShared GAN finetuning for Imagenet 64×64</head><p>We also performed the MHShared GAN finetuning on the SAGAN in <ref type="table" target="#tab_2">Table 2</ref> (trained for 1M steps) and after 15k steps the IS went to 30.47 and the FID went to 10.03 (and the validation classification accuracy went to 23.60% while the generator classification self accuracy went to 62.66%, both started at 1/n class). Again the diversity of the generator visibly declined; some examples of this are in <ref type="figure" target="#fig_0">Figure 12</ref>.  <ref type="figure">Figure 9</ref>: The architectures of the semi supervised GANs that we train. 9a shows the auxiliary classifier architecture <ref type="bibr" target="#b21">[23]</ref> of our SAGAN, the same as used for fully supervised experiments. When x ∼ p d the discriminator treats the example as shown in 9a. In the diagram the green "Embedding" block is the projection discriminator embedding for a class, φ is the discriminator up to the penultimate feature layer, ψ is a linear layer with scalar output, ψ C is a linear layer with output size n classes. When x ∼ p u and the label is not available, it is treated as shown in 9b. This is similar to S 2 GAN <ref type="bibr" target="#b18">[20]</ref>. Both ACGAN-SLL and MHGAN-SLL use these architectures, the only difference between the two is the form of the classification loss for labelled examples.    And the bottom row is that same model after 15k steps using the MHSharedGAN strategy, where the projection discriminator weights are shared between a itself and a classifier optimized with multi-hinge loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed MHGAN generates images while leveraging the information from the margins of a classifier. All the images in this figure are conditionally sampled from our MHGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>This illustration of MHingeGAN training for L Gaux shows a classifier (solid lines and shaded regions) learned on real samples (filled dots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) MHGAN IFID 134.82 (b) MHGAN IFID 126.52 (c) MHGAN IFID 158.00 (d) MHGAN IFID 147.14 (e) MHGAN 132.38 (f) SAGAN IFID 287.50 (g) SAGAN IFID 275.03 (h) SAGAN IFID 339.48 (i) SAGAN IFID 307.16 (j) SAGAN IFID 296.28</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Classes for which Intra-FID gets better for our proposed MHGAN. We show random samples from the 5 classes that improved the most inFigure 4cby points, excluding the trout class where SAGAN completely collapses.(a) MHGAN IFID 27.92 (b) MHGAN IFID 93.79 (c) MHGAN IFID 154.82 (d) MHGAN IFID 102.92 (e) MHGAN IFID 182.66 (f) SAGAN IFID 19.48 (g) SAGAN IFID 81.93 (h) SAGAN IFID 146.08 (i) SAGAN IFID 94.21 (j) SAGAN IFID 173.95</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>3) (SAGAN) to equation (5) (MHSharedGAN) Inception score skyrockets from 47.79 to 169.68 and Frechet Inception Distance drops to 8.87 (evaluated on 50k images). The catch is that (a) SAGAN trained for 580k steps. IS 47.79 and FID 17.10 (50k) at this stage. (b) The model after 5k steps of MHSharedGAN training. IS 169.68 and FID 8.87 (50k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Enforcing class fidelity without an auxiliary classifier, and training class margins to be respected in the projection discriminator using equation (4) instead leads to low diversity, but higher quality images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) For x ∼ p d (b) For x ∼ pu</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>The best FID of our models plotted by duration of training. The multi-hinge loss accelerates training over SAGAN and ACGAN baselines in both fully supervised and semi-supervised settings. Inception Scores and FIDs (10k) for supervised image generation on Imagenet 64 × 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Before/After performing the MHSharedGAN finetuning to lower the diversity, but raise the quality (according to IS and FID metrics) of samples on Imagenet 64 × 64. The top row of images is sampled from SAGAN trained for 1M steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 Million D-steps for SAGAN and MHGAN can be completed in about 48 hours.</figDesc><table><row><cell>Method</cell><cell cols="2">Imagenet 64x64</cell></row><row><cell></cell><cell>IS</cell><cell>FID</cell></row><row><cell>Real data</cell><cell>56.67</cell><cell></cell></row><row><cell>Baseline 1M</cell><cell>19.60</cell><cell>15.49</cell></row><row><cell cols="2">MHGAN 1M 22.16</cell><cell>13.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Inception Scores and FIDs (50k) for supervised image generation on Imagenet 64 × 64. Though the margin of improvement is not as great as for Imagenet 128 × 128, adding the auxiliary MHGAN loss proves fruitful. Diversity is maintained at the same level (it improved in Imagenet 128 × 128).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow Datasets, a collection of ready-to-use datasets</title>
		<ptr target="https://www.tensorflow.org/datasets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow/Gan</surname></persName>
		</author>
		<idno>original-date: 2019-04- 29T16:24:10Z</idno>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<idno>arXiv: 1701.07875</idno>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taufik</forename><surname>Li Chongxuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4088" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
		</imprint>
	</monogr>
	<note>Journal of machine learning research</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Good semi-supervised learning that requires a bad gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6510" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08753</idno>
		<idno>arXiv: 1901.08753</idno>
		<title level="m">Towards a Deeper Understanding of Adversarial Losses</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial training in semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Margingan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="10440" to="10449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Learned Representation For Artistic Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<idno>arXiv: 1705.02894</idno>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are GANs Created Equal? A Large-Scale Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Highfidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02271</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<idno>arXiv: 1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">McGan: Mean and covariance feature matching GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Convention Centre</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<idno>arXiv: 1511.06390</idno>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Kumar Sricharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shreve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Saketh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05789</idno>
		<title level="m">Semi-supervised conditional gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5523" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
