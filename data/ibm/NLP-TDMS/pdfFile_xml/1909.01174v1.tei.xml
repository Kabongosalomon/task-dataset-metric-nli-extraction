<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Demucs: Deep Extractor for Music Sources with extra unlabeled data remixed</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Défossez</surname></persName>
							<email>defossez@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research INRIA</orgName>
								<orgName type="institution">École Normale Supérieure PSL Research University Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
							<email>usunier@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
							<email>francis.bach@ens.fr</email>
							<affiliation key="aff3">
								<orgName type="department">INRIA / École Normale Supérieure</orgName>
								<orgName type="institution">PSL Research University Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Demucs: Deep Extractor for Music Sources with extra unlabeled data remixed</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of source separation for music using deep learning with four known sources: drums, bass, vocals and other accompaniments. State-of-the-art approaches predict soft masks over mixture spectrograms while methods working on the waveform are lagging behind as measured on the standard MusDB <ref type="bibr" target="#b21">[22]</ref> benchmark. Our contribution is two fold. (i) We introduce a simple convolutional and recurrent model that outperforms the state-of-the-art model on waveforms, that is, Wave-U-Net [28], by 1.6 points of SDR (signal to distortion ratio). (ii) We propose a new scheme to leverage unlabeled music. We train a first model to extract parts with at least one source silent in unlabeled tracks, for instance without bass. We remix this extract with a bass line taken from the supervised dataset to form a new weakly supervised training example. Combining our architecture and scheme, we show that waveform methods can play in the same ballpark as spectrogram ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cherry first noticed the "cocktail party effect" <ref type="bibr" target="#b4">[5]</ref>: how the human brain is able to separate a single conversation out of a surrounding noise from a room full of people chatting. Bregman later tried to understand how the brain was able to analyse a complex auditory signal and segment it into higher level streams. His framework for auditory scene analysis <ref type="bibr" target="#b3">[4]</ref> spawned its computational counterpart, trying to reproduce or model accomplishment of the brains with algorithmic means <ref type="bibr" target="#b35">[36]</ref>.</p><p>When producing music, recordings of individual instruments called stems are arranged together and mastered into the final song. The goal of source separation is then to recover those individual stems from the mixed signal. Unlike the cocktail problem, there is not a single source of interest to differentiate from an unrelated background noise, but instead a wide variety of tones and timbres playing in a coordinated way. As part of the SiSec Mus evaluation campaign for music separation <ref type="bibr" target="#b28">[29]</ref>, a choice was made to regroup those individual stems into 4 broad categories: (1) drums, (2) bass, (3) other, (4) vocals.</p><p>Each source is represented by a waveform s i ∈ R C,T where C is the number of channels (1 for mono, 2 for stereo) and T the number of samples. We define s := (s i ) i the concatenation of sources in a Preprint version. </p><p>for some dataset D, reconstruction error l, model architecture g with 4 outputs g i , and model weights θ ∈ R d .</p><p>As presented in the next section, most methods to solve (1) learn a mask per source σ i on the mixture spectrogram S := STFT(s) (Short-Time Fourier Transform). The estimated sources are then s i := ISTFT(σ i S) (Inverse Short-Time Fourier Transform). The mask σ i can either be a binary mask valued in {0, 1} or a soft assignment valued in [0, 1]. Those methods are state-of-the-art and perform very well without requiring large models. However, they come with two limitations:</p><p>1. There is no reason for σ i S to be a real spectrogram (i.e., obtained from a real signal). In that case the ISTFT step will perform a projection step that is not accounted for in the training loss and could result in artifacts. 2. Such methods do not try to model the phase but reuse the input mixture. Let us imagine that a guitar plays with a singer at the same pitch but the singer is doing a slight vibrato, i.e., a small modulation of the pitch. This modulation will impact the spectrogram phase, as the derivative of the phase is the instant frequency. Let say both the singer and the guitar have the same intensity, then the ideal mask would be 0.5 for each. However, as we reuse for each source the original phase, the vibrato from the singer would also be applied to the guitar. While this could be consider a corner case, its existence is a motivation for the search of an alternative.</p><p>Learning a model from/to the waveform could allow to lift some of the aforementioned limitations. Because a waveform is directly generated, the training loss is end-to-end, with no extra synthesis step that could add artifacts which solves the first point above. As for the second point, it is unknown whether any model could succeed in separating such a pathological case. In the fields of speech or music generation, direct waveform synthesis has replaced spectrogram based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7]</ref>. When doing generation without an input signal s, the first point is more problematic. Indeed, there is no input phase to reuse and the inversion of a power spectrogram will introduce significant artifacts <ref type="bibr" target="#b7">[8]</ref>. Those successes were also made possible by the development of very large scale datasets (30GB for the NSynth dataset <ref type="bibr" target="#b7">[8]</ref>). In comparison the standard MusDB dataset is only a few GB. This explains, at least partially, the worse performance of waveform methods for source separation <ref type="bibr" target="#b28">[29]</ref>.</p><p>In this paper we aim at taking waveform based methods one step closer to spectrogram methods.</p><p>We contribute a simple model architecture inspired by previous work in source separation from the waveform and audio synthesis. We show that this model outperforms the previous state of the art on the waveform domain. Given the limited data available, we further refine the performance of our model by using a novel semi-supervised data augmentation scheme that allows to leverage 2,000 unlabeled songs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A first category of methods for supervised music source separation work on power spectrograms. They predict a power spectrogram for each source and reuse the phase from the input mixture to synthesise individual waveforms. Traditional methods have mostly focused on blind (unsupervised) source separation. Non-negative matrix factorization techniques <ref type="bibr" target="#b25">[26]</ref> model the power spectrum as a weighted sum of a learnt spectral dictionary, whose elements can then be grouped into individual sources. Independent component analysis <ref type="bibr" target="#b11">[12]</ref> relies on independence assumptions and multiple microphones to separate the sources. Learning a soft/binary mask over power spectrograms has been done using either HMM-based prediction <ref type="bibr" target="#b24">[25]</ref> or segmentation techniques <ref type="bibr" target="#b2">[3]</ref>.</p><p>With the development of deep learning, fully supervised methods have gained momentum. Initial work was performed on speech source separation <ref type="bibr" target="#b8">[9]</ref> then for music using simple fully connected networks over few spectrogram frames <ref type="bibr" target="#b31">[32]</ref>, LSTMs <ref type="bibr" target="#b32">[33]</ref>, or multi scale convolutional / recurrent networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. State-of-the-art performance is obtained with those models when trained with extra labeled data. We show that our model architecture combined with our semi-supervised scheme can provide performance almost on par, while being trained on 5 times less labeled data.  On the other hand, working directly on the waveform only became possible with deep learning models. A Wavenet-like but regression based approach was first used for speech denoising <ref type="bibr" target="#b22">[23]</ref> and then adapted to source separation <ref type="bibr" target="#b18">[19]</ref>. Concurrently, a convolutional network with a U-Net structure called Wave-U-Net was used first on spectrograms <ref type="bibr" target="#b13">[14]</ref> and then adapted to the waveform domain <ref type="bibr" target="#b27">[28]</ref>. Those methods performs significantly worse than the spectrogram ones as shown in the latest SiSec Mus source separation evaluation campaign <ref type="bibr" target="#b28">[29]</ref>. As shown in Section 5, we outperform Wave-U-Net by a large margin with our architecture alone.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, the problem of semi-supervised source separation is tackled for 2 sources separation where a dataset of mixtures and unaligned isolated examples of source 1 but not source 2 is available. Using specifically crafted adversarial losses the authors manage to learn a separation model. In <ref type="bibr" target="#b10">[11]</ref>, the case of blind, i.e., completely unsupervised source separation is covered, combining adversarial losses with a remixing trick similar in spirit to our unlabeled data remixing presented in Section 4. Both papers are different from our own setup, as they assume that they completely lack isolated audio for some or all sources. Finally, when having extra isolated sources, previous work showed that it was possible to use adversarial losses to leverage them without using them to generate new mixtures <ref type="bibr" target="#b26">[27]</ref>. Unfortunately, extra isolated sources are exactly the kind of data that is hard to come by. As far as we know, no previous work tried to leverage unlabeled songs in order to improve supervised source separation performance. Besides, most previous work relied on adversarial losses, which can prove expensive while our remixing trick allows for direct supervision of the training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>Our network architecture is a blend of ideas from the SING architecture <ref type="bibr" target="#b6">[7]</ref> developed for music note synthesis and Wave-U-Net. We reuse the synthesis with large strides and large number of channels as well as the combination of a LSTM and convolutional layers from SING, while retaining the U-Net <ref type="bibr" target="#b23">[24]</ref> structure of Wave-U-Net. The model is composed of a convolutional encoder, an LSTM and a convolutional decoder, with the encoder and decoder linked with skip U-Net connections. The model takes a stereo mixture s = i s i as input and outputs a stereo estimateŝ i for each source. Similarly to other work in generation in both image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> and sound <ref type="bibr" target="#b6">[7]</ref>, we do not use batch normalization <ref type="bibr" target="#b12">[13]</ref> as our early experiments showed that it was detrimental to the model performance.</p><p>The encoder is composed of L := 6 stacked layers numbered from 1 to L. Layer i is composed of a convolution with kernel size K := 8, stride S := 4, C i−1 input channels, C i output channels and ReLU activation followed by a 1x1 convolution with GLU activation <ref type="bibr" target="#b5">[6]</ref>. As the GLU outputs C/2 channels with C channels as input, we double the number of channels in the 1x1 convolution. We define C 0 := 2 the number of channels in the input mixture and C 1 := 48 the initial number of channels for our model. For i ∈ {2, . . . , L} we take C i := 2C i−1 so that the final number of channels is C L = 1536. We then use a bidirectional LSTM with 2 layers and a hidden size C L . The LSTM outputs 2C L channels per time position. We use a 1x1 convolution with ReLU activation to take that number down to C L .</p><p>The decoder is almost the symmetric of the encoder. It is composed of L layers numbered in reverse order from L to 1. The i-th layer starts with a convolution with kernel size 3 and stride 1, input/output channels C i and a ReLU activation. We concatenate its result with the output of the i-th layer of the encoder to form a U-Net and take back the number of channels to C i using a 1x1 convolution with GLU activation. Finally, we use a transposed convolution with kernel size K = 8 and stride S = 4, C i−1 outputs and ReLU activation. For the final layer, we instead output 4C 0 channels and do not use any activation function.</p><p>Weights rescaling The weights of a convolutional layer in a deep learning model are usually initialized in a way that account for the number of input channels and receptive field of the convolutions (i.e., fan in), as introduced by He et al. <ref type="bibr" target="#b9">[10]</ref>. The initial weights of a convolution will roughly scale as 1 √ KCin where K is the kernel size and C in the number of input channels. For instance, the standard deviation after initialization of the weights of the first layer of our encoder is about 0.2, while that of the last layer is 0.01. Modern optimizers such as Adam <ref type="bibr" target="#b16">[17]</ref> normalize the gradient update per coordinate so that, on average, each weight will receive updates of the same magnitude. Thus, if we want to take a learning rate large enough to tune the weights of the first layer, it will most likely be too large for the last layer.</p><p>In order to remedy this problem, we use a trick that is equivalent to using specific learning rates per layer. Let us denote w the weights at initialization used to compute the convolution w * x. We take α := std(w)/a, where a is a reference scale. We replace w by w = w/ √ α and the output of the convolution by √ αw * x, so that the output of the layer is unchanged. This is similar to the equalized learning rate trick used for image generation with GAN <ref type="bibr" target="#b15">[16]</ref>. We observed both faster decay of the training loss and convergence to a better optimum when using the weight rescaling trick, see Section 5.3. Optimal performance was obtained for a reference level a := 0.1. We also tried rescaling the weights by 1/α rather than 1/ √ α however this made the training loss diverge.</p><p>Synthesis vs. filtering Let us denote e i (s) the output of the i-th layer of the encoder and d i (s) the output of the i-th layer of the decoder. Wave-U-Net takes d i (s) and upsamples it using linear interpolation. It then concatenates it with e i−1 (s) (with e 0 (s) := s) and applies a convolution with a stride of 1 to obtain d i−1 (s). Thus, it works by taking a coarse representation, upsampling it, adding back the fine representation from e i−1 (s) and filtering it out to separate channels.</p><p>On the other hand, our model takes d i (s) and concatenates it with e i (s) and uses a transposed convolution to obtain d i−1 (s). A transposed convolution is different from a linear interpolation upsampling. With a sufficient number of input channels, it can generate any signal, while a linear upsampling will generate a signal with higher sampling rate but no high frequencies.</p><p>High frequencies are injected using a U-Net skip connection. Separation is performed by applying various filters to the obtained signal (aka convolution with a stride of 1).</p><p>Thus, Wave-U-Net generates its output by iteratively upsampling, adding back the high frequency part of the signal from the matching encoder output (or from the input for the last decoder layer) and filtering. On the other hand, our approach consist in a direct synthesis. The main benefit of synthesis is that we can use a relatively large stride in the decoder, thus speeding up the computations and allowing for a larger number of channels. We believe this larger number of channels is one of the reasons for the better performance of our model as shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unlabeled Data Remixing</head><p>In order to leverage unlabeled songs, we propose to first train a classifier to detect the absence or presence of each source on small time frames, using a supervised train set for which we know the <ref type="figure">Figure 2</ref>: Overall representation of our unlabeled data remixing pipeline. When we detect an excerpt of at least 5 seconds with one source silent, here the bass, we recombine it with a single bass sample from the training set. We can then provide strong supervision for the silent source, and weak supervision for the other 3 as we only know the ground truth for their sum.</p><p>contribution of each source. When we detect an audio excerpt m i with at least 5 seconds of silence for source i, we add it to a new set D i . We can then mix an example m i ∈ D i with a single source s i taken from the supervised train set in order to form a new mixture s = s i + m i , noting s j the ground truth for this example (potentially unknown to us) for each source j. As the source i is silent in m i we can provide strong supervision for source i as we have s i = s i and weak supervision for the other sources as we have j =i s j = m i . The whole process pipeline is represented in <ref type="figure">Figure 2</ref>.</p><p>Motivation for this approach comes from our early experiments with the available supervised data which showed a clear tendency for overfitting when training our separation models. We first tried using completely unsupervised regularization, for instance given an unlabeled track m, we want iŝ i = m whereŝ i is the estimated source i. This proved too weak to improve performance. We then tried to detect passages with a single source present however this proved too rare of an event in Pop/Rock music: for the standard MusDB dataset presented in Section 5.1, source other is alone 2.6% of the time while the others are so less than 0.5% of the time. Accounting for the fact that our model will never reach a recall of 100%, this represents too few extractable data to be interesting. On the other hand, a source being silent happen quite often, 11% of the time for the drums, 13% for the bass or 32% for the vocals. This time, the other is the least frequent with 2.6% and hardest to extract as noted hereafter.</p><p>We first formalize our classification problem and then describe the extraction procedure. The use of the extracted data for training is detailed in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Silent source detector</head><p>Given a mixture s = i s i , we define for all sources i the relative volume V i (s) := 10 log 10 si 2 s 2 and a source being silent as S i (s) := 1 Vi(s)≥V thres . For instance, having V i = −20 means that source i is 100 times quieter than the mixture. Doing informal testing, we observe that a source with a relative volume between 0 and -10 will be perceived clearly, between -10 and -20 it will feel like a whisper and almost silent between -20 and -30. A source with a relative volume under -30 is perceptually zero.</p><p>We can then train a classifier to estimate P i := P {S i = 1|s}, the probability that source i is silent given the input mixture s. Given the limited amount of supervised data, we use a Wavelet scattering transform <ref type="bibr" target="#b0">[1]</ref> of order two as input features rather than the raw waveform. This transformation is computed using the Kymatio package <ref type="bibr" target="#b1">[2]</ref>. The model is then composed of convolutional layers with max pooling and batch normalization and a final LSTM that produces an estimateP i for every window of 0.64 seconds with a stride of 64 ms. We detail the architecture in the Section 2 of the supplementary material. We have observed that using a downsampled (16kHz) and mono representation of the mixture further helps prevent overfitting.</p><p>The silence threshold is set to -13dB. Although this is far from silent, this allows for a larger number of positive samples and better training. We empirically observe that the true relative volumes decreases as the estimated probabilityP i increases. Thus, in order to select only truly silent samples (V i ≤ −30), one only needs to select a high threshold onP i .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extraction procedure</head><p>We assume we have a few labeled data from the same distribution as the unlabeled data available, in our case we used 100 labeled tracks for 2,000 unlabeled ones, as explained in Section 5.1. If such data is not available, it is still possible to annotate part of the unlabeled data, but only with weak labels (source present or absence) which is easier than obtaining the exact waveform for each source. We perform extraction by first setting thresholds probabilities p i for each source. We define p i as the lowest limit so that for at least 95% of the samples withP i (s) ≥ p i , we have V i (s) ≤ −20 on the stem set. We then only keep audio extracts whereP i ≥ p i for at least 5 seconds, which reduces the amount of data extracted by roughly 50% but also reduces the 95% percentile of the relative volume from -20 to -30. We assemble all the 5 seconds excerpt where source i is silent into a new dataset D i .</p><p>In our case, we did not manage to obtain more than a few minutes of audio with source other silent. Indeed, as noted above, it is the most frequent source, training examples without it are rare leading to unreliable prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We present here the datasets, metrics and baselines used for evaluating our architecture and unlabeled data remixing. We mostly reuse the framework setup for the SiSec Mus evaluation campaign for music source separation <ref type="bibr" target="#b28">[29]</ref> and their MusDB dataset <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation framework</head><p>MusDB and unsupervised datasets We use the MusDB <ref type="bibr" target="#b21">[22]</ref> dataset, which is composed of 150 songs with full supervision in stereo and sampled at 44100Hz. For each song, we have the exact waveform of the drums, bass, other and vocals parts, i.e. each of the sources. The actual song, a.k.a. the mixture, is the sum of those four parts. The first 100 songs form the train set while the remaining 50 are kept for the test set.</p><p>To test out the semi-supervised scheme described in Section 4, we exploit our own set of 2,000 unlabeled tracks, which represents roughly 4.5 days of audio. It is composed of 4% of Heavy Metal, 4% of Jazz, 37% of Pop and 55% of Rock music. Although we do not release this set, we believe that a similarly composed digital music collection will allow to replicate our results. We refer to this data as the unsupervised or unlabled set.</p><p>We also collected raw stems for 100 tracks, i.e., individual instrument recordings used in music production software to make a song. Those tracks come from the same distribution as our unsupervised dataset but do not overlap. We manually assigned each instrument to one of the sources using simple rules on the filenames (for instance "Lead Guitar.wav" is assigned to the other source) or listening to the stems in case of ambiguity. We will call this extra supervised data the stem set. As some of the baslises used additional labeled data (807 songs), we also provide metrics for our own architecture trained using this extra stem set.</p><p>We applied our extraction pipeline to the 2,000 unlabeled songs, and obtained about 1.5 days of audio (with potential overlap due to our extraction procedure) for with the source drums, bass or vocals silent which form respectively the datasets D 0 , D 1 , D 3 . We could not retrieve a significant amount of audio for the other source. Indeed, this last source is the most frequently present (there is almost always a melodic part in a song), and with the amount of data available, we could not train a model that would reliably predict the absence of this source. As a consequence, we do not extract a dataset D 2 for it. We did manage to extract a few hours with only the other source, but we have not tried to inject it into our separation model training. Although we trained our model on mono and 16kHz audio, we perform the extraction on the original 44kHz stereo data. Source separation metrics Measurements of the performance of source separation models was developed by Vincent et al. for blind source separation <ref type="bibr" target="#b34">[35]</ref> and reused for supervised source separation in the SiSec Mus evaluation campaign <ref type="bibr" target="#b28">[29]</ref>. Reusing the notations from <ref type="bibr" target="#b34">[35]</ref>, let us take a source j ∈ 1, 2, 3, 4 and introduce P sj (resp P s ) the orthogonal projection on s j (resp on Span(s 1 , . . . , s 4 )).</p><p>We then take withŝ j the estimate of source s j , s target := P sj (ŝ j ), e interf := P s (ŝ j ) − P sj (ŝ j ) and e artif :=ŝ j − P s (ŝ j ). The signal to distortion ratio is then defined as SDR := 10 log 10 s target 2 e interf + e artif 2 .</p><p>Note that this definition is invariant to the scaling ofŝ j . We used the python package museval 1 which provide a reference implementation for the SiSec Mus 2018 evaluation campaign. It also allows time invariant filters to be applied toŝ j as well as small delays between the estimate and ground truth <ref type="bibr" target="#b34">[35]</ref>. As done in the SiSec Mus competition, we report the median over all tracks of the median of the metric over each track computed using the museval package. Similarly to previous work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, we focus in this section on the SDR, but other metrics can be defined (SIR an SAR) and we present them in the Appendix, Section B. Baselines We selected the best performing models from the last SiSec Mus evaluation campaign <ref type="bibr" target="#b28">[29]</ref> as baselines. MMDenseNet <ref type="bibr" target="#b29">[30]</ref> is a multiscale convolutional network with skip and U-Net connections. This model was submitted as TAK1 when trained without extra labeled data and as TAK3 when trained with 804 extra labeled songs 2 . MMDenseLSTM <ref type="bibr" target="#b30">[31]</ref> is an extension of MMDenseNet that adds LSTMs at different scales of the encoder and decoder. This model was submitted as TAK2 and was trained with the same 804 extra labeled songs. Unlike MMDenseNet, this model was not submitted without supplementary training data. The only Waveform based method submitted to the evaluation campaign is Wave-U-Net <ref type="bibr" target="#b27">[28]</ref> with the identifier STL2. Metrics from all baselines were downloaded from the SiSec submission repository 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training procedure</head><p>We define one epoch over the dataset as a pass over all 5 second extracts with a stride of 0.5 seconds. We train the classifier described in Section 4 on 4 V100 GPUs for 40 epochs with a batch size of 64 using Adam <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 5e-4. We use the sum of the binary cross entropy loss for each source as a training loss. The Demucs separation model described in Section 3 is trained for 400 epochs on 16 V100 GPUs, with a batch size of 128 using Adam with a learning rate of 5e-4 and decaying the learning rate every 160 epochs by a factor of 5. We perform the following data augmentation, partially inspired by <ref type="bibr" target="#b32">[33]</ref>: shuffling sources within one batch, randomly shifting sources in time (same shift for both channels), randomly swapping channels, random multiplication by ±1 per channel. Given the cost of fitting those models, we perform a single run for each configuration.</p><p>We use the L1 distance between the estimated sourcesŝ i and the ground truth s i as we observed it improved the performance quite a bit, as shown on <ref type="table" target="#tab_1">Table 2</ref>. We have tried replacing or adding to this loss the L1 distance between the power spectrogram ofŝ i and that of s i , as done in <ref type="bibr" target="#b6">[7]</ref>, however it only degraded the final SDR of the model. When using the unlabeled data remixing trick describe in Section 4, we perform an extra step with probability 0.25 after each training batch from the main training step. We sample one source i at random out of (0) drums, (1) bass or (3) vocals (remember that we could not extract excerpt for source other) and obtain m i ∈ D where source i is silent and s i from the training set where only i is present. We take s := m i + s i and perform a gradient step on the following loss</p><formula xml:id="formula_2">ŝ i − s i 1 + λ j =iŝ j − m i 1 .<label>(3)</label></formula><p>Given that the extracted examples m i are noisier than those coming from the train set, we use a separate instance of Adam for this step with a learning rate 10 times smaller than the main one. Furthermore, as we only have weak supervision over sources j = i, the second term is too be understood as a regularizer rather than a leading term, thus we take λ := 10 −6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation results</head><p>We compare the performance of our approach with the state of the art in <ref type="table" target="#tab_0">Table 1</ref>. On the top half, we show all methods trained without supplementary data. We can see a clear improvement coming from our new architecture alone compared to Wave-U-Net while MMDenseNet keeps a clear advantage. We then look at the impact of adding unlabeled remixed data. We obtain a gain of nearly 0.3 of the median SDR. As a reference, adding 100 labeled tracks to the train set gives a gain of 0.6. Interestingly, even when training with the extra tracks, our model still benefits from the unlabeled data, gaining an extra 0.2 points of SDR. MMDenseLSTM and MMDenseNet still obtain the best performance overall but we can notice that Demucs trained with unlabeled data achieved state of the art performance for the separation of the bass source. It only had access to 100 extra labeled songs, which is far from the 804 extra labeled songs used for MMDenseNet/LSTM and it would be interesting to see how waveform based models perform with a dataset that large. Box plots with quantiles can be found in the Appendix, Section B. Audio samples from different Demucs variant and the baselines are provided online <ref type="bibr" target="#b3">4</ref> . We provide an ablation study of the main novelties of this paper on <ref type="table" target="#tab_1">Table 2</ref>. on the train set of MusDB plus our remixed unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented Demucs, a simple architecture inspired by previous work in source separation from the waveform and audio synthesis that reduces the gap between spectrogram and waveform based methods from 2.2 points of median SDR to 0.5 points when trained only on the standard MusDB dataset. We have also demonstrated how to leverage 2,000 unlabeled mp3s by first training a classifier to detect excerpt with at least one source silent and then remixing it with an isolated source from the training set. To our knowledge, this is the first semi-supervised approach to source separation that does not rely on adversarial losses. Finally, training our model with remixed unlabeled data as well as 100 extra training examples, we obtain performance almost on par with that of state of the art spectrogram based methods, even better for the bass source, making waveform based method a legitimate contender for supervised source separation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>s∈D 4 i=1l</head><label>4</label><figDesc>arXiv:1909.01174v1 [cs.SD] 3 Sep 2019 tensor of size (4, C, T ) and the mixture s := 4 i=1 s i . We aim at training a model that minimises min θ (g i θ (s), s i )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Encoder1(Cin = 2 , 2 *</head><label>22</label><figDesc>Cout = 64) Encoder2(Cin = 64, Cout = 128) 128, Cout = 64) Decoder1(Cin = 2 * 64, Cout = 4 * 2) (a) Demucs architecture with the mixture waveform as input and the four sources estimates as output. Arrows represents U-Net connections. Relu(Conv1d(Cin, Cin, K = 3, S = 1)) GLU(Conv1d(2Cin, 2Cin, K = 1, S = 1)) Relu(ConvTr1d(Cin, Cout, K = 8, S = 4)) Encoderi Decoderi+1 or LSTM Decoderi−1 or output Relu(Conv1d(Cin, Cout, K = 8, S = 4)) GLU(Conv1d(2Cout, 2Cout, K = 1, S = 1)) Decoderi Encoderi−1 or input Encoderi+1 or LSTM (b) Detailed view of the layers Decoderi on the top and Encoderi on the bottom. Arrows represent connections to other parts of the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Demucs complete architecture on the left, with detailed representation of the encoder and decoder layers on the right. Key novelties compared to the previous Wave-U-Net are the GLU activation in the encoder and decoder, the bidirectional LSTM in-between and exponentially growing number of channels, allowed by the stride of 4 in all convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Demucs with the state of the art in the waveform domain (Wave-U-Net) and in the spectrogram domain (MMDenseNet, MMDenseNetLSTM) on the MusDB test set. Extra data is the number of extra songs used, either labeled with the waveform for each source or unlabeled. We report the median over all tracks of the median SDR over each track, as done in the SiSec Mus evaluation campaign<ref type="bibr" target="#b28">[29]</ref>. For easier comparison, the All column is obtained by concatenating the metrics from all sources and then taking the median.</figDesc><table><row><cell></cell><cell cols="2">Extra data</cell><cell></cell><cell cols="3">Test SDR in dB</cell><cell></cell></row><row><cell>Architecture</cell><cell cols="7">Wav? labeled unlabed All Drums Bass Other Vocals</cell></row><row><cell>MMDenseNet</cell><cell></cell><cell></cell><cell>5.34</cell><cell>6.40</cell><cell>5.14</cell><cell>4.13</cell><cell>6.57</cell></row><row><cell>Wave-U-Net</cell><cell></cell><cell></cell><cell>3.17</cell><cell>4.16</cell><cell>3.17</cell><cell>2.24</cell><cell>3.05</cell></row><row><cell>Demucs</cell><cell></cell><cell></cell><cell>4.81</cell><cell>5.38</cell><cell>5.07</cell><cell>3.01</cell><cell>5.44</cell></row><row><cell>Demucs</cell><cell></cell><cell>2,000</cell><cell>5.09</cell><cell>5.79</cell><cell>6.23</cell><cell>3.45</cell><cell>5.51</cell></row><row><cell>Demucs</cell><cell>100</cell><cell></cell><cell>5.41</cell><cell>5.99</cell><cell>5.72</cell><cell>3.65</cell><cell>6.17</cell></row><row><cell>Demucs</cell><cell>100</cell><cell>2,000</cell><cell>5.67</cell><cell>6.50</cell><cell>6.21</cell><cell>3.80</cell><cell>6.21</cell></row><row><cell>MMDenseLSTM</cell><cell>804</cell><cell></cell><cell>5.97</cell><cell>6.75</cell><cell>5.28</cell><cell>4.72</cell><cell>7.15</cell></row><row><cell>MMDenseNet</cell><cell>804</cell><cell></cell><cell>5.85</cell><cell>6.81</cell><cell>5.21</cell><cell>4.37</cell><cell>6.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for the novel elements in our architecture or training procedure. Unlike onTable 1, we report the simple SDR defined in Section 5.1 rather than the extended version SDR ext . We also report average values rather than medians as this make small changes more visible. This explains the SDR reported here being much smaller than onTable 1. Both metrics are averaged over the last 3 epochs and computed on the MusDB test set. Reference is trained with remixed unlabeled data, with stereo channels input resampled at 22kHz, on the train set of MusDB.</figDesc><table><row><cell></cell><cell cols="2">Train set</cell><cell cols="2">Test set</cell></row><row><cell>Difference</cell><cell cols="4">L1 loss SDR L1 loss SDR</cell></row><row><cell>Reference</cell><cell>0.090</cell><cell>8.82</cell><cell>0.169</cell><cell>5.09</cell></row><row><cell>no remixed</cell><cell>0.089</cell><cell>8.87</cell><cell>0.175</cell><cell>4.81</cell></row><row><cell>no GLU</cell><cell>0.099</cell><cell>8.00</cell><cell>0.174</cell><cell>4.68</cell></row><row><cell>no BiLSTM</cell><cell>0.156</cell><cell>8.42</cell><cell>0.182</cell><cell>4.83</cell></row><row><cell>MSE loss</cell><cell>N/A</cell><cell>8.84</cell><cell>N/A</cell><cell>5.04</cell></row><row><cell>no weight rescaling</cell><cell>0.094</cell><cell>8.39</cell><cell>0.171</cell><cell>4.68</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/sigsep/sigsep-mus-eval 2 Source: https://sisec18.unmix.app/#/methods/TAK2 3 https://github.com/sigsep/sigsep-mus-2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://ai.honu.io/papers/demucs/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/sigsep/sigsep-mus-2018-analysis</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Architecture of the silent sources detector</head><p>We use as input a scattering transform of order 2, computed using the Kymatio package <ref type="bibr" target="#b1">[2]</ref> with J := 8 wavelets per octave. Coefficients of order 1 are indexed by one frequency f 1 and of order two by f 1 and f 2 with f 2 the frequency of the second order filter. We organize the coefficient in a tensor of dimension (C, F, T ) where T is the number of time windows, F is the number of order 1 frequencies. The first channel is composed of order 1 coefficients, while the next ones contains the order two coefficient ordered by f 2 . Thanks to this reorganization, we can now use 2D convolutions over the output of the scattering transform. The model is then composed of</p><p>• frequency dimension is eliminated with a final convolution of kernel size 14 in the frequency axis and 1 in the time axis with 512 input channels and 1024 channels as output, • BiLSTM with hidden size of 1024, 2 layers, dropout at 0.18, • Conv1d(C in = 2048, C out = 1024, K = 1, S = 1),batch normalization, then Relu, • Conv1d(1024, 4, K = 1, S = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results for all metrics with boxplots</head><p>Reusing the notations from <ref type="bibr" target="#b34">[35]</ref>, let us take a source j ∈ 1, 2, 3, 4 and introduce P sj (resp P s ) the orthogonal projection on s j (resp on Span(s 1 , . . . , s 4 )). We then take withŝ j the estimate of source s j s target := P sj (ŝ j ) e interf := P s (ŝ j ) − P sj (ŝ j ) e artif :=ŝ j − P s (ŝ j )</p><p>We can now define various signal to noise ratio, expressed in decibels (dB): the source to distortion ratio SDR := 10 log 10 s target As explained in the main paper, extra invariants are added when using the museval package. We refer the reader to <ref type="bibr" target="#b34">[35]</ref> for more details. We provide hereafter box plots for each metric and each target, generated using the notebook provided specifically by the organizers of the SiSec Mus evaluation <ref type="bibr" target="#b4">5</ref> . An "Extra" suffix means that extra training data has been used and the "Remixed" suffix means that our unlabeled data remixing scheme has been used.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep scattering spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Andén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Kymatio: Scattering transforms in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Mathieu Andreux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Angles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Exarchakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaspar</forename><surname>Leonarduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thiry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruna</surname></persName>
		</author>
		<idno>1812.11214</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind one-microphone speech separation: A spectral learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustic Society of America</title>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sing: Symbol-to-instrument neural generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Défossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usunier</forename><surname>Nicolas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cinjon</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno>1704.01279</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for single channel source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Umut Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustic, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards unsupervised single-channel blind source separation using adversarial pair unmix-and-remix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno>1812.07504</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tillman</forename><surname>Weyde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno>1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising auto-encoder with recurrent skip connections and residual regression for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end music source separation: is it possible in the waveform domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
		<idno>1810.12187</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Samplernn: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>1612.07837</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised monaural singing voice separationwith a masking network trained on synthetic mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Michelashvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno>1812.06087</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The musdb18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafar</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stylianos Ioannis Mimilakis, and Rachel Bittner</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Static and dynamic source separation using nonnegative factorizations: A unified view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial semi-supervised audio source separation applied to singing voice extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2391" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
		<idno>1806.03185</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The 2018 signal separation evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobutaka</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Latent Variable Analysis and Signal Separation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band densenets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabarun</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno>1805.02410</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep neural network based instrument extraction from music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Févotte</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/inria-00544230" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">Computational Auditory Scene Analysis</title>
		<editor>DeLiang Wang and Guy J. Brown</editor>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
