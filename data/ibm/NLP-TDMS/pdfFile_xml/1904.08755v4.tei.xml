<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
							<email>jgwak@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
						</author>
						<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many robotics and VR/AR applications, 3D-videos are readily-available sources of input (a continuous sequence of depth images, or LIDAR scans). However, these 3D-videos are processed frame-by-frame either through 2D convnets or 3D perception algorithms in many cases. In this work, we propose 4-dimensional convolutional neural networks for spatio-temporal perception that can directly process such 3D-videos using high-dimensional convolutions. For this, we adopt sparse tensors [8, 9]  and propose the generalized sparse convolution which encompasses all discrete convolutions. To implement the generalized sparse convolution, we create an open-source auto-differentiation library for sparse tensors that provides extensive functions for highdimensional convolutional neural networks. 1  We create 4D spatio-temporal convolutional neural networks using the library and validate them on various 3D semantic segmentation benchmarks and proposed 4D datasets for 3D-video perception. To overcome challenges in the high-dimensional 4D space, we propose the hybrid kernel, a special case of the generalized sparse convolution, and the trilateral-stationary conditional random field that enforces spatio-temporal consistency in the 7D space-time-chroma space. Experimentally, we show that convolutional neural networks with only generalized sparse convolutions can outperform 2D or 2D-3D hybrid methods by a large margin. 2 Also, we show that on 3D-videos, 4D spatio-temporal convolutional neural networks are robust to noise, outperform 3D convolutional neural networks and are faster than the 3D counterpart in some cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work, we are interested in 3D-video perception. A 3D-video is a temporal sequence of 3D scans such as a video from a depth camera, a sequence of LIDAR scans, or a multiple MRI scans of the same object or a body part <ref type="figure" target="#fig_6">(Fig. 1</ref>). As LIDAR scanners and depth cameras become more affordable and widely used for robotics applications, 3D-videos became readily-available sources of input for robotics systems or AR/VR applications. However, there are many technical challenges in using 3Dvideos for high-level perception tasks. First, 3D data requires heterogeneous representations and processing those either alienates users or makes it difficult to integrate into larger systems. Second, the performance of the 3D convolutional neural networks is worse or on-par with 2D convolutional neural networks. Third, there are limited number of opensource libraries for fast large-scale 3D data.</p><p>To resolve most, if not all, of the challenges in the highdimensional perception, we adopt a sparse tensor <ref type="bibr">[8,</ref><ref type="bibr">9]</ref> for our problem and propose the generalized sparse convolutions. The generalized sparse convolution encompasses all discrete convolutions as its subclasses and is crucial for high-dimensional perception. We implement the generalized sparse convolution and all standard neural network functions in Sec. 4 and open-source the library.</p><p>We adopt the sparse representation for several reasons. Currently, there are various concurrent works for 3D perception: a dense 3D convolution <ref type="bibr">[5]</ref>, pointnet-variants <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, continuous convolutions <ref type="bibr">[11,</ref><ref type="bibr">15]</ref>, surface convolutions <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b29">29]</ref>, and an octree convolution <ref type="bibr" target="#b24">[24]</ref>. Out of these representations, we chose a sparse tensor due to its expressiveness and generalizability for high-dimensional spaces. Also, it allows homogeneous data representation within traditional neural network libraries since most of them support sparse tensors.</p><p>Second, the sparse convolution closely resembles the standard convolution (Sec. 3) which is proven to be successful in 2D perception as well as 3D reconstruction <ref type="bibr">[4]</ref>, feature learning <ref type="bibr" target="#b33">[33]</ref>, and semantic segmentation <ref type="bibr" target="#b30">[30]</ref>.</p><p>Third, the sparse convolution is efficient and fast. It only computes outputs for predefined coordinates and saves them into a compact sparse tensor (Sec. 3). It saves both memory and computation especially for 3D scans or high-dimensional data where most of the space is empty.</p><p>Thus, we adopt the sparse representation for the our problem and create the first large-scale 3D/4D networks or Minkowski networks. <ref type="bibr">3</ref> We named them after the space-time continuum, Minkowski space, in Physics.</p><p>However, even with the efficient representation, merely scaling the 3D convolution to high-dimensional spaces results in significant computational overhead and memory consumption due to the curse of dimensionality. A 2D convolution with kernel size 5 requires 5 2 = 25 weights which increases exponentially to 5 3 = 125 in a 3D cube, and 625 in a 4D tesseract <ref type="figure">(Fig. 2</ref>). This exponential increase, however, does not necessarily lead to better performance and slows down the network significantly. To overcome this challenge, we propose custom kernels with non-(hyper)-cubic shapes using the generalized sparse convolution.</p><p>Finally, the predictions from the 4D spatio-temporal generalized sparse convnets are not necessarily consistent throughout the space and time. To enforce consistency, we propose high-dimensional conditional random fields defined in a 7D trilateral space (space-time-color) with a stationary pairwise consistency function. We use variational inference to convert the conditional random field to differentiable recurrent layers which can be implemented in as a 7D generalized sparse convnet and train both the 4D and 7D networks end-to-end.</p><p>Experimentally, we use various 3D benchmarks that cover both indoor <ref type="bibr">[5,</ref><ref type="bibr">2]</ref> and outdoor spaces <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b25">25]</ref>. First, we show that networks with only generalized sparse 3D conv nets can outperform 2D or hybrid deep-learning algorithms by a large margin. <ref type="bibr">4</ref> Also, we create 4D datasets from Synthia <ref type="bibr" target="#b27">[27]</ref> and Varcity <ref type="bibr" target="#b25">[25]</ref> and report ablation studies of temporal components. Experimentally, we show that the generalized sparse conv nets with the hybrid kernel outperform sparse convnets with tesseract kernels. Also, the 4D generalized sparse convnets are more robust to noise and sometimes more efficient in some cases than the 3D counterpart. <ref type="bibr">3</ref> At the time of submission, our proposed method was the first very deep 3D convolutional neural networks with more than 20 layers. <ref type="bibr">4</ref> We achieved 67.9% mIoU on the ScanNet benchmark outperforming all algorithms including the best peer-reviewed work <ref type="bibr">[6]</ref> by 19% mIoU at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The 4D spatio-temporal perception fundamentally requires 3D perception as a slice of 4D along the temporal dimension is a 3D scan. However, as there are no previous works on 4D perception using neural networks, we will primarily cover 3D perception, specifically 3D segmentation using neural networks. We categorized all previous works in 3D as either (a) 3D-convolutional neural networks or (b) neural networks without 3D convolutions. Finally, we cover early 4D perception methods. Although 2D videos are spatio-temporal data, we will not cover them in this paper as 3D perception requires radically different data processing, implementation, and architectures.</p><p>3D-convolutional neural networks. The first branch of 3D-convolutional neural networks uses a rectangular grid and a dense representation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr">5]</ref> where the empty space is represented either as 0 or the signed distance function. This straightforward representation is intuitive and is supported by all major public neural network libraries. However, as the most space in 3D scans is empty, it suffers from high memory consumption and slow computation. To resolve this, OctNet <ref type="bibr" target="#b24">[24]</ref> proposed to use the Octree structure to represent 3D space and convolution on it.</p><p>The second branch is sparse 3D-convolutional neural networks <ref type="bibr" target="#b28">[28,</ref><ref type="bibr">9]</ref>. There are two quantization methods used for high dimensions: a rectangular grid and a permutohedral lattice <ref type="bibr">[1]</ref>. <ref type="bibr" target="#b28">[28]</ref> used a permutohedral lattice whereas <ref type="bibr">[9]</ref> used a rectangular grid for 3D classification and semantic segmentation.</p><p>The last branch is 3D pseudo-continuous convolutional neural networks <ref type="bibr">[11,</ref><ref type="bibr">15]</ref>. Unlike the previous works, they define convolutions using continuous kernels in a continuous space. However, finding neighbors in a continuous space is expensive, as it requires KD-tree search rather than a hash table, and are susceptible to uneven distribution of point clouds.</p><p>Neural networks without 3D convolutions. Recently, we saw a tremendous increase in neural networks without 3D convolutions for 3D perception. Since 3D scans consist of thin observable surfaces, <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b29">29]</ref> proposed to use 2D convolutions on the surface for semantic segmentation.</p><p>Another direction is PointNet-based methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>. PointNets use a set of input coordinates as features for a multi-layer perceptron. However, this approach processes a limited number of points and thus a sliding window for cropping out a section from an input was used for large spaces making the receptive field size rather limited. <ref type="bibr" target="#b52">[14]</ref> tried to resolve such shortcomings with a recurrent network on top of multiple pointnets, and <ref type="bibr">[15]</ref> proposed a variant of 3D continuous convolution for lower layers of a PointNet and got a significant performance boost.</p><p>4D perception. The first 4D perception algorithm <ref type="bibr" target="#b18">[18]</ref> proposed a dynamic deformable balloon model for 4D car-diac image analysis. Later, <ref type="bibr" target="#b16">[16]</ref> used a 4D Markov Random Fields for cardiac segmentation. Recently, a "Spatio-Temporal CNN" <ref type="bibr" target="#b34">[34]</ref> combined a 3D-UNet with a 1D-AutoEncoder for temporal data and applied the model for auto-encoding brain fMRI images, but it is not a 4Dconvolutional neural network.</p><p>In this paper, we propose the first high-dimensional convolutional neural networks for 4D spatio-temporal data, or 3D videos and the 7D space-time-chroma space. Compared with other approaches that combine temporal data with a recurrent neural network or a shallow model (CRF), our networks use a homogeneous representation and convolutions consistently throughout the networks. Instead of using an RNN, we use convolution for the temporal axis since it is proven to be more effective in sequence modeling <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Tensor and Convolution</head><p>In traditional speech, text, or image data, features are extracted densely. Thus, the most common representations of these data are vectors, matrices, and tensors. However, for 3-dimensional scans or even higher-dimensional spaces, such dense representations are inefficient due to the sparsity. Instead, we can only save the non-empty part of the space as its coordinates and the associated features. This representation is an N-dimensional extension of a sparse matrix; thus it is known as a sparse tensor. There are many ways to save such sparse tensors compactly <ref type="bibr" target="#b31">[31]</ref>, but we follow the COO format as it is efficient for neighborhood queries (Sec. 3.1). Unlike the traditional sparse tensors, we augment the sparse tensor coordinates with the batch indices to distinguish points that occupy the same coordinate in different batches <ref type="bibr">[9]</ref>. Concisely, we can represent a set of 4D coordinates as C = {(x i , y i , z i , t i )} i or as a matrix C and a set of associated features F = {f i } i or as a matrix F . Then, a sparse tensor can be written as</p><formula xml:id="formula_0">C =    x 1 y 1 z 1 t 1 b 1 . . . x N y N z N t N b N    , F =    f T 1 . . . f T N    (1)</formula><p>where b i and f i are the batch index and the feature associated to the i-th coordinate. In Sec. 6, we augment the 4D space with the 3D chromatic space and create a 7D sparse tensor for trilateral filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generalized Sparse Convolution</head><p>In this section, we generalize the sparse convolution <ref type="bibr">[8,</ref><ref type="bibr">9]</ref> for generic input and output coordinates and for arbitrary kernel shapes. The generalized sparse convolution encompasses not only all sparse convolutions but also the conventional dense convolutions. Let x in u ∈ R N in be an N in -dimensional input feature vector in a D-dimensional space at u ∈ R D (a D-dimensional coordinate), and convolution kernel weights be W ∈ R K D ×N out ×N in . We break down the weights into spatial weights with K D matrices of size N out × N in as W i for |{i} i | = K D . Then, the conventional dense convolution in D-dimension is</p><formula xml:id="formula_1">x out u = i∈V D (K) W i x in u+i for u ∈ Z D ,<label>(2)</label></formula><p>where V D (K) is the list of offsets in D-dimensional hypercube centered at the origin. e.g. V 1 (3) = {−1, 0, 1}. The generalized sparse convolution in Eq. 3 relaxes Eq. 2.</p><formula xml:id="formula_2">x out u = i∈N D (u,C in ) W i x in u+i for u ∈ C out<label>(3)</label></formula><p>where N D is a set of offsets that define the shape of a kernel and N D (u, C in ) = {i|u + i ∈ C in , i ∈ N D } as the set of offsets from the current center, u, that exist in C in . C in and C out are predefined input and output coordinates of sparse tensors. First, note that the input coordinates and output coordinates are not necessarily the same. Second, we define the shape of the convolution kernel arbitrarily with N D . This generalization encompasses many special cases such as the dilated convolution and typical hypercubic kernels. Another interesting special case is the "sparse submanifold convolution" <ref type="bibr">[9]</ref> when we set</p><formula xml:id="formula_3">C out = C in and N D = V D (K). If we set C in = C out = Z D and N D = V D (K)</formula><p>, the generalized sparse convolution becomes the conventional dense convolution (Eq. 2). If we define the C in and C out as multiples of a natural number and N D = V D (K), we have a strided dense convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Minkowski Engine</head><p>In this section, we propose an open-source autodifferentiation library for sparse tensors and the generalized sparse convolution (Sec. 3). As it is an extensive library with many functions, we will only cover essential forward-pass functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sparse Tensor Quantization</head><p>The first step in the sparse convolutional neural network is the data processing to generate a sparse tensor, which converts an input into unique coordinates, associated features, and optionally labels when training for semantic segmentation. In Alg. 1, we list the GPU function for this process. When a dense label is given, it is important that we ignore voxels with more than one unique labels. This can be done by marking these voxels with IGNORE_LABEL. First, we convert all coordinates into hash keys and find all unique hashkey-label pairs to remove collisions. Note that SortByKey, UniqueByKey, and ReduceByKey are all standard Thrust library functions <ref type="bibr" target="#b19">[19]</ref>. The reduction function f ((l x , i x ), (l y , i y )) =&gt; (IGNORE_LABEL, i x ) takes Algorithm 1 GPU Sparse Tensor Quantization</p><formula xml:id="formula_4">Inputs: coordinates C p ∈ R N ×D , features F p ∈ R N ×N f , target labels l ∈ Z N + , quantization step size v l C p ← floor(C p / v l ) k ← hash(C p ), i ← sequence(N) ((i , l ), k ) ← SortByKey((i, l), key=k) (i , (k , l )) ← UniqueByKey(i , key=(k , l )) (l , i ) ← ReduceByKey((l , i ), key=k , fn=f ) return C p [i , :], F p [i , :], l</formula><p>label-key pairs and returns the IGNORE_LABEL since at least two label-key pairs in the same key means there is a label collision. A CPU-version works similarly except that all reduction and sorting are processed serially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generalized Sparse Convolution</head><p>The next step in the pipeline is generating the output coordinates C out given the input coordinates C in (Eq. 3). When used in conventional neural networks, this process requires only a convolution stride size, input coordinates, and the stride size of the input sparse tensor (the minimum distance between coordinates). The algorithm is presented in the supplementary material. We create this output coordinates dynamically allowing an arbitrary output coordinates C out for the generalized sparse convolution.</p><p>Next, to convolve an input with a kernel, we need a mapping to identify which inputs affect which outputs. This mapping is not required in conventional dense convolutions as it can be inferred easily. However, for sparse convolution where coordinates are scattered arbitrarily, we need to specify the mapping. We call this mapping the kernel maps and define them as pairs of lists of input indices and output indices,</p><formula xml:id="formula_5">M = {(I i , O i )} i for i ∈ N D .</formula><p>Finally, given the input and output coordinates, the kernel map, and the kernel weights W i , we can compute the generalized sparse convolution by iterating through each of the offset i ∈ N D (Alg. 2) where I[n] and O[n] indicate the n-th element of the list of</p><formula xml:id="formula_6">Algorithm 2 Generalized Sparse Convolution Require: Kernel weights W, input features F i , output fea- ture placeholder F o , convolution mapping M, 1: F o ← 0 // set to 0 2: for all W i , (I i , O i ) ∈ (W, M) do 3: F tmp ← W i [F i I i [1] , F i I i [2] , ..., F i I i [n] ] // (cu)BLAS 4: F tmp ← F tmp + [F o O i [1] , F o O i [2] , ..., F o O i [n] ] 5: [F o O i [1] , F o O i [2] , ..., F o O i [n]</formula><p>] ← F tmp 6: end for indices I and O respectively and F i n and F o n are also n-th input and output feature vectors respectively. The transposed generalized sparse convolution (deconvolution) works simi-larly except that the role of input and output coordinates is reversed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Max Pooling</head><p>Unlike dense tensors, on sparse tensors, the number of input features varies per output. Thus, this creates nontrivial implementation for a max/average pooling. Let I and O be the vector that concatenated all {I i } i and {O i } i for i ∈ N D respectively. We first find the number of inputs per each output coordinate and indices of the those inputs. Alg. 3 reduces the input features that map to the same output coordinate. Sequence(n) generates a sequence of integers from 0 to n -1 and the reduction function</p><formula xml:id="formula_7">f ((k 1 , v 1 ), (k 2 , v 2 )) = min(v 1 , v 2 )</formula><p>which returns the minimum value given two key-value pairs. MaxPoolKernel is a custom CUDA kernel that reduces all features at a specified channel using S , which contains the first index of I that maps to the same output, and the corresponding output indices O".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 GPU Sparse Tensor MaxPooling</head><formula xml:id="formula_8">Input: input feature F , output mapping O (I , O ) ← SortByKey(I, key=O) S ← Sequence(length(O )) S , O" ← ReduceByKey(S, key=O , fn=f ) return MaxPoolKernel(S , I , O", F )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Global / Average Pooling, Sum Pooling</head><p>An average pooling and a global pooling layer compute the average of input features for each output coordinate for average pooling or one output coordinate for global pooling. This can be implemented in multiple ways. We use a sparse matrix multiplication since it can be optimized on hardware or using a faster sparse BLAS library. In particular, we use the cuSparse library for sparse matrixmatrix (cusparse_csrmm) and matrix-vector multiplication (cusparse_csrmv) to implement these layers. Similar to the max pooling algorithm, M is the (I, O) input-tooutput kernel map. For the global pooling, we create the kernel map that maps all inputs to the origin and use the same Alg. 4. The transposed pooling (unpooling) works similarly.</p><p>On the last line of the Alg. 4, we divide the pooled features by the number of inputs mapped to each output. However, this process could remove density information. Thus, we propose a variation that does not divide the number of inputs and named it the sum pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Non-spatial Functions</head><p>For functions that do not require spatial information (coordinates) such as ReLU, we can apply the functions directly Algorithm 4 GPU Sparse Tensor AvgPooling</p><formula xml:id="formula_9">Input: mapping M = (I, O), features F , one vector 1 S M = coo2csr(row=O, col=I, val=1) F = cusparse_csrmm(S M , F ) N = cusparse_csrmv(S M , 1) return F /N</formula><p>to the features F . Also, for batch normalization, as each row of F represents a feature, we could use the 1D batch normalization function directly on F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Minkowski Convolutional Neural Networks</head><p>In this section, we introduce 4-dimensional spatiotemporal convolutional neural networks for spatio-temporal perception. We treat the time dimension as an extra spatial dimension and create networks with 4-dimensional convolutions. However, there are unique problems arising from high-dimensional convolutions. First, the computational cost and the number of parameters in the networks increase exponentially as we increase the dimension. However, we experimentally show that these increases do not necessarily lead to better performance. Second, the networks do not have an incentive to make the prediction consistent throughout the space and time with conventional cross-entropy loss alone.</p><p>To resolve the first problem, we make use of a special property of the generalized sparse convolution and propose non-conventional kernel shapes that not only save memory and computation, but also perform better. Second, to enforce spatio-temporal consistency, we propose a high-dimensional conditional random field (7D space-time-color space) that filters network predictions. We use variational inference to train both the base network and the conditional random field end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tesseract Kernel and Hybrid Kernel</head><p>The surface area of 3D data increases linearly to time and quadratically to the spatial resolution. However, when we use a conventional 4D hypercube, or a tesseract <ref type="figure">(Fig. 2)</ref>, for a convolution kernel, the exponential increase in the number of parameters leads to over-parametrization, overfitting, as well as high computational-cost and memory consumption. Instead, we propose a hybrid kernel (non-hypercubic, nonpermutohedral) to save computation. We use the arbitrary kernel offsets N D of the generalized sparse convolution to implement the hybrid kernel.</p><p>The hybrid kernel is a combination of a cross-shaped kernel a conventional cubic kernel <ref type="figure">(Fig. 3)</ref>. For spatial dimensions, we use a cubic kernel to capture the spatial geometry accurately. For the temporal dimension, we use the crossshaped kernel to connect the same point in space across time.</p><p>We experimentally show that the hybrid kernel outperforms the tesseract kernel both in speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross</head><p>Hypercross Cube Hypercube Hybrid <ref type="figure">Figure 3</ref>: Various kernels in space-time. The red arrow indicates the temporal dimension and the other two axes are for spatial dimensions. The third spatial dimension is hidden for better visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Residual Minkowski Networks</head><p>The generalized sparse convolution allows us to define strides and kernel shapes arbitrarily. Thus, we can create a high-dimensional network only with generalized sparse convolutions, making the implementation easier and generic. In addition, it allows us to adopt recent architectural innovations in 2D directly to high-dimensional networks. To demonstrate, we create a high-dimensional version of a residual network on <ref type="figure">Fig. 4</ref>. For the first layer, instead of a 7 × 7 2D convolution, we use a 5×5×5×1 generalized sparse convolution. However, for the rest of the networks, we follow the original network architecture.</p><p>For the U-shaped variants, we add multiple strided sparse convolutions and strided sparse transpose convolutions with skip connections connecting the layers with the same stride size <ref type="figure" target="#fig_1">(Fig. 5)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Trilateral Stationary-CRF</head><p>For semantic segmentation, the cross-entropy loss is applied for each pixel or voxel. However, the loss does not enforce consistency as it does not have pair-wise terms. To make such consistency more explicit, we propose a highdimensional conditional random field (CRF) similar to the one used in image semantic segmentation <ref type="bibr" target="#b35">[35]</ref>. In image segmentation, the bilateral space that consists of 2D space and 3D color is used for the CRF. For 3D-videos, we use the trilateral space that consists of 3D space, 1D time, and 3D chromatic space. The color space creates a "spatial" gap between points with different colors that are spatially adjacent (e.g., on a boundary). Thus, it prevents information from "leaking out" to different regions. Unlike conventional CRFs with Gaussian edge potentials and dense connections <ref type="bibr">[13,</ref><ref type="bibr" target="#b35">35]</ref>, we do not restrict the compatibility function to be a Gaussian. Instead, we relax the constraint and only apply the stationarity condition.</p><p>To find the global optima of the distribution, we use the variational inference and convert a series of fixed point update equations to a recurrent neural network similar to <ref type="bibr" target="#b35">[35]</ref>. We use the generalized sparse convolution in 7D space to implement the recurrence and jointly train both the base network that generates unary potentials and the CRF end-toend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Definition</head><p>Let a CRF node in the 7D (space-time-chroma) space be x i and the unary potential be φ u (x i ) and the pairwise potential as φ p (x i , x j ) where x j is a neighbor of x i , N 7 (x i ). The conditional random field is defined as</p><formula xml:id="formula_10">P (X) = 1 Z exp i   φ u (x i ) + j∈N 7 (xi) φ p (x i , x j )  </formula><p>where Z is the partition function; X is the set of all nodes; and φ p must satisfy the stationarity condition φ p (u, v) = φ p (u + τ u , v + τ v ) for τ u , τ v ∈ R D . Note that we use the camera extrinsics to define the spatial coordinates of a node x i in the world coordinate system. This allows stationary points to have the same coordinates throughout the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Variational Inference</head><p>The optimization arg max X P (X) is intractable. Instead, we use the variational inference to minimize divergence between the optimal P (X) and an approximated distribution Q(X). Specifically, we use the mean-field approximation, Q = i Q i (x i ) as the closed form solution exists. From the Theorem 11.9 in <ref type="bibr">[12]</ref>, Q is a local maximum if and only if</p><formula xml:id="formula_11">Q i (x i ) = 1 Z i exp E X−i∼Q−i   φ u (x i ) + j∈N 7 (xi) φ p (x i , x j )   .</formula><p>X −i and Q −i indicate all nodes or variables except for the ith one. The final fixed-point equation is Eq. 4. The derivation is in the supplementary material.</p><formula xml:id="formula_12">Q + i (x i ) = 1 Z i exp    φ u (x i ) + j∈N 7 (xi) xj φ p (x i , x j )Q j (x j )    (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Learning with 7D Sparse Convolution</head><p>Interestingly, the weighted sum φ p (x i , x j )Q j (x j ) in Eq. 4 is equivalent to a generalized sparse convolution in the 7D space since φ p is stationary and each edge between x i , x j can be encoded using N 7 . Thus, we convert fixed point update equation Eq. 4 into an algorithm in Alg. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5 Variational Inference of TS-CRF</head><p>Require: Input: Logit scores φ u for all x i ; associated coor-</p><formula xml:id="formula_13">dinate C i , color F i , time T i Q 0 (X) = exp φ u (X), C crf = [C, F, T ] for n from 1 to N dõ Q n = SparseConvolution((C crf , Q n−1 ), kernel=φ p ) Q n = Softmax(φ u +Q n ) end for return Q N</formula><p>Finally, we use φ u as the logit predictions of a 4D Minkowski network and train both φ u and φ p end-to-end using one 4D and one 7D Minkowski network using Eq. 5.</p><formula xml:id="formula_14">∂L ∂φ p = N n ∂L ∂Q n+ ∂Q n+ ∂φ p , ∂L ∂φ u = N n ∂L ∂Q n+ ∂Q n+ ∂φ u<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>To validate the proposed high-dimensional networks, we first use multiple standard 3D benchmarks for 3D semantic segmentation. It allows us to gauge the performance of the high-dimensional networks with the same architecture with other state-of-the-art methods. Next, we create multiple 4D datasets from 3D datasets that have temporal sequences and analyze each of the proposed components for ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Implementation</head><p>We implemented the Minkowski Engine (Sec. 4) using C++/CUDA and wrap it with PyTorch <ref type="bibr" target="#b21">[21]</ref>. Data is prepared in parallel data processes that load point clouds, apply data augmentation, and quantize them with Alg. 1 on the fly. For non-spatial functions, we use the PyTorch functions directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Training and Evaluation</head><p>We use Momentum SGD with the Poly scheduler to train networks from learning rate 1e-1 and apply data augmentation including random scaling, rotation around the gravity axis, spatial translation, spatial elastic distortion, and chromatic translation and jitter.</p><p>For evaluation, we use the standard mean Intersection over Union (mIoU) and mean Accuracy (mAcc) for metrics following the previous works. To convert voxel-level predictions to point-level predictions, we simply propagated predictions from the nearest voxel center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Datasets</head><p>ScanNet. The ScanNet <ref type="bibr">[5]</ref> 3D segmentation benchmark consists of 3D reconstructions of real rooms. It contains 1.5k rooms, some repeated rooms captured with different sensors. We feed an entire room to a MinkowskiNet fully convolutionally without cropping.</p><p>Stanford 3D Indoor Spaces (S3DIS). The dataset <ref type="bibr">[2]</ref> contains 3D scans of six floors of three different buildings. We use the Fold #1 split following many previous works. We use 5cm and 2cm voxel for the experiment.</p><p>RueMonge 2014 (Varcity). The RueMonge 2014 dataset <ref type="bibr" target="#b25">[25]</ref> provides semantic labels for a multi-view 3D reconstruction of the Rue Mongue. To create a 4D dataset, we crop the 3D reconstruction on-the-fly to generate a temporal sequence. We use the official split for all experiments.</p><p>Synthia 4D. We use the Synthia dataset <ref type="bibr" target="#b27">[27]</ref> to create 3D video sequences. We use 6 sequences of driving scenarios in 9 different weather conditions. Each sequence consists of 4 stereo RGB-D images taken from the top of a car. We back-project the depth images to the 3D space to create 3D videos. We visualized a part of a sequence in <ref type="figure" target="#fig_6">Fig. 1</ref>.</p><p>We use the sequence 1-4 except for sunset, spring, and fog for the train split; the sequence 5 foggy weather for validation; and the sequence 6 sunset and spring for the test. In total, the train/val/test set contain 20k/815/1886 3D scenes respectively.</p><p>Since the dataset is purely synthetic, we added various noise to the input point clouds to simulate noisy observations. We used elastic distortion, Gaussian noise, and chromatic shift in the color for the noisy 4D Synthia experiments. ScanNet &amp; Stanford 3D Indoor The ScanNet and the Stanford Indoor datasets are one of the largest non-synthetic datasets, which make the datasets ideal test beds for 3D segmentation. We were able to achieve +19% mIOU on ScanNet, and +7% on Stanford compared with the bestpublished works by the CVPR deadline. This is due to the depth of the networks and the fine resolution of the space. We trained the same network for 60k iterations with 2cm voxel and achieved 72.1% mIoU on ScanNet after the deadline. For all evaluation, we feed an entire room to a network and process it fully convolutionally. TA denotes temporal averaging. As the input pointcloud coordinates are noisy, averaging along the temporal dimension introduces noise.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D analysis</head><p>The RueMongue dataset is a small dataset that ranges one section of a street, so with the smallest network, we were able to achieve the best result (Tab. 5). However, the results quickly saturate. On the other hand, the Synthia 4D dataset has an order of magnitude more 3D scans than any other datasets, so it is more suitable for the ablation study.</p><p>We use the Synthia datasets with and without noise for 3D and 4D analysis and results are presented in Tab. 2 and  Tab. 3. We use various 3D and 4D networks with and without TS-CRF. Specifically, when we simulate noise in sensory inputs on the 4D Synthia dataset, we can observe that the 4D networks are more robust to noise. Note that the number of parameters added to the 4D network compared with the 3D network is less than 6.4 % and 6e-3 % for the TS-CRF. Thus, with a small increase in computation, we could get  <ref type="bibr" target="#b25">[25]</ref> Method mIOU MV-CRF <ref type="bibr" target="#b26">[26]</ref> 42.3 Gradde et al. <ref type="bibr">[7]</ref> 54.4 RF+3D CRF <ref type="bibr" target="#b17">[17]</ref> 56.4 OctNet (256 3 ) <ref type="bibr" target="#b24">[24]</ref> 59.2 SPLATNet (3D) <ref type="bibr" target="#b28">[28]</ref> 65.4 3D MinkNet20 66.46 4D MinkNet20 66.56 4D MinkNet20 + TS-CRF 66.59</p><p>The performance saturates quickly due to the small training set. Per class IoU in the supplementary material. a more robust algorithm with higher accuracy. In addition, when we process temporal sequence using the 4D networks, we could even get small speed gain as we process data in a batch mode. On Tab. 6, we vary the voxel size and the sequence length and measured the runtime of the 3D and 4D networks, as well as the 4D networks with TS-CRF. Note that for large voxel sizes, we tend to get small speed gain on the 4D networks compared with 3D networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we propose a generalized sparse convolution and an auto-differentiation library for sparse tensors and the generalized sparse convolution. Using these, we create 4D convolutional neural networks for spatio-temporal perception. Experimentally, we show that 3D convolutional neural networks alone can outperform 2D networks and 4D perception can be more robust to noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The supplementary material for 4D Spatio-Temporal ConvNets:</head><p>Minkowski Convolutional Neural Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Minkowski Engine</head><p>In this section, we provide detailed breakdowns of few algorithms in the main paper as well as the coordinate initialization and the hash function. Please note that the actual implementation may not faithfully reflect the algorithms presented here for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Sparse Quantization</head><p>First, we provide line-by-line comments for the sparse quantization algorithm. Overall, the algorithm finds unique voxels and creates labels associated with them. If there are different labels within the same voxel, we assign the IGNORE_LABEL. This allows the network to ignore the cross-entropy loss for those voxels during training.</p><p>The Alg. 1 starts by converting all coordinates to hash keys and generating a sequence i, which is used to identify the original indices. Next, we sort the sequence, label pairs by the hash key (L5). This will place indices and labels that fall into the same cell to be adjacent in the GPU memory. Next, we reduce the index sequence with the key-label pair (L6). This step will collapse all points with the same labels. However, if there are different labels within the same cell, we will have at least two key-label pairs in the same cell. Thus, in L7, we use the reduction function f ((l x , i x ), (l y , i y )) =&gt; (IGNORE_LABEL, i x ) will simply assign IGNORE_LABEL to the cell. Finally, we use the reduced unique indices i to return unique coordinates, features, and transformed labels.</p><p>Algorithm 1 GPU Sparse Tensor Quantization 1: Inputs: coordinates C p ∈ R N ×D , features F p ∈ R N ×N f , target labels l ∈ Z N + , quantization step size v l 2: C p ← floor(C p / v l ) 3: k ← hash(C p ) 4: i ← sequence(N) 5: ((i , l ), k ) ← SortByKey((i, l), key=k) 6: (i , (k , l )) ← UniqueByKey(i , key=(k , l )) 7: (l , i ) ← ReduceByKey((l , i ), key=k , fn=f ) <ref type="bibr">8:</ref> </p><formula xml:id="formula_15">return C p [i , :], F p [i , :], l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Coordinate Hashing</head><p>We used a variation of Fowler-Noll-Vo hash function, FNV64-1A, and modified it for coordinates in int32. We list the algorithm in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Creating output coordinates</head><p>The first step in the generalized sparse convolution is defining the output coordinates C out . In most cases, we use the same coordinates for the output coordinates. However, when we use a strided convolution or a strided pooling, we use the Alg. 3 to create new output coordinates. For transposed strided convolutions, we use the same coordinates with the same input-stride to preserve the order of the coordinates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Derivation of the Trilateral Stationary-Conditional Random Field Fixed Point Update Equation</head><p>The TS-CRF optimization arg max X P (X) is intractable. Instead, we use the variational inference to minimize divergence between the optimal P (X) and an approximated distribution Q(X). Commonly, the divergence is measured using the Iprojection: D(Q P ), the number of bits lost when coding a distribution Q using P . To simplify the approximated distribution, we use the mean-field approximation, Q = i Q i (x i ) as the closed form solution exists. From the Theorem 11.9 in <ref type="bibr">[4]</ref>, Q is a local maximum if and only if</p><formula xml:id="formula_16">Q i (x i ) = 1 Z i exp E X−i∼Q−i   φ u (x i ) + j∈N (xi) φ p (x i , x j )  <label>(1)</label></formula><p>. where X −i and Q −i indicate all variables except for the i-th variable.</p><formula xml:id="formula_17">Q i (x i ) = 1 Z i exp E X−i∼Q−i   φ u (x i ) + j∈N (xi) φ p (x i , x j )   (2) = 1 Z i exp   E X−i∼Q−i φ u (x i ) + E X−i∼Q−i j∈N (xi) φ p (x i , x j )   (3) = 1 Z i exp   φ u (x i ) + j∈N (xi) E j∈N (xi) φ p (x i , x j )   (4) = 1 Z i exp   φ u (x i ) + j∈N (xi) xj φ p (x i , x j )Q j (x j )  <label>(5)</label></formula><p>Thus, the final fixed-point update equation is</p><formula xml:id="formula_18">Q + i (x i ) = 1 Z i exp    φ u (x i ) + j∈N (xi) xj φ p (x i , x j )Q j (x j )   <label>(6)</label></formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Analysis</head><p>We present per-class IoU numbers for all experiments and more qualitative results. We also visualize the entire RueMongue 2014 (Varcity) dataset on <ref type="figure" target="#fig_6">Fig. 1</ref>. The dataset is quite small for deep learning so the Minkowski Networks results are all saturated around 66% mIoU.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F O X W W H U E H D P E R D U G E R R N F D V H F H L O L Q J F K D L U F R O X P Q G R R U I O R R U V R I D W D E O H</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>An example of 3D video: 3D scenes at different time steps. Best viewed on display. 2D projections of hypercubes in various dimensions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of MinkowskiUNet32. × indicates a hypercubic kernel, + indicates a hypercross kernel. (best viewed on display) variations of the same architecture for semantic segmentation experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Visualizations of 3D (top), and 4D networks (bottom) onSynthia. A road (blue) far away from the car is often confused as sidewalks (green) with a 3D network, which persists after temporal averaging. However, 4D networks accurately captured it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of Scannet predictions. From the top, a 3D input pointcloud, a network prediction, and the ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of Stanford dataset Area 5 test results.From the top, RGB input, prediction, ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 :</head><label>6</label><figDesc>Time (s) to process 3D videos with 3D and 4D MinkNet, the volume of a scan at each time step is 50m× 50m × 50m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the RueMonge 2014 dataset ground-truth. The left half of the pointcloud is the training set and the right half is the test set (Black: IGNORE_LABEL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Visualization of the 4D Synthia predictions and ground truth labels. Visualization of the RueMonge 2014 dataset TASK3 results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :Figure 5 :Figure 6 :Figure 7 :</head><label>4567</label><figDesc>The confusion matrix of the MinkowskiNet32 predictions on the Stanford dataset Area 5 (Fold #1). Visualization of the Stanford dataset Area 5 test result. Visualization of the noisy 4D Synthia dataset and predictions. Visualization of MinkNet predictions on the Scannet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>on the base residual network. We use multiple Note the structural similarity. × indicates a hypercubic kernel, + indicates a hypercross kernel. (best viewed on display)</figDesc><table><row><cell>Sparse Conv 5×5×5×1, 64</cell><cell>pool</cell><cell>Sparse Conv 3×3×3+3, 64</cell><cell>Sparse Conv 3×3×3+3, 64</cell><cell>Sparse Conv 3×3×3+3, 64</cell><cell>Sparse Conv 3×3×3+3, 64</cell><cell>Sparse Conv 2×2×2×1, /2</cell><cell>Sparse Conv 3×3×3+3, 128</cell><cell>Sparse Conv 3×3×3+3, 128</cell><cell>Sparse Conv 3×3×3+3, 128</cell><cell>Sparse Conv 3×3×3+3, 128</cell><cell>Sparse Conv 2×2×2×1, /2</cell><cell>Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 64 Sparse Conv 3×3×3+3, 64 Sparse Conv 3×3×3+3, 64 Sparse Conv 3×3×3+3, 64 Sparse Conv 3×3×3+3, 128 Sparse Conv 3×3×3+3, 128 Sparse Conv 3×3×3+3, 128 Sparse Conv 3×3×3+3, 128 Sparse Conv 5×5×5×1, 64 pool Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Linear Figure 4: Architecture of ResNet18 (left) and MinkowskiNet18 Conv 3×3, 256 Conv 3×3, 64 Conv 3×3, 64 Conv 3×3, 64 Conv 3×3, 64 Conv 3×3, 128 Conv 3×3, 128 Conv 3×3, 128 Conv 3×3, 128 Conv 7×7, 64 pool Conv 3×3, 256 Conv 3×3, 256 Conv 3×3, 256 Conv 3×3, 512 Conv 3×3, 512 Conv 3×3, 512 Conv 3×3, 512 Linear (right). Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 256 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 1×1×1×1, LABELS Sparse Conv ×2 2×2×2×1, /2 Sparse Conv Tr 2×2×2×1, Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv 3×3×3+3, 512 Sparse Conv Tr 2×2×2×1, ×2 Sparse Conv Tr 2×2×2×1, ×2 Sparse Conv 3×3×3+3, 512 Sparse Sparse Sparse Conv Conv Conv 3×3×3+3, 512 3×3×3+3, 512 3×3×3+3, 512</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>3D Semantic Label Benchmark on ScanNet † [5] † 73.4 † : post-CVPR submissions. ‡ : uses 2D images additionally. Per class IoU in the supplementary material. The parenthesis next to our methods indicate the voxel size.</figDesc><table><row><cell>Method mIOU</cell></row><row><cell>ScanNet [5] 30.6 SSC-UNet [10] 30.8 PointNet++ [23] 33.9 ScanNet-FTSDF 38.3 SPLATNet [28] 39.3 TangetConv [29] 43.8 SurfaceConv [20] 44.2</cell></row><row><cell>3DMV  ‡ [6] 48.4 3DMV-FTSDF  ‡ 50.1 PointNet++SW 52.3</cell></row><row><cell>MinkowskiNet42 (5cm) 67.9</cell></row><row><cell>SparseConvNet [10]  † 72.5 MinkowskiNet42 (2cm)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Segmentation results on the 4D Synthia dataset</figDesc><table><row><cell>Method</cell><cell>mIOU mAcc</cell></row><row><cell>3D MinkNet20 3D MinkNet20 + TA</cell><cell>76.24 89.31 77.03 89.20</cell></row><row><cell cols="2">4D Tesseract MinkNet20 75.34 89.27 4D MinkNet20 77.46 88.013 4D MinkNet20 + TS-CRF 78.30 90.23 4D MinkNet32 + TS-CRF 78.67 90.51</cell></row><row><cell cols="2">TA denotes temporal averaging. Per class IoU in the supplementary</cell></row><row><cell>material.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Segmentation results on the noisy Synthia 4D dataset</figDesc><table><row><cell>IoU</cell><cell>Building</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Fence</cell><cell>Vegetation</cell><cell>Pole</cell><cell>Car</cell><cell cols="3">Traffic Sign Pedestrian Lanemarking Traffic Light</cell><cell>mIoU</cell></row><row><cell>3D MinkNet42 3D MinkNet42 + TA</cell><cell cols="4">87.954 97.511 78.346 84.307 87.796 97.068 78.500 83.938</cell><cell>96.225 96.290</cell><cell cols="2">94.785 87.370 94.764 85.248</cell><cell>42.705 43.723</cell><cell>66.666 62.048</cell><cell>52.665 50.319</cell><cell>55.353 54.825</cell><cell>76.717 75.865</cell></row><row><cell cols="5">4D Tesseract MinkNet42 89.957 96.917 81.755 82.841 4D MinkNet42 88.890 97.720 85.206 84.855</cell><cell>96.556 97.325</cell><cell cols="2">96.042 91.196 96.147 92.209</cell><cell>52.149 61.794</cell><cell>51.824 61.647</cell><cell>70.388 55.673</cell><cell>57.960 56.735</cell><cell>78.871 79.836</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Stanford Area 5 Test (Fold #1) (S3DIS) [2]</cell></row><row><cell>Method mIOU mAcc</cell></row><row><cell>PointNet [22] 41.09 48.98</cell></row><row><cell>SparseUNet [9] 41.72 64.62</cell></row><row><cell>SegCloud [30] 48.92 57.35</cell></row><row><cell>TangentConv [29] 52.8 60.7</cell></row><row><cell>3D RNN [32] 53.4 71.3</cell></row><row><cell>PointCNN [15] 57.26 63.86</cell></row><row><cell>SuperpointGraph [14] 58.04 66.5</cell></row><row><cell>MinkowskiNet20 62.60 69.62 MinkowskiNet32 65.35 71.71</cell></row><row><cell>Per class IoU in the supplementary material.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>RueMonge 2014 dataset (Varcity) TASK3</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Algorithm 2 Coordinate HashingRequire: Inputs: coordinate c ∈ Z D in int32 1: function HASH(c) Create output coordinates Require: input stride s i , layer stride s l , input coordinates C 1: function CREATEOUTPUTCOORDINATES(C, s i , s l )</figDesc><table><row><cell>2: 3: 4: 5: 6: 7: 8: end function h = UINT64(14695981039346656037) for all c i ∈ c do h = h ⊕ UINT32(c i ) h = h × UINT64(1099511628211) end for return h</cell></row><row><cell>Algorithm 3 2: if s l &gt; 1 then</cell></row><row><cell>3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end function s ← s l × s i C ← {} // Create an empty set for all c i ∈ C do c ← c i /s × s if c is not in C then Append c to C end if end for return C else return C end if</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 :</head><label>1</label><figDesc>ScanNet [1] 3D Segmentation Benchmark Results Method bath bed bksf cab chair cntr curt desk door floor othr pic ref show sink sofa tab toil wall wind mIoU</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 :</head><label>2</label><figDesc>Results on the Stanford 3D Indoor Spaces Dataset Area 5 Test (Fold #1) (S3DIS) Method ceiling floor wall beam clmn windw door chair table bkcase sofa board clutter mIOU mAcc PointNet [8] 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22 41.09 48.98 SegCloud [13] 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60 48.92 57.35 TangentConv* [12] 90.47 97.66 73.99 0.0 20.66 38.98 31.34 77.49 69.43 57.27 38.54 48.78 39.79 52.8 60.7 3D RNN [15] 95.2 98.6 77.4 0.8 9.83 52.7 27.9 76.8 78.3 58.6 27.4 39.1 51.0 53.4 71.3 PointCNN [6] 92.31 98.24 79.41 0.00 17.60 22.77 62.09 80.59 74.39 66.67 31.67 62.05 56.74 57.26 63.86 SuperpointGraph [5] 89.35 96.87 78.12 0.0 42.81 48.93 61.58 84.66 75.41 69.84 52.60 2.1 52.22 58.04 66.5 PCCN [14] 90.26 96.20 75.89 0.27 5.98 69.49 63.45 66.87 65.63 47.28 68.91 59.10 46.22 58.27 67.01 MinkowskiNet20 91.55 98.49 84.99 0.8 26.47 46.18 55.82 88.99 80.52 71.74 48.29 62.98 57.72 62.60 69.62 MinkowskiNet32 91.75 98.71 86.19 0.0 34.06 48.90 62.44 89.82 81.57 74.88 47.21 74.44 58.57 65.35 71.71</figDesc><table /><note>*Data from the authors</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation results on the 4D Synthia dataset Method Bldn Road Sdwlk Fence Vegittn Pole Car T. Sign Pedstrn Bicycl Lane T. Light mIOU mAcc MinkNet20 89.394 97.684 69.425 86.519 98.106 97.256 93.497 79.450 92.274 0.000 44.609 66.691 76.24 89.31 MinkNet20 + TA 88.096 97.790 78.329 87.088 96.540 97.486 94.203 78.831 92.489 0.000 46.407 67.071 77.03 89.198 4D MinkNet20 90.125 98.262 73.467 87.187 99.099 97.502 94.010 79.041 92.622 0.000 50.006 68.138 77.46 88.013 4D Tesseract MinkNet20 89.346 97.291 60.712 86.451 98.000 96.632 93.191 74.906 91.030 0.000 47.113 69.343 75.335 89.272 4D MinkNet20 + TS-CRF 89.438 98.291 79.938 86.254 98.707 97.142 95.045 81.592 91.540 0.000 54.596 67.067 78.30 90.23 4D MinkNet32 + TS-CRF 89.694 98.632 86.893 87.801 98.776 97.284 94.039 80.292 92.300 0.000 49.299 69.060 78.67 90.51</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Semantic segmentation results on the RueMongue [10] (Varcity) dataset .978 80.813 64.303 47.256 61.486 76.901 72.498 66.462 4D MinkNet20 61.938 81.591 65.394 35.089 68.554 80.546 72.846 66.565 4D MinkNet20 + TS-CRF 62.600 81.821 63.599 40.795 66.229 80.464 70.639 66.592</figDesc><table><row><cell>Method window wall balcony door</cell><cell>roof</cell><cell>sky</cell><cell>shop mIOU</cell></row><row><cell>3D MinkNet20 61</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/StanfordVL/MinkowskiEngine 2 At the time of submission, we achieved the best performance on Scan-Net[5]  with 67.9% mIoU</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgements</head><p>Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We acknowledge the support of the System X Fellowship and the companies sponsored: NEC Corporation, Nvidia, Samsung, and Tencent. Also, we want to acknowledge the academic hardware donation from Nvidia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myers Abraham</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">753</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiley Online Library</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scannet: Richlyannotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient 2d and 3d facade segmentation using autocontext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3d convolutional neural networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on nonuniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH Asia 2018)</title>
		<meeting>SIGGRAPH Asia 2018)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques -Adaptive Computation and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segmentation of 4d cardiac mr images using a probabilistic atlas and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Lorenzo-Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">I</forename><surname>Sanchez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Elkington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mohiaddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d all the way: Semantic segmentation of urban scenes from start to end in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andelo</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dynamic finite element surface model for segmentation and tracking in multidimensional medical images with application to cardiac 4d image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="83" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Thrust: Parallel algorithm library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Hao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04952</idno>
		<title level="m">Convolutional neural networks on 3d surfaces using parallel frames</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning where to classify in multi-view semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning where to classify in multi-view semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vangelis</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08275</idno>
		<title level="m">Splatnet: Sparse lattice networks for point cloud processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Tangent convolutions for dense prediction in 3D. CVPR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<title level="m">Semantic segmentation of 3d point clouds. International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An investigation of sparse tensor formats for tensor libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tew</forename><surname>Parker Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning the matching of local 3d geometry in range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Modeling 4d fmri data via spatio-temporal convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Makkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12564</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">st-cnn). arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minkowskinet42</surname></persName>
		</author>
		<idno>5cm) 81.1 73.4 73.9</idno>
		<imprint/>
	</monogr>
	<note>1 80.4 41.3 75.9 69.6 54.5 93.8 51.8 14.1 62.3 75.7 68.0 72.3 68.4 89.6 82.1 65.1 67.9</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minkowskinet42</surname></persName>
		</author>
		<idno>2cm) 83.7 80.4 80.0 72.1 84.3 46.0 83.5 64.7 59.7 95.3 54.2 21.4 74.6 91.2 70.5 77.1 64.0 87.6 84.2 67.2 72.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<ptr target="http://kaldir.vc.in.tum.de/scannet_benchmark" />
		<title level="m">Entries without citation are from unpublished works. ‡ : uses 2D images additionally</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Sparse 3d convolutional neural networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques -Adaptive Computation and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Pointcnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Hao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04952</idno>
		<title level="m">Convolutional neural networks on 3d surfaces using parallel frames</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning where to classify in multi-view semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayko</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Bódis-Szomorú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vangelis</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08275</idno>
		<title level="m">Splatnet: Sparse lattice networks for point cloud processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Tangent convolutions for dense prediction in 3D. CVPR</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<title level="m">Semantic segmentation of 3d point clouds. International Conference on 3D Vision (3DV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu Ma3 Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Liang Du, and Xiaolin Zhang. 3d recurrent neural networks with context fusion for point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiamao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
