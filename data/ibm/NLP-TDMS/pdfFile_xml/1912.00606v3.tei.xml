<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEGAS: Differentiable Efficient Generator Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEGAS: Differentiable Efficient Generator Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network architecture search (NAS) achieves state-of-theart results in various tasks such as classification and semantic segmentation. Recently, a reinforcement learning-based approach has been proposed for Generative Adversarial Networks (GANs) search. In this work, we propose an alternative strategy for GAN search by using a method called DEGAS (Differentiable Efficient GenerAtor Search), which focuses on efficiently finding the generator in the GAN. Our search algorithm is inspired by the differential architecture search strategy and the Global Latent Optimization (GLO) procedure. This leads to both an efficient and stable GAN search. After the generator architecture is found, it can be plugged into any existing framework for GAN training. For CTGAN, which we use in this work, the new model outperforms the original inception score results by 0.25 for CIFAR-10 and 0.77 for STL. It also gets better results than the RL based GAN search methods in shorter search time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref> have become a very successful framework for image generation. This scheme includes a generator that creates images and a discriminator that tries to discriminate between real and synthesized images. Both are trained together using a minmax based optimization.</p><p>GANs are difficult to train, due to problems such as mode collapse, non-convergence to the Nash equilibrium and vanishing gradients. Overcoming these problems is the focus of many recent works. Notable among them is the Wasserstein GAN (WGAN) <ref type="bibr" target="#b0">[1]</ref>, which suggests replacing the KL-divergence loss with the earth mover's distance (Wasserstein) loss for solving the diminishing gradient problem, and is the basis for many other works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21]</ref>. Another approach, GLO <ref type="bibr" target="#b1">[2]</ref>, changes the traditional adversarial framework, by removing the discrimi-nator and using a reconstruction loss of the input instead, which avoids many of the instabilities that appear in joint generator-discriminator training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>The architectures that are used for the generator in the GAN works are usually inspired by DC-GAN <ref type="bibr" target="#b25">[26]</ref> and ResNet <ref type="bibr" target="#b10">[11]</ref>. However, all of them are manually designed. In recent years, neural architecture search (NAS) based approaches have successfully found models that outperform the state-of-the-art in various tasks <ref type="bibr" target="#b13">[14]</ref>. The first line of works used reinforcement learning <ref type="bibr" target="#b34">[35]</ref> and genetic algorithms <ref type="bibr" target="#b26">[27]</ref> to generate a target classifier for a certain task. Yet, these approaches require a large number of computational resources, thus, making them computationally demanding. The second line of works manages to reduce the computational cost significantly to only a few days on a single GPU. Two notable works among them are Efficient NAS <ref type="bibr" target="#b24">[25]</ref> and Differentiable Architecture Search (DARTS) <ref type="bibr" target="#b17">[18]</ref>.</p><p>Contribution. In this work, we aim at finding a generator model using an efficient neural architecture search. Similar to DARTS, we use the concept of learning the weights of connections between feature maps and then performing pruning to get the final network structure.</p><p>While we use the DARTS MixedOp to connect between feature maps and the concept of connection based differentiable search, we have four major differences from it: a. Global search. DARTS uses the cell method, i.e., learning a repeating structure in the network and then concatenating the learned repeated structures to create the whole large network. We do not use the cell method: Although many generator models are built from several repeating operations, we exploit the relatively short length of the generator network and the efficiency of the search algorithm to make the search more flexible in finding new structures. Thus, we search for the whole network altogether.</p><p>b. Operation types. Instead of operations that reduce the size of the data that are part of the DARTS search space, we use up-sample operations that increase the size of their input and are inspired by the DC-GAN and ResNet models.</p><p>c. Operations' connections. DARTS concatenate the outputs of cells to form the input of the next cell. To allow having DC-GAN and ResNet type networks, we eliminate the concatenation and forward only operations that are selected in the search. This is crucial to get the results we present in our work. Yet, it requires changing the pruning phase that is used in DARTS. More details on this issue and the changes it requires, appear in Section 3.</p><p>d. The objective function. While DARTS uses classification losses in its training, we use a different objective. Since their introduction, GANs have been very popular for image generation. Yet, their training is considered to be a difficult task. The GAN framework is composed of two components: (i) a generator that tries to generate images that look real, and (ii) a discriminator that discriminates between generated and real images. Learning both architectures simultaneously can turn into a very hard problem. In this work, we relax this problem by searching only for generator architecture. Moreover, to overcome the sensitivity of the GAN training, we search for the generator model using the GLO training protocol, i.e., search for the generator using a reconstruction loss. This gives us a very important advantage: The generator search is decoupled from the discriminator and thus we do not need to deal in the search with an untrained discriminator that will have a negative influence on the learning of the generator.</p><p>While we use GLO for the search, we do not use it for the final training of the generator since the visual results produced by GANs that have a discriminator are more favorable. Thus, to be on par with other existing GANs, we train the found generator architecture using an existing framework for GAN training. Specifically, we use the CTGAN protocol <ref type="bibr" target="#b31">[32]</ref> and show that our automatically designed generator improves the performance in this framework compared to the original manually designed GAN used with it both in terms of FID <ref type="bibr" target="#b11">[12]</ref> and inception scores <ref type="bibr" target="#b28">[29]</ref>. By this, we show that a generator search that is based on image reconstruction is a valid strategy for finding generators that produce good images.</p><p>On all the datasets used in this work, the search took only a couple of days on a single GPU, achieving better results than <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b7">[8]</ref> that use reinforcement learning. Moreover, when we plug our generator to an existing GAN procedure, we also improve the results compared to the vanilla generator (in CTGAN). This further demonstrates the advantage of the proposed approach for the generator search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Generative Adversarial Networks. The Generative Adversarial Networks (GANs) models <ref type="bibr" target="#b8">[9]</ref> include a generator that synthesizes new data given a latent representation and a discriminator, whose role is to discriminate between real data and synthesized data. The generator and the discriminator are trained together using an adversarial game between the generator that tries to 'fool' the discriminator, which on its side tries to discriminate between true and syn-thesized examples. This is implemented by a min-max optimization problem whose goal is to reach the optimal point, which is the Nash equilibrium.</p><p>The common practice shows that training GANs is a difficult task. Frequent problems in their training include mode collapse, diminishing gradients and non-convergence to the Nash equilibrium. These hinders can cause significant difficulty in performing an automatic search for generator architecture. Much attention has been given recently to find a solution to these deficiencies.</p><p>The Wasserstein GAN (WGAN) <ref type="bibr" target="#b0">[1]</ref> aims at reducing the imbalance between the discriminator and the generator in the training, in which the discriminator outperforms the generator. When this happens, the generator gradients according to the Jensen or KL divergence are zero, and thus it does not learn anything. To solve this, the Wasserstein loss function (known also as the earth mover's distance) is used. For using it, the discriminator function needs to be a differentiable 1-Lipchitz function.</p><p>In the original WGAN paper, the 1-Lipchitz property has been enforced by clipping the weights of the network using a hyperparameter, which leads to a network that is very sensitive to the tuning of this hyperparameter. To fix this, the use of gradient penalty (GP) has been proposed <ref type="bibr" target="#b9">[10]</ref>, under the assumption that the gradient of a 1-Lipchitz function needs to be one almost everywhere. This condition is added as a regularizer and leads to the WGAN-GP technique.</p><p>Another notable work is CAGAN <ref type="bibr" target="#b20">[21]</ref>, which introduced the adversarial consistency loss. They used the training process of WGAN-GP with a large number of critics (different discriminators). Each critic was created by using a dropout on the hidden layers of the discriminator. They presented a consistency loss that reduced the redundancy between these critics.</p><p>More recently, CTGAN has been proposed <ref type="bibr" target="#b31">[32]</ref>, which adds regularization on the discriminator loss function. The loss takes two random perturbations of the inputs to the discriminator and adds a penalty on the distance between their two outputs. In this work, we use the CTGAN training framework and discriminator for testing the generator architecture we have found.</p><p>The Generative Latent Optimization (GLO) framework <ref type="bibr" target="#b1">[2]</ref> takes an alternative route for bypassing the challenge of training both the generator and the discriminator networks. It replaces the adversarial training that requires having a discriminator, with another optimization strategy that only trains a generator. This approach pairs to each image in the training set a latent representation and then optimizes the generator to decode each latent vector to its corresponding image (see <ref type="figure" target="#fig_0">Fig. 1</ref>). This strategy allows training generative models very easily without the need for a discriminator training. Another related approach is NAM <ref type="bibr" target="#b12">[13]</ref>, which is an unsupervised method for mapping without adversar- Note that the overall performances of the GLO strategy in terms of image quality are not as good as training a generator with a discriminator. However, the simplicity of this method is perfect for generator architecture search and therefore we use it in our work in the search phase.</p><p>Architecture search techniques. Several methods have been proposed for optimizing the parameters of neural networks and for finding new architectures. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref>, it has been shown that one may unfold a recurrent neural network to approximate some given functionals in a better way than other manually designed approaches. These strategies may allow better tuning of the hyper-parameters of a given network compared to Bayesian approaches. Yet, they are not designed searching for new neural architectures.</p><p>Zoph et al. <ref type="bibr" target="#b34">[35]</ref> used a reinforcement learning-based approach for neural architecture search (NAS). A recurrent network was used to generate the model description of a target neural network for a certain task. They showed improvement in their resulted architecture (NASnet) comparing to existing hand-crafted network models at its time. They were outperformed by AmoebaNet <ref type="bibr" target="#b26">[27]</ref>. The work in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref> introduces a different technique for finding neural architectures. Given a task, they use an evolutionary (genetic) algorithm to find a neural structure.</p><p>Another approach for improving accuracy was proposed in <ref type="bibr" target="#b5">[6]</ref>. New data augmentation techniques for neural network training have been sought for using reinforcement learning. While this search process is computationally demanding, it has been demonstrated that the found augmentation techniques, even when produced by a relatively small dataset, are transferable across different problems (e.g., augmentation developed for CIFAR-10 were useful for Im-ageNet).</p><p>All these methods require very large computational resources (some needs thousands of GPU days!), therefore although they achieve state-of-the-art performances, in practice it is unfeasible to use them.</p><p>Recent works have managed to overcome the high computational cost, without reducing the neural architectures performances significantly. Notable among them are the Efficient NAS (ENAS) <ref type="bibr" target="#b24">[25]</ref> and the Differentiable Architecture Search (DARTS) <ref type="bibr" target="#b17">[18]</ref>. Both of these work manage to search neural architectures in only a few GPU days.</p><p>ENAS is a reinforcement learning-based method. A subgraph (child model) is searched in a large graph and to reduce the search time, the child models share weights between the same operations.</p><p>DARTS consists of two phases. Its network is built from a concatenation of cells, which are learned during the first phase. Then, in the second phase, some of the graph connections and operations are reduced according to a pruning algorithm. Each operation has a learned weight multiplier, which indicates connection importance. They are learned as continues variables during the first phase. After the second phase, the remaining weights after the pruning are the ones with the highest weight multiplier magnitude.</p><p>DARTS uses a harsh pruning at the end, which makes the found architecture sub-optimal. This problem may be addressed by performing gradual pruning <ref type="bibr" target="#b22">[23]</ref> or layer-wise search <ref type="bibr" target="#b3">[4]</ref>, which leads to better results in less time.</p><p>A one-shot low memory-efficient solution has been suggested in <ref type="bibr" target="#b2">[3]</ref>. It uses a learned binary mask to select only a single path of the network and load it on the GPU. Their strategy searches global architecture and not cells.</p><p>NAS methods have been in used also for other problems than classification. These include semantic image segmentation <ref type="bibr" target="#b16">[17]</ref>, medical image segmentation <ref type="bibr" target="#b32">[33]</ref>, volumetric medical image segmentation <ref type="bibr" target="#b33">[34]</ref>, object detection <ref type="bibr" target="#b21">[22]</ref> and active learning <ref type="bibr" target="#b6">[7]</ref>.</p><p>Very recently NAS approaches have been proposed for GANs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>. These works, which are the closest to ours, use reinforcement learning, which is very different from our approach. An important advantage of our approach over them is search time. The search time in <ref type="bibr" target="#b30">[31]</ref>, which targets directly the inception score that requires class labels information, is extremely high: they use 200 TITAN GPUs for 6 days to perform the search, which is significantly more than the time and computations required by our search. The approach in <ref type="bibr" target="#b7">[8]</ref>, use RL with parameter sharing and dynamicresetting. The authors in this work do not perform a search on STL due to search time, which is possible in our proposed strategy due to its better efficiency. As we show hereafter, our achieved results are also better than the ones in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref> in terms of FID <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GAN search</head><p>Our goal here is to efficiently search for a suitable generator architecture. As mentioned above, in this work we avoid the search for both a generator and a discriminator due to the instabilities involved in their joint training. Instead, we make the search simpler by using the GLO method, which does not contain a discriminator.</p><p>Our assumption, which is demonstrated later empirically, is that a generator that is found based on an image reconstruction criterion can also be used to produce improved images when plugged in an existing GAN procedure.</p><p>Our generator search method is inspired by the DARTS algorithm <ref type="bibr" target="#b17">[18]</ref>. It learns in an efficient way what operations to use in each layer of the generator from a pre-fixed set of operations that are all used at the beginning. Before we describe our strategy we briefly survey the DARTS technique and then mention the innovation that allows us to have an efficient search technique for generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">DARTS</head><p>The DARTS strategy <ref type="bibr" target="#b17">[18]</ref> contains two phases: In the first one, the algorithm searches for the network architecture; and in the second, the new architecture is evaluated after training it from scratch. The work in <ref type="bibr" target="#b34">[35]</ref> has noticed that recent convolution neural networks are built from a repeating structure of operations that when concatenated together form a network. DARTS follows this routine and in the search phase, it searches for this repeating structure of operations, which is called a 'cell'. Each cell is composed of a feed-forward graph of feature maps that are connected between them by a mixture of operations. This Mixed Operation denoted byō (i,j) is equal tō</p><formula xml:id="formula_0">o i,j (x) = o∈O exp(α (i,j) o )o(x) o ∈O exp(α (i,j) o ) ,<label>(1)</label></formula><p>where o(x) is an operation on x, O is the group of all operations in the search space, and α i,j o is the learned weight for the operation o. The α values in (1) are learned by optimizing the loss function w.r.t their values.</p><p>In DARTS, two types of cells are being learned -Normal and Reduction. Normal cell outputs a feature map of the same size as the input. The reduction cell outputs a feature map of a smaller size than the input. The operations o (i,j) can be average/max-pool or types of convolutions with varying kernel sizes and strides. The kind of operations that are used depend on the cell type. The complete network is composed of a concatenation of several cells.</p><p>In the search phase, to make the process faster, the network is smaller than the one that will be trained and evaluated in the second phase. Because the search space is continuous, the optimization process is much faster than prior NAS works.</p><p>At the end of the search, pruning is done to most of the operations, based on their α values. The remaining operations (non-pruned ones) are the final cell structure that was learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The generator search</head><p>In our search strategy, we do not use the cells method. Since the commonly used generator networks are not as deep as the ones used in classification, we can search for the whole network altogether without using cells.</p><p>Search phase. In order to search only for a generator, we employ the GLO approach. This strategy trains a generator alone without the use of a discriminator by optimizing both with respect to the latent space input (z) and the generator weights. As we stated before, the advantage of using GLO is its stability, its elimination of the need for a discriminator and its smaller memory use in the search phase-As we do not need to backpropagate gradients from the discriminator. This allows us to find more complex models in a shorter time. Also, its monotonic stable loss function helps the search to converge.</p><p>The method works as follow: For each train image it matches a random noise vector z in the latent space, in order to train the generator to output for each z its corresponding image. Then, a reconstruction loss w.r.t the train image (compared to the generator output) is calculated. The reconstruction loss is a combination of a squared-loss function and a Laplacian pyramid (Lap1loss):</p><formula xml:id="formula_1">2 (x,x) = x −x 2 2 ,<label>(2)</label></formula><formula xml:id="formula_2">Lap 1 (x,x) = j 2 2j | L j (x) − L j (x) | 1 ,</formula><p>where L j (x) is the jth level of the Laplacian pyramid. Using alternating optimization between the GLO and α values, we perform one step of optimization with respect to the latent space input (Z) and generator weights as in GLO, and one step of optimization with respect to the α values of the MixedOp. The loss function is a combination of the losses in (2) as in GLO.</p><p>Searched network structure. The searched network is divided into three parts. The first is fixed and inspired by ResNet and contains a linear operation with reshape. The second contains the architecture and operations that we search for. The last part is also fixed and contains bn+relu+conv+tanh, again, similar to ResNet.</p><p>As in DARTS, we use MixedOp to connect between feature maps but with different types of operations in each MixedOp. We use two types: (i) Normal operations that keep the size of their input; and (ii) up-sample operations that increase the size of their input. We also prune connections at the end based on their value, as we explain hereafter.</p><p>The normal operations employed are inspired by DARTS and ResNet architecture. They include a combination of bn+Relu+conv with kernel sizes of 1 and 3, Max and Average pooling, skip connection, separate convolutions(see <ref type="bibr" target="#b17">[18]</ref>) and dilated convolution with kernel sizes of 3 or 5.</p><p>The up-sample operations used are inspired by the DC-GAN and ResNet architectures and include bn+Relu+deconvolution with kernel sizes of 4 and 6 and bn+Relu+nearest neighbor up-sampling + convolution with a kernel size of 1 or 3. <ref type="figure" target="#fig_1">Figure 2</ref> presents the connections Pruning phase. At the end of the search phase, we perform pruning. The rationale behind our pruning strategy is to allow having skip connections in the network structure but without enforcing them. Because we do not use the same cell configuration as in DARTS (we do not concatenate the outputs), we have to change the pruning procedure. Otherwise, two close nodes may not be connected if we keep using the DARTS pruning procedure. Our new pruning does not enforce any macro architecture but lets the search decide whether to select residual or not, based on the found alpha value.</p><p>The pruning consists of two stages. In the first stage, for each feature map, we keep only one connection to its previous feature maps. The connection is selected to be the operation with the largest value of α. In the second stage, if the connection that was selected in the first stage is residual (which may leave the previous feature map unconnected as the operation skips it), we will also add another operation by selecting from the direct connections the one with the highest α. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the search does choose some residuals connections in some connections but not in all of them.</p><p>Search space complexity. We turn to analyze the complexity of the search space. From looking at <ref type="figure" target="#fig_1">Figure 2</ref>, note that we have 3 MixedOps with 7 up-sample operations, 3 direct MixedOps with 4 normal operations, and 5 MixedOps on residual connection with 4+1 up-sample operations each (+1 because they can have lack of connection). Thus, the number of possible discretized networks (i.e. combination of operations after pruning) is 4 3 · 7 3 · 5 · 9 2 ≈ 10 7 .</p><p>The continuous search space (which is the search space before the pruning) has 9 3 options for normal operations and up-sample operations, 4 3 options for direct connection and (4 + 1) 5 residual connection (+1 includes the zero operation, that stands for no connection). Overall, we have 1.458 * 10 8 possible networks in this case.</p><p>GAN training. After finding the generator architecture with our search, we can use it to replace the generator in any given GAN framework, e.g., CTGAN <ref type="bibr" target="#b31">[32]</ref>. Then, we simply train the new generator with the discriminator of that framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We used CIFAR-10, CelebA and STL datasets for our experiments. For having a quantitative evaluation, we measured the FID <ref type="bibr" target="#b11">[12]</ref> and Inception score (IS) <ref type="bibr" target="#b28">[29]</ref> on CIFAR-10 and STL. A larger version of the figures presented here appears in the supp. material.</p><p>For calculating the IS, we employed the same method as in <ref type="bibr" target="#b23">[24]</ref> and other works and for calculating the FID we used the same method as in <ref type="bibr" target="#b11">[12]</ref>. In their computation, we generated random 50,000 images with the currently trained model and then use 50,000 real images to calculate the IS and FID. The inception score is calculated over 10 splits of the generated images, then mean and std(± sign) are calculated from these splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Search results and generalization</head><p>We searched for a generator for CIFAR-10, STL, and CelebA using the scheme described above. In all cases, we do not use the labels provided with the data (in CIFAR-10 and STL) in the search. Yet, we show hereafter that the generator architecture that is found without labels shows good performance also when the labels are added. We show also the transferability of the model across datasets.</p><p>We use the DARTS <ref type="bibr" target="#b17">[18]</ref> hyperparameters of CIFAR-10, except for the learning rate that is set to 3e-1 for CIFAR-10, and 3e-2 for CelebA and STL.</p><p>On 1 Nvidia TITAN-X, the search took 28 hours for CIFAR-10, 100 hours for STL, and 57 hours for CelebA. The unsupervised generator training time was 37 hours for CIFAR-10, 85 hours for STL and 33 hours for CelebA.</p><p>Models size. The model size of the CIFAR10 generator is 1.65 MB and the STL generator is 1.17 MB.</p><p>CIFAR-10. <ref type="figure" target="#fig_2">Figure 3</ref> presents the network found for CIFAR-10. Notice that different operations are selected in each layer (both for the up-sample and normal operations), which demonstrates the advantage of not forcing the network into the cell structure. Note also that the found archi-   <ref type="table">Table 1</ref> compares the IS and FID scores of our model to other existing works. Notice that our results are on par with the state of the art methods and better than the other searched GANs (we are on the same scale like <ref type="bibr" target="#b30">[31]</ref> in terms of IS but much better than it in terms of FID, which is considered to be a better perceptual measure <ref type="bibr" target="#b11">[12]</ref>. Thus, we may claim that our result is better). Supervised CIFAR-10. For CIFAR-10, we also train the found architecture in a supervised way, following the procedure in <ref type="bibr" target="#b31">[32]</ref>. By that, we demonstrate that the model we have found is transferable to supervised training. The results of the supervised case appear in <ref type="table" target="#tab_0">Table 2</ref>. Note that although the generator was searched in an unsupervised form, our supervised results are competitive with the other super-vised generators and better as before than AGAN <ref type="bibr" target="#b30">[31]</ref> (no result was reported for this case for AutoGAN in <ref type="bibr" target="#b7">[8]</ref>).</p><p>STL. For STL, we performed two experiments: (i) searching on STL and then training on it; and (ii) taking the network found on CIFAR-10 and training it on STL. As the image size in CIFAR-10 and STL is different, we have increased the latent vector size by a factor of the image size ratio between CIFAR-10 and STL, in the CIFAR-10 network, which leads to the desired size at the output for STL.</p><p>As <ref type="table" target="#tab_1">Table 3</ref> shows, the architecture we found on CIFAR-10 transfers well to STL and even outperforms the one searched on STL. We believe that the difference between the results is due to the hyperparameters (initially designed for CIFAR-10) used in the search. We expect that a better hyperparameter setting will improve the STL search. Note   <ref type="bibr" target="#b19">[20]</ref> 9.10 ± 0.04 40.1 -WGAN-GP <ref type="bibr" target="#b9">[10]</ref> 9.05 ± 0.12 --CAGAN <ref type="bibr" target="#b20">[21]</ref> 9.51 ± 0.14 --AGAN <ref type="bibr" target="#b30">[31]</ref> 9.23 ± 0.08 52.7 1200 AutoGAN <ref type="bibr" target="#b7">[8]</ref> 9.16 ± 0.12 31.01 n/a DEGAS(ours) -searched 9.22 ± 0.08 40.25 4.167 DEGAS(ours) -CIFAR-10 net 9.71 ± 0.11</p><p>28.76 1.167 8.37 ± 0.08 <ref type="table">Table 4</ref>. Ablation studies on CIFAR-10 toGAN. Although its search time is not reported, it was claimed that they did not search on STL due to the search time. Note that our search time on STL is just a few days on a single GPU, which is a reasonable time.</p><p>CelebA. Similar to STL, also for CelebA we considered two generators: One that was searched on CelebA images of size 32x32; and one that was searched on CIFAR-10. <ref type="figure">Figure 6</ref> presents the 32x32 generated images. Notice that the quality of images is similar for both generators, which shows the transferability of the generator found on CIFAR-10 to a different type of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Training random block. Taking random continues architecture, pruning it and then applying it to CIFAR-10 leads to IS and FID of 8.127 ± 0.098, 15.312, which is significantly inferior to our found network. This further confirms the effectiveness of DEGAS.</p><p>Effect of different n values. We evaluate the effect of the number of normal feature maps (n) between up-sample feature maps on the search. When searching on CIFAR-10 using n=2, the IS is low (7.24 ± 0.06) and the FID is high (48.83), which shows that n=1 is better suited for the generator task. The found generator can be found in the supp. material.</p><p>Effect of search with smaller image size. We also check whether decreasing the size of the images in the search affects the performance of the found architecture. For CIFAR-10, we searched using images with a size 16x16 and trained on regular-sized images. On STL we searched using images of size 32x32 and then trained the found generator on regular images (of size 48x48). Although the search time was smaller, the IS was much lower for both (8.03 ± 0.012 and 8.97 ± 0.02).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduced DEGAS, a method for searching efficiently generator architectures without the need to search for a discriminator. On CIFAR-10 and STL, DE-GAS outperforms parallel works to us for automated GAN search <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b7">[8]</ref> both in terms of the search cost and terms of FID score. We have also demonstrated the transferability of our found generator both across datasets and between unsupervised and supervised training.</p><p>The paper's main contribution is providing an efficient generator search strategy. It avoids long search time by using a continuous search space and the GLO framework. We believe our work has various interesting follow-up GANrelated search directions. We mention two of them:</p><p>1. Most GANs generate low-resolution images. A solution to this problem is to train GAN progressively <ref type="bibr" target="#b14">[15]</ref>. This means that we start with a generator and discriminator for small image output, and after stabilizing the network, we progressively expand the network output. A possible future work is combining our search with the progressive GAN framework to create a search strategy for high-resolution GANs and further improve results.</p><p>2. In this work, we did not select the discriminator, which may have a great impact on the results. Future work should explore also this important aspect of GAN training. Discriminators are mostly built from down-sample and normal operations as in the DARTS searched classifiers and can be added as another optimizing phase to our scheme, before or together with the generator search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>GLO method: each randomly sampled Z is matched to specific train image (X) and Z, G(z)=X are being learned ial training. In this method, a pre-trained generative model aligns each source image with a synthesized image from the target domain. This optimization is done together with the optimization of the domain mapping function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The network is built from two types of operations -normal and up-sample. The arrows between the feature maps are learned and called MixedOps. The arrows that go into the blue blocks are up-sample operations and the arrows that go into the pink blocks are normal operations. The architecture of the big arrows is constant used in the search between the different feature maps. We define normal and up-sampled feature maps as the feature maps that are created using normal and up-sample operations respectively. In other words, the input MixedOp to a normal/up-sampled feature map is a normal/up-sample MixedOp. As can be seen in the figure, the up-sampled feature maps are connected between them with residual connections and are connected to the normal feature map before them. The number of normal feature maps between the upsample feature maps is determined by the hyperparameter n. The figure shows the case of n=1, where there is only one normal feature map between two up-sample feature maps. In this case, each normal feature map is connected to the up-sample feature map before it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The generators found for: (a) CIFAR-10 (n=1); (b) STL (n=1); (c) CelebA (n=1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>CIFAR-10: (left) unsupervised generated images. (right) supervised generated images tecture does contains skip connections, which resemble the current state-of-the-art generators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>STL unsupervised generated images using: (left) CIFAR-10 based generator (right) STL based generator CelebA unsupervised generated images using: (left) CIFAR-10 network (right) CelebA network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>CIFAR-10 supervised image generation that our FID score is better than the compared methods.Note that the fact that we can search on STL makes us conclude that our search time is better than the one of Au-</figDesc><table><row><cell>Method</cell><cell>Inception</cell><cell>FID</cell><cell>Search</cell><cell>Method</cell><cell>Inception</cell><cell>FID</cell><cell>Search</cell></row><row><cell></cell><cell>Score</cell><cell></cell><cell>GPU</cell><cell></cell><cell>Score</cell><cell></cell><cell>GPU</cell></row><row><cell></cell><cell></cell><cell></cell><cell>days</cell><cell></cell><cell></cell><cell></cell><cell>days</cell></row><row><cell>Real data</cell><cell>11.24</cell><cell>2.1</cell><cell>-</cell><cell>Real data</cell><cell>11.24</cell><cell>2.1</cell><cell>-</cell></row><row><cell>SN-GAN [20]</cell><cell>8.22 ± 0.5</cell><cell>11.8*</cell><cell></cell><cell cols="2">WGAN-GP [10] 8.42 ± 0.1</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">WGAN-GP [10] 7.86 ± 0.07</cell><cell>14.1*</cell><cell>-</cell><cell>CAGAN [21]</cell><cell>8.89 ± 0.11</cell><cell>-</cell><cell>-</cell></row><row><cell>CTGAN [32]</cell><cell>8.12 ± 0.12</cell><cell>-</cell><cell>-</cell><cell>CTGAN [32]</cell><cell>8.81 ± 0.13</cell><cell>-</cell><cell>-</cell></row><row><cell>CAGAN [21]</cell><cell>8.35 ± 0.09</cell><cell>-</cell><cell>-</cell><cell>AGAN [31]</cell><cell>8.82 ± 0.9</cell><cell>23.8</cell><cell>1200</cell></row><row><cell>AGAN [31]</cell><cell>8.29 ± 0.9</cell><cell>30.5</cell><cell>1200</cell><cell>DEGAS(ours)</cell><cell>8.85 ± 0.07</cell><cell>9.83</cell><cell>1.167</cell></row><row><cell>AutoGAN [8]</cell><cell>8.55 ± 0.1</cell><cell>12.42</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DEGAS(ours)</cell><cell>8.37 ± 0.08</cell><cell>12.01</cell><cell>1.167</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Table 1. CIFAR-10 unsupervised image generation. *FID taken</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>from [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>STL unsupervised image generation. The first row indicates the score for a network searched on STL dataset. The second row indicates the score for a network searched on CIFAR-10.</figDesc><table><row><cell>Method</cell><cell>Inception</cell></row><row><cell></cell><cell>Score</cell></row><row><cell>Search with n = 2</cell><cell>7.24 ± 0.06</cell></row><row><cell cols="2">Images size 16x16 8.03 ± 0.012</cell></row><row><cell>Random block</cell><cell>8.127 ± 0.098</cell></row><row><cell>DEGAS(ours)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by Alibaba and the NSF-BSF grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Lopez</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian Q Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to learn without gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501v2</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep active learning with a neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El-Yaniv</forename><surname>Ran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nam: Non-adversarial unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="455" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hy-Oukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpipe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">Efficient training of giant neural networks using pipeline parallelism</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Marcin Michalski, and Sylvain Gelly. A large-scale study on regularization and normalization in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lučić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1" to="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrychowicz</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 02 2018</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cagan: Consistent adversarial training enhanced gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lejian</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJ-CAI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nas-fcos: Fast neural architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Chunhua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04423</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Asap: Architecture search, anneal and prune</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Nayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04123</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Melody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning -ICML AutoML Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How good is my gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Agan: Towards automated design of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11080</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving the improved training of wasserstein gans: A consistency term and its dual effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation(ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nasunet: Neural architecture search for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Qiu</surname></persName>
		</author>
		<idno>2019. 3</idno>
	</analytic>
	<monogr>
		<title level="j">In IEEE Access</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">V-nas: Neural architecture search for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daguang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.0281</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
