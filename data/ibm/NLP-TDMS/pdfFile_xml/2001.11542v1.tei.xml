<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHANNEL-ATTENTION DENSE U-NET FOR MULTICHANNEL SPEECH ENHANCEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahareh</forename><surname>Tolooshams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">H</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CHANNEL-ATTENTION DENSE U-NET FOR MULTICHANNEL SPEECH ENHANCEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Channel-Attention</term>
					<term>U-Net</term>
					<term>Complex Ratio Masking</term>
					<term>Multichannel Speech Enhancement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised deep learning has gained significant attention for speech enhancement recently. The state-of-the-art deep learning methods perform the task by learning a ratio/binary mask that is applied to the mixture in the time-frequency domain to produce the clean speech. Despite the great performance in the single-channel setting, these frameworks lag in performance in the multichannel setting as the majority of these methods a) fail to exploit the available spatial information fully, and b) still treat the deep architecture as a black box which may not be well-suited for multichannel audio processing. This paper addresses these drawbacks, a) by utilizing complex ratio masking instead of masking on the magnitude of the spectrogram, and more importantly, b) by introducing a channelattention mechanism inside the deep architecture to mimic beamforming. We propose Channel-Attention Dense U-Net, in which we apply the channel-attention unit recursively on feature maps at every layer of the network, enabling the network to perform nonlinear beamforming. We demonstrate the superior performance of the network against the state-of-the-art approaches on the CHiME-3 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multichannel speech enhancement is the problem of obtaining a clean speech estimate from multiple channels of noisy mixture recordings. Traditionally, beamforming techniques have been employed, where a linear spatial filter is estimated, per frequency, to boost the signal from the desired target direction while attenuating the interferences from other directions by utilizing second-order statistics, e.g., spatial covariance of speech and noise <ref type="bibr" target="#b0">[1]</ref>.</p><p>In recent years, deep learning (DL) based supervised speech enhancement techniques have achieved significant success <ref type="bibr" target="#b1">[2]</ref>, specifically for monaural/single-channel case. Motivated by this success, a recent line of work proposes to combine supervised single-channel techniques with unsupervised beamforming methods for multichannel case <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. These approaches are broadly known as neural beamforming, where a neural network estimates the second-order statistics of speech and noise, using estimated time-frequency (TF) masks, after which the beamformer is applied to linearly combine the multichannel mixture to produce clean speech. However, the performance of neural beamforming is limited by the nature of beamforming, a linear spatial filter per frequency bin.</p><p>This work was done while B. Tolooshams and A. H. Song were interns at Amazon Web Services.</p><p>Another line of work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> proposes to use spatial features along with spectral information to estimate TF masks. Most of these approaches have an explicit step to extract spatial features such as interchannel time/phase/level difference (ITD/IPD/ILD). Recent work <ref type="bibr" target="#b6">[7]</ref> automatically extracts phase information from the input mixture by incorporating IPD as a block inside the neural network. <ref type="bibr" target="#b7">[8]</ref> takes a more general approach to predict the TF mask by directly feeding magnitude and phase of the complex spectrogram from all microphones to a convolutional neural network (CNN). Despite incorporating spatial information, these methods still focus on predicting a real mask, hence resort to using the noisy phase, and ignore phase-enhancement.</p><p>To overcome the aforementioned limitations, this paper proposes an end-to-end neural architecture for multichannel speech enhancement, which we call Channel-Attention Dense U-Net. The distinguishing feature of the proposed framework is a Channel-Attention (CA) mechanism inspired by beamforming. CA is motivated by the self-attention mechanism, which captures global dependencies within the data. Self-attention has been previously used in various fields <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, as well as speech enhancement in the single-channel setting <ref type="bibr" target="#b11">[12]</ref>. This paper incorporates CA into a CNN to guide the network to decide, at every layer, which feature maps to pay the most attention to. This work, therefore, extends the idea of beamforming on the input space to a latent space.</p><p>In addition to the CA units, the network is a variation of U-Net <ref type="bibr" target="#b12">[13]</ref>, a popular architecture for source separation, and DenseNet <ref type="bibr" target="#b13">[14]</ref>. Motivated by the success of complex ratio masking in the single-channel case <ref type="bibr" target="#b14">[15]</ref>, our approach takes both real and imaginary part of the complex mixture short-time Fourier transform (STFT) and estimates a complex ratio mask (CRM) unlike in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. The CRM is then applied to the mixture STFT to obtain the clean speech. Channel-Attention Dense U-Net does not require an explicit spatial feature extraction step; instead it implicitly identifies and exploits the relevant spatial information.</p><p>Rest of the paper is organized as follows: Section 2 introduces the proposed network, Channel-Attention Dense U-Net, and discusses mechanism of CA in detail. Section 3 describes the dataset, network parameters, and evaluation criteria. This is followed by Section 4, in which we demonstrate the outperformance of our network against state-of-the-art methods. Finally, Section 5 concludes the paper and discusses some future directions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CHANNEL-ATTENTION DENSE U-NET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Description</head><p>Let y c ∈ R N be the discrete-time signal of a noisy mixture at microphone c. We assume that {y c } C c=1 , for c = 1, . . . , C and arXiv:2001.11542v1 [cs.SD] 30 Jan 2020 n = 1, . . . , N , follows the generative model</p><formula xml:id="formula_0">y c [n] = s c [n] + n c [n],<label>(1)</label></formula><p>where s c [n] and n c [n] represent the clean speech and noise recorded at channel c, at time n, respectively. The goal of speech enhancement is to estimateŝ ref , where ref ∈ {1, . . . , C} denotes a reference channel from the multichannel mixtures {y c } C c=1 . We also denote Y c ∈ C F ×T as the STFT of y c , where F and T are the number of frequency bins and time frames, respectively, and</p><formula xml:id="formula_1">Y = [Y 1 , . . . , Y C ] ∈ C F ×T ×C as multichannel STFT. Let Y f = [Y 1 f , . . . , Y C f ] ∈ C T ×C</formula><p>be the multichannel STFT at frequency bin f , the traditional beamformers, such as the popular MVDR beamformer, linearly combine {Y c f } C c=1 with the estimated beamforming weightsŵ f ∈ C C , to produce the estimated clean speechŜ f at each frequency f (e.i.,Ŝ f = Y fŵ H f ∈ C T ). As will be made clearer in subsequent sections, our proposed framework applies attention weights, i.e., a weight matrix similar to beamforming weights, recursively to the multichannel input and feature maps, extending the beamforming analogy to a non-linear combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Framework Overview</head><p>Channel-Attention Dense U-Net consists of an encoder, a mask estimation network, and a decoder. The encoder performs STFT on the mixture {y c } C c=1 to produce Y. Given Y, the mask estimation network computes both the speech mask M and noise mask M noise , which are multiplied to input, to obtain the clean speech estimatê S ∈ C F ×T ×C and the noise estimateN ∈ C F ×T ×C . Finally, the decoder performs inverse-STFT onŜ andN to produce time-domain estimates of the speechŝ ∈ R N ×C and the noisen ∈ R N ×C .</p><p>We now expand on how the outputs of the network,Ŝ andN, are computed from Y. Stacking the real and imaginary components Ystack = [Yr, Yi] where subscript r and i denote real and imaginary parts, respectively, the mask estimation network aims to estimate a mask Mstack = [Mr, Mi] ∈ [R F ×T ×C , R F ×T ×C ]. The complex multiplication between Y and M produces estimated speechŜ ∈ C F ×T ×C <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b7">8]</ref>. Hence, M can be considered as the CRM for speech. Given Mstack, the noise mask M noise stack is computed for the estimate of the noiseN as follows:</p><formula xml:id="formula_2">M noise r = 1 − Mr, M noise i = −Mi.<label>(2)</label></formula><p>The clean speechŜ and noiseN are estimated by the element-wise complex multiplication (denoted as * )</p><formula xml:id="formula_3">S = Y * M,N = Y * M noise .<label>(3)</label></formula><p>To train the network, we minimize the weighted 1 loss of the audio in time domain and the magnitude of its spectrogram as follows:</p><formula xml:id="formula_4">L(u,û) = u∈{s,n} α u −û 1 + |U| − |Û| 1 ,<label>(4)</label></formula><p>where α is determined based on the relative importance of the two error terms. The framework is trained in a supervised manner, requiring ground truth speech and noise signals <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network Architecture</head><p>The mask estimation network is a variant of U-Net that consists of a series of blocks. The first block is a single unit of CA to perform beamforming-like operation on the input mixture, and the last block is a convolutional layer with ReLU non-linearity to generate the mask. We explain the middle blocks for the rest of this section.</p><p>We note that the real version of Channel-Attention Dense U-Net takes the magnitude of the STFT as its input, and estimates a real mask <ref type="bibr" target="#b15">[16]</ref>. This implies that the denoising of the noisy mixture is performed only with respect to the magnitude, hence the estimated clean speech contains phase of the noisy mixture.</p><formula xml:id="formula_5">Encoder CA ∈ R F ×T ×4C Dense-block Down-block Down-block Up-block Up-block Conv Mask Generator * * Decoder sn y ∈ R N ×C Ystack = [Yr, Yi] ∈ R F ×T ×2C ∈ R F ×T ×K 1 ∈ R F 2 × T 2 ×4K 1 ∈ R F 4 × T 4 ×8K 1 skip-connection ∈ R F 2 × T 2 ×4K 1 skip-connection ∈ R F ×T ×2K 1 M noise stack Mstack Fig. 1. Architecture of Channel-Attention Dense U-Net for L = 2. 2.3.1. U-Net with DenseNet Blocks</formula><p>U-Net, a convolutional network previously proposed for image segmentation, is a popular network for source separation <ref type="bibr" target="#b16">[17]</ref> and speech enhancement <ref type="bibr" target="#b17">[18]</ref>. U-Net consists of a series of blocks (L down-blocks and L up-blocks), and skip-connections between down and up-blocks. Each down-block consists of a pooling layer for down-sampling, a convolutional layer, and an exponential nonlinearity. Each up-block consists of an up-sampling through a transposed convolution with stride 2, a transposed convolutional layer, and an exponential non-linearity. In Channel-Attention Dense U-Net, each convolutional layer in each block is replaced by a DenseNet block followed by a CA unit. The output of each down-block or up-block is the concatenation of the input and output of its CA unit. DenseNet applies convolution to the concatenation of several previous-layer feature maps, which eases the gradient flow in deep networks and helps each layer learn features that are not similar to the neighbouring layers <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure">Figure 1</ref> shows the architecture of Channel-Attention Dense U-Net for when L = 2, and <ref type="figure" target="#fig_0">Figure 2</ref> shows the detail of down and up blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Channel-Attention</head><p>In this section, we introduce the Channel-Attention unit inspired by self-attention and beamforming. Self-attention is a mechanism for capturing global dependencies, which has gained attention in various fields such as machine translation <ref type="bibr" target="#b8">[9]</ref>, natural language processing <ref type="bibr" target="#b9">[10]</ref>, and image processing <ref type="bibr" target="#b10">[11]</ref>.</p><p>Given input x ∈ R F × T ×2 C , which is also the output of the DenseNet in mid blocks, our proposed CA unit transforms x into key k(x) ∈ R F ×d×2 C , query q(x) ∈ R F ×d×2 C , and value v(x) ∈ R F × T ×2 C feature maps through a convolution followed by an exponential non-linearity. Note that we use (·) to indicate that due to down/up-sampling layer in each block, the dimensions of the input are different for CA unit at different blocks. The k, q, and v are 1 × 1 convolutional operators in the 2-dimensional space of F × 2 C with T input channels and d, d, and T output channels, respectively, followed by an exponential non-linearity.</p><formula xml:id="formula_6">Pooling Dense-block CA ∈ R F ×T ×K ∈ R F 2 × T 2 ×K ∈ R F 2 × T 2 ×2K ∈ R F 2 × T 2 ×4K Up-sampling Conv Dense-block CA ∈ R F 2 × T 2 ×4K ∈ R F ×T ×4K ∈ R F ×T ×K ∈ R F ×T ×2K ∈ R F ×T ×K skip ∈ R F ×T ×2K (b) Up-block (a) Down-block</formula><p>We treat the key, query, and value as stack of real and imaginary where the first C channels are real and the second C channels are imaginary. For a given frequency bin f , we define the key and query as k f (x) ∈ C d× C and q f (x) ∈ C d× C . The CA mechanism computes the similarity matrix, P = [P1, . . . , P F ] ∈ C F × C× C between key and query for every frequency bin as follows:</p><formula xml:id="formula_7">P f = k f (x) T q f (x) ∈ C C× C , for f = 1, . . . , F .<label>(5)</label></formula><p>Having k f (x) = [k f 1 , . . . , k f C ] and q f (x) = [q f 1 , . . . , q f C ], for f = 1, . . . , F , the similarity matrix is,</p><formula xml:id="formula_8">P f = k T f q f =       k T f 1 q f 1 . . . k T f 1 q f C k T f 2 q f 1 . . . k T f 2 q f C . . . . . . . . . k T f C q f 1 . . . k T f C q f C       .<label>(6)</label></formula><p>The attention weights matrix W ∈ C F × C× C is normalized (by softmax function) P with respect to the second dimension. The weight matrix entry is thus given as </p><formula xml:id="formula_9">o f = v f (x)W f ∈ C T × C ,<label>(8)</label></formula><p>where v f (x) ∈ C T × C , and W f ∈ C C× C . For real-valued input, multiplication and similarity operations happen in real domain. <ref type="figure">Figure 3</ref> shows the detailed architecture of CA.</p><p>x q(x) query <ref type="figure">Fig. 3</ref>. CA unit. Given input x, CA computes the attention mask W and apply it to value, a variant of the input.</p><formula xml:id="formula_10">k(x) key v(x) × P W value softmax × o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Connection of Channel-Attention to Beamforming</head><p>The motivation for incorporating the CA concept into our framework is two-fold. First, inspired by the traditional beamformers which linearly combine multichannel mixtures to produce a clean signal estimate, we expect the trained CA unit to learn to 'optimally' combine multichannel information to produce a clean speech signal. Specifically, the fact that a CA unit is applied to features maps at every layer, and that nonlinearity layers exist throughout the architecture suggests that this combination is not confined to the linear regime. In Eq. <ref type="formula" target="#formula_8">(6)</ref>, each column c resembles beamforming weights as if channel c is chosen as reference. Therefore, in Eq. <ref type="formula" target="#formula_9">(8)</ref>, v f (x) can be seen as a variant of the input signal to CA, and W f decides which channel of v f (x) to pay more attention to. Indeed, our proposed CA can be seen as a mechanism to automatically pick a reference channel and perform beamforming. Interestingly, we observe that the attention weights in a trained model learn to represent the signalto-noise-ratio (SNR) (importance) of each feature map.</p><p>We verified this behaviour by examining the weights of the trained CA unit W, located right after the encoder, from the trained CA unit of real Channel-Attention Dense U-Net for the following two input scenarios: 1) a noisy mixture from the CHiME-3 dataset <ref type="bibr" target="#b19">[20]</ref> and 2) a toy example with the simulated input where channel 1 has the highest SNR among all channels. We chose to examine the real network instead of the complex network, for easier interpretation and visualization.   . . , WF ×C×C ] for the CHiME-3 data example, where we observe that channel 5 has received the most attention. This matches with the fact that channel 4 and 5 of CHiME-3 recordings have the highest SNR on average. Another interesting observation is that CA learns to pay more attention to low frequencies, which are known to contain the majority of the speech information. <ref type="figure" target="#fig_3">Figure 4(b)</ref> demonstrates the similar results for a toy example. In this case, we clearly observe that channel 1, the channel with the highest SNR, gets the most attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We used the publicly available CHiME-3 dataset <ref type="bibr" target="#b19">[20]</ref>, made available as part of a speech separation and recognition challenge, for training and evaluating speech enhancement performance. The dataset is a 6-channel (C = 6) microphone recording of talkers speaking in a noisy environment, sampled at 16 kHz. It consists of 7,138, 1,640, and 1,320 simulated utterances with an average length of 3 s for training, development, and test, respectively. At every iteration of training, a random segment of length N = 19,200 is selected from the utterance, and a random attenuation of the background noise in the range of [−20, 0] dB is applied as a data augmentation scheme. This augmentation was done to make the network robust against various SNRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training and Network Parameters</head><p>The encoder and decoder are initialized with STFT and Inverse-STFT coefficients, respectively, using a Hanning window of length 1,024, hence F = 512 (we discard the last bin, since, for the downsampling step of the network, the number of frequency bins needs to be even), and hop size of 256. Consequently, the number of time frames for each input is T = 80. We design the network to have 4 down-blocks and 4 up-blocks (L = 4) where the kernel size for all convolutions is 2 × 2. The number of convolutional filters in the first layer is set to 32, with a maximum of 256 possible number of filters at every convolution. For all the CA units inside the network, we set the depth of query and key to d = 20.</p><p>The network is trained with ADAM optimizer with learning rate of 10 −4 , and batch size of 8. For the loss function (Eq. (4)), we set α such that the error in time domain, u −û 1, is twice as important as the error in the magnitude of the spectrogram, |U|−|Û| 1 . This is done based on loss magnitude at the beginning of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation</head><p>We evaluated the network performance with the following metrics: signal-to-distortion ratio (SDR) using BSS Eval library <ref type="bibr" target="#b20">[21]</ref> and Perceptual Evaluation of Speech Quality (PESQ) -more specifically the wideband version recommended in ITU-T P.862.2 (0.5 to 4.5).</p><p>Given the source estimatesŝ from all C channels at the output, we computed the posterior SNR for each channel and selected the channel with the highest posterior SNR as the final estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>We trained four networks as follows:</p><p>• U-Net (Real): U-Net without any dense blocks, which performs magnitude ratio masking.</p><p>• Dense U-Net (Real): U-Net (Real) with dense blocks (D=4).</p><p>• Dense U-Net (Complex): U-Net with dense blocks (D=4), which takes real and imaginary part of STFT as input and performs complex ratio masking.</p><p>• CA Dense U-Net (Complex): Dense U-Net (Complex) with Channel-Attention. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the improvement in the performance of the network, as we add a new component to the architecture, such as dense-blocks, complex ratio masking scheme, and finally, the Channel-Attention. We note that for U-Net (Real), and Dense U-Net (Real), the only spatial information network has access to is ILD, the level difference between the channel. Hence the performance improvement from Dense U-Net (Real) to Dense U-Net (Complex) is primarily for two reasons: a) access to IPD information, and b) complex ratio masking instead of magnitude ratio masking. Finally, we observe that Channel-Attention improves the performance of Dense U-Net (Complex) further. We compare the performance of our method to the following three state-of-the-art methods on CHiME-3 dataset:</p><p>• Neural Beamforming <ref type="bibr" target="#b2">[3]</ref>: An MVDR beamforming with mask estimation through bidirectional-LSTM.</p><p>• NMF-Informed Beamforming <ref type="bibr" target="#b21">[22]</ref>: An online MVDR beamforming through the decomposition of TF bins of the mixture into the sum of speech and noise, by performing nonnegative matrix factorization (NMF).</p><p>• Forgetting Factor Optimization <ref type="bibr" target="#b22">[23]</ref>: An MVDR beamforming with simultaneous estimation of TF masks and forgetting factors. <ref type="table">Table 2</ref> shows the results where ∆PESQ represents PESQ improvement with respect to the channel 5 of the noisy mixtures (row 1 in <ref type="table" target="#tab_0">Table 1</ref>). Results for the competing methods are taken from the corresponding papers and the missing entries in the table indicate that the metric is not reported in the reference paper. Overall, our proposed approach significantly outperforms state-of-the-art results on the CHiME-3 speech enhancement task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This paper proposed a channel-attention mechanism inspired by beamforming for speech enhancement of multichannel recordings. The paper combined time-frequency masking <ref type="bibr" target="#b15">[16]</ref>, UNet <ref type="bibr" target="#b12">[13]</ref>, and DenseNet <ref type="bibr" target="#b13">[14]</ref> into a unified network along with channel-attention mechanism. Our interpretation of the channel-attention mechanism is that the network performs recursive non-linear beamforming on the data represented in a latent space. We showed that the proposed network outperforms all the published state-of-the-art algorithms on the CHiME-3 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Down Block, (b) Up Block. The skip connection is the output from the corresponding down-block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>|w f,c,c | = e |p f,c,c | C c=1 e |p f,c,c | , ∠w f,c,c = ∠p f,c,c ,(7)for f = 1, . . . , F, and c, c = 1, . . . , C. The output of the attention unit for frequency f is the concatenation of the real and imaginary parts of o f computed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) W from the CHiME-3 dataset (b) W from the SNR example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 (</head><label>4</label><figDesc>a) shows the [WF ×C×1, .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of trained networks on CHiME-3 dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">sim-dev</cell><cell></cell><cell>sim-test</cell></row><row><cell>Methods</cell><cell></cell><cell>SDR</cell><cell>PESQ</cell><cell></cell><cell>SDR</cell><cell>PESQ</cell></row><row><cell cols="2">Channel-5 (Noisy)</cell><cell>5.79</cell><cell>1.27</cell><cell></cell><cell>6.50</cell><cell>1.27</cell></row><row><cell>U-Net (Real)</cell><cell></cell><cell cols="4">14.651 2.105 15.967 2.176</cell></row><row><cell cols="2">Dense U-Net (Real)</cell><cell cols="4">14.901 2.242 16.855 2.378</cell></row><row><cell cols="2">Dense U-Net (Complex)</cell><cell cols="2">16.962 2.33</cell><cell cols="2">18.402 2.404</cell></row><row><cell cols="6">CA Dense U-Net (Complex) 17.169 2.368 18.635 2.436</cell></row><row><cell cols="6">Table 2. Performance comparison of Channel-Attention Dense U-</cell></row><row><cell cols="4">Net with state-of-the-art results on CHiME 3.</cell><cell></cell></row><row><cell></cell><cell cols="2">sim-dev</cell><cell cols="3">sim-test</cell></row><row><cell>Methods</cell><cell>SDR</cell><cell>∆PESQ</cell><cell>SDR</cell><cell></cell><cell>∆PESQ</cell></row><row><cell>NMF B [22]</cell><cell>-</cell><cell>-</cell><cell>16.16</cell><cell></cell><cell>0.52</cell></row><row><cell cols="2">Forgetting F [23] 16.07</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Neural B [3]</cell><cell>15.80</cell><cell>0.92</cell><cell>15.12</cell><cell></cell><cell>1.02</cell></row><row><cell cols="2">CA Dense U-Net 17.169</cell><cell>1.09</cell><cell cols="2">18.635</cell><cell>1.16</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Microphone array signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiteng</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved mvdr beamforming using single-channel mask prediction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nuclear Physics A</title>
		<imprint>
			<biblScope unit="volume">08</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1981" to="1985" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural network based spectral mask estimation for acoustic beamforming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining spectral and spatial features for deep learning based blind speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-channel deep clustering: Discriminative spectral and spatial embeddings for speaker-independent speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end multi-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Timefrequency masking based online speech enhancement with multi-channel data using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2016 Conference on Empirical Methods in Natural Language essing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Selfattention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08318</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention wave-u-net for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvindh</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting>of IEEE Workshop on Applications of Signal essing to Audio and Acoustics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Complex ratio masking for monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="483" to="492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multichannel Speech Enhancement Based on Time-frequency Masking Using Subband Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<meeting>of 2019 IEEE Workshop on Applications of Signal essing to Audio and Acoustics (WASPAA)<address><addrLine>New Paltz, NY, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wave-u-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>of IEEE International Conference on Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2391" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phaseaware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale multi-band densenets for audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third ?chime? speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised speech enhancement based on multichannel nmf-informed beamforming for noise-robust automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech and Lang. Proc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="960" to="971" />
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous optimization of forgetting factor and time-frequency mask for block online multi-channel speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="2702" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
