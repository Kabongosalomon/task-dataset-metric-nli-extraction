<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects TbD-NC 3D position &amp; angular velocity 6-DOF Trajectory Input Video 2D trajectory Object snapshots shape &amp; appearance Piece-wise Deblatting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">Visual Recognition Group</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">UTIA</orgName>
								<orgName type="institution" key="instit2">Czech Academy of Sciences</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filipšroubek</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">UTIA</orgName>
								<orgName type="institution" key="instit2">Czech Academy of Sciences</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">Visual Recognition Group</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects TbD-NC 3D position &amp; angular velocity 6-DOF Trajectory Input Video 2D trajectory Object snapshots shape &amp; appearance Piece-wise Deblatting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Estimation of appearance, shape and 6D pose (3D position and rotation) of fast moving objects. The input video and 2D trajectories estimated by Non-Causal Tracking by Deblatting, , are processed by the proposed piecewise deblatting that generates, with sub-frame temporal resolution, the object appearance and shape (snapshots), from which the complete 6-DOF trajectory is estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a novel method that tracks fast moving objects, mainly non-uniform spherical, in full 6 degrees of freedom, estimating simultaneously their 3D motion trajectory, 3D pose and object appearance changes with a time step that is a fraction of the video frame exposure time. The sub-frame object localization and appearance estimation allows realistic temporal super-resolution and precise shape estimation. The method, called TbD-3D (Tracking by Deblatting in 3D) relies on a novel reconstruction algorithm which solves a piece-wise deblurring and matting problem. The 3D rotation is estimated by minimizing the reprojection error. As a second contribution, we present a new challenging dataset with fast moving objects that change their appearance and distance to the camera. High speed camera recordings with zero lag between frame exposures were used to generate videos with different frame rates annotated with ground-truth trajectory and pose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking encompasses a broad class of problems that have received significant interest <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Current stateof-the-art methods employ a range of techniques, such as deep Siamese networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> and discriminative correlation filters <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12]</ref>. The standard output of tracking methods is a 2D bounding box, either axis aligned or rotated. Video segmentation methods output precise segmentation masks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Recently, fast moving objects (FMOs) have been introduced as one of the problems in tracking <ref type="bibr" target="#b14">[15]</ref>. Such objects are recorded as blurred streaks. They are common in sport videos and many other scenarios, such as videos of falling objects, hailstorm and flying insects, or more specialized ones, e.g. visual navigation of microrobots in a magnetic field. To avoid FMOs and the related phenomena, one can use high-speed cameras operating at high frame rates, e.g. 240 fps or more. However, this brings additional requirements on resources, such as data transfer bandwidth and storage. When capturing such objects, camera settings have to be considered a priori before video acquisition.</p><p>The blurred trace of an object encodes information about its velocity, shape and appearance. Estimating these quantities should be thus in principle possible even from more affordable cameras with 30 fps, but it is a challenging task as the problem is heavily ill-posed. As shown in <ref type="bibr" target="#b14">[15]</ref>, standard tracking methods do not perform well on FMOs.</p><p>For a fast moving object, a bounding box or a segmentation mask is not an adequate representation of its trajectory, as it travels a non-negligible path in a single frame. Such Input 1 2 3 4 5 6 7 8 <ref type="figure">Figure 2</ref>: Sub-frame appearance estimation of fast moving objects. Left: 30 fps input images with overlaid 2D projections of recovered 3D trajectories in green. White points correspond to time instants in the middle of high-speed camera frames. Right: cropped objects from a high-speed camera (top) and output of the proposed TbD-3D (bottom). 3D rotation is estimated by minimizing the reprojection error, assuming a spherical object. The estimated rotation axis is visualized by a yellow cross.</p><p>object may be localized more precisely, with a sub-frame accuracy.</p><p>Tracking by Deblatting (TbD) <ref type="bibr" target="#b4">[5]</ref> was the first method to track fast moving objects by solving a joint deblurring and matting (deblatting) problem. These techniques are closer to blind deconvolution than to visual tracking methods. Non-causal post-processing proposed in <ref type="bibr" target="#b13">[14]</ref> gives more precise and complete trajectories. The output of both above-mentioned methods is only a 2D trajectory. They assume a 2D appearance and mask of an object to stay unchanged over the duration of a frame. This is equivalent to ignoring the 3D rotation of the object, the change of its distance to camera and of appearance due to the non-uniform light field, reflections, shadows or deformations. Such simplifications are only adequate for objects with almost uniform texture and moving in a plane parallel to the camera projection plane. To date, the full nature of 3D object motion and appearance has not been considered, nor object location in 3D nor angular velocity in 3D.</p><p>In this paper we are the first to estimate continuous-time sub-frame changes in appearance of the object. While solving for the shape and appearance, we recover the 3D rotation of the object and distance to the camera (currently we are able to handle only close to spherical object). The output of the proposed method is a continuous object pose with 6 degrees of freedom. The reconstruction pipeline is summarized in <ref type="figure">Figure 1</ref>.</p><p>We make the following contributions:</p><p>• We propose TbD-3D (Tracking by Deblatting in 3D)the first method to reconstruct the appearance and the shape of blurred moving objects with sub-frame temporal resolution using piece-wise deblatting. We call these reconstructions snapshots (as in <ref type="figure">Figure 2</ref>).</p><p>• The method estimates continuous-time pose with 6 degrees of freedom (3D location and rotation) for nonuniform spherical objects. The rotation is estimated by a new method which minimizes the reprojection error.</p><p>• We collect and make available a new challenging dataset with fast moving objects that change their appearance and distance to the camera. High speed camera with zero lag between frame exposures is used to generate videos with different low frame rates annotated with ground-truth trajectory and pose data.</p><p>• Sub-frame reconstruction accuracy on object deformations that occur during contact with other objects is demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Detection and tracking of fast moving objects was introduced by Rozumnyi et al. <ref type="bibr" target="#b14">[15]</ref>. Their work was limited by several assumptions on object trajectory and appearance, such as linear trajectory parallel to the camera projection plane, uniform 2D appearance of the object, high contrast to the background and no contact of the moving object with other objects. Some of these assumptions were relaxed in a method called Tracking by Deblatting (TbD) <ref type="bibr" target="#b4">[5]</ref>, which tracks fast moving objects by solving a deblurring problem in every frame and processing the video in a causal manner.</p><p>TbD outperforms the previous approach by a wide margin, yet trajectories estimated at adjacent frames are independent and the final trajectory for the whole sequence is a set of segments. These limitations were addressed by a followup method called non-causal tracking by deblatting (TbD-NC) <ref type="bibr" target="#b13">[14]</ref>. TbD-NC takes the output of TbD and estimates the final trajectory which is continuous over the duration of the whole sequence.</p><p>All these methods assume that the object trajectory is parallel to the camera plane and that the object appearance is static within one frame (no rotation). The appearance can change between frames, but in arbitrary fashion as a longterm appearance template was learned online. The only exception is the work of Kotera andŠroubek <ref type="bibr" target="#b5">[6]</ref> that estimates object rotation, yet only 2D in-plane rotation is considered and the object shape is assumed to be known. The method is thus applicable only to nearly flat objects moving on a flat surface.</p><p>Deep learning has been applied to motion deblurring of videos <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref> and to the generation of intermediate shortexposure frames <ref type="bibr" target="#b3">[4]</ref>. Their proposed convolutional neural networks are trained only on small blurs; blur parameters are not available as they are not directly estimated. Tracking methods that consider blurred objects in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10]</ref> assume object motion that is approximately linear and relatively small compared to the object size. Alpha blending of the tracked object with the background is ignored and their output per frame is only a bounding box, which is insufficient for fast moving objects.</p><p>The tracking methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref> for fast moving objects use an image formation model that is defined as</p><formula xml:id="formula_0">I = H * F + (1 − H * M )B<label>(1)</label></formula><p>for a single color video frame I. The formation model is a mixture of two components. The first component is the object appearance F (after projection to the image plane) blurred by motion along the object trajectory, which is represented as a blur kernel H. The second part is the background B attenuated by object occlusion, where M , equivalent to the indicator function of F , is the object shape after projection to the image plane. Following <ref type="bibr" target="#b4">[5]</ref>, the blur is simplified to a 2D convolution. The background B is estimated as a median of the last 5 frames. They assume either an almost static camera or a stabilized sequence. The output of TbD-NC <ref type="bibr" target="#b13">[14]</ref> is a 2D object trajectory C f (t): [0, N ] → R 2 in an analytical form where N is the number of frames in the video sequence. This output is then used as an input to the proposed TbD-3D method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose the following pipeline to reconstruct a 6DoF pose of a fast moving object: <ref type="figure">Figure 3</ref>: TbD-3D applied to 30 fps videos compared to images from a high-speed camera at 240 fps (marked with * ). F : snapshots of object appearance estimates of fast moving objects. Each row corresponds to one sub-frame instant (red dot on a green trajectory) of the input frame on the left. For visualization purposes, the masks M are inverted.</p><formula xml:id="formula_1">Input F F * M M *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">From a given 2D trajectory, in our case computed</head><p>by the TbD-NC algorithm <ref type="bibr" target="#b13">[14]</ref>, reconstruct sub-frame blur-free snapshots of the object by piece-wise deblatting (Section 3.1).</p><p>2. Estimate the relative distance from the object to the camera from the estimated object shape mask (Section 3.2).</p><p>3. Using the assumption of a spherical object with locally constant rotation find the rotation axis and velocity by minimizing the reprojection error (Section 3.3).</p><p>An alternative method to estimate the 3D rotation of fast moving objects from their snapshots would be to run a classical 3D reconstruction pipeline such as COLMAP <ref type="bibr" target="#b15">[16]</ref>. We tried reconstruction and structurefrom-motion pipelines <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and none of them were able to deal with small objects containing few features. They do not perform well even on snapshots from a highspeed camera sequence, where the motion blur is negligible.</p><p>Tracking by Deblatting in 3D thus extends TbD and TbD-NC by using trajectories estimated by these methods to infer more attributes about the object and its motion. The core of TbD consists of two alternating optimization steps. The first step updates the object shape and appearance (F, M ) while the trajectory H is fixed, and the second one updates the trajectory H while the object (F, M ) is fixed. Both steps are formulated as convex optimization problems with non-smooth terms and constraints and solved Ground Truth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TbD-3D with Oracle</head><p>TbD-3D <ref type="figure">Figure 4</ref>: 3D trajectory estimation for selected sequences from the TbD-3D dataset. Curve thickness codes distance from the object to the camera (thicker curve means that the object is closer to the camera). TbD-3D with Oracle means that the 2D trajectory is estimated from the original high-speed footage and only the third dimension is estimated. Otherwise, the output of TbD-NC <ref type="bibr" target="#b13">[14]</ref> is used as the 2D trajectory. Sequences correspond to 30 fps.</p><p>using the ADMM method <ref type="bibr" target="#b0">[1]</ref>. Throughout processing of the video sequence, TbD maintains a long-term appearance modelF that is used to regularize the estimation of F in the new frame.</p><p>We have made two modifications to the TbD core steps. First, we added a new regularization term to the shape-andappearance (F, M ) estimation step to facilitate shape estimation in cases when the tracked object is a ball and its shape is thus circularly symmetric. The modified optimization problem is</p><formula xml:id="formula_2">min F,M 1 2 H * F + (1 − H * M )B − I 2 2 + λ 2 F −F 2 2 + α F ∇F 1 + λ R 2 RM − M 2 2 , (2) s.t. 0 ≤ F ≤ M ≤ 1,</formula><p>where matrix inequalities are considered element-wise. The first term is the data likelihood given by the image formation model <ref type="bibr" target="#b0">(1)</ref>. The second term constrains the solution to be close to the templateF and the third term is Total Variation that enforces piece-wise smooth object appearance. In the last, λ R -weighted term, R is a linear operator that performs circular averaging, i.e. the shape mask M is forced to be rotationally symmetric. Second, in the estimation of H we replaced the L 1 regularization of H by the constraint i H[i] = 1, which is free of weighting parameters that have to be tuned for different sequences. The modified optimization problem is then</p><formula xml:id="formula_3">min H 1 2 H * F + (1 − H * M )B − I 2 2 , s.t. H ≥ 0, i H[i] = 1.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Piece-wise Deblatting</head><p>TbD assumes that the appearance and shape of the object is constant during single frame exposure. In reality, the appearance changes even within a single video frame due to the object rotation and camera projection. We propose to approximately model this gradual change as a sequence of constant snapshots which we estimate. The snapshots can be used for temporal super-resolution and also to determine the intra-frame rotation of the object.</p><p>Suppose that the object trajectory in the form of a parametric curve C : [0, 1] → R 2 has been estimated for a given video frame. We partition this curve to multiple contiguous segments C i with their corresponding blurs denoted H i and estimate the appearance and shape (F i , M i ) of the object at the time interval corresponding to C i . From the piecewise constant appearance assumption, we get the formation model of the video frame I as  <ref type="table">Table 1</ref>: TbD-3D dataset -comparison of TbD <ref type="bibr" target="#b4">[5]</ref>, TbD-NC <ref type="bibr" target="#b13">[14]</ref> and the proposed TbD-3D. For each sequence, we report: TIoU-3D <ref type="bibr" target="#b14">(15)</ref> to measure the accuracy of 3D object location, radius error, axis error as the average angle between the estimated axis and the ground truth axis, and the angle error in degrees. For each sequence and each score, we highlight the best performing method in bold. TbD-3D-O means TbD-3D with oracle: the 2D object location is known from the ground truth. The TbD-3D dataset corresponds to 30 fps frame rate, 8 times lower than the ground truth data from the high speed camera. Results for other frame rates are shown in <ref type="figure">Figure 5</ref>.</p><formula xml:id="formula_4">I = i H i * F i + 1 − i H i * M i · B.<label>(4)</label></formula><p>The optimization problem (2) for joint estimation of</p><formula xml:id="formula_5">(F i , M i ) on segments C i becomes min Fi,Mi 1 2 i H i * F i + (1 − i H i * M i )B − I 2 2 + λ 2 F i −F i 2 2 + α F ∇F 1 + λ R 2 i RM i − M i 2 2 + γ F i F i − F i+1 1 + γ M i M i − M i+1 1 , (5) s.t. 0 ≤ F i ≤ M i ≤ 1.</formula><p>The last two terms, weighted by γ F and γ M , are new regularization terms enforcing similarity of both appearance and shape of the object in neighboring time intervals.F i is a sub-frame extension of the appearance template used in TbD, regularizing the appearance estimation in corresponding segments.</p><p>The piecewise appearance estimation is implemented in a hierarchical manner. First, we split C into two segments C 1 1 and C 1 2 (superscript denotes the hierarchy level) and solve (5) for F 1 1 , F 1 2 with both templatesF 1 1 =F 1 2 := F 0 where F 0 is the initial result of TbD. On the next level, we do another binary splitting of C 1 1 to C 2 1 , C 2 2 and C 1 2 to C 2 3 , C 2 4 and again solve (5) with templates set to results from the previous level,F 2 1 =F 2 2 := F 1 1 andF 2 3 =F 2 4 := F 1 2 . This process continues until the desirable number of splitting of C is achieved. Results of this estimation process are illustrated in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Trajectory</head><p>TbD-NC <ref type="bibr" target="#b13">[14]</ref> provides a 2D part of the estimated trajectory by fitting piece-wise polynomial curves. We extend this approach to fitting piece-wise polynomial curve in 3D, where the third dimension is the object distance to the camera. We assume that the object is approximately spherical with radius r, i.e. the area of mask defined as sum of all pixel values is area(M ) := i M [i] = πr 2 . The distance d is inversely proportional to the perceived object radius r and is given by</p><formula xml:id="formula_6">d ∝ π area(M ) .<label>(6)</label></formula><p>Note that the absolute distance can be calculated if we know camera parameters and the actual object radius. The estimated relative distances in sub-frame precision are expressed analytically by piece-wise continuous curve fitting. First, bounces are found as the ones initially estimated in 2D trajectory and then additional bounces which are only visible in 3D are added, e.g. during motion perpendicular to the camera plane. The bounces separate the trajectory into segments and in each segment we fit a polynomial of degree up to 6. The final trajectory is a function T (t):</p><formula xml:id="formula_7">[0, N ] ⊂ R → R 3</formula><p>where N is the number of frames. It is defined as</p><formula xml:id="formula_8">T (t) = ps k=0c s,k t k t ∈ [t s−1 , t s ], s = 1..S,<label>(7)</label></formula><p>with S polynomials, where polynomial with index s has degree p s and it is represented by its coefficient matrix c s ∈ R 3,ps+1 . Time stamps t s split the whole sequence into intervals between 0 and N , such that 0 = t 0 &lt; t 1 &lt; ... &lt; t S−1 &lt; t S = N . The degree of the polynomial depends on the number of frames it is fitted to; the interpolation scheme is similar to <ref type="bibr" target="#b13">[14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Location</head><p>2D Rotation Axis 1D Rotation Angle <ref type="figure">Figure 5</ref>: Evaluation of the TbD-3D method on the TbD-3D dataset with different frame rates. We report scores for three settings: 30, 60 and 120 fps. From left to right: TIoU-3D (15) of the proposed TbD-3D compared to the TbD <ref type="bibr" target="#b4">[5]</ref> and TbD-NC <ref type="bibr" target="#b13">[14]</ref> methods, error of rotation axis estimation, error of rotation angle estimation. The errors of rotation axis and angle are measured by a mean average angle between the estimate and the ground truth from the high-speed footage at 240 fps. Oracle with known 2D trajectory from ground truth is marked by "-O". The TbD-3D method performs better in the task of 3D location estimation and provides meaningful results for 3D rotation estimation w.r.t. the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Angular velocity</head><p>In the case of spherical objects, we are able to estimate their angular velocity ω ∈ R 3 . Following the standard definition in physics, ω is a 3D vector of the rotation axis orientation whose magnitude represents the rotation angle along the axis per time unit. Let R ω be an operator transforming a 2D image of a ball by performing 3D rotation given by ω of a virtual 3D representation of the ball. More specifically, if F 2 = R ω F 1 , then F 2 is given by mapping the 2D image F 1 to a virtual 3D sphere, rotating the sphere by ω and projecting back on the 2D image. The error of the transformation between the two images is defined as</p><formula xml:id="formula_9">E(F 1 , F 2 | ω) = R ω F 1 − F 2 1 .<label>(8)</label></formula><p>Since different parts of the ball are visible before and after rotation, the sum in eq. (8) is carried out only in some predefined region visible in both images after arbitrary considered rotation, so that errors corresponding to different rotations are mutually comparable.</p><p>Having recovered the object appearance F 1 and F 2 at two different video sequence timestamps t 1 and t 2 , we can find the average angular velocity ω between t 1 and t 2 as the minimizer of the transformation error E(F 1 , F 2 | (t 2 − t 1 )ω). Velocity estimation from just two restored images at close timestamps is prone to errors, especially if either of the images is estimated with artifacts. We therefore state an assumption that angular velocity is locally constant in small time interval of the motion (which is physically nearly correct even in the long term if the ball is in free flight) and estimate angular velocity more robustly in a slidingwindow manner from several restored images belonging to the corresponding time-window.</p><p>Let F 1 , . . . , F n be a set of estimated ball appearances at timestamps t 1 , . . . , t n ; a short time-window of the whole sequence. We estimate a single average angular velocity ω at this time-window as follows. Let ω ij be the minimizer of the transformation error from F i to F j and S ij inverse of the attained error ('score'):</p><formula xml:id="formula_10">ω ij = argmin ω E(F i , F j | (t j − t i )ω),<label>(9)</label></formula><formula xml:id="formula_11">S ij = 1 E(F i , F j | (t j − t i )ω ij ) + ε .<label>(10)</label></formula><p>In other words, ω ij is the vote of the corresponding image pair for the true ω and S ij is the confidence of such vote. We minimize (9) by searching the discretized space of feasible angular velocities. Simply averaging ω ij results in non-robust estimate that is sensitive to outliers. Instead we proceed with RANSAClike approach. Based on the discretization step used in the minimization of (9), an inlier threshold ρ is defined as the maximum acceptable error in determining ω. We treat ω ij 's as hypotheses for the final estimate ω and for each hypothesis calculate its consensus set C ij as</p><formula xml:id="formula_12">C ij = {(k, l) : ω kl − ω ij ≤ ρ} .<label>(11)</label></formula><p>The winning candidate ω pq is the one with the best total score of its consensus set,</p><formula xml:id="formula_13">(p, q) = argmax (i,j) (k,l)∈Cij S kl .<label>(12)</label></formula><p>The final estimate is then the weighted average of the votes in the consensus set of the winning candidate </p><formula xml:id="formula_14">ω = kl S kl ω kl kl S kl , (k, l) ∈ C pq .<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Kotera et al. <ref type="bibr" target="#b4">[5]</ref> introduced Trajectory Intersection over Union (TIoU) to measure the accuracy of estimated trajectories, which is defined as</p><formula xml:id="formula_15">TIoU(C, C * ) = t IoU M * C(t) , M * C * (t) dt,<label>(14)</label></formula><p>where M * C(t) corresponds to ground truth object appearance mask M * placed at a 2D point on either the estimated trajectory C(t) or ground truth trajectory C * (t). Integral is approximated by sum, sampling time at 8 evenly-separated instants. We extend this measure to 3D trajectories and define TIoU-3D as</p><formula xml:id="formula_16">TIoU-3D(T , T * ) = t IoU S * T (t) , S * T * (t) dt,<label>(15)</label></formula><p>where S * T (t) is a ball corresponding to the ground truth radius and located at T (t), a 3D point along trajectory T at time-stamp t. Similarly, T * stands for the ground truth trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TbD-3D Dataset</head><p>We created a new annotated dataset containing fast moving objects. All previous datasets with FMOs, such as FMO dataset <ref type="bibr" target="#b14">[15]</ref> and TbD dataset <ref type="bibr" target="#b4">[5]</ref>, included only objects moving in a 2D plane parallel to the camera plane and their appearance was close to static. Ground truth 2D object location was provided, but no angular velocity.</p><p>The introduced dataset is the first dataset with nonnegligible 3D object motion and with changing appearance of non-uniform fast moving objects. Objects are from a set of three balls with complex texture. The dataset is called TbD-3D and it contains nine sequences with annotated object location, pose, and size from a high-speed camera. In contrast to previous datasets, the perceived size of objects in TbD-3D dataset varies throughout the whole sequence due to depth of the scene, as shown in <ref type="figure">Figure 3</ref>.</p><p>Videos were recorded in raw format using a high-speed camera at 240 fps with exposure time 1/240s (so called 360 • shutter angle -negligible lag between two frames). The dataset sequences were generated by averaging 2, 4 and 8 frames, which corresponds to real videos captured at 30, 60, 120 fps, respectively. Ground truth annotation was done on the original raw footage at 240 fps. 3D object location (2D position and radius) was annotated manually and 3D object rotation was estimated using the proposed method in Section 3.3 and validation; see Section 4.2 for details about ground-truth annotation of the object rotation.</p><p>The proposed method is evaluated on the TbD-3D dataset for all three frame-rate settings. <ref type="figure">Figure 5</ref> shows accuracy of the estimated 6DoF object pose: 3D location error measured by TIoU-3D (left), 2D rotation axis error measured as a mean average deviation from the GT axis in degrees (middle) and mean average error of 1D rota-  tion angle (right). We use TbD <ref type="bibr" target="#b4">[5]</ref> and TbD-NC <ref type="bibr" target="#b13">[14]</ref> as baselines, which only estimate 2D trajectory. These methods ignore depth changes and assume one object size for the whole sequence. To show the performance of TbD-3D when the input 2D trajectory has no errors, we also provide scores of TbD-3D with oracle (TbD-3D-O) where we use 2D trajectory from the annotated 240-fps videos. TbD-3D-O estimates only additional 4DoF of object pose and compare to TbD-3D it performs better in average. The performance drop of TbD-3D can be thus attributed to errors in 2D trajectories estimated by TbD-NC. <ref type="table">Table 1</ref> provides more detailed comparison on every sequence at the lowest frame rate of 30 fps. Three examples of 3D trajectory reconstruction on sequences 'depth2', 'depthf1' and 'depthb2' are shown in <ref type="figure">Figure 4</ref> and one example of angular velocity estimation on sequence 'out2' is in <ref type="figure" target="#fig_4">Figure 7</ref>.</p><p>Acknowledgements. This work was supported by Czech Science Foundation grant GA18-05360S. Denys Rozumnyi was also</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Rotation Estimation</head><p>Calculating ground truth rotation of FMOs, even when the high speed camera footage is available, is a challenging task. To estimate the accuracy of the proposed method for rotation estimation (Section 3.3) when applied on highspeed footage, we captured sequences of a ball rolling on the ground along a straight trajectory of known length. The ground truth angular velocity is derived from physical properties of the rolling ball as we know its actual circumference and the distance it traverses. The average angle between the estimated rotation axis using the proposed method and the GT axis was 4.052 degrees. The average angle between the estimated and GT rotation angle was 0.037 degrees, which corresponds to 1.2 % relative error.</p><p>A special case appears during contact with another object in the scene. The object is deformed and modeling the object there is out of the scope of this paper. However, we can still detect such deformations as shown in <ref type="figure" target="#fig_2">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Applications</head><p>Temporal super-resolution is among the most interesting applications of the proposed method. First, a video free of FMOs is produced by replacing blurred objects with the estimated background. Second, a higher frame rate video is created by linear interpolation. Last, the trajectory is split into the desired number of segments and the object is blended into the sequence with its 6DoF appearance at desired snapshot time-scale, following the image formation model <ref type="bibr" target="#b0">(1)</ref>. Compared to the previous methods, which use the same appearance for all frames among one low rate trajectory, we synthesize the object at much higher temporal resolution. Videos generated using temporal superresolution are provided in the supplementary material.</p><p>Other applications and future work include for instance rotation estimation for tennis or table tennis serves, or full 3D reconstruction of the blurred object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a method for estimating up to 6DoF trajectory of fast moving objects which are severely blurred by object motion. The assumption of a non-uniform spherical object is needed, otherwise only a 3D object location is estimated. The proposed TbD-3D method achieves good results on a newly created dataset of non-uniform FMOs with significant changes of appearance and distance to the camera within the sequence or even a frame. Sub-frame appearance estimation enables us to see deformations which last shorter than the exposure duration. We showed a more precise temporal super-resolution compared to the previous methods. The dataset and implementation will be made publicly available. supported by Google Focused Research Award.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Deformations found using the TbD-3D method. They are not modeled explicitly, but are visible during contact with other objects in the scene. Left: input images with trajectories overlaid in green. Right: crops from high-speed camera footage (top), object appearance F and mask M reconstructions by the proposed TbD-3D method with the uniform split of trajectories. For this experiment, we set the term on rotational symmetry λ R in eq. (5) to zero. We estimate sub-frame snapshots using only the input frame on the left and the background. The trajectory is split into 8 (top row) or 25 (bottom row) segments. Deformation during a soft ball bounce is visible between the two red bars in the bottom row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Rotation velocity magnitude and direction in different parts of the sequence (color coded). TbD-3D resultssolid lines, ground truth -dashed. Rotation is estimated only between bounces.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view reconstruction preserving weakly-supported surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Jancosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3121" to="3128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiguang</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="6334" to="6342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Intra-frame Object Tracking by Deblatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipšroubek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion estimation and deblurring of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filipšroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2860" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukačehovin</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Eldesokey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018 Workshops</title>
		<editor>Laura Leal-Taixé and Stefan Roth</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni-Kristian</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukezic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time robust tracking for motion blur and fast motion via correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Xu Lingyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1443</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sba: A software package for generic sparse bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Manolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><forename type="middle">A</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Argyros</surname></persName>
		</author>
		<idno>2:1-2:30</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojíř</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4847" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Esmblur: Handling rendering blur in 3d tracking and augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woontack</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-Causal Tracking by Deblatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipšroubek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The world of fast moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Rozumnyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kotera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Filipšroubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Novotný</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4838" to="4846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep video deblurring for hand-held cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuochen</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning blind motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5596" to="5609" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superpixel-based tracking-by-segmentation using markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghun</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Hee</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
