<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARADE: Passage Representation Aggregation for Document Reranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
							<email>licanjia17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
							<email>ayates@mpi-inf.mpg.desean@ir.cs.georgetown.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">IR Lab</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
							<email>benhe@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<email>yfsun@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARADE: Passage Representation Aggregation for Document Reranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present PARADE, an end-to-end Transformer-based model that considers document-level context for document reranking. PARADE leverages passage-level relevance representations to predict a document relevance score, overcoming the limitations of previous approaches that perform inference on passages independently. Experiments on two ad-hoc retrieval benchmarks demonstrate PARADE's effectiveness over such methods. We conduct extensive analyses on PARADE's efficiency, highlighting several strategies for improving it. When combined with knowledge distillation, a PARADE model with 72% fewer parameters achieves effectiveness competitive with previous approaches using BERT-Base. Our code is available at https: //github.com/canjiali/PARADE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (PLMs), e.g., BERT <ref type="bibr" target="#b16">(Devlin et al., 2019)</ref>, ELECTRA <ref type="bibr" target="#b9">(Clark et al., 2020)</ref> and T5 <ref type="bibr" target="#b44">(Raffel et al., 2019)</ref>, have achieved state-of-the-art results on standard ad-hoc retrieval benchmarks and in many NLP tasks. The success of PLMs mainly relies on learning contextualized representations of input sequences using the Transformer <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref>. The Transformer uses a self-attention mechanism whose computational complexity is quadratic with respect to the input sequence's length, so PLMs generally limit the sequence's length (e.g., to 512 tokens) to reduce computational costs. Consequently, when applied to the ad-hoc ranking task, PLMs are commonly used to predict the relevance of passages or individual sentences. <ref type="bibr" target="#b13">(Dai and Callan, 2019b;</ref>. The max or k-max passage scores (e.g., top 3) are then aggregated to produce a document relevance score. Such approaches have achieved state-of-the-art results on a variety of ad-hoc retrieval benchmarks.</p><p>Documents are often much longer than a single passage, however, and intuitively there are many types of relevance signals that can only be observed in a full document. For example, the Verbosity Hypothesis <ref type="bibr" target="#b45">(Robertson and Walker, 1994)</ref> states that relevant excerpts can appear at different positions in a document. It is not necessarily possible to account for all such excerpts by considering only the top passages. Similarly, the ordering of passages itself may affect a document's relevance; a document with relevant information at the beginning is intuitively more useful than a document with the information at the end . On the other hand, the amount of non-relevant information in a document can also be a signal, because relevant excerpts would make up a large fraction of an ideal document. IR Axioms encode this idea in the first length normalization constraint (LNC1), which states that adding non-relevant information to a document should decrease its score <ref type="bibr" target="#b18">(Fang et al., 2011)</ref>. Considering a full document as input has the potential to incorporate signals like these. Furthermore, from the perspective of training a supervised ranking model, the common practice of applying document-level relevance labels to individual passages is undesirable, because it introduces unnecessary noise into the training process.</p><p>Empirical studies support the importance of fulldocument signals. <ref type="bibr">Wu et al. study</ref> how passagelevel relevance labels correspond to document-level labels, finding that more relevant documents also contain a higher number of relevant passages . Additionally, experiments in several works suggest that aggregating passage-level relevance scores to predict the document's relevance score outperforms the common practice of using the maximum passage's score <ref type="bibr">(Bendersky and Kur-land, 2008;</ref><ref type="bibr" target="#b17">Fan et al., 2018;</ref><ref type="bibr" target="#b0">Ai et al., 2018)</ref>.</p><p>In this work, we study how PLMs like BERT can be applied to the ad-hoc document ranking task while preserving many document-level signals. To this end, we propose PARADE, an end-to-end document reranking model. PARADE predicts a document's relevance by learning passage-level relevance representations that are aggregated in a way that preserves document-level context. These aggregation approaches include 1) a passage weighting method, 2) a pooling technique, and 3) using the Transformer in a hierarchical way. PARADE is optimized end-to-end at the document level, which eliminates the noise introduced by using the document relevance label as a proxy for passage relevance labels. Since the utilization of full-text causes more memory usage, we investigate using knowledge distillation to create smaller, more efficient PARADE models that remain effective.</p><p>In the recent TREC-COVID challenge that studies the problem of identifying literature relevant to COVID-19 information needs, PARADE performed well and was among the top positions in the second round (as measured by nDCG@10). The details of our TREC-COVID submissions can be found in Appendix A.1.</p><p>Our contributions are threefold:</p><p>• The proposal of the end-to-end PARADE method for predicting a document's relevance by aggregating passage representations, • An evaluation on standard TREC ad-hoc benchmark collections confirming the effectiveness of our approach, and • Analyses of how PARADE's efficiency can be improved by decreasing the model size, and of how its effectiveness is influenced by the number of passages considered and by the initial ranking method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We review three lines of related research. Contextualized Language Models for IR. Neural IR models like DSSM <ref type="bibr" target="#b26">(Huang et al., 2013)</ref>, DRMM <ref type="bibr" target="#b20">(Guo et al., 2016)</ref>, (Co-)PACRR <ref type="bibr" target="#b27">(Hui et al., 2017</ref>, and (Conv-)KNRM <ref type="bibr" target="#b59">(Xiong et al., 2017;</ref><ref type="bibr" target="#b14">Dai et al., 2018)</ref> have been proposed for the ad-hoc retrieval task. However, their contextual capacity is limited by using pre-trained unigram embeddings. Benefiting from BERT's pre-trained contextual embeddings, BERT-based IR models have been shown to be superior to neural IR mod-els. Nogueira et al. first adopted BERT to passage reranking tasks  using BERT's [CLS] vector. Birch  and BERT-MaxP <ref type="bibr" target="#b13">(Dai and Callan, 2019b)</ref> explore the sentence-level and passage-level relevance signals using BERT for document reranking, respectively. CEDR proposed a joint approach that combines BERTs outputs with existing neural IR models <ref type="bibr" target="#b39">(MacAvaney et al., 2019)</ref>. Other researchers trade off PLM effectiveness for efficiency by utilizing the PLM to improve document indexing <ref type="bibr" target="#b12">Dai and Callan, 2019a)</ref>, pre-computing intermediate Transformer representations <ref type="bibr" target="#b31">(Khattab and Zaharia, 2020;</ref><ref type="bibr" target="#b37">MacAvaney et al., 2020a;</ref><ref type="bibr" target="#b19">Gao et al., 2020;</ref><ref type="bibr" target="#b29">Humeau et al., 2020)</ref>, using the PLM to build sparse representations <ref type="bibr" target="#b38">(MacAvaney et al., 2020b)</ref>, or reducing the number of Transformer layers <ref type="bibr">(Hofstätter et al., 2020b,a)</ref>.</p><p>While several works have recently investigated approaches for improving the Transformer's efficiency by reducing the computational complexity of its attention module, none of these approaches have been shown to be effective for the document ranking task. The Sparse Transformer <ref type="bibr" target="#b8">(Child et al., 2019)</ref> and Reformer <ref type="bibr" target="#b32">(Kitaev et al., 2020)</ref> focus on text generation. We were unable to effectively use Transformer-XL <ref type="bibr" target="#b15">(Dai et al., 2019)</ref> in pilot experiments, while Longformer <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref> is an interesting contemporaneous work. We note that PARADE could be used in conjunction with such models.</p><p>Passage-based Document Retrieval. Given the increasing lengths of documents in full-text collections, Callan first experimented with paragraphbased and window-based methods of defining passages <ref type="bibr" target="#b5">(Callan, 1994)</ref>. Several works drive passagebased document retrieval in the language modeling context <ref type="bibr" target="#b36">(Liu and Croft, 2002;</ref><ref type="bibr" target="#b4">Bendersky and Kurland, 2008)</ref>, indexing context <ref type="bibr" target="#b35">(Lin, 2009)</ref>, and learning to rank context <ref type="bibr" target="#b48">(Sheetrit et al., 2020)</ref>. In the realm of neural networks, HiNT demonstrated that aggregating representations of passage level relevance can perform well in the context of pre-BERT models <ref type="bibr" target="#b17">(Fan et al., 2018)</ref>. Others have investigated sophisticated evidence aggregation approaches <ref type="bibr" target="#b63">(Zhao et al., 2020;</ref>. Wu et al. explicitly modeled the importance of passages based on position decay, passage length, length with position decay, exact match, etc . In a contemporaneous study, they proposed a model that considers passage-level representations of relevance in order to predict the passage-level cumulative gain of each passage <ref type="bibr" target="#b57">(Wu et al., 2020)</ref>. In this approach the final passage's cumulative gain can be used as the document-level cumulative gain. Our approaches share some similarities, but theirs differs in that they use passagelevel labels to train their model and perform passage representation aggregation using a LSTM.</p><p>Knowledge Distillation. Knowledge distillation is the process of transferring knowledge from a large model to a smaller student model <ref type="bibr" target="#b1">(Ba and Caruana, 2014;</ref><ref type="bibr" target="#b21">Hinton et al., 2015)</ref>. Ideally, the student model performs well while consisting of fewer parameters. One line of research investigates the use of specific distilling objectives for intermediate layers in the BERT model <ref type="bibr" target="#b30">(Jiao et al., 2019;</ref>. Turc et al. pre-train a family of compact BERT models and explore transferring task knowledge from large fine-tuned models <ref type="bibr" target="#b52">(Turc et al., 2019)</ref>. <ref type="bibr">Tang et al. distill</ref> knowledge from the BERT model into Bi-LSTM <ref type="bibr" target="#b51">(Tang et al., 2019)</ref>. Tahami et al. propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model for fast retrieval <ref type="bibr">(Tahami et al., 2020)</ref>. We demonstrate this approach can be applied to PARADE to improve efficiency without substantial reductions in effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present the proposed PARADE method for end-to-end document reranking. Given a query q and a document D, a ranking method aims to generate a relevance score rel(q, D) that estimates to what degree document D satisfies the query q. As described in the following sections, we perform this relevance estimation by aggregating passage-level relevance representations into a document-level representation, which is then used to produce a relevance score. Representing a Document as Passages. As introduced in Section 1, a long document cannot be considered directly by the BERT model due to its fixed sequence length limitation. Following <ref type="bibr" target="#b13">(Dai and Callan, 2019b;</ref><ref type="bibr" target="#b5">Callan, 1994)</ref>, we split a document into passages that can be handled by BERT individually. To do so, a sliding window of 150 words is applied to the document with a stride of 100 words, formally expressed as D = {P 1 , . . . , P n } where n is the number of passages. Afterward, these passages are taken as input to the BERT model for relevance estimation.</p><p>Creating Passage Relevance Representations. Following prior work , we concatenate a pair of query q and passage P i with a [SEP] token in between and another [SEP] token at the end. The special [CLS] token is also prepended, in which the corresponding output in the last layer is parameterized as a relevance representation p cls i ∈ R d , denoted as follows:</p><formula xml:id="formula_0">p cls i = BERT(q, P i )<label>(1)</label></formula><p>Aggregating Passage Relevance Representations. Given the passage relevance representations D cls = {p cls 1 , . . . , p cls n }, PARADE summarizes D cls into a single dense representation d cls ∈ R d in three different ways, coined as PARADE Max , PARADE Attn , and PARADE.</p><p>PARADE Max utilizes a robust max-pooling operation on the passage relevance features 1 in D cls . As widely applied in Convolution Neural Network, max-pooling has been shown to be effective in obtaining position-invariant features <ref type="bibr" target="#b47">(Scherer et al., 2010)</ref>. Herein, each element at index j in d cls is obtained by a element-wise max-pooling operation on the passage relevance representations over the same index.</p><formula xml:id="formula_1">d cls [j] = max(p cls 1 [j], . . . , p cls n [j])<label>(2)</label></formula><p>PARADE Attn assumes that each passage contributes differently to the relevance of a document to the query. A simple yet effective way to learn the importance of a passage is to apply a feed-forward network to predict passage weights:</p><formula xml:id="formula_2">w 1 , . . . , w n = softmax(W p cls 1 , . . . , W p cls n ) (3) d cls = n i=1 w i p cls i (4)</formula><p>where softmax is the normalization function and W ∈ R d is a learnable weight. For completeness of study, we also introduce a PARADE Avg that simply averages the passage relevance representations. This can be regarded as manually assigning equal weights to all passages (i.e., w i = 1/n). PARADE Transformer , which as shorthand we simply call PARADE, enables passage relevance representations to interact by adopting the Transformer <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref> in a hierarchical way. Specifically, BERT's [CLS] 2 token embedding and all p cls i are concatenated, resulting in an input x l = (emb cls , p cls 1 , . . . , p cls n ) that is consumed by Transformer layers to exploit the ordering of and dependencies among passages. That is,</p><formula xml:id="formula_3">h = LayerNorm(x l + MultiHead(x l ) (5) x l+1 = LayerNorm(h + FFN(h))<label>(6)</label></formula><p>where LayerNorm is the layer-wise normalization as introduced in <ref type="bibr" target="#b2">(Ba et al., 2016)</ref>, MultiHead is the multi-head self-attention <ref type="bibr" target="#b53">(Vaswani et al., 2017)</ref>, and FFN is a two-layer feed-forward network with a ReLu activation in between. The [CLS] vector of the last Transformer output layer, regarded as a pooled representation of the relevance between query and the whole document, is taken as d cls . The sequence length of the Transformer layers in PARADE is equal to the number of passages used in a document, usually dozens, hence this approach adds only a small amount of computation compared with PARADE Attn and PARADE Max . Generating the Relevance Score. For all three PARADE variants, after obtaining the final d cls embedding, a single-layer feed-forward network is adopted to generate a relevance score, as follows:</p><formula xml:id="formula_4">rel(q, D) = W d d cls<label>(7)</label></formula><p>where W d ∈ R d is a learnable weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We experiment with two ad-hoc collections: Ro-bust04 3 and <ref type="formula" target="#formula_1">GOV2</ref>   making it more challenging to train an end-to-end ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare PARADE against the following traditional and neural baselines:</p><p>BM25 is an unsupervised ranking model based on IDF-weighted counting <ref type="bibr" target="#b46">(Robertson et al., 1995)</ref>. The documents retrieved by BM25 also serve as the candidate documents used with reranking methods.</p><p>BM25+RM3 is a query expansion model based on RM3 <ref type="bibr" target="#b33">(Lavrenko and Croft, 2001)</ref>. We used Anserini's <ref type="bibr" target="#b61">(Yang et al., 2018)</ref> implementations of BM25 and BM25+RM3. Documents are indexed and retrieved with the default settings for keywords queries. For description queries, we set b = 0.6 and changed the number of expansion terms to 20.</p><p>Birch (MS) and Birch (MS→ MB) aggregate sentence-level evidence provided by BERT to rank documents . Birch (MS) is the fine-tuned BERT-Large model on the MSMARCO passage dataset while Birch (MS→MB) is further fine-tuned on TREC MicroBlog datasets. We use BM25 rather than BM25+RM3 as an initial ranking method for a fair comparison.</p><p>BERT-MaxP (MS) adopts the maximum score of passages within a document as an overall relevance score <ref type="bibr" target="#b13">(Dai and Callan, 2019b)</ref>. However, rather than fine-tuning BERT-base on a Bing search log, we improve performance by fine-tuning on the MSMARCO passage ranking dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training PARADE</head><p>To prepare the BERT model for the ranking task, we first fine-tune BERT on the MSMARCO passage ranking dataset <ref type="bibr" target="#b40">(Nguyen et al., 2016</ref>  </p><formula xml:id="formula_5">0.4251 † 0.4917 † 0.4482 † ‡ 0.5324 † ‡ 0.6107 † 0.5362 † 0.5872 † 0.5288 † PARADE Max 0.4432 † § 0.5115 † § 0.4657 † ‡ § 0.5487 † ‡ 0.6319 † 0.5399 † 0.6148 † 0.5419 † PARADE Attn 0.4410 † § 0.5134 † § 0.4614 † ‡ § 0.5517 † ‡ 0.6319 † 0.5554 † 0.6198 † ‡ 0.5513 † PARADE 0.4486 † § 0.5252 † § 0.4661 † ‡ § 0.5605 † ‡ § 0.6530 † § 0.5750 † 0.6299 † ‡ § 0.5674 † ‡ PARADE (ELECTRA) 0.4604 † ‡ § 0.5399 † ‡ § 0.4717 † ‡ § 0.5713 † ‡ § 0.6678 † ‡ § 0.5851 † ‡ 0.6470 † ‡ § 0.5762 † ‡</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>Following prior work <ref type="bibr" target="#b13">(Dai and Callan, 2019b;</ref><ref type="bibr" target="#b39">MacAvaney et al., 2019)</ref>, we use 5-fold crossvalidation. We set the reranking threshold to 100 on the test fold as trade-off between latency and effectiveness. The reported results are based on the average of all test folds. Performance is measured in terms of the P@20 and nDCG@20 ranking metrics using trec eval 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>The reranking effectiveness of PARADE is shown in <ref type="table" target="#tab_3">Table 2</ref>. It can be seen that the performance of PARADE Max and PARADE Attn is comparable, while nDCG@20 of PARADE Attn can always surpass PARADE Max . PARADE Avg underperforms other models by a large margin, which confirms that passages differ in their contributions to the overall relevance of a document. PARADE consistently outperforms the other models across both datasets, suggesting that the multi-head selfattention mechanism in the Transformer is a superior method for passage-level relevance aggregation.</p><p>Compared with other baseline models, Birch has two innate advantages: it uses the BERT-Large model with 3x more parameters than BERT-Base, 5 https://trec.nist.gov/trec_eval and it is an ensemble model that additionally considers the original ranking scores. Nevertheless, PARADE still outperform it, especially on description queries. 6 For BERT-MaxP, the reported results are better than those reported in <ref type="bibr" target="#b13">(Dai and Callan, 2019b)</ref> with approximately a 0.02 nDCG@20 increase on Robust04 title queries. On the Robust04 collection with deeper judgments, PARADE outperforms BERT-MaxP significantly.</p><p>When applying PARADE to the more recent and efficiently trained LM model ELECTRA-Base <ref type="bibr" target="#b9">(Clark et al., 2020)</ref>, PARADE's performance increases substantially. This model significantly improves over all baselines on nDCG@20 for the Robust04 collection and P@20 for both collections. These results illustrate that as Transformer pretraining techniques advance, PARADE is able to take advantage of improved pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we consider the following research questions:</p><p>• RQ1: How can BERT's efficiency be improved while maintaining its effectiveness? • RQ2: How does the number of document passages preserved influence effectiveness? • RQ3: Is it beneficial to rerank documents from a more effective initial ranking method? In particular, is reranking BM25+RM3 better than reranking BM25? Additionally, we study the effectiveness of PA-RADE on the TREC-COVID Challenge in Ap-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reranking Effectiveness vs. Efficiency</head><p>While BERT-based models are effective at producing high-quality ranked lists, they are computationally expensive. However, the reranking task is sensitive to efficiency concerns, because documents must be reranked in real time after the user issues a query. In this section we consider two strategies for improving PARADE's efficiency, which also answers RQ1. Using a Smaller BERT Variant. As smaller models require fewer computations, we study the reranking effectiveness of PARADE when using pre-trained BERT models of various sizes, providing an important guidance for deploying a retrieval system. Pre-trained BERT models of various sizes were provided by <ref type="bibr" target="#b52">(Turc et al., 2019)</ref>. From <ref type="table" target="#tab_5">Table 3</ref>, it can be seen that as the size of models is reduced, their effectiveness decline monotonously. The hidden layer size (#6 vs #7, #8 vs #9) plays a more critical role for performance than the number of layers (#3 vs #4, #5 vs #6). An example is the comparison between models #7 and #8. Model #8 performs better; it has fewer layers but contains more parameters. The number of parameters and inference time are also given in <ref type="table" target="#tab_5">Table 3</ref> to facilitate the study of tradeoffs between model complexity and effectiveness.</p><p>Distilling Knowledge from a Large Model. To further explore the limits of smaller PARADE models, we apply knowledge distillation to leverage knowledge from a large teacher model. We use PA-RADE trained with BERT-Base on the target collection as the teacher model. Smaller student models then learn from the teacher at the output level. We use mean squared error as the distilling objective, which has been shown to work effectively <ref type="bibr">(Tahami et al., 2020;</ref><ref type="bibr" target="#b51">Tang et al., 2019)</ref>. The learning objective penalizes the student model based on both the ground-truth and the teacher model:</p><formula xml:id="formula_6">L = α · L CE + (1 − α) · ||z t − z s || 2 (8)</formula><p>where L CE is the cross-entropy loss with regard to the logit of the student model and the ground truth, α weights the importance of the learning objectives, and z t and z s are logits from the teacher model and student model, respectively.</p><p>As shown in <ref type="table" target="#tab_5">Table 3</ref>, the nDCG@20 of distilled models always increases. The PARADE model using 8 layers (#4) can achieve comparable results with the teacher model. Moreover, the PARADE model using 10 layers (#3) can outperform the teacher model with 11% fewer parameters. The PA-RADE model trained with BERT-Small achieves a nDCG@20 above 0.5, which outperforms BERT-MaxP using BERT-Base, while requiring only 1.14 ms to perform inference on one document. The inference time for each query is only 0.114 second by reranking top 100 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Number of Passages Considered</head><p>One hyper-parameter in PARADE is the maximum number of passages being used, i.e., preserved data size, which is studied to answer RQ2 in this section. We consider title queries on the GOV2 dataset given that these documents are longer on average than in Robust04. <ref type="figure" target="#fig_0">Figure 1 depicts nDCG@20</ref> of PARADE with the number of passages varying from 8 to 64. Generally, larger preserved data size results in better performance for PARADE, which suggests that a document can be better un-  derstood from document-level context with more preservation of its content. For PARADE Max and PARADE Attn , however, the performance degrades a little when using 64 passages. Both max-pooling (Max) and simple attention mechanism (Attn) have limited capacity and are challenged when dealing with such longer documents. PARADE is able to improve nDCG@20 as the number of passages increases, demonstrating its superiority in identifying relevant and non-relevant documents when documents become much longer. However, considering more passages also increases the number of computations performed. One advantage of the PARADE models is that the number of parameters remains constant as the number of passages in a document varies. Thus, we consider the impact of varying the number of passages considered between training and inference. As shown in <ref type="table" target="#tab_6">Table 4</ref>, rows indicate the number of passages considered at training time while columns indicate the number used to perform inference. The diagonal indicates that preserving more of the passages in a document consistently improves nDCG. Similarly, increasing the number of passages considered at inference time (columns) or at training time (rows) usually improves nDCG. In conclusion, the number of passages considered plays a crucial role in PARADE's effectiveness. When trading off efficiency for effectiveness, PARADE models' effectiveness can be improved by training on more passages than will be used at inference time. This generally yields a small nDCG increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Understanding Reranking Behavior</head><p>Query expansion methods based on pseudorelevance feedback, like RM3 <ref type="bibr" target="#b33">(Lavrenko and Croft, 2001)</ref> and NPRF <ref type="bibr" target="#b34">(Li et al., 2018)</ref>, have been shown to increase the effectiveness of a search system. The use of PRF methods in prior work on BERT ranking models varies, however. Thus, in this section we consider the question (i.e., RQ3) of whether reranking a stronger initial ranking method (e.g., RM3) improves retrieval results. To do so, we compare the reranking effectiveness of PARADE on top of BM25 and BM25+RM3. To simplify the analysis, we focus on the ranking distribution of relevant documents. On the Robust04 dataset with title queries, we examine the top 1,000 documents retrieved by BM25 and BM25+RM3. We then divide all relevant documents retrieved into three partitions, D both , D BM 25 and D QE , defined as follows: • D both : the relevant documents retrieved by both BM25 and BM25+RM3 • D BM 25 : the relevant documents retrieved by BM25 but not retrieved by BM25+RM3 • D QE : the relevant documents retrieved by BM25+RM3 but not retrieved by BM25 For all methods, D both is the same; differences come from D BM 25 , D QE , and non-relevant documents. In total, Count(D both ) = 9863, Count(D QE ) = 1538, and Count(D BM 25 ) = 409, which means that BM25 and BM25+RM3 share a large number of relevant documents.</p><p>Different from the previous setting, we set the reranking threshold to 1,000 to increase recall. The most effective PARADE is adopted as a reranker. The (re-)ranking effectiveness of these models is shown in <ref type="table" target="#tab_8">Table 5</ref>. Replacing BM25 with BM25+RM3 increases Recall@1k by about 8% and MAP@1k by about 4%, which may be a result of the nearly 1,000 relevant documents introduced by RM3. The differences for the other metrics are minor, with RM3 slightly reducing P@20. These findings are in line with recent work demonstrating that there is little difference in effec-   tiveness between reranking BM25 and reranking BM25+RM3 <ref type="bibr" target="#b42">(Nogueira et al., 2020)</ref>.</p><p>To investigate why there is little difference between reranking BM25 and BM25+RM3 for metrics considering top positions, we provide four sub-figures in <ref type="figure" target="#fig_1">Figure 2</ref> that depict the number of relevant documents placed in different position bins (averaged by the number of queries). <ref type="figure" target="#fig_1">Figures 2a, 2b, 2c, 2d</ref> depict the ranking distribution of BM25+RM3, PARADE (reranking BM25+RM3), BM25, PARADE (reranking BM25), respectively. Due to the change in bin size from 10 to 100, there is a steep increase in the bin 101-200 across all figures. The distribution is mono-decreasing if the bin size is unchanged. It can be seen that:</p><p>• From figures 2a and 2c, the documents from D QE and D BM 25 are more likely to be ranked at the low positions (behind 100) by the initial ranking models, which suggests that both models are less confident in these documents. For BM25+RM3, it might be that the documents from D QE are mostly retrieved by the expanded terms; for BM25, it may be these documents are retrieved by terms with lower weights. • Comparing <ref type="figure" target="#fig_1">Figure 2a</ref> with 2b, as well as <ref type="figure" target="#fig_1">Figure 2c</ref> with 2d, the documents from D QE and D BM 25 can be boosted to higher positions by PARADE. Mostly, documents in D QE are retrieved using the expanded terms. PARADE can boost these documents without even knowing these terms, which confirms contextualization benefits by BERT. • Comparing <ref type="figure" target="#fig_1">Figure 2c</ref> with 2d, it can be seen that a large amount of documents from D BM 25 , especially the documents behind position 100, are boosted to higher positions, which closes the large gap in MAP between BM25 and BM25+RM3 as shown in <ref type="table" target="#tab_8">Table 5</ref>. The advantage of using BM25+RM3 may be that its relevance scores are good source for model ensemble. As shown in <ref type="table" target="#tab_8">Table 5</ref>, an ensemble method that linearly interpolates the scores achieves the best results. In conclusion, while BM25+RM3 does retrieve more relevant documents than BM25, these documents are not effectively utilized by the reranking methods. BM25+RM3 is thus more of a reranking method than an initial ranking method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed the PARADE end-to-end document reranking model and demonstrated its effectiveness on two TREC ad-hoc benchmark collections.</p><p>Our results indicate the importance of incorporating diverse relevance signals from the full text into ad-hoc ranking, rather than basing it on a single passage. We additionally investigated how model size and the initial ranking method affect performance. Knowledge distillation on PARADE boosts the performance of smaller PARADE models while substantially reducing their parameters.    In response to the urgent demand for reliable and accurate retrieval of COVID-19 academic literature, TREC has been developing the TREC-COVID challenge to build a test collection during the pandemic <ref type="bibr" target="#b54">(Voorhees et al., 2020)</ref>. The challenge uses the CORD-19 data set <ref type="bibr">(Wang et al., 2020a)</ref>, which is a dynamic collection enlarged over time. There are supposed to be 5 rounds for the researchers to iterate their systems. TREC develops a set of COVID-19 related topics, including queries (keyword based), questions, and narratives. A retrieval system is supposed to generate a ranking list corresponding to these queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>We began submitting PARADE runs to TREC-COVID from Round 2. The Round 5 results are not yet available at the time of writing. By using PARADE, we are able to utilize the full-text of the COVID-19 academic papers. We used the question topics since it works much better than other types of topics. In all rounds, we employ the full PARADE model. In Round 3, we additionally tested PARADE Attn and a combination of PARADE and PARADE Attn using reciprocal rank fusion <ref type="bibr" target="#b10">(Cormack et al., 2009)</ref>.</p><p>Results from TREC-COVID Rounds 2-4 are shown in <ref type="table" target="#tab_10">Table 6</ref>, <ref type="table" target="#tab_11">Table 7, and Table 8</ref> <ref type="bibr">, respectively. 7</ref> In Round 2, PARADE achieves the highest nDCG, further supporting its effectiveness. 8 In Round 3, our runs are not as competitive as the previous round. One possible reason is that the collection doubles from Round 2 to Round 3, which can introduce more inconsistencies between training and testing data as we trained PARADE on Round 2 data and tested on Round 3 data. In particular, our run mpiid5 run3 performed poorly. We found that it tends to retrieve more documents that are not likely to be included in the judgment pool. When considering the bpref metric that takes only the judged documents into account, its performance is comparable to that of the other variants. As measured by nDCG, PARADE's performance improved in Round 4 <ref type="table" target="#tab_12">(Table 8</ref>), but is again outperformed by other approaches. It is worth noting that the PARADE runs were created by single models (excluding the fusion run from Round 3), whereas e.g. the UPrrf38rrf3-r4 run in Round 4 is an ensemble of more than 20 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results on the TREC 2019 DL Document Ranking Task</head><p>The MSMARCO document ranking dataset 9 is a large-scale collection and is used in TREC 2019 Deep Learning track <ref type="bibr" target="#b11">(Craswell et al., 2019)</ref> 0.237 0.517 ucas runid1  0.264 0.644 TUW19-d3-re <ref type="bibr" target="#b23">(Hofstätter et al., 2019)</ref>    MaxP <ref type="bibr" target="#b13">(Dai and Callan, 2019b)</ref> as the reranking method, TUW19-d3-re <ref type="bibr" target="#b23">(Hofstätter et al., 2019)</ref> is a Transformer-based non-BERT method, and idst bert r1 <ref type="bibr" target="#b60">(Yan et al., 2019)</ref> utilizes struct-BERT <ref type="bibr" target="#b56">(Wang et al., 2020b)</ref>, which is intended to strengthen the modeling of sentence relationships. All PARADE variants outperform ucas runid1 and TUW19-d3-re in terms of nDCG@10, but cannot outperform idst bert r1. Since this run's pre-trained structBERT model is not publicly available, we are not able to embed it into PARADE and make a fair comparison. In contrast with the previous results, the other variants outperform PARADE in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Effectiveness of Domain Adaptation</head><p>As previously described, the BERT model used in PARADE is fine-tuned on the MSMARCO passage ranking dataset before being embedded into PA-RADE. This training set consists of approximately 400M tuples of query, relevant passage, and nonrelevant passage. The dev set and test set consist of approximately 6,900 and 6,800 queries, respectively. For each passage, we use BERT's [CLS] vector as in Equation 1 to a single-layer feed-forward network to obtain the probability of being relevant. We follow the training setup in  and fine-tune the model with a batch size of 32 for 400k iterations. After that, the fine-tuned model is used as weight initialization in the BERT layers of PARADE.</p><p>As mentioned in , finetuning BERT on different domains can result in different model effectiveness. We verify the performance of PARADE using the BERT models fine-tuned on the above mentioned MSMARCO domain as well as Bing search log 10 . Results on Robust04 when using the original BERT model, a BERT model fine-tuned on Bing search logs, and using a BERT model fine-tuned on MSMARCO are shown in <ref type="table" target="#tab_1">Table 10</ref>. It can be seen that fine-tuning on MSMARCO outperforms the other approaches by a large margin. Considering the model size, BERT-Base shows comparable ability to BERT-Large while requiring fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hyper-parameters</head><p>In our pilot study, we tune learning rates from {1e-6, 3e-6, 5e-6, 1e-5, 5e-5}, α for knowledge distillation from {0.25, 0.5, 0.75}, numbers of Transformer layers from 1 to 4, and the numbers of training epochs from 1 to 10. Then we fix the learning rate as 3e-6, the number of Transformer layers as 2, the number of training epochs as 3, and α as 0.75 for all experiments. For PARADE, the configuration of Transformer layers (e.g., number of attention heads, hidden size, etc.) is the same as the Transformer block being used in BERT.</p><p>Documents are split into passages. We set the maximum number of passages in each document as 16 and 32 for Robust04 and GOV2 respectively. As we split the documents using a sliding window of 150 words with a stride of 100 words, a maximum number of 1650 words in each document are retained on the Robust04 collection while 3250 on GOV2. The maximum sequence length in BERT is set as 256. When running PARADE, documents with less number of required passages are padded and later masked out by passage level masks. For documents longer than required, the first and last passages are always kept while the remaining are selected using a uniform sampling strategy as in <ref type="bibr" target="#b13">(Dai and Callan, 2019b)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Reranking effectiveness of PARADE when different number of passages are being used on Gov2 title dataset. nDCG@20 is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(Re)ranking distributions by different models. The X-axis represents the ranking position bins while Y-axis represents the average number of relevant documents dropped in each bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Collection statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). The fine-tuned BERT model is then used to initialize PARADE's BERT component. Training of PA-RADE was performed on a single Google TPU v3-8 using a cross entropy loss where rel(q, D) in Equation 7 is the logits. We train on the top 1,000 documents returned by BM25; documents that are labeled relevant in the ground-truth are taken as positive samples and all other documents as negative samples. We train PARADE for 3 epochs with batches of 32 training instances. Each instance</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Robust04</cell><cell></cell><cell></cell><cell></cell><cell>GOV2</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Title</cell><cell cols="2">Description</cell><cell></cell><cell>Title</cell><cell cols="2">Description</cell></row><row><cell>Model</cell><cell>P@20</cell><cell cols="2">nDCG@20 P@20</cell><cell cols="2">nDCG@20 P@20</cell><cell cols="2">nDCG@20 P@20</cell><cell>nDCG@20</cell></row><row><cell>BM25</cell><cell>0.3631</cell><cell>0.4240</cell><cell>0.3345</cell><cell>0.4058</cell><cell>0.5362</cell><cell>0.4774</cell><cell>0.4705</cell><cell>0.4264</cell></row><row><cell>BM25+RM3</cell><cell>0.3821</cell><cell>0.4407</cell><cell>0.3661</cell><cell>0.4255</cell><cell>0.5634</cell><cell>0.4851</cell><cell>0.4966</cell><cell>0.4212</cell></row><row><cell>Birch (MS)</cell><cell>0.3616</cell><cell>0.4227</cell><cell>0.3341</cell><cell>0.4053</cell><cell>0.5352</cell><cell>0.4722</cell><cell>0.4701</cell><cell>0.4260</cell></row><row><cell>Birch (MS→MB)</cell><cell>0.4404</cell><cell>0.5137</cell><cell>0.4211</cell><cell>0.5069</cell><cell>0.6409</cell><cell>0.5608</cell><cell>0.5973</cell><cell>0.5307</cell></row><row><cell>BERT-MaxP (MS)</cell><cell>0.4277</cell><cell>0.4931</cell><cell>0.4522</cell><cell>0.5453</cell><cell>0.6356</cell><cell>0.5600</cell><cell>0.6087</cell><cell>0.5506</cell></row><row><cell>PARADE Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Reranking effectiveness of different models on Robust04 and GOV2 dataset. Best results are in bold. Significant improvements over Birch (MS), Birch (MS→MB) and BERT-MaxP (MS) are marked with †, ‡ and §, respectively. (p &lt; 0.01, two-tailed paired t-test.</figDesc><table /><note>) comprises a query and all split passages in a doc- ument. We use a learning rate of 3e-6 with warm- up over the first 10 proportions of training steps. Training takes approximately 2.5 hours for each fold on the Robust04 collection. Further details on fine-tuning and hyper-parameters are available in Appendix A.3 and A.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: PARADE's effectiveness using BERT models of varying sizes on Robust04 title queries. Significant</cell></row><row><cell>improvements of distilled over non-distilled models are marked with  †. (p &lt; 0.01, two-tailed paired t-test.)</cell></row><row><cell>pendix A.1, on the TREC 2019 DL document rank-</cell></row><row><cell>ing task in Appendix A.2, and the impact of fine-</cell></row><row><cell>tuning on different domains in Appendix A.3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Reranking effectiveness of PARADE using various preserved data size on GOV2 title dataset. nDCG@20 is reported. The indexes of columns and rows are number of passages being used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>(Re)ranking effectiveness of different models.</figDesc><table><row><cell>(a) Ranking with BM25+RM3</cell><cell>(b) Reranking with PARADE (BM25+RM3)</cell></row><row><cell>(c) Ranking with BM25</cell><cell>(d) Reranking with PARADE (BM25)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>1</head><label></label><figDesc>Results on the TREC-COVID Challenge</figDesc><table><row><cell>runid</cell><cell cols="2">nDCG@10 P@5</cell><cell>bpref</cell><cell>MAP</cell></row><row><cell>1 mpiid5 run3</cell><cell>0.6893</cell><cell cols="3">0.8514 0.5679 0.3380</cell></row><row><cell>2 mpiid5 run2</cell><cell>0.6864</cell><cell cols="3">0.8057 0.4943 0.3185</cell></row><row><cell cols="2">3 SparseDenseSciBert 0.6772</cell><cell cols="3">0.7600 0.5096 0.3115</cell></row><row><cell>4 mpiid5 run1</cell><cell>0.6677</cell><cell cols="3">0.7771 0.4609 0.2946</cell></row><row><cell>5 UIowaS Run3</cell><cell>0.6382</cell><cell cols="3">0.7657 0.4867 0.2845</cell></row></table><note>A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">: Ranking effectivenes of different retrieval sys-</cell></row><row><cell cols="4">tems in the TREC-COVID Round 2.</cell></row><row><cell></cell><cell>runid</cell><cell cols="2">nDCG@10 P@5</cell><cell>bpref</cell><cell>MAP</cell></row><row><cell>1</cell><cell>covidex.r3.t5 lr</cell><cell>0.7740</cell><cell cols="2">0.8600 0.5543 0.3333</cell></row><row><cell>2</cell><cell>BioInfo-run1</cell><cell>0.7715</cell><cell cols="2">0.8650 0.5560 0.3188</cell></row><row><cell>3</cell><cell>UIowaS Rd3Borda</cell><cell>0.7658</cell><cell cols="2">0.8900 0.5778 0.3207</cell></row><row><cell>4</cell><cell cols="2">udel fang lambdarank 0.7567</cell><cell cols="2">0.8900 0.5764 0.3238</cell></row><row><cell cols="2">11 sparse-dense-SBrr-2</cell><cell>0.7272</cell><cell cols="2">0.8000 0.5419 0.3134</cell></row><row><cell cols="2">13 mpiid5 run2</cell><cell>0.7235</cell><cell cols="2">0.8300 0.5947 0.3193</cell></row><row><cell cols="3">16 mpiid5 run1 (Fusion) 0.7060</cell><cell cols="2">0.7800 0.6084 0.3010</cell></row><row><cell cols="2">43 mpiid5 run3 (Attn)</cell><cell>0.3583</cell><cell cols="2">0.4250 0.5935 0.2317</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="4">: Ranking effectivenes of different retrieval sys-</cell></row><row><cell cols="3">tems in the TREC-COVID Round 3.</cell></row><row><cell>runid</cell><cell cols="2">nDCG@20 P@20 bpref</cell><cell>MAP</cell></row><row><cell>1 UPrrf38rrf3-r4</cell><cell>0.7843</cell><cell cols="2">0.8211 0.6801 0.4681</cell></row><row><cell>2 covidex.r4.duot5.lr</cell><cell>0.7745</cell><cell cols="2">0.7967 0.5825 0.3846</cell></row><row><cell>3 UPrrf38rrf3v2-r4</cell><cell>0.7706</cell><cell cols="2">0.7856 0.6514 0.4310</cell></row><row><cell>4 udel fang lambdarank</cell><cell>0.7534</cell><cell cols="2">0.7844 0.6161 0.3907</cell></row><row><cell cols="2">5 run2 Crf A SciB MAP 0.7470</cell><cell cols="2">0.7700 0.6292 0.4079</cell></row><row><cell>6 run1 C A SciB</cell><cell>0.7420</cell><cell cols="2">0.7633 0.6256 0.3992</cell></row><row><cell>7 mpiid5 run1</cell><cell>0.7391</cell><cell cols="2">0.7589 0.6132 0.3993</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ranking effectiveness of different retrieval systems in the TREC-COVID Round 4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">: Ranking effectiveness on TREC 2019 DL</cell></row><row><cell>Track document task test set.</cell><cell></cell><cell></cell></row><row><cell>Fine-tuned model</cell><cell cols="2">P@20 nDCG@20</cell></row><row><cell>BERT-Base</cell><cell>0.4333</cell><cell>0.4970</cell></row><row><cell>BERT-Base (Bing)</cell><cell>0.4223</cell><cell>0.4930</cell></row><row><cell cols="2">BERT-Base (MSMARCO) 0.4486</cell><cell>0.5252</cell></row><row><cell>BERT-Large</cell><cell>0.4408</cell><cell>0.5046</cell></row><row><cell cols="2">BERT-Large (MSMARCO) 0.4508</cell><cell>0.5243</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Rereanking effectiveness of PARADE using different fine-tuned BERT models on Robust04 dataset with Title queries.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that max pooling is performed on passage representations, not over passage relevance scores as in prior work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In our pilot study, there is no difference among using [CLS], [SEP], and [UNK] as the prepended token in x l . 3 https://trec.nist.gov/data/qa/T8_ QAdata/disks4_5.html 4 http://ir.dcs.gla.ac.uk/test_ collections/gov2-summary.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note the Birch results presented here are lower than those in the original work, because we rerank 100 documents. PA-RADE continues to outperform Birch when reranking 1,000 documents in a comparable setting, as shown later inTable 5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://boston.lti.cs.cmu.edu/ appendices/SIGIR2019-Zhuyun-Dai</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by Google Cloud and the TensorFlow Research Cloud.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural passage model for ad-hoc document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10772</biblScope>
			<biblScope unit="page" from="537" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Utilizing passage-based language models for document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">4956</biblScope>
			<biblScope unit="page" from="162" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Passage-level evidence in document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm/Springer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">UCAS at TREC-2019 deep learning track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buettcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2019 deep learning track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Context-aware sentence/passage term importance estimation for first stage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno>abs/1910.10687</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeper text understanding for IR with contextual neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for soft-matching n-grams in ad-hoc search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling diverse relevance patterns in ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SI-GIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Diagnostic evaluation of information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">EARL: Speedup transformer-based rankers with precomputed representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<idno>abs/2004.13313</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local self-attention over long text for efficient document retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TU wien @ TREC deep learning &apos;19 -simple contextualization for re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Interpretable &amp; time-budgetconstrained contextualization for re-ranking</title>
		<idno>abs/2002.01854</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PACRR: A position-aware neural IR model for relevance matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-pacrr: A context-aware neural IR model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tinybert: Distilling BERT for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1909.10351</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ColBERT: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relevancebased language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NPRF: A neural pseudo relevance feedback framework for ad-hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4482" to="4491" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Is searching full text more effective than searching abstracts? BMC Bioinform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Passage retrieval based on language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient document re-ranking for transformers by precomputing term representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expansion via prediction of importance with contextualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CEDR: contextualized embeddings for document ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Passage re-ranking with BERT. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1901.04085</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Document ranking with a pretrained sequence-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/2003.06713</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Document expansion by query prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM/Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Okapi at TREC-4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6354</biblScope>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A passage-based approach to learning to rank documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eilon</forename><surname>Sheetrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr. J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="186" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for BERT model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Kamyar Ghajar, and Azadeh Shakery. 2020. Distilling knowledge for fast retrievalbased chat-bots. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Vakili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahami</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Distilling taskspecific knowledge from BERT into simple neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1903.12136</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Well-read students learn better: The impact of student initialization on knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">TREC-COVID: constructing a pandemic information retrieval test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasmeer</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2005.04474</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewey</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devvret</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oren Etzioni</title>
		<imprint/>
	</monogr>
	<note>and Sebastian Kohlmeier. 2020a. CORD-19: the covid-19 open research dataset. CoRR, abs/2004.10706</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Leveraging passage-level cumulative gain for document ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<editor>WWW. ACM</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Investigating passage-level relevance and its role in document-level relevance judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">End-to-end neural adhoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">IDST at TREC 2019 deep learning track: Deep cascade ranking with generation-based document expansion and pretrained language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Anserini: Reproducible ranking baselines using lucene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>16:1-16:20</idno>
	</analytic>
	<monogr>
		<title level="j">J. Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Applying BERT to document retrieval with birch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Zeynep Akkalyoncu Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">GEAR: graph-based evidence aggregating and reasoning for fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://microsoft.github.io/TREC-2019-Deep-Learning" />
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics. 7 Further details and system descriptions can be</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
