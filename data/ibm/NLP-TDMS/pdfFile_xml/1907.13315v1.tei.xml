<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: tinyurl.com/PASTReID</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function basing on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (Re-ID) is a crucial task in surveillance and security, which aims to locate a target pedestrian across non-overlapping camera views using a probe image. With the advantages of convolutional neural networks (CNN), many person Re-ID works focus on supervised learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref> and achieve satisfactory improvements. Despite the great * Work was done when X. Zhang was visiting The University of Adelaide. First two authors contributed to this work equally. C. Shen is the corresponding author: chunhua.shen@adelaide.edu.au  Here we use Duke <ref type="bibr" target="#b42">[43]</ref> as the source domain and Market-1501 <ref type="bibr" target="#b41">[42]</ref> as the target domain.</p><p>success, they depend on large labelled datasets which are costly and sometime impossible to obtain. To tackle this problem, a few unsupervised learning methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref> propose to take advantage of abundant unlabelled data, which are easier to collect in general. Unfortunately, due to lack of supervision information, the performance of unsupervised methods is typically weak, thus being less effective for practical usages. In contrast, unsupervised cross-domain methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> propose to use both labelled datasets (source domain) and unlabelled datasets (target domain). However, directly applying the models trained in the source domain to the target domain leads to unsatisfactory performances due to the inconsistent characteristics between the two domains, which is known as the domain shift problem <ref type="bibr" target="#b18">[19]</ref>. In unsupervised cross-domain Re-ID, the problem becomes how to transfer the learned information of a pre-trained model from the source domain to the target domain effectively in an unsupervised manner.</p><p>Some domain transfer methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref> have taken great efforts to address this challenge, where the majority are based on pseudo label estimation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref>. They extract embedding features of unlabelled target datasets from the pre-trained model and apply unsupervised clustering methods (e.g., k-means and DBSCAN <ref type="bibr" target="#b8">[9]</ref>) to separate the data into different clusters. The samples in the same cluster are assumed to belong to the same person, which are adapted for training as in supervised learning. The drawback of these methods is that the performance highly depends on the clustering quality, reflecting on whether samples with the same identity are assigned to one cluster. In other words, performance relies on to what extent are the pseudo labels from clustering consistent with ground truth identity labels. Since the percentage of corrupted labels largely affect the model generalization on the target dataset <ref type="bibr" target="#b39">[40]</ref>, we propose a method to improve the quality of labels in a progressive way which results in considerable improvement of model generalization on the unseen target dataset.</p><p>Here we propose a new Self-Training with Progressive Augmentation framework (PAST) to: 1) restrain error amplification at early training epochs when the quality of pseudo label can be low; and 2) progressively incorporate more confidently labelled examples for self-training when the label quality is becoming better. PAST has two learning stages, i.e., conservative and promoting stage, which consider complementary data information via different learning strategies for self-training. Conservative Stage. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the percentage of correctly labelled data is low at first due to the domain shift. In this scenario, we need to select confidently labelled examples to reduce label noise. We consider the similarity score between images as a good indicator of confidence measure. Beside the widely used clustering-based triplet loss (CTL) <ref type="bibr" target="#b14">[15]</ref>, which is sensitive to the quality of pseudo labels generated from clustering method, we propose a novel label-free loss function, ranking-based triplet loss (RTL), to better capture the characteristic of data distribution in the target domain.</p><p>Specifically, we calculate the ranking score matrix for the whole target dataset and generate triplets by selecting the positive and negative examples from the top η and (η, 2η] ranked images for each anchor. The triplets are then fed into the model and trained with the proposed RTL. In the conservative stage, we mainly consider the local structure of data distribution which is crucial for avoiding model collapse when the label quality is mediocre at early learning epochs. Promoting Stage. However, as the number of training triplets dramatically grows in large datasets and triplets only focus on local information, the learning process with triplet loss inevitably becomes instability and suffers from the local-optimal result, as shown by the "CTL" and "CTL+RTL" in <ref type="figure" target="#fig_1">Figure 1</ref>. To remedy this issue, we propose to use the global distribution of data points for network training at the promoting stage. That is, we treat each cluster as a class and convert the learning process into a classification problem. Softmax cross-entropy loss is used to force different categories staying apart for encouraging inter-class separability. After the promoting stage, the model is prone to be more stable which facilitates learning the discriminative features. Since the error is most likely amplified when training on images with extremely corrupted labels using the softmax cross-entropy loss, we employ this stage following the conservative learning stage and carry out two stages interchangeably. With this alternate process, our proposed PAST framework can stabilize the training process and progressively improve the capability of model generalization on the target domain.</p><p>To summarize, our main contributions are as follows. 1) We present a novel self-training with progressive augmentation framework (PAST) to solve the unsupervised cross-domain person Re-ID problem. By executing the twostage self-training process, namely, conducting conservative and promoting stage alternately, our method considerably improve the model generalization on unlabelled targetdomain datasets.</p><p>2) We propose a ranking-based triplet loss (RTL), solely relying on similarity scores of data points, to avoid selecting triplet samples using unreliable pseudo labels.</p><p>3) We take advantage of global data distribution for model training with softmax cross-entropy loss, which is beneficial for training stability and promoting the capability of model generalization. 4) Experimental results on three large-scale datasets indicate the effectiveness of our proposed method on the task of unsupervised cross-domain person Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Supervised Person Re-ID. Most existing deep person Re-ID methods follow a supervised setting. They are mainly based on either well-designed model architectures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>, additional attributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b3">4]</ref> or metric learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44]</ref>. Although significant progress has been obtained by these methods, they all require a large amount of labelled training data, which are costly and impractical to be annotated due to drastic appearance change among different datasets. Unsupervised Person Re-ID. To alleviate the above limitation, unsupervised person Re-ID methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> are proposed to make full use of large-scale unlabelled datasets. Most of them exploit cross-view identity-specific information to capture discriminate features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33]</ref>   During training, we first carry out a sampling process, which consists of extracting embedding features of unlabelled target dataset with the current model and calculating the ranking score matrix with Eq. (2). We then assign pseudo labels to training samples via HDBSCAN <ref type="bibr" target="#b0">[1]</ref> clustering method. After that, we conduct conservative stage by using clustering-based triplet loss (CTL) and the proposed ranking-based triplet loss (RTL) simultaneously to update the model. In promoting stage, the softmax cross-entropy loss is employed to further improve the capability of the model. Note that the conservative stage and promoting stage alternate iteratively during the whole learning process. For Re-ID evaluation, we extract the embedding features for both query and gallery images and use the cosine distance for ranking.</p><p>corporate clustering methods into training to separate unlabelled images into different classes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. However, since specific identity labels are unavailable, these unsupervised learning methods are not able to achieve comparable results as supervised-based approaches.</p><p>Unsupervised Cross-Domain Person Re-ID. Recently, researchers pay intensive attention to unsupervised crossdomain person Re-ID algorithms <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref> to leverage Re-ID models pre-trained in the source domain to improve the performance on unlabelled target domain. They all focus on overcoming domain shift so as to learn domain-invariant feature representation. Among these existing works, PTGAN <ref type="bibr" target="#b35">[36]</ref> and SP-GAN <ref type="bibr" target="#b7">[8]</ref> transfer source images into target-domain style by CycleGAN and then use translated images to train a model. However, due to unable to guarantee the identity of generated images, these style transfer learning methods can not result in satisfactory performance. Another line of unsupervised cross-domain person Re-ID works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref> combine other auxiliary information as an assistant task to improve the model generalization. For instance, TFusion <ref type="bibr" target="#b22">[23]</ref> integrates spatio-temporal patterns to improve the Re-ID precision, while EANet <ref type="bibr" target="#b15">[16]</ref> uses pose segmentation. TJ-AIDL <ref type="bibr" target="#b33">[34]</ref> learns an attribute-semantic and identity discriminative feature representation space simultaneously, which can be transferred to any new target domain for re-id tasks. Similar as the difficulty of supervised learning, these domain adaptation approaches suffer from the requirement of collecting attribute annotations.</p><p>Beyond the above methods, some approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23</ref>] focus on estimating pseudo identity labels on the target domain so as to learn deep models in a supervised manner. Usually, clustering methods are used in the feature space to generate a series of clusters which are used to update networks with an embedding loss (e.g., triplet loss <ref type="bibr" target="#b14">[15]</ref> or contrastive loss) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref> or classification loss (e.g., softmax cross-entropy loss) <ref type="bibr" target="#b9">[10]</ref>. Whereas, embedding loss functions suffer from the limitation of sub-optimal results and slow convergence, while classification loss extremely depends on the quality of pseudo labels. While the work in <ref type="bibr" target="#b38">[39]</ref> introduces a simple domain adaptation framework which also use both triplet loss and softmax cross-entropy loss jointly, it aims at solving one-shot leaning problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Method</head><p>For unsupervised cross-domain person Re-ID, the problem that we concentrate on is how to learn robust feature representations for unlabelled target datasets using the prior knowledge from the labelled source datasets. In this section, we present our proposed self-training with progressive augmentation framework (PAST) in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview of Our Proposed Framework</head><p>The overall framework of our proposed self-training with progressive augmentation framework (PAST) is described in <ref type="figure" target="#fig_2">Figure 2</ref>. The framework is based on a deep neural network M trained on ImageNet <ref type="bibr" target="#b6">[7]</ref>, which contains two main components: conservative stage and promoting stage.</p><p>We first fine-tune the model M using labelled source training dataset S in a supervised manner. Then, this pre-trained model is utilized to extract features F on all training images in the target domain T , which are used as the input features of our framework. For the conservative stage, based on the ranking score matrix D R learned from the input features, we can generate a more reliable training set T U via the HDBSCAN <ref type="bibr" target="#b0">[1]</ref> clustering method (other clustering methods can be employed here too). This updated training set T U is a subset of the whole training data T . Combining with two triplet-based loss functions, i.e., clustering-based triplet loss (CTL) and the proposed ranking-based triplet loss (RTL), local structure of the current updated training set can be captured for model optimization. After that, we can use the new model to extract features F U of the current training set T U . Next, in the promoting stage, with the new features F U from the conservative stage, we propose to employ softmax cross-entropy loss for further optimizing the network. At this stage, the global distribution of the training set is considered to improve the discrimination of feature representation. Finally, the capability of model generalization is improved gradually by training the network with the conservative stage and promoting stage alternately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Conservative Stage</head><p>In the task of unsupervised domain adaptation, it is a natural goal to gather samples of the same identity together and push samples from different classes away from each other. Triplet loss <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23]</ref> has been proved to be able to discover meaningful underlying local structure of data distribution by generating reliable triplets of the target data. Different from the supervised setting, pseudo labels are assigned to unlabelled samples, which is more difficult to construct high-quality triplets. Therefore, our goal is to design a learning strategy to not only generate reliable samples but also improve the model performance.</p><p>In practice, we conduct the following procedure in the conservative stage. At the beginning, on the whole training dataset T :</p><formula xml:id="formula_0">{x 1 , x 2 , ..., x N }, we extract features F: {f (x 1 ), f (x 2 ), ..., f (x N )</formula><p>} from the current model, and adopt the k-reciprocal encoding <ref type="bibr" target="#b43">[44]</ref>, which is a variation of the Jaccard distance between nearest neighbors sets, to generate the distance matrix D as:</p><formula xml:id="formula_1">D = [D J (x 1 ) D J (x 2 ) . . . D J (x N )] T , D J (x i ) = [d J (x i , x 1 ) d J (x i , x 2 ) . . . d J (x i , x N )], ∀i ∈ {1, 2, . . . , N },<label>(1)</label></formula><p>where D J (x i ) represents the distance vector of one specific person x i with all training images. d J (x i , x j ) is the Jaccard distance between sample x i and x j .</p><p>According to the fact that a smaller distance reflects more similarities between two images, we sort every distance vector D J (x i ) from smallest value to largest value, yielding ranking score matrix D R as:</p><formula xml:id="formula_2">D R = [D R (x 1 ) D R (x 2 ) . . . D R (x N )] T , D R (x i ) = [d J (x i , x 1 ) d J (x i , x 2 ) . . . d J (x i , x N )], ∀i ∈ {1, 2, . . . , N }, (2) where D R (x i ) is the ranking format of D J (x i ) from small to large. Given a specific sample x i , x j in d J (x i , x j ) repre- sents the j-th most similar sample.</formula><p>Then, we apply a hierarchical density-based clustering algorithm (HDBSCAN) <ref type="bibr" target="#b0">[1]</ref> on D R to split the whole training images into different clusters, which are considered as pseudo labels. After HDBSCAN, some images, not belonging to any clusters, are discarded. Thus, we use images with assigned labels as the updated training set T U for further model optimization.</p><p>We combine two types of triplet loss functions together to update the model, i.e., clustering-based triplet loss (CTL) and ranking-based triplet loss (RTL), which are different from the way of triplets selection as well as the way for model optimization.</p><p>Clustering-based Triplet Loss (CTL). One loss function that we use is batch hard mining triplet loss <ref type="bibr" target="#b14">[15]</ref>, proposed to mine relations among samples within a mini-batch. We randomly sample P clusters and K instances in each cluster to compose a mini-batch with size of P K. For each anchor image x a , the corresponding hardest positive sample x p and the hardest negative sample x n within the batch are selected to form a triplet. Since the pseudo labels are from a clustering method, we rename this loss function as clustering-based triplet loss (CTL), which is formulated as,</p><formula xml:id="formula_3">L CT L = P K a=1 [m + ||f (x a ) − f (x p )|| 2 − ||f (x a ) − f (x n )|| 2 ] + = P i=1 K a=1 [m + hardest positive max p=1...K ||f (x i,a ) − f (x i,p )|| 2 − min n=1...K j=1...P j =i ||f (x i,a ) − f (x j,n )|| 2 hardest negative ] + ,<label>(3)</label></formula><formula xml:id="formula_4">where x i,j is a data point representing the j-th image of the i-th cluster in the batch. f (x i,j ) is the feature vector of x i,j .</formula><p>Ranking-based Triplet Loss (RTL). However, it is clear that the effect of CTL highly depends on the quality of label estimation, which is hard to decide whether the clustering result is correct or not. Therefore, we propose a Ranking-based Triplet Loss (RTL) to make full use of the ranking score matrix D R . It is a label-free method reflecting the relation between data pairs. Specifically, given a training anchor x a , positive sample x p is randomly selected from the top η nearest neighbors according to the ranking score vector D R (x a ), and negative sample x n is from the location (η, 2η]. In addition, instead of hard margin in CTL,we introduce a soft margin based on the relative ranking position of x p and x n , which can adapt well to different scales of intra-class variation. The formula of RTL is shown as,</p><formula xml:id="formula_5">L RT L = P K a=1 [ |P p − P n | η m + ||f (x a ) − f (x p )|| 2 − ||f (x a ) − f (x n )|| 2 ] + ,<label>(4)</label></formula><p>where the selected anchors in each batch are the same as CTL. m is a basic hard margin same as Eq. <ref type="formula" target="#formula_3">(3)</ref>. η is the maximum of ranking position for positive sample selection. P p and P n are the ranking positions of x p and x n with respect to x a . To summarize, we optimize the network using the combination of CTL and RTL to better capture the localconstraint information of data distribution. Our final tripletbased loss function in conservative stage is shown in Eq. <ref type="formula" target="#formula_6">(5)</ref>:</p><formula xml:id="formula_6">L C = L RT L + λL CT L ,<label>(5)</label></formula><p>where λ is the loss weight to trade off the influence of two loss functions. Experiments show that this combined tripletbased loss function can certainly improve the capability of model representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Promoting Stage</head><p>Nevertheless, since triplet-based loss functions only focus on the data relation within each triplet, the model will be prone to instability and stuck into a suboptimal local minimum. To alleviate this problem, we propose to apply classification loss to further improve model generalization by taking advantage of global information of training samples. In the promoting stage, a fully-connected layer is added at the end of the model as a classifier layer, which is initialized according to the features of current training set. Softmax cross-entropy loss is used as the objective function, which is formulated as:</p><formula xml:id="formula_7">L P = − P K i=1 log e W T y i xi C c=1 e W T c xi ,<label>(6)</label></formula><p>whereŷ i is the pseudo label of the sample x i . C is the number of clusters from the HDBSCAN clustering method with updated training set T U .</p><p>Feature-based Weight Initialization for Classifier. Due to the variation of cluster numbers C, the newly added classifier layer CL should be initialized every time executing HDBSCAN. Instead of random initialization, we exploit the mean features of each cluster as the initial parameters. Specifically, for each cluster c, we calculate the mean feature F c by averaging all the embedding features of its elements. The parameters W of CL are initialized as follows: where W ∈ R d×C , W c is the c-th column of W, and d is the feature dimensionality. An advantage of this initialization is that we can use the previous information to avoid the fluctuation of accuracy caused by random initialization, which is useful for the convergence of model training.</p><formula xml:id="formula_8">W c = F c , c ∈ {1, 2, . . . , C},<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Alternate Training</head><p>The learning process is expected to progressively improve the model capability of generalization, which can avoid model to fall into local optimum. In this paper, we carefully develop a simple yet effective self-training strategy which can capture local structure and global information of training images. That is, the conservative stage and the promoting stage are conducted alternately. At the beginning, the model is trained only using the local relations between data points alone, so that the difficulty of error amplification brought by softmax loss can be prevented. After several training steps in the conservative stage, the ability of model representation and the quality of clusters are more trusty. Then model capability is further augmented using Softmax cross-entropy loss in the promoting stage and the updated model is used as the initial state for conservative stage alternately. As the training goes on, model generalization is improved, allowing to learn more discriminate feature representation of training images. The details of this two-stage alternate self-training are included in Algorithm 1. We also list one visual example of this alternate self-training process, shown in <ref type="figure" target="#fig_4">Figure 3</ref>. It is proved that our proposed PAST framework is also useful for refining the quality of clusters.  <ref type="table" target="#tab_1">3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23   24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate our unsupervised self-learning method on cross-domain Person Re-ID tasks. Three common largescale person Re-ID datasets are used, Market-1501 <ref type="bibr" target="#b41">[42]</ref>, DukeMTMC-Re-ID <ref type="bibr" target="#b42">[43]</ref>, and CUHK03 <ref type="bibr" target="#b16">[17]</ref>.</p><p>Market-1501 <ref type="bibr" target="#b41">[42]</ref> contains 32,668 labelled images of 1,501 identities taken by 6 cameras, which are detected and cropped via Deformable Part Model (DPM) <ref type="bibr" target="#b10">[11]</ref>. The dataset is split into training set with 12,936 images of 751 identities and test set with 19,732 images of 750 identities.</p><p>DukeMTMC-Re-ID <ref type="bibr" target="#b42">[43]</ref> consists of 36,411 labelled images belonging to 1,404 identities observed by 8 camera views. As the format of Matket-1501 dataset, it has 16,522 images of 702 identities for the training set and the remaining 19,889 images of 702 identities for the test set. Hereafter Duke refers to this dataset.</p><p>CUHK03 <ref type="bibr" target="#b16">[17]</ref> is composed of 14,096 images from 1,467 identities captured by 2 cameras. This dataset was constructed by both manual labelling and DPM. In this work, we experiment on the images detected using DPM. To be in consistency with the protocol of Market-1501 and Duke, new train/test evaluation protocol <ref type="bibr" target="#b43">[44]</ref> are used: 7,365 images with 767 identities for training and the remaining 6,732 images with 700 identities for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation Details</head><p>Model and Preprocessing. We adopt PCB <ref type="bibr" target="#b28">[29]</ref> as our model structure, in which ResNet-50 <ref type="bibr" target="#b13">[14]</ref> without last classification layer is used as backbone model. Similar as EANet <ref type="bibr" target="#b15">[16]</ref>, we use 9 regions for feature representation. Instead of using part aligned pooling <ref type="bibr" target="#b15">[16]</ref>, we change to use even parts like PCB for simplification. The dimension of each embedding layer is set to 256. Following each embedding layer, we also implement the classifier layer with one fully connected layer in the promoting stage. The classifier output changes according to the number of clusters generated from HDBSCAN clustering process.</p><p>All input images are resized to 384×128×3. It is noting that we only apply random flipping as data augmentation.</p><p>Training Settings. We use the SGD optimizer with a momentum of 0.9 and weight decay of 5 × 10 −4 to train the model. Without otherwise specification, in all experiments we set batch size to 64 and the iteration step to 4. Instead of directly using same learning rates for both conservative and promoting stage, we believe that individually setting the specialized learning rates can work better for our PAST framework. The reason is that the parameters from the conservative stage should be updated slower in the promoting stage for avoiding error amplification caused by Softmax cross-entropy loss. Specifically, the learning rate is initialized to 10 −4 on fine-tune layers and 2×10 −4 on embedding layers in the conservative stage, while for the promoting stage, newly added classifier layers use an initial learning rate of 10 −3 and all other layers 5 × 10 −5 . After 3 iterations, all learning rates are multiplied by 0.1. The margin hyper parameter m is set to 0.3 in both Eq. (3) and Eq. (4).</p><p>Evaluating Settings. For performance evaluation, feature vectors from embedding layers of 9 parts are normalized separately and then concatenated as the output representation. Given a query image, we calculate cosine distance with all gallery images and then sort it as final ranking result. We utilize the Cumulated Matching Characteristics (CMC) <ref type="bibr" target="#b12">[13]</ref> and mean Average Precision (mAP) <ref type="bibr" target="#b41">[42]</ref> as the performance evaluation measures. CMC curve shows the probability that a query appears in different size of candidate lists. As for mAP, given a single query, the Average Precision (AP) is computed from the area under its precision-recall curve. The mAP is then calculated as the mean value of AP across all queries. Note that single-shot setting is adopted similar to <ref type="bibr" target="#b28">[29]</ref> in all experiments.  <ref type="bibr" target="#b42">[43]</ref> as source domain and Market-1501 <ref type="bibr" target="#b41">[42]</ref> as target domain. * denotes that the results are produced by us. DT means Direct Transfer from PCB with 9 regions. R means applying k-reciprocal encoding method <ref type="bibr" target="#b43">[44]</ref>. CTL represents clustering-based triplet loss <ref type="bibr" target="#b14">[15]</ref>, while RTL is our proposed rankingbased triplet loss. Our PAST framework consists of conservative stage and promoting stage that are denoted by C and P respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation Study</head><p>In this subsection, we aim to thoroughly analyse the effectiveness of each components in our PAST framework.</p><p>Effectiveness of the Conservative Stage. As shown in <ref type="table" target="#tab_1">Table 1</ref>, we conduct several experiments to verify the effectiveness of the individual components CTL, RTL and the combination of these two triplet loss functions on the task of M→D and D→M. First, only with CTL, we improve the performance by 18.49% and 12.14% at Rank-1 accuracy compared with the results from k-reciprocal encoding method <ref type="bibr" target="#b43">[44]</ref> on M→D and D→M respectively. Second, we observe that containing only our proposed RTL, the Rank-1 accuracy and mAP increase by 21% and 12.64% for M→D, while 12.91% and 5.69% on D→M. This obvious improvement shows that both CTL and RTL are useful for increasing model generalization. And CTL obtains slightly lower performance than RTL. Then, as described in Eq. (5), we combine CTL and RTL together to jointly optimize model in our conservative stage. It is clear that we achieve better results on both M→D and D→M. Especially for D→M, we gain 2.38% and 4.42% on Rank-1 and mAP comparing to only using CTL, which shows the significant benefit of our RTL. Through this conservative stage, we can learn a relative powerful model for target domain.</p><p>Effectiveness of the Promoting Stage. However, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, there is no further gains even with more training iterations when only using triplet-based loss functions. We believe that it is because during conservative stage, the model only sees local structure of data distribution brought by triplet samples. Thus, in our PAST framework, we employ softmax cross-entropy loss as the objective function in the promoting stage to train the model with the conservative stage alternately. Refer to <ref type="table" target="#tab_1">Table 1</ref> again, compared with only using conservative stage, our PAST can further improve mAP and Rank-1 by 2.21% and 0.72% on M→D task, and 4.03% and 4.12% for D→M. Meanwhile, from <ref type="figure" target="#fig_4">Figure 3</ref>, the quality of clusters is also improved with our PAST framework. This shows that the promoting stage does play an important role in model generalization. Through the above experiments, different components in our PAST have been evaluated and verified. We show that our PAST framework is not only beneficial for improving model generation but also refining clustering quality.</p><p>Comparison with Different Clustering Methods. We evaluate three different clustering methods, i.e., k-means, DBSCAN <ref type="bibr" target="#b8">[9]</ref> and HDBSCAN <ref type="bibr" target="#b0">[1]</ref> in the conservative stage. The performance of utilizing these clustering methods under different settings are specified in <ref type="table">Table 2</ref>. For k-means, the number of cluster centroids k is set to 702 and 751 on target data of Market-1501 and Duke respectively, which is the same as the number of identities of source training data. It is clear that HDBSCAN performs better than k-means and DBSCAN under either only using conservative stage or whole PAST framework. For instance, using HDBSCAN can achieve mAP 54.26% and Rank-1 72.35% for M→D task in PAST framework, which are 4.29% and 3.41% higher than using k-means, and 1.19% and 0.45% than using DBSCAN. In addition, we also observe that whatever clustering method we use, our PAST framework always outperforms only using conservative stage. This means that on the one hand, HDBSCAN clustering method has more powerful effect in our framework; on the other hand, our PAST framework indeed provides improvement of feature representation on target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with State-of-the-art Methods</head><p>Following evaluation setting in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b44">45]</ref>, we compare our proposed PAST framework with state-of-the-art unsupervised cross-domain methods, shown in <ref type="table">Table 3</ref>. It can be seen that only using conservative stage with CTL and RTL for training, the performance is already competitive with other cross-domain adaptive methods. For example, although EANet <ref type="bibr" target="#b15">[16]</ref> proposes complex part-aligned pooling and combines pose segmentation to provide more information for adaptation, our conservative stage still outperforms it by 3.93% in Rank- <ref type="bibr" target="#b0">1</ref>  79.48%, 69.88% in Rank-1 accuracy for M→D, M→D, C→M, C→D. We can also prove that it is useful to alternately use conservative and promoting stage by comparing with the last two rows in <ref type="table">Table 3</ref>. Especially, our PAST can improve 4.71% and 5.21% in Rank-1 and mAP for C→D compared with only using conservative stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parameter Analysis</head><p>Besides, we conduct additional experiments to evaluate the parameter sensitivity.</p><p>Analysis of the Loss Weight λ. λ is a hyper parameter which is used to trade off the effect between rankingbased triplet loss (RTL) and clustering-based triplet loss (CTL). We evaluate the impact of λ, which is sampled from {0.1, 0.2, 0.5, 1.0, 2.0}, on the task of D→M. The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref> (a). We observe that the best result is obtained when λ is set to 0.5. Note that large or small λ has limitation on the improvement of performance.</p><p>Analysis of the Minimum Samples S min . In addition, we analyse how the number of minimum samples (S min ) for every cluster in HDBSCAN clustering affects the Re-ID results. We test the impact of {5, 10, 15, 20} minimum samples on the performance of our PAST framework on D→M setting. As shown in <ref type="figure" target="#fig_5">Figure 4 (b)</ref>, we can see that setting S min to 10 yields superior accuracy. Meanwhile, different S min has large variance on the final number of pseudo identities from HDBSCAN. We believe that it is because samples from the same class will be separated to several clusters when S min is too small, while low-density classes will be abandoned if S min is too large. This can be verified from <ref type="figure" target="#fig_5">Figure 4 (c)</ref>, the number of identity from HDBSCAN with minimum sample 10 is 625, which is the closest one to the true value 751 in Market-1501 training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we have presented a self-training with progressive augmentation framework (PAST) for unsupervised cross-domain person re-identification. Our PAST consists of two different stages, i.e., the conservative and promoting stage, which are adopted alternately to offer complementary information for each other. Specifically, the conservative stage mainly captures local information with triplet-based loss functions, while the promoting stage is used for extracting global information. For alleviating the dependence on clustering quality, we also propose a novel label-free ranking-based triplet loss. With these proposed method, the model generalization gains significant improvement, as well as the capability of feature representation on target domain. Extensive experiments show that our PAST outperforms the state-of-the-art unsupervised cross-domain algorithms by a large margin.</p><p>We plan to extend our work to other unsupervised crossdomain applications, such as face recognition and image retrieval tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">More Qualitative Analyses</head><p>Qualitative Analysis of the Feature Representation. To demonstrate the results intuitively, we visualize the feature embeddings calculated by our PAST framework in 2-D using t-SNE <ref type="bibr" target="#b30">[31]</ref>.  negative samples. As illustrated in <ref type="figure">Figure 5</ref>, images belonging to the same identity are almost well gathered together, while those from different classes usually stay apart from each other. It implies that our PAST framework can improve the capability of model generalization which is beneficial for learning discriminative feature representation on the target-domain dataset. Qualitative Analysis of the Triplet Selection. In <ref type="figure">Figure 6</ref>, we visualize the triplet samples generated in the conservative stage for CTL and RTL, respectively. We summarize the main advantages of the proposed PAST method in the following.</p><p>1. The proposed PAST algorithm can significantly improve the quality of the clustering assignments during training. As shown in the first row of the iterations from 1 to 4, the images assigned to the same class by the proposed method tend to be more and more similar. On the other hand, the quality of the pseudo labels assigned to each images is steadily improved during training. It means that our PAST framework is beneficial for learning discriminating feature presentation and can assign more reliable pseudo labels to target images. The accurate pseudo labels can be used to promoting stage to improve the model generalization further. 2. RTL is useful for remedying the variance caused by CTL. Refer to <ref type="figure">Figure 6</ref> again, we can observe that the third cluster in iteration 2 is noisy and the selected triplets from CTL are not faithful. However, RTL can select correct positive sample even the cluster is dirty. We believe that the reason is that RTL just depends on the similarity ranking matrix and the top η similar images are used for generating positive samples, which is more reliable when the features representation is not so discriminative. 3. RTL helps to further optimize the network, especially in the later iteration. From <ref type="figure">Figure 6</ref>, we can also see that different clusters in one mini-batch may look different due to unique color of clothes, which results in extremely simple negative samples and slows down the optimization when training on CTL. Whereas, considering the triplets generated from the RTL, negative images are extremely similar to the anchors, which is even hard to be well recognized by human beings. For example, at the second column in iteration 4, all images look like one person, although images from the first two rows are same person, while those from the third row belong to another person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>True Positive</head><p>False Positive False Negative <ref type="figure">Figure 5</ref> -Qualitative analysis of the feature representation by using t-SNE <ref type="bibr" target="#b30">[31]</ref> visualization on a subset of Market-1501 <ref type="bibr" target="#b41">[42]</ref> training data. According to the clustering result, we choose the Top-50 identities which contain Top-50 the largest number of images. Points with the same color have the same (ground-truth) identity. The green circle means images from the same identity are gathered together, and the cluster is extremely reliable. Images in orange circle are both from same identity, yet they are clustered to two different classes. We can see that due to the camera style, images from the two classes have different appearances. In the red circle, although our algorithm may gather images from different (ground-truth) identities into the same cluster, these images usually share very similar appearances and are hard to distinguish with each other. For instances, every image in the red circle contains one person with white clothes and a black bicycle.  <ref type="figure">Figure 6</ref> -Quality of the triplet selection over training iterations. Images from different clusters are divided by yellow line. The red line means generated triplets are not completely correct, while green line represents generated triplets are completely correct. The solid line and dashed line are for triplets, which are generated from CTL and RTL respectively. We use Duke <ref type="bibr" target="#b42">[43]</ref> as the source domain and Market-1501 <ref type="bibr" target="#b41">[42]</ref> as the target domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 -</head><label>1</label><figDesc>Label quality vs. model generalization. The accuracy of pseudo labels prediction (top) and performance comparison (bottom) of different training processes over training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 -</head><label>2</label><figDesc>The overview of our self-training framework with progressive augmentation (PAST). The model is pre-trained on the labelled source dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 -</head><label>3</label><figDesc>The alternate self-training process of our PAST framework on one visual example. All images belong to same person in truth. Samples with same color denotes that they are assigned to same pseudo label generated by HDBSCAN clustering method. Gray figure means the sample not belonging to any cluster and not being used for model training. From training iteration 1 to iteration 4, more samples are selected for training. At the same time, the pseudo labels are more reliable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 -</head><label>4</label><figDesc>Analysis of hyper parameters on D→M setting. (a): The impact of the loss weight λ; (b): The impact of the minimum samples S min at each cluster in HDBSCAN clustering method; (c): The number of clusters from HDBSCAN with different minimum sample S min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>The Self-training with Progressive Augmentation Framework (PAST) Input : labelled source domain dataset S; whole unlabelled target domain training dataset T ; CNN model M pre-trained on ImageNet; maximum iteration Imax; HDBSCAN clustering method; minimal samples in each cluster for HDBSCAN S min . Output: Model M . Initialization: Initialize model M on S; Initial selected training set T U = T . Extract embedding features F on training data T from M ; Compute ranking score matrix D R on whole training data T with F according to Eq. (2);</figDesc><table /><note>1 for i = 1 to Imax do2 Conservative Stage:345 Update training set T U using HDBSCAN(D R ; S min );6 Update model M using T U according to Eq. (5);7 Extract embedding features F U on T U from M ;8 Promoting Stage:9 Initialize classifier layer CL based on F U according to Eq. (7);10 Update model M with classifier layer using T U according to Eq. (6); 11 end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 -</head><label>1</label><figDesc>The effectiveness of conservative stage and promoting stage in our proposed Self-training with Progressive Augmentation Framework (PAST). D→M represents that we use Duke</figDesc><table><row><cell>Method</cell><cell>Stage</cell><cell cols="2">M→D Rank-1 mAP</cell><cell cols="2">D→M Rank-1 mAP</cell></row><row><cell>PCB  *  [29] (DT)</cell><cell>-</cell><cell>42.73</cell><cell>25.70</cell><cell>57.57</cell><cell>29.01</cell></row><row><cell>PCB-R  *  [44]</cell><cell>-</cell><cell>49.69</cell><cell>39.38</cell><cell>59.74</cell><cell>41.93</cell></row><row><cell>PCB-R-CTL</cell><cell>C</cell><cell>68.18</cell><cell>49.06</cell><cell>71.88</cell><cell>46.17</cell></row><row><cell>PCB-R-RTL</cell><cell>C</cell><cell>70.69</cell><cell>52.02</cell><cell>72.65</cell><cell>47.62</cell></row><row><cell>PCB-R-CTL+RTL</cell><cell>C</cell><cell>71.63</cell><cell>52.05</cell><cell>74.26</cell><cell>50.59</cell></row><row><cell>PCB-R-PAST</cell><cell>C+P</cell><cell>72.35</cell><cell>54.26</cell><cell>78.38</cell><cell>54.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and 4.05% in mAP when testing on M→D. Moreover, our PAST framework surpasses all previous methods by a large margin, which achieves 54.26%, 54.62%, 57.34%, 51.79% in mAP and 72.35%, 78.38%,</figDesc><table><row><cell>Method</cell><cell>M→D Rank-1</cell><cell>mAP</cell><cell>D→M Rank-1</cell><cell>mAP</cell><cell>C→M Rank-1</cell><cell>mAP</cell><cell>C→D Rank-1</cell><cell>mAP</cell></row><row><cell>UMDL [25]'16</cell><cell>18.5</cell><cell>7.3</cell><cell>34.5</cell><cell>12.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PUL [10]'18</cell><cell>30.0</cell><cell>16.4</cell><cell>45.5</cell><cell>20.5</cell><cell>41.9</cell><cell>18.0</cell><cell>23.0</cell><cell>12.0</cell></row><row><cell>PTGAN [36]'18</cell><cell>27.4</cell><cell>-</cell><cell>38.6</cell><cell>-</cell><cell>31.5</cell><cell>-</cell><cell>17.6</cell><cell>-</cell></row><row><cell>SPGAN [8]'18</cell><cell>46.4</cell><cell>26.2</cell><cell>57.7</cell><cell>26.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TJ-AIDL [34]'18</cell><cell>44.3</cell><cell>23.0</cell><cell>58.2</cell><cell>26.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HHL [45]'18</cell><cell>46.9</cell><cell>27.2</cell><cell>62.2</cell><cell>31.4</cell><cell>56.8</cell><cell>29.8</cell><cell>42.7</cell><cell>23.4</cell></row><row><cell>ARN [19]'18</cell><cell>60.2</cell><cell>33.4</cell><cell>70.3</cell><cell>39.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EANet [16]'19</cell><cell>67.7</cell><cell>48.0</cell><cell>78.0</cell><cell>51.6</cell><cell>66.4</cell><cell>40.6</cell><cell>45.0</cell><cell>26.4</cell></row><row><cell>Theory [27]'18</cell><cell>68.4</cell><cell>49.0</cell><cell>75.8</cell><cell>53.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PCB  *  [29] (DT)'18</cell><cell>42.73</cell><cell>25.70</cell><cell>57.57</cell><cell>29.01</cell><cell>51.43</cell><cell>27.28</cell><cell>29.40</cell><cell>16.72</cell></row><row><cell>PCB-R  *  [44]</cell><cell>49.69</cell><cell>39.38</cell><cell>59.74</cell><cell>41.93</cell><cell>55.91</cell><cell>38.95</cell><cell>35.19</cell><cell>26.89</cell></row><row><cell>PCB-R-CTL+RTL (Ours)</cell><cell>71.63</cell><cell>52.05</cell><cell>74.26</cell><cell>50.59</cell><cell>77.70</cell><cell>54.36</cell><cell>65.71</cell><cell>46.58</cell></row><row><cell>PCB-R-PAST (Ours)</cell><cell>72.35</cell><cell>54.26</cell><cell>78.38</cell><cell>54.62</cell><cell>79.48</cell><cell>57.34</cell><cell>69.88</cell><cell>51.79</cell></row><row><cell cols="9">Table 3 -Comparison with state-of-the-art methods under unsupervised cross-domain setting. In each column, the 1st and 2nd highest scores are marked</cell></row><row><cell cols="7">by red and blue respectively. D, M, C represent Duke [43], Market-1501 [42] and CUHK03 [17] respectively.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Three representative classes are displayed by showing the corresponding images in the bottom, i.e., true positive samples, false positive samples and false</figDesc><table><row><cell></cell><cell></cell><cell>D→M</cell><cell></cell><cell></cell></row><row><cell>η</cell><cell cols="4">Rank-1 Rank-5 Rank-10 mAP</cell></row><row><cell>5</cell><cell>71.85</cell><cell>83.22</cell><cell>87.00</cell><cell>44.37</cell></row><row><cell>10</cell><cell>73.78</cell><cell>84.09</cell><cell>87.62</cell><cell>47.64</cell></row><row><cell>15</cell><cell>77.43</cell><cell>86.70</cell><cell>89.99</cell><cell>51.77</cell></row><row><cell>20</cell><cell>78.38</cell><cell>88.63</cell><cell>92.01</cell><cell>54.62</cell></row><row><cell>25</cell><cell>78.27</cell><cell>88.63</cell><cell>91.63</cell><cell>55.27</cell></row><row><cell>30</cell><cell>78.15</cell><cell>88.93</cell><cell>91.95</cell><cell>54.46</cell></row><row><cell>35</cell><cell>78.59</cell><cell>88.48</cell><cell>91.83</cell><cell>55.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 -</head><label>4</label><figDesc></figDesc><table /><note>The influence of maximum ranking position η for triplet se- lection of RTL in our PAST framework on D→M setting.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">More Experiments for Parameter Analysis</head><p>Analysis of the Maximum Ranking Position η for Positive Sample. The maximum ranking position η is a tunable hyper-parameter in the ranking-based triplet loss (RTL), as shown in Eq. (4) in the main paper, which defines the range (0, η] for selecting positive samples and the range (η, 2η] for negative samples. We conduct several experiments to evaluate the sensitivity of our method to η when transferring from Duke <ref type="bibr" target="#b42">[43]</ref> to Market-1501 <ref type="bibr" target="#b41">[42]</ref>, as shown in <ref type="table">Table 4</ref>. It shows that when η is equal or larger than 20, we can obtain nearly same and competitive results. And we set η = 20 in all experiments except this part. The performance drops quickly when η is extremely small. We believe that it is due to the unbalanced identities, e.g., the minimal and maximal numbers of images are 2 and 72 respectively in the training set of Market-1501, which results in a large probability that the selected positive and negative samples are from the same (ground-truth) identity.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific-Asia. Conf. Knowledge discovery &amp; data mining</title>
		<meeting>Pacific-Asia. Conf. Knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person search via a mask-guided two-stream cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multi-task deep network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning multi-scale representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2590" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person re-identification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A densitybased algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge discovery &amp; data mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ACM Trans. Multimedia Computing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05244</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop on Performance Evaluation for Tracking and Surveillance</title>
		<meeting>IEEE Int. Workshop on Performance Evaluation for Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Eanet: Enhancing alignment for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11369</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adaptation and re-identification network: An unsupervised deep transfer learning approach to person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="172" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intell</title>
		<meeting>AAAI Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="7948" to="7956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1306" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11334</idno>
		<title level="m">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiregion bilinear convolutional neural networks for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Advanced Video Signal-based Surveillance</title>
		<meeting>IEEE Int. Conf. Advanced Video Signal-based Surveillance</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person reidentification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentionaware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Making classification competitive for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12649</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
