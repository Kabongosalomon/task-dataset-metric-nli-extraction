<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBSPECTRAL NORMALIZATION FOR NEURAL AUDIO DATA PROCESSING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simyung</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research †</orgName>
								<address>
									<region>YH</region>
									<country>Qualcomm Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungwoo</forename><surname>Park</surname></persName>
							<email>hwoopark@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research †</orgName>
								<address>
									<region>YH</region>
									<country>Qualcomm Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Cho</surname></persName>
							<email>janghoon@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research †</orgName>
								<address>
									<region>YH</region>
									<country>Qualcomm Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsin</forename><forename type="middle">Park</forename><surname>Sungrack</surname></persName>
							<email>hyunsinp@qti.qualcomm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research †</orgName>
								<address>
									<region>YH</region>
									<country>Qualcomm Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Kyuwoong</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qualcomm AI Research †</orgName>
								<address>
									<region>YH</region>
									<country>Qualcomm Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUBSPECTRAL NORMALIZATION FOR NEURAL AUDIO DATA PROCESSING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-SubSpectral Normalization</term>
					<term>CNNs</term>
					<term>Au- dio</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks are widely used in various machine learning domains. In image processing, the features can be obtained by applying 2D convolution to all spatial dimensions of the input. However, in the audio case, frequency domain input like Mel-Spectrogram has different and unique characteristics in the frequency dimension. Thus, there is a need for a method that allows the 2D convolution layer to handle the frequency dimension differently. In this work, we introduce SubSpectral Normalization (SSN), which splits the input frequency dimension into several groups (sub-bands) and performs a different normalization for each group. SSN also includes an affine transformation that can be applied to each group. Our method removes the inter-frequency deflection while the network learns a frequency-aware characteristic. In the experiments with audio data, we observed that SSN can efficiently improve the network's performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Convolutional Neural Networks (CNNs) have been widely used in the recent studies on deep neural networks for various domains such as image, audio, and text. Early researches on CNNs have been mainly studied in the image domain, and enormous improvements and achievements are obtained with some architectures, VGG <ref type="bibr" target="#b0">[1]</ref> or ResNet <ref type="bibr" target="#b1">[2]</ref>, for many computer vision tasks. These research results have been applied to audio and speech tasks with various modifications in the architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Most methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> based on the frequency domain feature (e.g. Mel-Spectrogram) use the architecture consisting of multiple 2D convolution layers.</p><p>The 2D convolution operation equally processes the input data in vertical and horizontal directions. As illustrated in <ref type="figure">Figure 1</ref>(a), this processing is proper for image-domain tasks to extract the features of objects placed in different locations given an image. However, the audio feature, Mel-spectrogram † Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.</p><p>Translation Invariance (a) Image (b) Mel-Spectrogram of Audio <ref type="figure">Fig. 1</ref>. 2D convolution on image and audio input Unlike image processing, the feature in different audio frequency bands has different information.</p><p>in <ref type="figure">Figure 1</ref>(b), shows some unique characteristics depending on the frequency dimension (vertical direction). Thus, the same 2D convolution operation used in image-domain tasks may not be appropriate for the audio-domain tasks. Several studies have been reported to address this problem and propose an architecture where separate convolution layers are designed for each frequency sub-band <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, this leads to much computation and memory with the increase of the number of sub-bands. It's hard to apply this architecture to other applications since it's designed for a specific task.</p><p>To handle these problems, we consider a normalization layer commonly used in CNN. Batch normalization, one of the most widely used normalization methods, uses batch statistics to normalize each channel. But the normalization is equally performed in the frequency and temporal direction. Thus, it may not be easy to interpret the unique characteristics of each frequency band differently. Furthermore, if there is an imbalance of the scale in data, this is also kept in the normalized feature. To overcome these limitations, we propose a novel normalization technique, SubSpectral Normalization (SSN). Our method divides the frequency dimension into several sub-bands and normalizes each sub-band. By applying SSN, each band's scale imbalance can be adjusted. The convolution kernel for each band acts as a different filter by performing other affine transformations for each group.</p><p>We applied our method on two different tasks to confirm SSN's effectiveness: acoustic scene classification and keyword spotting. SSN can replace batch normalization layers of the models without increasing computation. The experimental results show that our method could significantly improve Our contributions are summarized as follows: (1) We propose SubSpectral Normalization (SSN), which splits the frequency dimension into multiple sub-bands and normalizes each group.</p><p>(2) SSN can normalize each sub-frequency band and allows a convolution filter to behave like multiple filters with only small additional parameters.</p><p>(3) SSN can improve performance by just replacing the normalization layer of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Normalization. Many normalization methods have been proposed in deep neural networks. Batch normalization (BN) <ref type="bibr" target="#b12">[13]</ref> operates normalization along the batch dimension. Some recent studies do not compute along batch dimensions to overcome the drawbacks of using batch statistics. Layer normalization <ref type="bibr" target="#b13">[14]</ref> operates along the channel dimension to improve performance in small mini-batch size in the recurrent neural networks (RNNs). Instance normalization (IN) <ref type="bibr" target="#b14">[15]</ref> normalizes each channel independently and applies it at test time and training. Group normalization (GN) <ref type="bibr" target="#b15">[16]</ref> proposes group-wise computation along the channel axis to solve the degradation of performance because of dependency on the batch size in case of small mini-batch size. Weight normalization (WN) <ref type="bibr" target="#b16">[17]</ref> performs normalization for the filter weights. Despite these various studies on normalization, the previous methods still equally normalize all features of the same channel. Different from previous normalization methods, we propose subspectral normalization (SSN) that performs along the sub-bands of frequency dimension. It is similar to apply a different convolution filter at each sub-band in spectrogram. Using sub-frequency bands. SubSpectalNet <ref type="bibr" target="#b9">[10]</ref> trains separate CNNs on sub-spectrograms divided along the frequency axis from the spectrogram, and each CNN learns properties from different frequency bands. Mcdonell et al. <ref type="bibr" target="#b10">[11]</ref> apply two parallel paths for high and low frequencies and combine the two paths using late fusion along frequency axes. Subband CNN <ref type="bibr" target="#b11">[12]</ref> splits spectrogram into overlapped sub-bands and concatenates the different features that are extracted from each sub-band after the first convolutional layer. In this paper, we reconsider the normalization layer to handle the frequency band differently. Our method requires less additional computation and has little effect on the model size. SSN can be applied to conventional CNN models by replacing the BN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SUBSPECTRAL NORMALIZATION</head><p>In this section, we present a novel normalization method, SubSpectral Normalization (SSN), which can be applied to audio-domain tasks based on 2D convolutional networks. Our method splits the input frequency dimension into several groups (sub-bands) and performs a different normalization for each group. <ref type="figure" target="#fig_0">Figure 2</ref> shows the comparison of conventional normalization methods with SSN.</p><p>Normalization methods can be expressed as follows:</p><formula xml:id="formula_0">x = 1 σ (x − µ).<label>(1)</label></formula><p>Here, x denotes the input feature, and µ and σ are the mean and standard deviation of the x, respectively. In Batch Normalization (BN), x is a feature of the same channel in a mini-batch, and µ and σ denote the mean and standard deviation of this feature x. For the SSN, we divide the frequency dimension into multiple groups, and x represents one sub-band of these groups, not the entire feature of one channel. µ and σ are also calculated for each sub-band. <ref type="figure" target="#fig_1">Figure 3</ref> is the code that implements a training mode of SSN on PyTorch. As shown in the code, SSN can be performed by separately applying Batch Normalization to each sub-band. And the frequency groups are divided equally for efficient computation. SSN gives the effect that the parameters of the following convolution layer are defined differently for each sub-band. When the number of sub-bands is S and i denotes the ith sub-band, the normalized featurex i of the sub-band feature x i can be defined as:</p><formula xml:id="formula_1">x i = γ SSN · 1 σ i (x i − µ i ) + β SSN ,<label>(2)</label></formula><p>where µ i and σ i are the mean and standard deviation for the ith sub-band. γ SSN and β SSN denote scale and shift parameters of SSN, respectively. Here, SSN's affine transformation parameters are shared by the entire frequency dimension, not each sub-band. We define this transform type as All. The SSN can perform separate affine transformation for each sub-band, which is defined as follows:</p><formula xml:id="formula_2">x i = γ SSN i · 1 σ i (x i − µ i ) + β SSN i ,<label>(3)</label></formula><p>where γ SSN i and β SSN i are scale and shift parameters for the ith sub-bands. We define this transformation type as Sub.</p><p>If there is a convolution layer following SSN, the merged parameter of two layers for each sub-band can be defined as follows:</p><formula xml:id="formula_3">W conv i = γ SSN i · W conv ,<label>(4)</label></formula><p>and</p><formula xml:id="formula_4">B conv i = γ SSN i · B conv + β SSN i .<label>(5)</label></formula><p>Here, W conv ∈ R C×(C prev ·k 2 ) and B conv ∈ R C denote the weight and bias of the next convolution layer with k × k size kernels, where C prev and C are the number of input channels and output channels, respectively. Using SSN instead of BN, the next convolution layer for the ith sub-band is defined as a function of W conv , B conv , γ SSN i and β SSN i . It means that the convolution with SSN can operate differently on each sub-band compared to the convolution with BN, which works equally on the whole frequency dimension.</p><p>When applying SSN to CNNs, the user can control the number of sub-bands and the type of affine transformation as hyper-parameters, and we denote it as SSN(S=number of subbands, A=affine type) in this paper. To this, SSN(S=1, A=All), SSN(S=1, A=Sub) and BN are equivalent operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We have experimented with our method on two different tasks. One is an acoustic scene classification, and the other is  keyword spotting. In the following experiments, we demonstrate the potential of SubSpectral Normalization (SSN) by applying it to audio tasks dealing with ambient sound and speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Acoustic Scene Classification We evaluate SSN using the TAU Urban Acoustic Scenes 2019 10 class dataset <ref type="bibr" target="#b17">[18]</ref> which consists of acoustic scene samples recorded in 12 different European cities. Each recording has the audio scene label (one of 10 scenes: e.g., 'airport' or 'shopping mall'). For the task 1A, the dataset contains the ten acoustic scenes, and the development set includes 40 hours of data with 14,400 segments.</p><p>In the experiments, we select 9,185 segments and 4,185 segments for the training and evaluation dataset, respectively: we use the split in the first fold of the validation set. Keyword Spotting We select the google speech command dataset <ref type="bibr" target="#b18">[19]</ref> to evaludate SSN on speech data. The dataset has 65,000 one-second long utterances of 30 short words, by thousands of different people. Following Google's implementation, we distinguish 12 classes: yes, no, up, down, left, right, on, off, stop, go, silence and unknown. The utterances were then randomly split into training, development, and evaluation sets in the ratio of 80:10:10, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Acoustic Scene Classification</head><p>In this section, we conduct experiments using TAU Urban Acoustic Scenes 2019 dataset. We select CP-ResNet <ref type="bibr" target="#b8">[9]</ref> as the baseline, which shows high performance with simple ResNet <ref type="table">Table 2</ref>. Results on Google Speech Command dataset. The numbers marked with * are taken from each paper, and ‡ denotes the result of training the same epoch as <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test Accuracy #Params res8 w/ BN <ref type="bibr" target="#b3">[4]</ref> 94.1% ±0.35 * 111K res15 w/ BN <ref type="bibr" target="#b3">[4]</ref> 95.8% ±0.48 * 239K TC-ResNet14-1.5 <ref type="bibr" target="#b4">[5]</ref> 96.6% * 305K EdgeSpeechNet-A <ref type="bibr" target="#b19">[20]</ref> 96.8% * 107K architecture. It uses 256 bins Mel-Spectrogram as input, and the setting of the experiment follows <ref type="bibr" target="#b8">[9]</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows the performance when we applied SSN to the baseline models. By applying SSN to CP-ResNet (ch64) with 64 base channels, we got an accuracy improvement of 1.3%. It is higher than CP-ResNet (ch128), which is four times bigger model. Input Norm is the result of normalizing the input Spectrogram by all frequency bins. This result shows that just normalizing the input cannot reach the same effect as SSN. We also applied SSN to a bigger model, CP-ResNet (ch128), and obtained a 0.9% accuracy gain. This consistent improvement shows that SSN works very effectively in acoustic scene classification. We obtained the best results when the number of sub-bands S is two, and affine transformation is applied separately for each sub-band. <ref type="figure" target="#fig_2">Figure 4</ref> shows the validation accuracy according to hyper-parameters of SSN. SSN shows better performance when the individual affine transformation is performed (SSN-Sub) than when applied to the whole frequency dimension (SSN-All). And when the number of sub-bands is between 2 and 4, SSN performs quite better than BN. But performance decreases as the number of sub-bands increases. When applying the sub-bands affine transformation to BN (BN-Sub), there is a slight accuracy improvement, but it is quite lower than SSN-Sub. These results show that proper sub-band size is more important than eliminating all frequency bin's characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Keyword Spotting</head><p>To verify our method on speech data, we evaluate SSN on the baseline <ref type="bibr" target="#b3">[4]</ref>, which has multiple 2D convolution layers with a residual architecture. The baseline receives MFCC of 40 features with a window size of 30ms and a hope size of 10ms. We conduct experiments by replacing BN with SSN, and Table 2 shows the result. Unlike in acoustic scene classification, SSN shows the best results with S of 4 in these experiments. There was a notable increase in accuracy, with a small parameter increase within 2%. SSN showed a bigger performance improvement on res8 than the large model res15, which already has high accuracy. Even though res15 has a very simple structure consisting of several residual blocks, res15 w/ SSN shows similar performance to the recent models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. By using the same training budget with <ref type="bibr" target="#b4">[5]</ref>, res15 w/ SSN shows the state of the art performance among the methods that do not use any additional noise or data. These results show that the SSN allows better processing of audio input even in simple structured models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>We check how each frequency bin changes when applying SSN to confirm the effect of SSN. <ref type="figure" target="#fig_3">Figure 5</ref> shows the scale of the activation through the convolution layer for each frequency bin. We have obtained an activation scale with the L1 norm and averaged it for each sub-band. We normalize each activation scale with zero mean and unit variance to compare each method. After the model is trained using BN, there is no significant difference from the randomly initialized model's output (BN, rand init). BN is limited to reduce each frequency bin's deviation because BN equally normalizes all frequency dimensions. On the other hand, when SSN is applied, the results confirm that each sub-band is independently normalized. The green line, SSN(S=16, A=All), shows the effect of remarkably mitigating the scale deviation between sub-bands. When performing affine transformation for each sub-band, SSN(S=16, A=Sub), our method can control a specific band's scale. It also has the effect of embedding frequency information to each sub-band.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose a novel normalization method, SubSpectral Normalization (SSN), for the frequency domain audio input. SSN divides the frequency dimension into subbands and normalizes each of them. It can remove the weight deviation between sub-frequency groups while providing frequency-aware characteristics. By changing the existing normalization layer to SSN, the user can improve the model's performance without complex model design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Normalization methods on Frequency-Time audio input, with N as batch axis, C as channels, F as frequency and T as time axis. SSN shows the case of two sub-bands. model performance by changing the existing normalization layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>PyTorch code of SubSpectral Normalization with affine transormation type Sub</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Validation accuracy of CP-ResNet (ch64) depending on the hyper-parameters of SubSpectral Normalization. Each accuracy denotes an average of 5 runs. BN-Sub denotes BN with sub-bands affine transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of activation-norm according to normalization methods in CP-ResNet(ch64).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on TAU Urban Acoustic Scenes 2019.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell>#Params</cell></row><row><cell cols="3">CP-ResNet(ch64) w/ BN</cell><cell></cell><cell></cell><cell cols="2">82.3% ±0.19</cell><cell>899K</cell></row><row><cell cols="5">CP-ResNet(ch64) w/ BN + Input Norm</cell><cell cols="2">82.7% ±0.35</cell><cell>899K</cell></row><row><cell cols="3">CP-ResNet(ch128) w/ BN</cell><cell></cell><cell></cell><cell cols="2">83.2% ±0.22</cell><cell>3,567K</cell></row><row><cell cols="5">CP-ResNet(ch64) w/ SSN(S=2, A=Sub)</cell><cell cols="2">83.6% ±0.07</cell><cell>907K</cell></row><row><cell cols="7">CP-ResNet(ch128) w/ SSN(S=2, A=Sub) 84.1% ±0.20 3,583K</cell></row><row><cell>Validation Accuracy</cell><cell>82.0 82.5 83.0 83.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BN BN-Sub SSN-All SSN-Sub</cell></row><row><cell></cell><cell>81.5</cell><cell>1</cell><cell>2</cell><cell cols="2">4 Number of Sub-bands</cell><cell>8</cell><cell>16</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Track Proceedings, Yoshua Bengio and Yann LeCun</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>3rd International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal convolution for realtime keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kersner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3372" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ossama</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep cnn encoder and rnnlm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="949" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The receptive field as a regularizer in deep convolutional neural networks for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 27th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Subspectralnet-using sub-spectrogram based convolutional neural networks for acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Samarth R Phaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Benetos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="825" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Acoustic scene classification using deep residual networks with late fusion of separated high and low frequency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sub-band convolutional neural networks for small-footprint spoken term classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh-Chi</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2195" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multi-device dataset for urban acoustic scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamaria</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)</title>
		<meeting>the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)</meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="9" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Edgespeechnets: Highly efficient deep neural networks for speech recognition on the edge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrey</forename><forename type="middle">G</forename><surname>Zhong Qiu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08559</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
