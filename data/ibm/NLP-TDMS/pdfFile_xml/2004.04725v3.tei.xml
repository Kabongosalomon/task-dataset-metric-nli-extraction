<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly supervised learning has emerged as a compelling tool for object detection by reducing the need for strong supervision during training. However, major challenges remain: (1) differentiation of object instances can be ambiguous; (2) detectors tend to focus on discriminative parts rather than entire objects; (3) without ground truth, object proposals have to be redundant for high recalls, causing significant memory consumption. Addressing these challenges is difficult, as it often requires to eliminate uncertainties and trivial solutions. To target these issues we develop an instance-aware and context-focused unified framework. It employs an instance-aware self-training algorithm and a learnable Concrete DropBlock while devising a memoryefficient sequential batch back-propagation. Our proposed method achieves state-of-the-art results on COCO (12.1% AP, 24.8% AP 50 ), VOC 2007 (54.9% AP), and VOC 2012 (52.1% AP), improving baselines by great margins. In addition, the proposed method is the first to benchmark ResNet based models and weakly supervised video object detection. Code, models, and more details will be made available at: https://github.com/NVlabs/wetectron.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent works on object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27]</ref> have achieved impressive results. However, the training process often requires strong supervision in terms of precise bounding boxes. Obtaining such annotations at a large scale can be costly, time-consuming, or even infeasible. This motivates weakly supervised object detection (WSOD) methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b22">23]</ref> where detectors are trained with weaker forms of supervision such as image-level category labels. These works typically formulate WSOD as a multiple instance learning task, treating the set of object proposals in each image as a bag. The selection of proposals that truly cover objects is modeled using learnable latent variables.</p><p>While alleviating the need for precise annotations, exist-* Work partially done at NVIDIA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grouped Instance</head><p>Missing Instance Part Domination <ref type="figure" target="#fig_1">Figure 1</ref>: Typical WSOD issues: (1) Instance Ambiguity: missing less salient objects (top) or failing to differentiate clustered instances (middle); (2) Part Domination: focusing on most discriminative object parts (bottom).</p><p>ing weakly supervised object detection methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b60">61]</ref> often face three major challenges due to the underdetermined and ill-posed nature, as demonstrated in <ref type="figure" target="#fig_1">Fig. 1</ref>:</p><p>(1) Instance Ambiguity. This arguably the biggest challenge which subsumes two common types of issues: (a) Missing Instances: Less salient objects in the background with rare poses and smaller scales are often ignored (top row in <ref type="figure" target="#fig_1">Fig. 1</ref>). (b) Grouped Instances: Multiple instances of the same category are grouped into a single bounding box when spatially adjacent (middle row in <ref type="figure" target="#fig_1">Fig. 1</ref>). Both issues are caused by bigger or more salient boxes receiving higher scores than smaller or less salient ones.</p><p>(2) Part Domination. Predictions tend to be dominated by the most discriminative parts of an object ( <ref type="figure" target="#fig_1">Fig. 1 bottom)</ref>. This issue is particularly pronounced for classes with big intra-class difference. For example, on classes such as animals and people, the model often turns into a 'face detector' as faces are the most consistent appearance signal.</p><p>(3) Memory Consumption. Existing proposal generation methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref> often produce dense proposals. Without ground-truth localization, maintaining a large number of proposals is necessary to achieve a reasonable recall rate and good performance. This requires a lot of memory, especially for video object detection. Due to the large number of proposals, most memory is consumed in the intermediate layers after ROI-Pooling.</p><p>To address the above three challenges, we propose a unified weakly supervised learning framework that is instanceaware and context-focused. The proposed method tackles Instance Ambiguity by introducing an advanced selftraining algorithm where instance-level pseudo groundtruth, in forms of category labels and regression targets are computed by considering more instance-associative spatial diversification constraints (Sec. 4.1). The proposed method also addresses Part Domination by introducing a parametric spatial dropout termed 'Concrete DropBlock.' This module is learned end-to-end to adversarially maximize the detection objective, thus encouraging the whole framework to consider context rather than focusing on the most discriminative parts (Sec. <ref type="bibr">4.2)</ref>. Finally, to alleviate the issue of Memory Consumption, our method adopts a sequential batch back-propagation algorithm which processes data in batches at the most memory-heavy stage. This permits the assess to larger deep models such as ResNet <ref type="bibr" target="#b18">[19]</ref> in WSOD, as well as the exploration of weakly supervised video object detection (Sec. 4.3).</p><p>Tackling the aforementioned three challenges via our proposed framework leads to state-of-the-art performance on several popular datasets, including COCO <ref type="bibr" target="#b29">[30]</ref>, VOC 2007 and 2012 <ref type="bibr" target="#b10">[11]</ref>. The effectiveness and robustness of each proposed module is demonstrated in detailed ablation studies, and further verified through qualitative results. Finally, we conduct additional experiments on videos and give the first benchmark for weakly supervised video object detection on ImageNet VID <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Weakly supervised object detection <ref type="bibr">(WSOD)</ref>. Object detection is one of the most fundamental problems in computer vision. Recent supervised methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27]</ref> have shown great performance in terms of both accuracy and speed. For WSOD, most methods formulate a multiple instance learning problem where input images contain a bag of instances (object proposals). The model is trained with a classification loss to select the most confident positive proposals. Modifications w.r.t. initialization <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b42">43]</ref>, regularization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">55]</ref>, and representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref> have been shown to improve results. For instance, Bilen and Vedaldi <ref type="bibr" target="#b4">[5]</ref> proposed an end-to-end trainable architecture for this task. Follow-up works further improve by leveraging spatial relations <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23]</ref>, better optimization <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b50">51]</ref>, and multitasking with weakly supervised segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Self-training for WSOD. Among the above directions, self-training <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b65">66]</ref> has been demonstrated to be seminal. Self-training uses instance-level pseudo labels to augment training and can be implemented in an offline manner <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b62">63</ref>]: a WSOD model is first trained using any of the methods discussed above; then the confident predictions are used as pseudo-labels to train a final supervised detector. This iterative knowledge distillation procedure is beneficial since the additional supervised models learn form less noisy data and usually have better architectures for which training is time-consuming. A number of works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b46">47]</ref> studied end-to-end implementations of self-training: WSOD models compute and use pseudo labels simultaneously during training, which is commonly referred to as an online solution. However, these methods typically only consider the most confident predictions for pseudo-labels. Hence they tend to have overfitting issues with difficult parts and instances ignored.</p><p>Spatial dropout. To address the above issue, an effective regularization strategy is to drop parts of spatial feature maps during training. Variants of spatial-dropout have been widely designed for supervised tasks such as classification <ref type="bibr" target="#b13">[14]</ref>, object detection <ref type="bibr" target="#b53">[54]</ref>, and human joints localization <ref type="bibr" target="#b48">[49]</ref>. Similar approaches have also been applied in weakly supervised tasks for better localization in detection <ref type="bibr" target="#b39">[40]</ref> and semantic segmentation <ref type="bibr" target="#b55">[56]</ref>. However, these methods are non-parametric and cannot adapt to different datasets in a data-driven manner. As a further improvement, Kingma et al. <ref type="bibr" target="#b23">[24]</ref> designed variational dropout where the dropout rates are learned during training. Wang et al. <ref type="bibr" target="#b53">[54]</ref> proposed a parametric but non-differentiable spatial-dropout trained with REINFORCE <ref type="bibr" target="#b57">[58]</ref>. In contrast, the proposed 'Concrete DropBlock' module has a parametric and differentiable structured novel form.</p><p>Memory efficient back-propagation. Memory has always been a concern since deeper models <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> and larger batch size <ref type="bibr" target="#b32">[33]</ref> often tend to yield better results. One way to alleviate this concern is to trade computation time for memory consumption by modifying the back-propagation (BP) algorithm <ref type="bibr" target="#b36">[37]</ref>. A suitable technique <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6]</ref> is to not store some intermediate deep net representations during forward-propagation. One can recover those by injecting small forward passes during back-propagation. Hence, the one-stage back-propagation is divided into several stepwise processes. However, this method cannot be directly applied to our model where a few intermediate layers consume most of the memory. To address it, we suggest a batch operation for the memory-heavy intermediate layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>Bilen and Vedaldi <ref type="bibr" target="#b4">[5]</ref> are among the first to develop an end-to-end deep WSOD framework based on the idea of multiple instance learning. Specifically, given an input image I and the corresponding set of pre-computed <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref> proposals R, an ImageNet <ref type="bibr" target="#b7">[8]</ref> pre-trained neural network is used to produce classification logits f w (c, r) ∈ R and detection logits g w (c, r) ∈ R for every object category c ∈ C and for every region r ∈ R. The vector w subsumes all trainable parameters. Two score matrices, i.e., s(c|r) of a region r being classified as category c, and s(r|c) of detecting region r for category c are obtained through sw(c|r) = exp fw(c, r) c∈C exp fw(c, r)</p><p>, and sw(r|c) = exp gw(c, r) r∈R exp gw(c, r)</p><p>.</p><p>The final score s w (c, r) for assigning category c to region r is computed via an element-wise product:</p><formula xml:id="formula_1">s w (c, r) = s w (c|r)s w (r|c) ∈ [0, 1]. During training, s w (c, r)</formula><p>is summed for all regions r ∈ R to obtain the image evidence φ w (c) = r∈R s w (c, r). The loss is then computed via:</p><formula xml:id="formula_2">L img (w) = − c∈C y(c) log φw(c),<label>(2)</label></formula><p>where y(c) ∈ {0, 1} is the ground truth (GT) class label indicating image-level existence of category c. For inference, s w (c, r) is used for prediction followed by standard non-maximum suppression (NMS) and thresholding.</p><p>To integrate online self-training, the region score s w (c, r) is often used as teacher to generate instance-level pseudo category labelŷ(c, r) ∈ {0, 1} for every region r ∈ R <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b46">47]</ref>. This is done by treating the top-scoring region and its highly-overlapped neighbors as the positive examples for class c. The extra student layer is then trained for region classification via:</p><formula xml:id="formula_3">L roi (w) = − 1 |R| c∈Cŷ (c, r) logŝw(c|r),<label>(3)</label></formula><p>whereŝ w (c|r) is the output of this layer. During testing, the student predictionŝ w (c|r) will be used rather than s w (c, r). We build upon this formulation and develop two additional novel modules as described subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>Image-level labels are an effective form of supervision to mine for common patterns across images. Yet inexact supervision often causes localization ambiguity. To address the mentioned three challenges caused by this ambiguity, we develop the instance-aware and context-focused framework outlined in <ref type="figure" target="#fig_0">Fig. 2</ref>. It contains a novel online selftraining algorithm with ROI regression to reduce instance ambiguity and better leverage the self-training supervision (Sec. 4.1). It also reduces part-domination for classes with large intra-class variance via a novel end-to-end learnable 'Concrete DropBlock' (Sec. 4.2), and it is more memory friendly (Sec. 4.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multiple instance self-training (MIST)</head><p>With online or offline generated pseudo-labels <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b62">63]</ref>, self-training helps to eliminate localization ambiguities, benefiting mainly from two aspects: (1) Pseudolabels permit to model proposal-level supervision and interproposal relations; (2) Self-training can be broadly regarded as a teacher-student distillation process which has been found helpful to improve the student's representation. We take the following dimensions into account when designing our framework: Instance-associative: Object detection is often 'instanceassociative': highly overlapping proposals should be assigned similar labels. Most self-training methods for WSOD ignore this and instead treat proposals independently. Instead, we impose explicit instance-associative constraints into pseudo box generation. Representativeness: The score of each proposal in general is a good proxy for its representativeness. It is not perfect, especially in the beginning there is a tendency to focus on object parts. However, the score provides a high recall for being at least located on correct objects. Spatial-diversity: Imposing spatial diversity to the selected pseudo-labels can be a useful self-training inductive bias. It promotes better coverage on difficult (e.g., rare appearance, poses, or occluded) objects, and higher recall for multiple instances (e.g., diverse scales and sizes).</p><p>The above constraints and criteria motivate a novel algorithm to generate diverse yet representative pseudo boxes which are instance-associative. The details are provided in Alg. 1. Specifically, we first sort all the scores across the set R for each class c that appears in the category-label. We then pick the top p percent of the ranked regions to form an initial candidate pool R (c). Note that the size of the candidate pool R (c), i.e., |R (c)| is image-adaptive and contentdependent by being proportional to |R|. Intuitively, |R| is a meaningful prior for the overall objectness of an input image. A diverse set of high-scoring non-overlapping regions are then picked from R (c) as the pseudo boxesR(c) using non-maximum suppression. Even though being simple, this effective algorithm leads to significant performance improvements as shown in Sec. 5. for i in {2 ... |R (c)|} do // start from the second highest 7:</p><formula xml:id="formula_4">APPEND(R(c), r i ) if IoU(r i ,r j ) &lt; τ, ∀r j ∈ R (c) : j &lt; i 8: returnR(c)</formula><p>Self-training with regression. Bounding box regression is another module that plays an important role in supervised object detection but is missing in online self-training methods. To close the gap, we encapsulate a classification layer and a regression layer into 'student blocks' as shown via blue boxes in <ref type="figure" target="#fig_0">Fig. 2</ref>. We jointly optimize them using pseudo-labelsR. The predicted bounding boxes from the regression layer are referred to via µ w (r) for all regions r ∈ R. For each region r, if it is highly overlapping with a pseudo-boxr ∈R for ground-truth class c, we generate the regression targett(r) by using the coordinates ofr and by marking the classification labelŷ(c, r) = 1. The complete region-level loss for training the student block is:</p><formula xml:id="formula_5">L roi (w) = 1 |R| r∈R λr(L smooth-L1 (t(r), µw(r)) − 1 |C| c∈Cŷ (c, r) logŝw(c|r)),<label>(4)</label></formula><p>where L smooth-L1 is the Smooth-L1 objective used in <ref type="bibr" target="#b15">[16]</ref> and λ r is a scalar per-region weight used in <ref type="bibr" target="#b45">[46]</ref>. In practice, conflicts happen when we force theŷ(·, r) to be a one-hot vector since the same region can be chosen to be positive for different ground-truth classes, especially in the early stages of training. Our solution is to use that class for pseudo-labelr which has a higher predicted score s(c,r). In addition, the obtained pseudo-labels and the proposals are inevitably noisy. Imposing bounding box regression is able to correctly learn from the noisy labels by capturing the most consistent patterns among them, and refining the noisy proposal coordinates accordingly. We empirically verify in Sec. 5.3 that bounding box regression improves both robustness and generalization.</p><p>Self-ensembling. We follow <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref> to stack multiple student blocks to improve performance. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the first pseudo-labelR 1 is generated from the teacher branch, and then the student block N generates pseudo-labelR N for the next student block N + 1. This technique is similar to the self-ensembling method <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concrete DropBlock</head><p>Because of the intra-category variation, existing WSOD methods often mistakenly only detect the discriminative parts of an object rather than its full extent. A natural solution for this issue encourages the network to focus on the context which can be achieved by dropping the most discriminative parts. Hence, spatial dropout is an intuitive fit. Naïve spatial dropout has limition for detection since the discriminative parts of objects differ in location and size. A more structured DropBlock <ref type="bibr" target="#b13">[14]</ref> was proposed where spatial points on ROI feature maps are sampled randomly as blob centers, and the square regions around these centers of size H × H are then dropped across all channels on the ROI feature map. Finally, the feature values are re-scaled by a factor of the area of the whole ROI over the area of the undropped region so that no normalization has to be applied for inference when no regions are dropped.</p><p>DropBlock is a non-parametric regularization technique. While it is able to improve model robustness and alleviate part domination, it basically treats regions equally. We consider dropping more frequently at discriminative parts in an adversarial manner. To this end, we develop the Concrete DropBlock: a data-driven and parametric variant of Drop-Block which is learned end-to-end to drop the most relevant regions as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Given an input image, the feature maps ψ w (r) ∈ R H×H are computed for each region r ∈ R using the layers up until ROI-Pooling. H is the ROI-Pooling output dimension. We then feed ψ w (r) into a convolutional residual block to generate a probability map p θ (r) ∈ R H×H ∀r ∈ R where θ subsumes the trainable parameters of this module. Each element of p θ (r) is regarded as an independent Bernoulli variable, and this probability map is transformed via a spatial Gumbel-Softmax <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref> into a hard mask M θ (r) ∈ {0, 1} H×H ∀r ∈ R. This operation is a differentiable approximation of sampling. To avoid trivial solutions (e.g., everything will be dropped or a certain area is dropped consistently), we apply a threshold τ such that p θ (r) = min(p θ (r), τ ). This guarantees that the computed mask M θ (r) is sparse. We follow DropBlock to finally generate the structured mask and normalize the features. During training, we jointly optimize the original network parameters w and the residual block parameters θ with the following minmax objective:  <ref type="figure">Figure 7</ref>: Seq-BBP: blue, yellow, and green blobs represent activation, gradients, and the module that is being updated.</p><formula xml:id="formula_6">w * , θ * = arg min w max θ I L img (w, θ) + L roi (w, θ). (5)</formula><formula xml:id="formula_7">(b) Split A b , G n into sub-batches to update 'Neck'. G b accumulated. Base Gb : 2000×CHW Img ROI-P (c) Use G b to update 'Base' network.</formula><p>By maximizing the original loss w.r.t. the Concrete Drop-Block parameters, the Concrete DropBlock will learn to drop the most discriminative parts of the objects, as it is the easiest way to increase the training loss. This forces the object detector to also look at the context regions. We found this strategy to improve performance especially for nonrigid object categories, which usually have a large intraclass difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sequential batch back-propagation</head><p>In this section, we discuss how we propose to handle memory limitations particularly during training, which turn out to be a major bottleneck preventing previous WSOD methods from using state-of-the-art deep nets. We introduce our memory-efficient sequential batch forward and backward computation, tailored for WSOD models.</p><p>Vanilla training via back-propagation <ref type="bibr" target="#b36">[37]</ref> stores all intermediate activations during the forward pass, which are reused when computing gradients of network parameters. This method is computationally efficient due to memoization, yet memory-demanding for the same reason. More efficient versions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6]</ref> have been proposed, where only a subset of the intermediate activations are saved during a forward pass at key layers. The whole model is cut into smaller sub-networks at these key layers. When computing gradients for a sub-network, a forward pass is first applied to obtain the intermediate representations for this subnetwork, starting from the stored activation at the input key layer of the sub-network. Combined with the gradients propagated from earlier sub-networks, the gradients of sub-network weights are computed and gradients are also propagated to outputs of earlier sub-networks.</p><p>This algorithm is designed for extremely deep networks where the memory cost is roughly evenly distributed along the layers. However, when these deep nets are adapted for detection, the activations (after ROI-Pooling) grow from 1 × CHW (image feature) to N × CHW (ROI-features) where N is in the thousands for weakly supervised models. Without ground-truth boxes, all these proposals need to be maintained for high recall and thus good performance (see the evidence in Appendix F).</p><p>To address this training challenge, we propose a sequential computation in the 'Neck' sub-module as depicted in <ref type="figure">Fig. 7</ref>. During the forward pass, the input image is first passed through the 'Base' and 'Neck,' with only the activation A b after the 'Base' stored. The output of the 'Neck'   then goes into the 'Head' for its first forward and backward pass to update the weights of the 'Head' and the gradients G n as shown in <ref type="figure">Fig. 7</ref> (a). To update the parameters of the 'Neck,' we split the ROI-features into 'sub-batches' and run back-propagation on each small sub-batch sequentially. Hence we avoid storing memory-consuming feature maps and their gradients within the 'Neck.' An example of this sequential method is shown in <ref type="figure">Fig. 7</ref> (b), where we split 2000 proposals into two sub-batches of 1000 proposals each. The gradient G b is accumulated and used to update the parameters of the 'Base' network via regular backpropagation as illustrated in <ref type="figure">Fig. 7</ref> (c). For testing, the same strategy can be applied if either the number of ROIs or the size of the 'Neck' is too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We assess our proposed method subsequently after detailing dataset, evaluation metrics and implementation.</p><p>Dataset and evaluation metrics. We first conduct experiments on COCO <ref type="bibr" target="#b29">[30]</ref>, which is the most popular dataset used for supervised object detection but rarely studied in WSOD. We use the COCO 2014 train/val/test split and report standard COCO metrics including AP (averaged over IoU thresholds) and AP 50 (IoU threshold at 50%).</p><p>We then evaluate on both VOC 2007 and 2012 <ref type="bibr" target="#b10">[11]</ref>, which are commonly used to assess WSOD performance. Average Precision (AP) with IoU threshold at 50% is used to evaluate the accuracy of object detection (Det.) on the testing data. We also evaluate correct localization accuracy (CorLoc.), which measures the percentage of training images of a class for which the most confident predicted box has at least 50% IoU with at least one ground-truth box.</p><p>Implementation details. For a fair comparison, all settings of the VGG16 model are kept identical to <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref> except those mentioned below. We use 8 GPUs during training with one input image per device. SGD is used for optimization. The default p and IoU in our proposed MIST technique (Alg. 1) are set to 0.15 and 0.2. For the Concrete DropBlock τ = 0.3. The ResNet models are identical to <ref type="bibr" target="#b15">[16]</ref>. Please check Appendix A for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overall performance</head><p>VGG16-COCO. We compare to state-of-the-art WSOD methods on COCO in Tab. 1. Our single model without any post-processing outperforms all previous approaches (w/ bells and whistles) by a great margin. On the private Test-dev benchmark, we increase AP 50 by 11.2 (+82.3%). For the 2014 validation set, we increase AP and AP 50 by 0.6 (+5.6%) and 1.6 (+7.1%). Complete results are provided in Appendix B. Note that compared to supervised models shown in the first two rows, the performance gap is still relatively big: ours is 56.9% of Faster R-CNN on average. In addition, our model achieves 12.4 AP and 25.8 AP 50 on the COCO 2017 split as reported in Tab. 4, which is more commonly adopted in supervised papers. ResNet-COCO. ResNet models have never been trained and evaluated before for WSOD. Nonetheless, they are the most popular backbone networks for supervised methods. Part of the reason is the larger memory consumption of ResNet. Without the training techniques introduced in Sec. 4.3, it's impossible to train on a standard GPU using all proposals. In Tab. 2 we provide the first benchmark for the COCO dataset using ResNet-50 and ResNet-101. As expected we observe ResNet models to perform better than the VGG16 model. Moreover, we note that the difference between ResNet-50 and ResNet-101 is relatively small. VGG16-VOC. To fairly compare with most previous WSOD works, we also evaluate our approach on the VOC datasets <ref type="bibr" target="#b10">[11]</ref>. The comparison to most recent works is reported in Tab. <ref type="bibr" target="#b2">3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative results</head><p>Qualitatively, we compare our full model with Tang et al. <ref type="bibr" target="#b45">[46]</ref>. In <ref type="figure" target="#fig_3">Fig. 8</ref> we show a set of two pictures side by side, with baselines on the left and our results on the right. Our model is able to address instance ambiguity by: (1) detecting previously ignored instances ( <ref type="figure" target="#fig_3">Fig. 8 left)</ref>; (2) predicting tight and precise boxes for multiple instances instead of a big one ( <ref type="figure" target="#fig_3">Fig. 8 center)</ref>. Part domination is also alleviated since our model focuses on the full extent of objects ( <ref type="figure" target="#fig_3">Fig. 8  right)</ref>. Even though our model can greatly increase the score * http://host.robots.ox.ac.uk:8080/anonymous/DCJ5GA. html  of larger boxes (see the horse example), the predictions may still be dominated by parts in some difficult cases. More qualitative results are shown in <ref type="figure" target="#fig_4">Fig. 9</ref> for all three datasets we used, as well in Appendix D. Our model is able to detect multiple instances of the same category (cow, sheep, bird, apple, person) and various objects of different classes (food, furniture, animal) in relatively complicated scenes. The COCO dataset is much harder than VOC as the number of objects and classes is bigger. Our model still tells apart objects decently well ( <ref type="figure" target="#fig_4">Fig. 9 bottom row)</ref>. We also show some failure cases ( <ref type="figure" target="#fig_4">Fig. 9</ref> right column) of our model which can be roughly categorized into three types: (1) relevant parts are predicted as instances of objects (hands and legs, bike wheels); <ref type="bibr" target="#b1">(2)</ref> in extreme examples, part domination remains (model converges to a face detector); (3) object co-occurrence confuses the detector when it predicts the sea as a surfboard or the baseball court as a bat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing Instance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grouped Instance Part Domination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis</head><p>How much does each module help? We study the effectiveness of each module in Tab. 5. We first reproduce the method of Tang et al. <ref type="bibr" target="#b45">[46]</ref>, achieving similar results (first two rows). Applying the developed MIST module improves the results significantly. This aligns with our observation that instance ambiguity is the biggest bottleneck for WSOD. Our conceptually simple solution also outperforms an improved version <ref type="bibr" target="#b44">[45]</ref> (PCL), which is based on a computationally expensive and carefully-tuned clustering.</p><p>The devised Concrete DropBlock further improves the performance when using MIST as the basis. This module surpasses several variants including: (1) (Img Spa.-Dropout): spatial dropout applied on the image-level features; (2) (ROI-Spa.-Dropout): spatial dropout applied on each ROI where each feature point is treated independently. This setting is similar to <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b53">54]</ref>; (3) (DropBlock): the bestperforming DropBlock setting reported in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Has Instance Ambiguity been addressed? To validate that instance ambiguity is alleviated, we report Average Recall (AR) over multiple IoU values (.50 : .05 : .95), given 1, 10, 100 detections per image (AR 1 , AR 10 , AR 100 ) and for small, medium, annd large objects (AR s , AR m , AR l ) on VOC 2007. We compare the model with and without MIST in Tab. 6 where our method increases all recall metrics.</p><p>Has Part Domination been addressed? In <ref type="figure" target="#fig_1">Fig. 10</ref>     improvements on the VOC 2007 and VOC 2012 dataset after applying the Concrete DropBlock. The performance of animal classes including 'person' increases most, which matches our intuition mentioned in Sec. 1: the part domination issue is most prominent for articulated classes with rigid and discriminative parts. Across both datasets, three out of the five top classes are mammals.</p><p>Space-time analysis of sequential batch BP? We also study the effect of our sequential batch back-propagation. We fix the input image to be of size 600 × 600, and run two methods (vanilla back-propagation and ours with sub-batch size 500 using ResNet-101 for comparison. We change the number of proposals from 1k to 5k in 1k increments, and report average training iteration time and memory consumption in <ref type="figure" target="#fig_1">Fig. 11</ref>. We observe: (1) vanilla back-propagation cannot even afford 2k proposals (average number of ROIs widely used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b45">46]</ref>) on a standard 16GB GPU, but ours can easily handle up to 4k boxes; (2) the training process is not greatly slowed down, ours takes ∼1-2× more   time than the vanilla version. In practice, input resolution and total number of proposals can be bigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of MIST?</head><p>To assess robustness we test a baseline model plus this algorithm only using different toppercentage p and rejection IoU on the VOC 2007 dataset. Results are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. The best result is achieved with p = 0.15 and IoU = 0.2, which we use for all the other models and datasets. Importantly, we note that, overall, the sensitivity of the final results on the value of p is small and only slightly larger for IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Extension: video object detection</head><p>We finally generalize our models to video-WSOD, which hasn't been explored in the literature. Following supervised methods, we experiment on the most popular dataset: Im-ageNet VID <ref type="bibr" target="#b7">[8]</ref>. Frame-level category labels are available during training. Uniformly sampled key-frames are used for training following <ref type="bibr" target="#b63">[64]</ref> and evaluation settings are also kept identical. Results are reported in Tab. 7. The performance improvement of the proposed MIST and Concrete Drop-Block generalize to videos. The memory-efficient sequential batch back-propagation permits to leverage short-term motion patterns (i.e., we use optical-flow following <ref type="bibr" target="#b63">[64]</ref>) to further increase the performance. This suggests that videos are a useful domain where we can obtain more data to improve WSOD. Full details are provided in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we address three major issues of WSOD. For each we have proposed a solution and demonstrated its effectiveness through extensive experiments. We achieve state-of-the-art results on popular datasets (COCO, VOC 07 and 12) and are the first to benchmark ResNet backbones and weakly supervised video object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>In this section, we provide additional implementation details for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Backbones</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VGG-16</head><p>We use the standard VGG-16 (without batch normalization) as backbone. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, the 'Base' network contains all the convolutional layers before the fully-connected layers. Following <ref type="bibr" target="#b45">[46]</ref>, we remove the last max-pooling layer, and replace the penultimate maxpooling layer and the subsequent convolutional layers with dilated convolutional (dilation=2) layers to increase the feature map resolution. Standard RoI-pooling is used for computing region-level features. We use the fully-connected layers of VGG-16 except the last classifier layer as the 'Neck'. After 'Neck', the RoI features are projected to f w , g w ,ŝ w ,μ w using 4 single fully-connected layers.</p><p>ResNets We use the ResNet-50/101-C4 variant from Detectron code repository <ref type="bibr" target="#b14">[15]</ref>. Convolutional layers of the first 4 ResNet stages (C1-C4) are used as 'Base' and the last stage (C5) is used as 'Neck'. Standard RoI-pooling is used, and RoI features are projected using linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Concrete DropBlock</head><p>Concrete DropBlock is implemented as a standard residual block as in ResNets. It takes as input the RoI features and output a 1 channel heatmap p θ (r). On the skip connection we use 1×1 convolution to reduce feature channels. We then generate the hard mask M θ (r) using Gumbel-softmax, and the structured dropout region as in DropBlock <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Student Blocks</head><p>Following <ref type="bibr" target="#b45">[46]</ref>, we stack 3 student blocks. During training, student block N generates pseudo labels for the next student block N + 1. During testing, we average the predictions of all student blocks as final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Training</head><p>Our code is implemented in PyTorch and all the experiments are conducted on single 8-GPU (NVIDIA V100) machine. SGD is used for optimization with weight decay 0.0001 and momentum 0.9. The batch size and initial learning rate is set to 8 and 0.01 on VOC 2007; 16 and 0.02 on VOC 2012. On both datasets we train the model for 30k iterations and decay the learning rate by 0.1 at 20k and 26k steps. On COCO, we train the model for total 130k iterations and decay the learning rate at 90k and 120k steps with batch size 8 and initial learning rate 0.01. We use Selective-Search (SS) <ref type="bibr" target="#b49">[50]</ref> for VOC datasets and MCG <ref type="bibr" target="#b0">[1]</ref> for COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Data Augmentation &amp; Inference</head><p>Multi-scale inputs (480, 576, 688, 864, 1000, 1200) are used during both training and testing following <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b22">23]</ref> and the longest image side to set to less than 2000. At test time, the scores are averaged over all scales and their horizontal flips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional quantitative results on COCO</head><p>In Tab. 8, we report quantitative results at different thresholds and scales on COCO for different models. The reported metrics include: Average Prevision (AP ) over multiple IoU thresholds (.50 : .05 : .95), at IoU threshold 50% and 75% (AP 50 , AP 75 ), and for small, medium and large objects (AP s , AP m , AP l ); and Average Recall (AR) over multiple IoU values (.50 : .05 : .95), given 1, 10 and 100 detections per image (AR 1 , AR 10 , AR 100 ); and for small, medium and large objects (AR s , AR m , AR l ). The results in Tab. 8 show that object size is a significant factor that influences the detection accuracy. The detector tends to perform better on large objects rather than smaller ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional results on VOC C.1. Per-class detection results</head><p>In Tab. 9 and Tab. 10, we report the per-class detection APs on the test sets of both VOC 2007 and 2012. Compared to other WSOD methods we observe: (1) Our method outperforms all others on most categories (10 classes on VOC   EB -    2007, 14 classes on VOC 2012). <ref type="bibr" target="#b1">(2)</ref> The classes that are hard for our approach (e.g., boat, plant, and chair) are also challenging for other methods. This suggests that these categories are essentially hard examples for WSOD methods, for which a certain amount of strong supervision might still be needed. Compared to supervised models (Fast R-CNN, Faster R-CNN) we note: (1) Our weakly supervised model performs competitively for classes such as: airplane, bicycle, bus, car, cow, motorbike, sheep, tv-monitor, where the performance gap is usually less than 10% AP. Our model sometimes even outperforms supervised models on categories that are considered relatively easy with small intra-class difference (bicycle and motorbike in VOC 2007, motorbike and tv-monitor in VOC 2012). (2) For classes like boat, chair, dinning table, person, all WSOD methods are significantly worse than supervised methods. This is likely due to a large intra-class variation. WSOD methods fail to capture the consistent patterns of these classes.</p><formula xml:id="formula_8">- - - - - - - - - - - - - - - - - - - 37.9 Shen [38] SS - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_9">SS - - - - - - - - - - - - - - - - - - - - 40.8 Wan [52] SS - - - - - - - - - - - - - - - - - - - - 42.4 SDCN [29] SS - - - - - - - - - - - - - - - - - - - - 43.</formula><formula xml:id="formula_10">SS - - - - - - - - - - - - - - - - - - - - 46.7 WSOD2 [61] SS - - - - - - - - - - - - - - - - - - - - 47.2 Arun [2] SS - - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Per-class correct localization results</head><p>In Tab. 11 and Tab. 12, we report the per-class correct localization (CorLoc) results on the trainval sets of both VOC 2007 and VOC 2012. Consistent with prior <ref type="bibr">Wan</ref>     <ref type="bibr" target="#b27">[28]</ref> EB      </p><formula xml:id="formula_11">[52] SS - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_12">SS - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_13">- - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_14">SS - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_15">SS - - - - - - - - - - - - - - - - - - - - 63.</formula><formula xml:id="formula_16">SS - - - - - - - - - - - - - - - - - - - - 67.9 C-MIL [51] SS - - - - - - - - - - - - - - - - - - - -</formula><formula xml:id="formula_17">SS - - - - - - - - - - - - - - - - - - - - 69.5 WSOD2 [61] SS - - - - - - - - - - - - - - - - - - - - 71.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Results on static-image datasets</head><p>We show additional results that highlight cases of 'Instance Ambiguity' and 'Part Domination' in <ref type="figure" target="#fig_1">Fig. 13</ref> and <ref type="figure" target="#fig_1">Fig. 14,</ref> respectively. Following the main paper, we compare our final model to a baseline without the modules proposed in Sec. 4.1 and Sec. 4.2 of the main paper to demon- † http://host.robots.ox.ac.uk:8080/anonymous/DCJ5GA.html strate the effectiveness of these two modules visually. We show a set of two pictures side by side, the baseline on the left and ours on the right. From the results, we observe: <ref type="bibr" target="#b0">(1)</ref> we have addressed the 'Missing Instances' issue and previously ignored objects are detected with great recall (e.g., monitor, sheep, car, and person in <ref type="figure" target="#fig_1">Fig. 13</ref>); (2) we have addressed the 'Grouped Instances' issue as our model predicts tight and precise boxes for multiple instances rather than one big one (e.g., bus, motor, boat, car in <ref type="figure" target="#fig_1">Fig. 13</ref>); (3) we have also alleviated the 'Part Domination' issue for objects like dog, cat, sheep, person, horse, and sofa (see <ref type="figure" target="#fig_1">Fig. 14)</ref>.</p><p>We also provide additional visualization of our results on COCO in <ref type="figure" target="#fig_1">Fig. 15</ref>. We obtain these results by running the VGG16 based model on the COCO 2014 validation set. Our model is able to detect different instances of the same category (e.g., car, elephant, pizza, cow, umbrella) and various objects of different classes in relatively complicated scenes, and the obtained boxes can cover the whole objects pretty well rather than simply focusing on discriminative parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Results on ImageNet VID dataset</head><p>Additional visualizations of our obtained results on Im-ageNet VID are shown in <ref type="figure" target="#fig_1">Fig. 16</ref>, where the frames of the same video are illustrated in the same row. These results are obtained using the ResNet-101 based model. We ob-serve: our model is able to handle objects of different poses, scales, and viewpoints in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proposal statistics</head><p>For consistency with prior literature, we use Selective-Search (SS) <ref type="bibr" target="#b49">[50]</ref> for VOC and MCG <ref type="bibr" target="#b0">[1]</ref> for COCO. Both methods generate around 2K proposals on average as shown in Tab. 13 but occasionally yield more than 5K on certain images. Our Sequential batch back-propagation can handle these cases easily even with ResNet-101, while other methods quickly run out of memory ( <ref type="figure" target="#fig_1">Fig. 11</ref> in main paper).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Need for redundant proposals</head><p>In WSOD, since ground-truth boxes are missing, object proposals have to be redundant for high recall rates, consuming significant amounts of memory. To study the need for a large number of proposals we randomly sample p percent of all proposals. A VGG16 based model on VOC 2007 is used. The results are summarized in Tab. 14. Reducing the number of proposals even by a small amount significantly reduces accuracy: using 95% of the proposals causes a 2.8% AP drop. This suggests that all proposals should be used for best performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional details on video experiments</head><p>In this section, we provide additional details of Sec. 5.4. Following supervised methods for video object detection <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b58">59]</ref>, we experiment on the most popular dataset: ImageNet VID <ref type="bibr" target="#b7">[8]</ref>. Frame-level category labels are available during training. For each video, we use the uniformly sampled 15 key-frames from <ref type="bibr" target="#b63">[64]</ref> for training. For evaluation, we test on the standard validation set, where perframe spatial object detection results are evaluated for all the videos.</p><p>The two models 'Ours' and 'Ours (MIST only)' are two single-frame baselines with or without Concrete DropBlock (main paper Sec. 4.2). In addition, the memory-efficient sequential batch back-propagation (main paper Sec. 4.3) permits to leverage short-term motion patterns (i.e., opticalflow) to further increase the performance. For 'Ours+flow,'</p><p>we first use FlowNet2 <ref type="bibr" target="#b19">[20]</ref> to compute optical flow between neighboring frames and the reference frame. The estimated flow maps are then used to warp the nearby frames' feature maps to linearly sum with the reference frame for representation enhancement. The accumulated features are then fed into the proposed task head (modules after 'Base' in main paper <ref type="figure" target="#fig_0">Fig. 2)</ref> for weakly supervised training. This method combines the flow-guided feature warping method as discussed in <ref type="bibr" target="#b63">[64]</ref> to leverage temporal coherence and the proposed WSOD task head to handle frame-level weak supervision. Hence it achieves better results than the aforementioned two baselines ('Ours' and 'Ours (MIST only)') using both VGG16 and ResNet-101 as reported in Tab. 7.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall framework. ROI-Pooling and the operations in Eq. (1) are abstracted away for readability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Multiple Instance Self-Training Input: Image I, class label y, proposals R, threshold τ , percentage p Output: Pseudo boxesR 1 1: Feed I into model; get ROI scores s 2: for ground-truth class c do 3: R(c) sorted ← SORT(s(c, * )) //sort ROIs by scores of class c 4: R (c) ← top p percent of R(c) sorted 5:R(c) ← r 0 // save first region (top-scoring) r 0 ∈ R 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the Concrete DropBlock idea. Discriminative parts such as head are zeroed out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of our models (right picture in pair) to our baseline (left picture in pair).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>More visualization (top: VOC 2007, middle: VOC 2012, bottom: COCO) and some failure cases (right column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Top-5 classes with biggest performance boost when using Concrete DropBlock. Animal classes are emphasized using green color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>ResNet-101 model memory consumption using different methods and different number of proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>VOC 2007 results for different p and IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Examples that highlight cases of 'Instance Ambiguity'. For every pair: baseline (left) and our model (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Examples that highlight cases of 'Part Domination'. For every pair: baseline (left) and our model (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Additional visualization results of the proposed method on the COCO2014 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Additional visualization results of the proposed method on the ImageNet VID validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Forward and back-prop to update 'Head'. A b , G n saved.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">G n : 2000×C'H'W'</cell><cell>G b1 : 1000×CHW</cell><cell>G n1 : 1000×C'H'W' Sub-batch 1</cell><cell>Sub-batch 2</cell></row><row><cell>Img</cell><cell>Base</cell><cell>ROI-P</cell><cell>Neck</cell><cell>Head</cell><cell></cell><cell></cell><cell>G b2 : 1000×CHW Neck</cell><cell>G n2 : 1000×C'H'W'</cell></row><row><cell cols="6">Ab : 2000×CHW (a) A b1 : 1000×CHW</cell><cell>Neck</cell><cell>A b2 : 1000×CHW</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Single model results (VGG16) on COCO.</figDesc><table><row><cell>Methods</cell><cell cols="2">Proposal Backbone</cell><cell>AP</cell><cell>AP 50</cell></row><row><cell>Faster R-CNN</cell><cell>RPN</cell><cell>R101-C4</cell><cell>27.2</cell><cell>48.4</cell></row><row><cell>Ours</cell><cell>MCG</cell><cell>VGG16</cell><cell>11.4</cell><cell>24.3</cell></row><row><cell>Ours</cell><cell>MCG</cell><cell>R50-C4</cell><cell>12.6</cell><cell>26.1</cell></row><row><cell>Ours</cell><cell>MCG</cell><cell>R101-C4</cell><cell>13.0</cell><cell>26.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Single model results (ResNet) on COCO 2014 val.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>. All entries in this table are single model results. For object detection, our single-model results surpass all previous approaches on the publicly available 2007 test set (+1.3 AP 50 ) and on the private 2012 test set (+1.9 AP 50 ). In addition, our single model also performs better than all previous methods with bells and whistles (e.g., '+FRCNN': supervised re-training, '+Ens.': model ensemble). Combining the 2007 and 2012 training set, our model achieves 58.1% (+2.1 AP 50 ) on the 2007 test set as reported in Tab. 4. CorLoc results on the training set and per-class results are provided in Appendix C. Since VOC is easier</figDesc><table><row><cell>Methods</cell><cell>Proposal</cell><cell>07-AP 50</cell><cell>12-AP 50</cell></row><row><cell>Fast R-CNN</cell><cell>SS</cell><cell>66.9</cell><cell>65.7</cell></row><row><cell>Faster R-CNN</cell><cell>RPN</cell><cell>69.9</cell><cell>67.0</cell></row><row><cell>WSDDN [5]</cell><cell>EB</cell><cell>34.8</cell><cell>-</cell></row><row><cell>OICR [46]</cell><cell>SS</cell><cell>41.2</cell><cell>37.9</cell></row><row><cell>PCL [45]</cell><cell>SS</cell><cell>43.5</cell><cell>40.6</cell></row><row><cell>SDCN [29]</cell><cell>SS</cell><cell>50.2</cell><cell>43.5</cell></row><row><cell>Yang et al. [60]</cell><cell>SS</cell><cell>51.5</cell><cell>45.6</cell></row><row><cell>C-MIL [51]</cell><cell>SS</cell><cell>50.5</cell><cell>46.7</cell></row><row><cell>WSOD2 [61]</cell><cell>SS</cell><cell>53.6</cell><cell>47.2</cell></row><row><cell>Pred Net [2]</cell><cell>SS</cell><cell>52.9</cell><cell>48.4</cell></row><row><cell>C-MIDN [12]</cell><cell>SS</cell><cell>52.6</cell><cell>50.2</cell></row><row><cell>C-MIL [51]+FRCNN</cell><cell>SS</cell><cell>53.1</cell><cell>-</cell></row><row><cell>SDCN [29]+FRCNN</cell><cell>SS</cell><cell>53.7</cell><cell>46.7</cell></row><row><cell>Pred Net [2]+Ens.+FRCNN</cell><cell>SS</cell><cell>53.6</cell><cell>49.5</cell></row><row><cell>Yang et al. [60]+Ens.+FRCNN</cell><cell>SS</cell><cell>54.5</cell><cell>49.5</cell></row><row><cell>C-MIDN [12]+FRCNN</cell><cell>SS</cell><cell>53.6</cell><cell>50.3</cell></row><row><cell>Ours (single)</cell><cell>SS</cell><cell>54.9</cell><cell>52.1 *</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Single model (VGG16) detection results on VOC.</figDesc><table><row><cell>Data-Split</cell><cell>07-Trainval</cell><cell>12-Trainval</cell><cell>07-Test</cell></row><row><cell>Metrics</cell><cell>CorLoc</cell><cell>CorLoc</cell><cell>Det</cell></row><row><cell>Ours-07</cell><cell>68.8</cell><cell>-</cell><cell>54.9</cell></row><row><cell>Ours-12</cell><cell>-</cell><cell>70.9</cell><cell>56.3</cell></row><row><cell>WSOD2(07+12) [61]</cell><cell>71.4</cell><cell>72.2</cell><cell>56.0</cell></row><row><cell>Ours-(07+12)</cell><cell>71.8</cell><cell>72.9</cell><cell>58.1</cell></row><row><cell>Metrics</cell><cell>17-Val-AP</cell><cell>17-Val-AP 50</cell><cell>17-Val-AP 75</cell></row><row><cell>Ours-Train2014</cell><cell>11.4</cell><cell>24.3</cell><cell>9.4</cell></row><row><cell>Ours-Train2017</cell><cell>12.4</cell><cell>25.8</cell><cell>10.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Does more data help? than COCO, the performance gap to supervised methods is smaller: ours is 78.1% of Faster R-CNN on average. Additional training data. The biggest advantage of WSOD methods is the availability of more data. Therefore, we are interested in studying whether more training data im- proves results. We train our model on the VOC 2007 train- val (5011 images), 2012 trainval (11540 images), and the combination of both (16555 images) separately, and evalu- ate on the VOC 2007 test set. As shown in Tab. 4 (top), the performance increase consistently with the amount of train- ing data. We verify this on COCO where 2014-train (82783 images) and 2017-train (128287 images) are used for train- ing, and 2017-val (a.k.a. minival) for testing. Similar results are observed as shown in Tab. 4 (bottom).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study. (*: our implementation)</figDesc><table><row><cell>Metrics</cell><cell>AR 1</cell><cell>AR 10</cell><cell>AR 100</cell><cell>AR s</cell><cell>AR m</cell><cell>AR l</cell></row><row><cell>w/o MIST</cell><cell>18.6</cell><cell>30.6</cell><cell>32.5</cell><cell>8.8</cell><cell>25.8</cell><cell>38.9</cell></row><row><cell>w/ MIST</cell><cell>20.5</cell><cell>37.8</cell><cell>43.9</cell><cell>15.0</cell><cell>34.8</cell><cell>51.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Average Recall (AR) (%) comparison.</figDesc><table><row><cell>0.25 0.30 0.35</cell><cell>VOC 2007 Det. Relative mAP change</cell><cell>0.20 0.25</cell><cell>VOC 2012 Det. Relative mAP change</cell></row><row><cell>0.20</cell><cell></cell><cell>0.15</cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell>0.10</cell><cell></cell></row><row><cell>0.05 0.10</cell><cell></cell><cell>0.05</cell><cell></cell></row><row><cell>0.00</cell><cell>cat dog diningtable bird sofa</cell><cell>0.00</cell><cell>train person horseaeroplane dog</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Video Object Detection Results.</figDesc><table><row><cell>mAP / Acc</cell><cell>49 53 57 65 61</cell><cell></cell><cell></cell><cell></cell><cell>Det. CorLoc</cell><cell>mAP / Acc</cell><cell>46 62 50 54 58</cell><cell></cell><cell></cell><cell></cell><cell>Det. CorLoc</cell></row><row><cell></cell><cell>45</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20 P</cell><cell>0.30</cell><cell></cell><cell>42</cell><cell>0.1</cell><cell>0.2</cell><cell>IoU</cell><cell>0.3</cell><cell>0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Single model detection results on COCO.</figDesc><table><row><cell>Methods</cell><cell cols="6">Proposal Aero Bike Bird Boat Bottle Bus Car</cell><cell cols="11">Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train</cell><cell>TV</cell><cell>AP</cell></row><row><cell>Fast R-CNN</cell><cell>SS</cell><cell cols="3">73.4 77.0 63.4 45.4</cell><cell>44.6</cell><cell cols="6">75.1 78.1 79.8 40.5 73.7 62.2 79.4</cell><cell>78.1</cell><cell>73.1</cell><cell>64.2</cell><cell>35.6</cell><cell>66.8</cell><cell>67.2 70.4</cell><cell>71.1</cell><cell>66.0</cell></row><row><cell>Faster R-CNN</cell><cell>RPN</cell><cell cols="3">70.0 80.6 70.1 57.3</cell><cell>49.9</cell><cell cols="6">78.2 80.4 82.0 52.2 75.3 67.2 80.3</cell><cell>79.8</cell><cell>75.0</cell><cell>76.3</cell><cell>39.1</cell><cell>68.3</cell><cell>67.3 81.1</cell><cell>67.6</cell><cell>69.9</cell></row><row><cell>Cinbis [7]</cell><cell>SS</cell><cell>35.8 40.6</cell><cell>8.1</cell><cell>7.6</cell><cell>3.1</cell><cell cols="2">35.9 41.8 16.8</cell><cell>1.4</cell><cell>23.0</cell><cell>4.9</cell><cell>14.1</cell><cell>31.9</cell><cell>41.9</cell><cell>19.3</cell><cell>11.1</cell><cell>27.6</cell><cell>12.1 31.0</cell><cell>40.6</cell><cell>22.4</cell></row><row><cell>Bilen [4]</cell><cell>SS</cell><cell cols="3">46.2 46.9 24.1 16.4</cell><cell>12.2</cell><cell cols="2">42.2 47.1 35.2</cell><cell>7.8</cell><cell cols="3">28.3 12.7 21.5</cell><cell>30.1</cell><cell>42.4</cell><cell>7.8</cell><cell>20.0</cell><cell>26.8</cell><cell>20.8 35.8</cell><cell>29.6</cell><cell>27.7</cell></row><row><cell>Wang [53]</cell><cell>SS</cell><cell cols="3">48.8 41.0 23.6 12.1</cell><cell>11.1</cell><cell cols="6">42.7 40.9 35.5 11.1 36.6 18.4 35.3</cell><cell>34.8</cell><cell>51.3</cell><cell>17.2</cell><cell>17.4</cell><cell>26.8</cell><cell>32.8 35.1</cell><cell>45.6</cell><cell>30.9</cell></row><row><cell>Li [28]</cell><cell>EB</cell><cell cols="3">54.5 47.4 41.3 20.8</cell><cell>17.7</cell><cell cols="6">51.9 63.5 46.1 21.8 57.1 22.1 34.4</cell><cell>50.5</cell><cell>61.8</cell><cell>16.2</cell><cell>29.9</cell><cell>40.7</cell><cell>15.9 55.3</cell><cell>40.2</cell><cell>39.5</cell></row><row><cell>WSDDN [5]</cell><cell>EB</cell><cell cols="3">39.4 50.1 31.5 16.3</cell><cell>12.6</cell><cell cols="6">64.5 42.8 42.6 10.1 35.7 24.9 38.2</cell><cell>34.4</cell><cell>55.6</cell><cell>9.4</cell><cell>14.7</cell><cell>30.2</cell><cell>40.7 54.7</cell><cell>46.9</cell><cell>34.8</cell></row><row><cell>Teh [48]</cell><cell>EB</cell><cell cols="3">48.8 45.9 37.4 26.9</cell><cell>9.2</cell><cell cols="6">50.7 43.4 43.6 10.6 35.9 27.0 38.6</cell><cell>48.5</cell><cell>43.8</cell><cell>24.7</cell><cell>12.1</cell><cell>29.0</cell><cell>23.2 48.8</cell><cell>41.9</cell><cell>34.5</cell></row><row><cell>ContextLocNet [23]</cell><cell>SS</cell><cell cols="2">57.1 52.0 31.5</cell><cell>7.6</cell><cell>11.5</cell><cell cols="2">55.0 53.1 34.1</cell><cell>1.7</cell><cell cols="3">33.1 49.2 42.0</cell><cell>47.3</cell><cell>56.6</cell><cell>15.3</cell><cell>12.8</cell><cell>24.8</cell><cell>48.9 44.4</cell><cell>47.8</cell><cell>36.3</cell></row><row><cell>OICR [46]</cell><cell>SS</cell><cell cols="3">58.0 62.4 31.1 19.4</cell><cell>13.0</cell><cell cols="6">65.1 62.2 28.4 24.8 44.7 30.6 25.3</cell><cell>37.8</cell><cell>65.5</cell><cell>15.7</cell><cell>24.1</cell><cell>41.7</cell><cell>46.9 64.3</cell><cell>62.6</cell><cell>41.2</cell></row><row><cell>Jie [22]</cell><cell>?</cell><cell cols="3">52.2 47.1 35.0 26.7</cell><cell>15.4</cell><cell cols="2">61.3 66.0 54.3</cell><cell>3.0</cell><cell cols="3">53.6 24.7 43.6</cell><cell>48.4</cell><cell>65.8</cell><cell>6.6</cell><cell>18.8</cell><cell>51.9</cell><cell>43.6 53.6</cell><cell>62.4</cell><cell>41.7</cell></row><row><cell>Diba [9]</cell><cell>EB</cell><cell cols="3">49.5 60.6 38.6 29.2</cell><cell>16.2</cell><cell cols="6">70.8 56.9 42.5 10.9 44.1 29.9 42.2</cell><cell>47.9</cell><cell>64.1</cell><cell>13.8</cell><cell>23.5</cell><cell>45.9</cell><cell>54.1 60.8</cell><cell>54.5</cell><cell>42.8</cell></row><row><cell>PCL [45]</cell><cell>SS</cell><cell cols="3">54.4 69.0 39.3 19.2</cell><cell>15.7</cell><cell cols="6">62.9 64.4 30.0 25.1 52.5 44.4 19.6</cell><cell>39.3</cell><cell>67.7</cell><cell>17.8</cell><cell>22.9</cell><cell>46.6</cell><cell>57.5 58.6</cell><cell>63.0</cell><cell>43.5</cell></row><row><cell>Wei [57]</cell><cell>SS</cell><cell cols="3">59.3 57.5 43.7 27.3</cell><cell>13.5</cell><cell cols="6">63.9 61.7 59.9 24.1 46.9 36.7 45.6</cell><cell>39.9</cell><cell>62.6</cell><cell>10.3</cell><cell>23.6</cell><cell>41.7</cell><cell>52.4 58.7</cell><cell>56.6</cell><cell>44.3</cell></row><row><cell>Tang [47]</cell><cell>SS</cell><cell cols="2">57.9 70.5 37.8</cell><cell>5.7</cell><cell>21.0</cell><cell cols="2">66.1 69.2 59.4</cell><cell>3.4</cell><cell cols="3">57.1 57.3 35.2</cell><cell>64.2</cell><cell>68.6</cell><cell>32.8</cell><cell>28.6</cell><cell>50.8</cell><cell>49.5 41.1</cell><cell>30.0</cell><cell>45.3</cell></row><row><cell>Shen [38]</cell><cell>SS</cell><cell cols="3">52.0 64.5 45.5 26.7</cell><cell>27.9</cell><cell cols="6">60.5 47.8 59.7 13.0 50.4 46.4 56.3</cell><cell>49.6</cell><cell>60.7</cell><cell>25.4</cell><cell>28.2</cell><cell>50.0</cell><cell>51.4 66.5</cell><cell>29.7</cell><cell>45.6</cell></row><row><cell>Wan [52]</cell><cell>SS</cell><cell cols="3">55.6 66.9 34.2 29.1</cell><cell>16.4</cell><cell cols="6">68.8 68.1 43.0 25.0 65.6 45.3 53.2</cell><cell>49.6</cell><cell>68.6</cell><cell>2.0</cell><cell>25.4</cell><cell>52.5</cell><cell>56.8 62.1</cell><cell>57.1</cell><cell>47.3</cell></row><row><cell>SDCN [29]</cell><cell>SS</cell><cell cols="3">59.4 71.5 38.9 32.2</cell><cell>21.5</cell><cell cols="6">67.7 64.5 68.9 20.4 49.2 47.6 60.9</cell><cell>55.9</cell><cell>67.4</cell><cell>31.2</cell><cell>22.9</cell><cell>45.0</cell><cell>53.2 60.9</cell><cell>64.4</cell><cell>50.2</cell></row><row><cell>C-MIL [51]</cell><cell>SS</cell><cell cols="3">62.5 58.4 49.5 32.1</cell><cell>19.8</cell><cell cols="6">70.5 66.1 63.4 20.0 60.5 52.9 53.5</cell><cell>57.4</cell><cell>68.9</cell><cell>8.4</cell><cell>24.6</cell><cell>51.8</cell><cell>58.7 66.7</cell><cell>63.6</cell><cell>50.5</cell></row><row><cell>Yang [60]</cell><cell>SS</cell><cell cols="3">57.6 70.8 50.7 28.3</cell><cell>27.2</cell><cell cols="6">72.5 69.1 65.0 26.9 64.5 47.4 47.7</cell><cell>53.5</cell><cell>66.9</cell><cell>13.7</cell><cell>29.3</cell><cell>56.0</cell><cell>54.9 63.4</cell><cell>65.2</cell><cell>51.5</cell></row><row><cell>C-MIDN [12]</cell><cell>SS</cell><cell cols="3">53.3 71.5 49.8 26.1</cell><cell>20.3</cell><cell cols="6">70.3 69.9 68.3 28.7 65.3 45.1 64.6</cell><cell>58.0</cell><cell>71.2</cell><cell>20.0</cell><cell>27.5</cell><cell>54.9</cell><cell>54.9 69.4</cell><cell>63.5</cell><cell>52.6</cell></row><row><cell>Arun [2]</cell><cell>SS</cell><cell cols="3">66.7 69.5 52.8 31.4</cell><cell>24.7</cell><cell cols="6">74.5 74.1 67.3 14.6 53.0 46.1 52.9</cell><cell>69.9</cell><cell>70.8</cell><cell>18.5</cell><cell>28.4</cell><cell>54.6</cell><cell>60.7 67.1</cell><cell>60.4</cell><cell>52.9</cell></row><row><cell>WSOD2 [61]</cell><cell>SS</cell><cell cols="3">65.1 64.8 57.2 39.2</cell><cell>24.3</cell><cell cols="6">69.8 66.2 61.0 29.8 64.6 42.5 60.1</cell><cell>71.2</cell><cell>70.7</cell><cell>21.9</cell><cell>28.1</cell><cell>58.6</cell><cell>59.7 52.2</cell><cell>64.8</cell><cell>53.6</cell></row><row><cell>Ours</cell><cell>SS</cell><cell cols="3">68.8 77.7 57.0 27.7</cell><cell>28.9</cell><cell cols="6">69.1 74.5 67.0 32.1 73.2 48.1 45.2</cell><cell>54.4</cell><cell>73.7</cell><cell>35.0</cell><cell>29.3</cell><cell>64.1</cell><cell>53.8 65.3</cell><cell>65.2</cell><cell>54.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Single model per-class detection results using VGG16 on PASCAL VOC 2007.</figDesc><table><row><cell>Methods</cell><cell cols="5">Proposal Aero Bike Bird Boat Bottle Bus Car</cell><cell cols="12">Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train</cell><cell>TV</cell><cell>AP</cell></row><row><cell>Fast R-CNN</cell><cell>SS</cell><cell cols="2">80.3 74.7 66.9 46.9</cell><cell>37.7</cell><cell cols="6">73.9 68.6 87.7 41.7 71.1 51.1 86.0</cell><cell>77.8</cell><cell>79.8</cell><cell>69.8</cell><cell>32.1</cell><cell>65.5</cell><cell cols="2">63.8 76.4</cell><cell>61.7</cell><cell>65.7</cell></row><row><cell>Faster R-CNN</cell><cell>RPN</cell><cell cols="2">82.3 76.4 71.0 48.4</cell><cell>45.2</cell><cell cols="6">72.1 72.3 87.3 42.2 73.7 50.0 86.8</cell><cell>78.7</cell><cell>78.4</cell><cell>77.4</cell><cell>34.5</cell><cell>70.1</cell><cell cols="2">57.1 77.1</cell><cell>58.9</cell><cell>67.0</cell></row><row><cell>Li [28]</cell><cell>EB</cell><cell cols="2">62.9 55.5 43.7 14.9</cell><cell>13.6</cell><cell cols="4">57.7 52.4 50.9 13.3 45.4</cell><cell>4.0</cell><cell>30.2</cell><cell>55.6</cell><cell>67.0</cell><cell>3.8</cell><cell>23.1</cell><cell>39.4</cell><cell>5.5</cell><cell>50.7</cell><cell>29.3</cell><cell>35.9</cell></row><row><cell>ContextLocNet [23]</cell><cell>SS</cell><cell>64.0 54.9 36.4</cell><cell>8.1</cell><cell>12.6</cell><cell cols="2">53.1 40.5 28.4</cell><cell>6.6</cell><cell cols="3">35.3 34.4 49.1</cell><cell>42.6</cell><cell>62.4</cell><cell>19.8</cell><cell>15.2</cell><cell>27.0</cell><cell cols="2">33.1 33.0</cell><cell>50.0</cell><cell>35.3</cell></row><row><cell>OICR [46]</cell><cell>SS</cell><cell cols="2">67.7 61.2 41.5 25.6</cell><cell>22.2</cell><cell cols="6">54.6 49.7 25.4 19.9 47.0 18.1 26.0</cell><cell>38.9</cell><cell>67.7</cell><cell>2.0</cell><cell>22.6</cell><cell>41.1</cell><cell cols="2">34.3 37.9</cell><cell>55.3</cell><cell>37.9</cell></row><row><cell>Jie [22]</cell><cell>?</cell><cell cols="2">60.8 54.2 34.1 14.9</cell><cell>13.1</cell><cell cols="2">54.3 53.4 58.6</cell><cell>3.7</cell><cell>53.1</cell><cell>8.3</cell><cell>43.4</cell><cell>49.8</cell><cell>69.2</cell><cell>4.1</cell><cell>17.5</cell><cell>43.8</cell><cell cols="2">25.6 55.0</cell><cell>50.1</cell><cell>38.3</cell></row><row><cell>Diba [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10 :</head><label>10</label><figDesc>Single model per-class detection results using VGG16 on PASCAL VOC 2012.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>Single model per-class correct localization (CorLoc) results using VGG16 on PASCAL VOC 2007.</figDesc><table /><note>Methods Proposal Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV CorLoc Li</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 12 :</head><label>12</label><figDesc>Single model per-class correct localization (CorLoc) results using VGG16 on PASCAL VOC 2012.work<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b1">2]</ref> this metric is computed on the training set. Thus it does not reflect the true performance of the detection models and has not been widely adopted by supervised methods<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18]</ref>. For WSOD approaches, it serves as an indicator of the 'over-fitting' behavior. Compared with previous state-of-the-art, our method achieves the third best result on VOC 2007, winning on 2 categories. We also achieve the second best performance on VOC 2012 and win on 19 categories. We find that: (1) Our model performs well for classes like: airplane, bicycle, bottle, bus, motorbike, sheep, tv-monitor. This observation aligns very well with the detection results.(2) The best performing methods differ across classes, which suggest that methods could potentially be ensembled for further improvements.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 13 :</head><label>13</label><figDesc>Proposals statistics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 14 :</head><label>14</label><figDesc>Effect of using different number of proposals.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Change Log</head><p>• v1: ArXiv preprint.</p><p>• v2: Additional implementation details (Appendix A);</p><p>Fixed a minor mistake in <ref type="figure">Fig. 8</ref>; Re-organized Appendix for better readability.</p><p>• v3: Fixed a typo in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this section, we provide: (1) additional quantitative results on COCO; (2) per-class detection (AP) and correct localization (CorLoc) results on VOC; (3) additional qualitative results; (4) proposal statistics; <ref type="bibr" target="#b4">(5)</ref> ablation study on the amount of proposals; (6) implementation details and video demo of weakly supervised video object detection. Specifically, we show that our approach produces state-of-the-art results on COCO (see Tab. 8), outperforms all competing models on VOC 2007 and 2012 (see Tab. 9 and Tab. 10). We also provide correct localization results in Tab. 11 and Tab. 12 for completeness and illustrate the necessity of the sequential batch back-propagation (introduced in Sec. 4.3 of the main paper) in Tab. 13 and Tab. 14. Comprehensive visualizations are also provided ( <ref type="figure">Fig. 13 to Fig. 16</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dissimilarity coefficient based weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Pawan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-fold MIL training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Ramazan Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Mohammad</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object discovery by generative adversarial &amp; ranking networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCV</title>
		<meeting>IJCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fang Wan, Haihang You, and Dongrui Fan. C-midn: Coupled multiple instance detection network with segmentation guidance for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno>2018. 11</idno>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS. 2015</title>
		<meeting>NIPS. 2015</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with segmentation collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Memory-efficient implementation of densenets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1707.06990</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Parallel distributed processing: Explorations in the microstructure of cognition. chapter Learning Internal Representations by Error Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cyclic guidance for weakly supervised joint detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You reap what you sow: Using videos to generate high precision object proposals for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Track and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PCL: Proposal cluster learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Jianbin Jiao, and Qixiang Ye. C-MIL: continuation multiple instance learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A-fast-rcnn: Hard positive generation via adversary for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relaxed multiple-instance SVM with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TS2C: tight box mining with surrounding segmentation context for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatial-temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards precise end-to-end weakly supervised object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">WSOD2: Learning bottom-up and top-down objectness distillation for weakly-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Zigzag learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">W2f: A weakly-supervised to fully-supervised framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Methods Proposal Aero Bike Bird Boat Bottle Bus Car Cat Chair Cow Table Dog Horse Motor Person Plant Sheep Sofa Train TV CorLoc</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
