<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jiatang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
							<email>mnqu@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
							<email>wangmingzhe@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
							<email>junyan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
							<email>qmei@umich.edu</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2736277.2741093</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Categories and Subject Descriptors I26 [Artificial Intelligence]: Learning General Terms Algorithms, Experimentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the "LINE," which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Information networks are ubiquitous in the real world with examples such as airline networks, publication networks, social and communication networks, and the World Wide Web. The size of these information networks ranges from hundreds of nodes to millions and billions of nodes. Analyzing large information networks has been attracting increasing attention in both academia and industry. This paper studies the problem of embedding information networks into lowdimensional spaces, in which every vertex is represented as a low-dimensional vector. Such a low-dimensional embedding is very useful in a variety of applications such as visualization <ref type="bibr" target="#b20">[21]</ref>, node classification <ref type="bibr" target="#b2">[3]</ref>, link prediction <ref type="bibr" target="#b9">[10]</ref>, and recommendation <ref type="bibr" target="#b22">[23]</ref>.</p><p>Various methods of graph embedding have been proposed in the machine learning literature (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2]</ref>). They generally perform well on smaller networks. The problem becomes much more challenging when a real world information network is concerned, which typically contains millions of nodes and billions of edges. For example, the Twitter followee-follower network contains 175 million active users and around twenty billion edges in 2012 <ref type="bibr" target="#b13">[14]</ref>. Most existing graph embedding algorithms do not scale for networks of this size. For example, the time complexity of classical graph embedding algorithms such as MDS <ref type="bibr" target="#b3">[4]</ref>, IsoMap <ref type="bibr" target="#b19">[20]</ref>, Laplacian eigenmap <ref type="bibr" target="#b1">[2]</ref> are at least quadratic to the number of vertices, which is too expensive for networks with millions of nodes. Although a few very recent studies approach the embedding of large-scale networks, these methods either use an indirect approach that is not designed for networks (e.g., <ref type="bibr" target="#b0">[1]</ref>) or lack a clear objective function tailored for network embedding (e.g., <ref type="bibr" target="#b15">[16]</ref>). We anticipate that a new model with a carefully designed objective function that preserves properties of the graph and an efficient optimization technique should effectively find the embedding of millions of nodes.</p><p>In this paper, we propose such a network embedding model called the "LINE," which is able to scale to very large, arbitrary types of networks: undirected, directed and/or weighted. The model optimizes an objective which preserves both the local and global network structures. Naturally, the local structures are represented by the observed links in the networks, which capture the first-order proximity between the vertices. Most existing graph embedding algorithms are designed to preserve this first-order proximity, e.g., IsoMap <ref type="bibr" target="#b19">[20]</ref> and Laplacian eigenmap <ref type="bibr" target="#b1">[2]</ref>, even if they do not scale. We observe that in a real-world network many (if not the majority of) legitimate links are actually not observed. In other <ref type="figure">Figure 1</ref>: A toy example of information network. Edges can be undirected, directed, and/or weighted. Vertex 6 and 7 should be placed closely in the low-dimensional space as they are connected through a strong tie. Vertex 5 and 6 should also be placed closely as they share similar neighbors. words, the observed first-order proximity in the real world data is not sufficient for preserving the global network structures. As a complement, we explore the second-order proximity between the vertices, which is not determined through the observed tie strength but through the shared neighborhood structures of the vertices. The general notion of the second-order proximity can be interpreted as nodes with shared neighbors being likely to be similar. Such an intuition can be found in the theories of sociology and linguistics. For example, "the degree of overlap of two people's friendship networks correlates with the strength of ties between them," in a social network <ref type="bibr" target="#b5">[6]</ref>; and "You shall know a word by the company it keeps" <ref type="bibr">(Firth, J. R. 1957:11)</ref> in text corpora <ref type="bibr" target="#b4">[5]</ref>. Indeed, people who share many common friends are likely to share the same interest and become friends, and words that are used together with many similar words are likely to have similar meanings. <ref type="figure">Fig. 1</ref> presents an illustrative example. As the weight of the edge between vertex 6 and 7 is large, i.e., 6 and 7 have a high first-order proximity, they should be represented closely to each other in the embedded space. On the other hand, though there is no link between vertex 5 and 6, they share many common neighbors, i.e., they have a high second-order proximity and therefore should also be represented closely to each other. We expect that the consideration of the secondorder proximity effectively complements the sparsity of the first-order proximity and better preserves the global structure of the network. In this paper, we will present carefully designed objectives that preserve the first-order and the second-order proximities.</p><p>Even if a sound objective is found, optimizing it for a very large network is challenging. One approach that attracts attention in recent years is using the stochastic gradient descent for the optimization. However, we show that directly deploying the stochastic gradient descent is problematic for real world information networks. This is because in many networks, edges are weighted and the weights usually present a high variance. Consider a word co-occurrence network, in which the weights (co-occurrences) of word pairs may range from one to hundreds of thousands. These weights of the edges will be multiplied into the gradients, resulting in the explosion of the gradients and thus compromise the performance. To address this, we propose a novel edge-sampling method, which improves both the effectiveness and efficiency of the inference. We sample the edges with the probabilities proportional to their weights, and then treat the sampled edges as binary edges for model updating. With this sam-pling process, the objective function remains the same and the weights of the edges no longer affect the gradients.</p><p>The LINE is very general, which works well for directed or undirected, weighted or unweighted graphs. We evaluate the performance of the LINE with various real-world information networks, including language networks, social networks, and citation networks. The effectiveness of the learned embeddings is evaluated within multiple data mining tasks, including word analogy, text classification, and node classification. The results suggest that the LINE model outperforms other competitive baselines in terms of both effectiveness and efficiency. It is able to learn the embedding of a network with millions of nodes and billions of edges in a few hours on a single machine.</p><p>To summarize, we make the following contributions:</p><p>• We propose a novel network embedding model called the "LINE," which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.</p><p>• We propose an edge-sampling algorithm for optimizing the objective. The algorithm tackles the limitation of the classical stochastic gradient decent and improves the effectiveness and efficiency of the inference.</p><p>• We conduct extensive experiments on real-world information networks. Experimental results prove the effectiveness and efficiency of the proposed LINE model.</p><p>Organization. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 formally defines the problem of large-scale information network embedding. Section 4 introduces the LINE model in details. Section 5 presents the experimental results. Finally we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) <ref type="bibr" target="#b3">[4]</ref>, IsoMap <ref type="bibr" target="#b19">[20]</ref>, LLE <ref type="bibr" target="#b17">[18]</ref> and Laplacian Eigenmap <ref type="bibr" target="#b1">[2]</ref>. These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph <ref type="bibr" target="#b21">[22]</ref> into a low dimensional space. However, these algorithms usually rely on solving the leading eigenvectors of the affinity matrices, the complexity of which is at least quadratic to the number of nodes, making them inefficient to handle large-scale networks.</p><p>Among the most recent literature is a technique called graph factorization <ref type="bibr" target="#b0">[1]</ref>. It finds the low-dimensional embedding of a large graph through matrix factorization, which is optimized using stochastic gradient descent. This is possible because a graph can be represented as an affinity matrix. However, the objective of matrix factorization is not designed for networks, therefore does not necessarily preserve the global network structure. Intuitively, graph factorization expects nodes with higher first-order proximity are represented closely. Instead, the LINE model uses an objective that is particularly designed for networks, which preserves both the first-order and the second-order proximities. Practically, the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs.</p><p>The most recent work related with ours is DeepWalk <ref type="bibr" target="#b15">[16]</ref>, which deploys a truncated random walk for social network embedding. Although empirically effective, the DeepWalk does not provide a clear objective that articulates what network properties are preserved. Intuitively, DeepWalk expects nodes with higher second-order proximity yield similar low-dimensional representations, while the LINE preserves both first-order and second-order proximities. DeepWalk uses random walks to expand the neighborhood of a vertex, which is analogical to a depth-first search. We use a breadthfirst search strategy, which is a more reasonable approach to the second-order proximity. Practically, DeepWalk only applies to unweighted networks, while our model is applicable for networks with both weighted and unweighted edges.</p><p>In Section 5, we empirically compare the proposed model with these methods using various real world networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM DEFINITION</head><p>We formally define the problem of large-scale information network embedding using first-order and second-order proximities. We first define an information network as follows:</p><formula xml:id="formula_0">Definition 1. (Information Network) An informa- tion network is defined as G = (V, E),</formula><p>where V is the set of vertices, each representing a data object and E is the set of edges between the vertices, each representing a relationship between two data objects. Each edge e ∈ E is an ordered pair e = (u, v) and is associated with a weight wuv &gt; 0, which indicates the strength of the relation.</p><formula xml:id="formula_1">If G is undirected, we have (u, v) ≡ (v, u) and wuv ≡ wvu; if G is directed, we have (u, v) ≡ (v, u) and wuv ≡ wvu.</formula><p>In practice, information networks can be either directed (e.g., citation networks) or undirected (e.g., social network of users in Facebook). The weights of the edges can be either binary or take any real value. Note that while negative edge weights are possible, in this study we only consider nonnegative weights. For example, in citation networks and social networks, wuv takes binary values; in co-occurrence networks between different objects, wuv can take any nonnegative value. The weights of the edges in some networks may diverge as some objects co-occur many times while others may just co-occur a few times.</p><p>Embedding an information network into a low-dimensional space is useful in a variety of applications. To conduct the embedding, the network structures must be preserved. The first intuition is that the local network structure, i.e., the local pairwise proximity between the vertices, must be preserved. We define the local network structures as the firstorder proximity between the vertices: Definition 2. (First-order Proximity) The first-order proximity in a network is the local pairwise proximity between two vertices. For each pair of vertices linked by an edge (u, v), the weight on that edge, wuv, indicates the firstorder proximity between u and v. If no edge is observed between u and v, their first-order proximity is 0.</p><p>The first-order proximity usually implies the similarity of two nodes in a real-world network. For example, people who are friends with each other in a social network tend to share similar interests; pages linking to each other in World Wide Web tend to talk about similar topics. Because of this importance, many existing graph embedding algorithms such as IsoMap, LLE, Laplacian eigenmap, and graph factorization have the objective to preserve the first-order proximity.</p><p>However, in a real world information network, the links observed are only a small proportion, with many others missing <ref type="bibr" target="#b9">[10]</ref>. A pair of nodes on a missing link has a zero first-order proximity, even though they are intrinsically very similar to each other. Therefore, first-order proximity alone is not sufficient for preserving the network structures, and it is important to seek an alternative notion of proximity that addresses the problem of sparsity. A natural intuition is that vertices that share similar neighbors tend to be similar to each other. For example, in social networks, people who share similar friends tend to have similar interests and thus become friends; in word co-occurrence networks, words that always co-occur with the same set of words tend to have similar meanings. We therefore define the second-order proximity, which complements the first-order proximity and preserves the network structure.</p><p>Definition 3. (Second-order Proximity) The secondorder proximity between a pair of vertices (u, v) in a network is the similarity between their neighborhood network structures. Mathematically, let pu = (wu,1, . . . , w u,|V | ) denote the first-order proximity of u with all the other vertices, then the second-order proximity between u and v is determined by the similarity between pu and pv. If no vertex is linked from/to both u and v, the second-order proximity between u and v is 0.</p><p>We investigate both first-order and second-order proximity for network embedding, which is defined as follows. |V |. In the space R d , both the first-order proximity and the second-order proximity between the vertices are preserved.</p><p>Next, we introduce a large-scale network embedding model that preserves both first-and second-order proximities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LINE: LARGE-SCALE INFORMATION NETWORK EMBEDDING</head><p>A desirable embedding model for real world information networks must satisfy several requirements: first, it must be able to preserve both the first-order proximity and the second-order proximity between the vertices; second, it must scale for very large networks, say millions of vertices and billions of edges; third, it can deal with networks with arbitrary types of edges: directed, undirected and/or weighted. In this section, we present a novel network embedding model called the "LINE," which satisfies all the three requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Description</head><p>We describe the LINE model to preserve the first-order proximity and second-order proximity separately, and then introduce a simple way to combine the two proximity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">LINE with First-order Proximity</head><p>The first-order proximity refers to the local pairwise proximity between the vertices in the network. To model the first-order proximity, for each undirected edge (i, j), we define the joint probability between vertex vi and vj as follows:</p><formula xml:id="formula_2">p1(vi, vj) = 1 1 + exp(− u T i · uj) ,<label>(1)</label></formula><p>where ui ∈ R d is the low-dimensional vector representation of vertex vi. Eqn. (1) defines a distribution p(·, ·) over the space V × V , and its empirical probability can be defined</p><formula xml:id="formula_3">asp1(i, j) = w ij W , where W = (i,j)∈E wij.</formula><p>To preserve the first-order proximity, a straightforward way is to minimize the following objective function:</p><formula xml:id="formula_4">O1 = d(p1(·, ·), p1(·, ·)),<label>(2)</label></formula><p>where d(·, ·) is the distance between two distributions. We choose to minimize the KL-divergence of two probability distributions. Replacing d(·, ·) with KL-divergence and omitting some constants, we have:</p><formula xml:id="formula_5">O1 = − (i,j)∈E wij log p1(vi, vj),<label>(3)</label></formula><p>Note that the first-order proximity is only applicable for undirected graphs, not for directed graphs. By finding the { ui} i=1..|V | that minimize the objective in Eqn. (3), we can represent every vertex in the d-dimensional space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">LINE with Second-order Proximity</head><p>The second-order proximity is applicable for both directed and undirected graphs. Given a network, without loss of generality, we assume it is directed (an undirected edge can be considered as two directed edges with opposite directions and equal weights). The second-order proximity assumes that vertices sharing many connections to other vertices are similar to each other. In this case, each vertex is also treated as a specific "context" and vertices with similar distributions over the "contexts" are assumed to be similar. Therefore, each vertex plays two roles: the vertex itself and a specific "context" of other vertices. We introduce two vectors ui and u i , where ui is the representation of vi when it is treated as a vertex while u i is the representation of vi when it is treated as a specific "context". For each directed edge (i, j), we first define the probability of "context" vj generated by vertex vi as:</p><formula xml:id="formula_6">p2(vj|vi) = exp( u T j · ui) |V | k=1 exp( u T k · ui) ,<label>(4)</label></formula><p>where |V | is the number of vertices or "contexts." For each vertex vi, Eqn. (4) actually defines a conditional distribution p2(·|vi) over the contexts, i.e., the entire set of vertices in the network. As mentioned above, the second-order proximity assumes that vertices with similar distributions over the contexts are similar to each other. To preserve the second-order proximity, we should make the conditional distribution of the contexts p2(·|vi) specified by the low-dimensional representation be close to the empirical distributionp2(·|vi). Therefore, we minimize the following objective function:</p><formula xml:id="formula_7">O2 = i∈V λid(p2(·|vi), p2(·|vi)),<label>(5)</label></formula><p>where d(·, ·) is the distance between two distributions. As the importance of the vertices in the network may be different, we introduce λi in the objective function to represent the prestige of vertex i in the network, which can be measured by the degree or estimated through algorithms such as PageRank <ref type="bibr" target="#b14">[15]</ref>. The empirical distributionp2(·|vi) is defined asp2(vj|vi) = w ij d i , where wij is the weight of the edge (i, j) and di is the out-degree of vertex i, i.e. di = k∈N (i) w ik , where N (i) is the set of out-neighbors of vi. In this paper, for simplicity we set λi as the degree of vertex i, i.e., λi = di, and here we also adopt KL-divergence as the distance function. Replacing d(·, ·) with KL-divergence, setting λi = di and omitting some constants, we have:</p><formula xml:id="formula_8">O2 = − (i,j)∈E wij log p2(vj|vi).<label>(6)</label></formula><p>By</p><formula xml:id="formula_9">learning { ui} i=1..|V | and { u i } i=1.</formula><p>.|V | that minimize this objective, we are able to represent every vertex vi with a d-dimensional vector ui.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Combining first-order and second-order proximities</head><p>To embed the networks by preserving both the first-order and second-order proximity, a simple and effective way we find in practice is to train the LINE model which preserves the first-order proximity and second-order proximity separately and then concatenate the embeddings trained by the two methods for each vertex. A more principled way to combine the two proximity is to jointly train the objective function <ref type="formula" target="#formula_5">(3)</ref> and <ref type="formula" target="#formula_8">(6)</ref>, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Optimization</head><p>Optimizing objective <ref type="formula" target="#formula_8">(6)</ref> is computationally expensive, which requires the summation over the entire set of vertices when calculating the conditional probability p2(·|vi).</p><p>To address this problem, we adopt the approach of negative sampling proposed in <ref type="bibr" target="#b12">[13]</ref>, which samples multiple negative edges according to some noisy distribution for each edge (i, j). More specifically, it specifies the following objective function for each edge (i, j):</p><formula xml:id="formula_10">log σ( u j T · ui) + K i=1 E vn∼Pn(v) [log σ(− u n T · ui)],<label>(7)</label></formula><p>where σ(x) = 1/(1 + exp(−x)) is the sigmoid function. The first term models the observed edges, the second term models the negative edges drawn from the noise distribution and K is the number of negative edges. We set Pn(v) ∝ dv 3/4 as proposed in <ref type="bibr" target="#b12">[13]</ref>, where dv is the out-degree of vertex v.</p><p>For the objective function <ref type="formula" target="#formula_5">(3)</ref>, there exists a trivial solution: u ik = ∞, for i=1, . . . , |V | and k = 1, . . . , d. To avoid the trivial solution, we can still utilize the negative sampling approach (7) by just changing u T j to u T j . We adopt the asynchronous stochastic gradient algorithm (ASGD) <ref type="bibr" target="#b16">[17]</ref> for optimizing Eqn. <ref type="bibr" target="#b6">(7)</ref>. In each step, the ASGD algorithm samples a mini-batch of edges and then updates the model parameters. If an edge (i, j) is sampled, the gradient w.r.t. the embedding vector ui of vertex i will be calculated as:</p><formula xml:id="formula_11">∂O2 ∂ ui = wij · ∂ log p2(vj|vi) ∂ ui<label>(8)</label></formula><p>Note that the gradient will be multiplied by the weight of the edge. This will become problematic when the weights of edges have a high variance. For example, in a word cooccurrence network, some words co-occur many times (e.g., tens of thousands) while some words co-occur only a few times. In such networks, the scales of the gradients diverge and it is very hard to find a good learning rate. If we select a large learning rate according to the edges with small weights, the gradients on edges with large weights will explode while the gradients will become too small if we select the learning rate according to the edges with large weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Optimization via Edge Sampling</head><p>The intuition in solving the above problem is that if the weights of all the edges are equal (e.g., network with binary edges), then there will be no problem of choosing an appropriate learning rate. A simple treatment is thus to unfold a weighted edge into multiple binary edges, e.g., an edge with weight w is unfolded into w binary edges. This will solve the problem but will significantly increase the memory requirement, especially when the weights of the edges are very large. To resolve this, one can sample from the original edges and treat the sampled edges as binary edges, with the sampling probabilities proportional to the original edge weights. With this edge-sampling treatment, the overall objective function remains the same. The problem boils down to how to sample the edges according to their weights.</p><p>Let W = (w1, w2, . . . , w |E| ) denote the sequence of the weights of the edges. One can simply calculate the sum of the weights wsum = |E| i=1 wi first, and then to sample a random value within the range of [0, wsum] to see which interval [ i−1 j=0 wj, i j=0 wj) the random value falls into. This approach takes O(|E|) time to draw a sample, which is costly when the number of edges |E| is large. We use the alias table method <ref type="bibr" target="#b8">[9]</ref> to draw a sample according to the weights of the edges, which takes only O(1) time when repeatedly drawing samples from the same discrete distribution.</p><p>Sampling an edge from the alias table takes constant time, O(1), and optimization with negative sampling takes O(d(K+ 1)) time, where K is the number of negative samples. Therefore, overall each step takes O(dK) time. In practice, we find that the number of steps used for optimization is usually proportional to the number of edges O(|E|). Therefore, the overall time complexity of the LINE is O(dK|E|), which is linear to the number of edges |E|, and does not depend on the number of vertices |V |. The edge sampling treatment improves the effectiveness of the stochastic gradient descent without compromising the efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>We discuss several practical issues of the LINE model. Low degree vertices. One practical issue is how to accurately embed vertices with small degrees. As the number of neighbors of such a node is very small, it is very hard to accurately infer its representation, especially with the second-order proximity based methods which heavily rely on the number of "contexts." An intuitive solution to this is expanding the neighbors of those vertices by adding higher order neighbors, such as neighbors of neighbors. In this paper, we only consider adding second-order neighbors, i.e., neighbors of neighbors, to each vertex. The weight between vertex i and its second-order neighbor j is measured as</p><formula xml:id="formula_12">wij = k∈N (i) w ik w kj d k .<label>(9)</label></formula><p>In practice, one can only add a subset of vertices {j} which have the largest proximity wij with the low degree vertex i. New vertices. Another practical issue is how to find the representation of newly arrived vertices. For a new vertex i, if its connections to the existing vertices are known, we can obtain the empirical distributionp1(·, vi) andp2(·|vi) over existing vertices. To obtain the embedding of the new vertex, according to the objective function Eqn. <ref type="bibr" target="#b2">(3)</ref> or Eqn. <ref type="formula" target="#formula_8">(6)</ref>, a straightforward way is to minimize either one of the following objective functions</p><formula xml:id="formula_13">− j∈N (i) wji log p1(vj, vi), or − j∈N (i) wji log p2(vj|vi),<label>(10)</label></formula><p>by updating the embedding of the new vertex and keeping the embeddings of existing vertices. If no connections between the new vertex and existing vertices are observed, we must resort to other information, such as the textual information of the vertices, and we leave it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We empirically evaluated the effectiveness and efficiency of the LINE. We applied the method to several large-scale real-world networks of different types, including a language network, two social networks, and two citation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Data Sets.</p><p>(1) Language network. We constructed a word cooccurrence network from the entire set of English Wikipedia pages. Words within every 5-word sliding window are considered to be co-occurring with each other. Words with frequency smaller than 5 are filtered out. (2) Social networks. We use two social networks: Flickr and Youtube 2 . The Flickr network is denser than the Youtube network (the same network as used in DeepWalk <ref type="bibr" target="#b15">[16]</ref>). (3) Citation Networks. Two types of citation networks are used: an author citation network and a paper citation network. We use the DBLP data set <ref type="bibr" target="#b18">[19]</ref> 3 to construct the citation networks between authors and between papers. The author citation network records the number of papers written by one author and cited by another author. The detailed statistics of these networks are summarized into <ref type="table" target="#tab_0">Table 1</ref>. They represent a variety of information networks: directed and undirected, binary and weighted. Each network contains at least half a million nodes and millions of edges, with the largest network containing around two million nodes and a billion edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Algorithms.</head><p>We compare the LINE model with several existing graph embedding methods that are able to scale up to very large networks. We do not compare with some classical graph embedding algorithms such as MDS, IsoMap, and Laplacian eigenmap, as they cannot handle networks of this scale.</p><p>• Graph factorization (GF) <ref type="bibr" target="#b0">[1]</ref>. We compare with the matrix factorization techniques for graph factorization. An information network can be represented as an affinity matrix, and is able to represent each vertex with a 2 Available at http://socialnetworks.mpi-sws.org/ data-imc2007.html 3 Available at http://arnetminer.org/citation low-dimensional vector through matrix factorization. Graph factorization is optimized through stochastic gradient descent and is able to handle large networks. It only applies to undirected networks.</p><p>• DeepWalk <ref type="bibr" target="#b15">[16]</ref>. DeepWalk is an approach recently proposed for social network embedding, which is only applicable for networks with binary edges. For each vertex, truncated random walks starting from the vertex are used to obtain the contextual information, and therefore only second-order proximity is utilized.</p><p>• LINE-SGD. This is the LINE model introduced in Section 4.1 that optimizes the objective Eqn. (3) or Eqn. <ref type="formula" target="#formula_8">(6)</ref> directly with stochastic gradient descent. With this approach, the weights of the edges are directly multiplied into the gradients when the edges are sampled for model updating. There are two variants of this approach: LINE-SGD(1st) and LINE-SGD(2nd), which use first-and second-order proximity respectively.</p><p>• LINE. This is the LINE model optimized through the edge-sampling treatment introduced in Section 4.2. In each stochastic gradient step, an edge is sampled with the probability proportional to its weight and then treated as binary for model updating. There are also two variants: LINE(1st) and LINE(2nd). Like the graph factorization, both LINE(1st) and LINE-SGD(1st) only apply to undirected graphs. LINE(2nd) and LINE-SGD(2nd) apply to both undirected and directed graphs.</p><p>• LINE (1st+2nd): To utilize both first-order and secondorder proximity, a simple and effective way is to concatenate the vector representations learned by LINE(1st) and LINE(2nd) into a longer vector. After concatenation, the dimensions should be re-weighted to balance the two representations. In a supervised learning task, the weighting of dimensions can be automatically found based on the training data. In an unsupervised task, however, it is more difficult to set the weights. Therefore we only apply LINE (1st+2nd) to the scenario of supervised tasks.</p><p>Parameter Settings.</p><p>The mini-batch size of the stochastic gradient descent is set as 1 for all the methods. Similar to <ref type="bibr" target="#b12">[13]</ref>, the learning rate is set with the starting value ρ0 = 0.025 and ρt = ρ0(1−t/T ), where T is the total number of mini-batches or edge samples. For fair comparisons, the dimensionality of the embeddings of the language network is set to 200, as used in word embedding <ref type="bibr" target="#b12">[13]</ref>. For other networks, the dimension is set as 128 by default, as used in <ref type="bibr" target="#b15">[16]</ref>. Other default settings include: the number of negative samples K = 5 for LINE and LINE-SGD; the total number of samples T = 10 billion for LINE(1st) and LINE(2nd), T = 20 billion for GF; window size win = 10, walk length t = 40, walks per vertex γ = 40 for DeepWalk. All the embedding vectors are finally normalized by setting || w||2 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Language Network</head><p>We start with the results on the language network, which contains two million nodes and a billion edges. Two applications are used to evaluate the effectiveness of the learned embeddings: word analogy <ref type="bibr" target="#b11">[12]</ref> and document classification. Word Analogy. This task is introduced by Mikolov et al. <ref type="bibr" target="#b11">[12]</ref>. Given a word pair (a, b) and a word c, the task aims to find a word d, such that the relation between c and d is similar to the relation between a and b, or denoted as: a : b → c :?. For instance, given a word pair ("China","Beijing") and a word "France," the right answer should be "Paris" because "Beijing" is the capital of "China" just as "Paris" is the capital of "France." Given the word embeddings, this task is solved by finding the word d * whose embedding is closest to the vector u b − ua + uc in terms of cosine proximity, i.e., d * = argmax d cos(( u b − ua + uc), u d ). Two categories of word analogy are used in this task: semantic and syntactic. <ref type="table" target="#tab_1">Table 2</ref> reports the results of word analogy using the embeddings of words learned on the Wikipedia corpora (Skip-Gram) or the Wikipedia word network (all other methods). For graph factorization, the weight between each pair of words is defined as the logarithm of the number of cooccurrences, which leads to better performance than the original value of co-occurrences. For DeepWalk, different cutoff thresholds are tried to convert the language network into a binary network, and the best performance is achieved when all the edges are kept in the network. We also compare with the state-of-the-art word embedding model Skip-Gram <ref type="bibr" target="#b11">[12]</ref>, which learns the word embeddings directly from the original Wikipedia pages and is also implicitly a matrix factorization approach <ref type="bibr" target="#b7">[8]</ref>. The window size is set as 5, the same as used for constructing the language network.</p><p>We can see that LINE(2nd) outperforms all other methods, including the graph embedding methods and the Skip-Gram. This indicates that the second-order proximity bet- ter captures the word semantics compared to the first-order proximity. This is not surprising, as a high second-order proximity implies that two words can be replaced in the same context, which is a stronger indicator of similar semantics than first-order co-occurrences. It is intriguing that the LINE(2nd) outperforms the state-of-the-art word embedding model trained on the original corpus. The reason may be that a language network better captures the global structure of word co-occurrences than the original word sequences. Among other methods, both graph factorization and LINE(1st) significantly outperform DeepWalk even if DeepWalk explores second-order proximity. This is because DeepWalk has to ignore the weights (i.e., co-occurrences) of the edges, which is very important in a language network. The performance by the LINE models directly optimized with SGD is much worse, because the weights of the edges in the language network diverge, which range from a single digit to tens of thousands, making the learning process suffer. The LINE optimized by the edge-sampling treatment effectively addresses this problem, and performs very well using either first-order or second-order proximity.</p><p>All the models are run on a single machine with 1T memory, 40 CPU cores at 2.0GHZ using 16 threads. Both the LINE(1st) and LINE(2nd) are quite efficient, which take less than 3 hours to process such a network with 2 million nodes and a billion edges. Both are at least 10% faster than graph factorization, and much more efficient than DeepWalk (five times slower). The reason that LINE-SGDs are slightly slower is that a threshold-cutting technique has to be applied to prevent the gradients from exploding. Document Classification. Another way to evaluate the quality of the word embeddings is to use the word vectors to compute document representation, which can be evaluated with document classification tasks. To obtain document vectors, we choose a very simple approach, taking the average of the word vector representations in that document. This is because we aim to compare the word embeddings with different approaches instead of finding the best method for document embeddings. The readers can find advanced document embedding approaches in <ref type="bibr" target="#b6">[7]</ref>. We download the abstracts of Wikipedia pages from http://downloads.dbpedia.org/ 3.9/en/long_abstracts_en.nq.bz2 and the categories of these pages from http://downloads.dbpedia.org/3.9/en/ article_categories_en.nq.bz2. We choose 7 diverse categories for classification including "Arts,""History,""Human," "Mathematics," "Nature," "Technology," and "Sports." For each category, we randomly select 10,000 articles, and articles belonging to multiple categories are discarded. We randomly sample different percentages of the labeled documents for training and use the rest for evaluation. All document vectors are used to train a one-vs-rest logistic regression classifier using the LibLinear package 4 . We report the classification metrics Micro-F1 and Macro-F1 <ref type="bibr" target="#b10">[11]</ref>. The results are averaged over 10 different runs by sampling different training data. <ref type="table" target="#tab_2">Table 3</ref> reports the results of Wikipedia page classification. Similar conclusion can be made as in the word analogy task. The graph factorization outperforms DeepWalk as DeepWalk ignores the weights of the edges. The LINE-SGDs perform worse due to the divergence of the weights of the edges. The LINE optimized by the edge-sampling treatment performs much better than directly deploying SGD. The LINE(2nd) outperforms LINE(1st) and is slightly better than the graph factorization. Note that with the supervised task, it is feasible to concatenate the embeddings learned with LINE(1st) and LINE(2nd). As a result, the LINE(1st+2nd) method performs significantly better than all other methods. This indicates that the first-order and second-order proximities are complementary to each other.</p><p>To provide the readers more insight about the first-order and second-order proximities, <ref type="table" target="#tab_3">Table 4</ref> compares the most similar words to a given word using first-order and secondorder proximity. We can see that by using the contextual proximity, the most similar words returned by the secondorder proximity are all semantically related words. The most similar words returned by the first-order proximity are a mixture of syntactically and semantically related words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Social Network</head><p>Compared with the language networks, the social networks are much sparser, especially the Youtube network. We evaluate the vertex embeddings through a multi-label classification task that assigns every node into one or more communities. Different percentages of the vertices are randomly sampled for training and the rest are used for evaluation. The results are averaged over 10 different runs. Flickr Network. Let us first take a look at the results on the Flickr network. We choose the most popular 5 communities as the categories of the vertices for multi-label classification. <ref type="table" target="#tab_4">Table 5</ref> reports the results. Again, LINE(1st+2nd) significantly outperforms all other methods. LINE(1st) is slightly better than LINE(2nd), which is opposite to the results on the language network. The reasons are two fold: (1) first-order proximity is still more important than secondorder proximity in social network, which indicates strong ties; (2) when the network is too sparse and the average number of neighbors of a node is too small, the second-order proximity may become inaccurate. We will further investigate this issue in Section 5.4. LINE(1st) outperforms graph factorization, indicating a better capability of modeling the first-order proximity. LINE(2nd) outperforms DeepWalk, indicating a better capability of modeling the second-order proximity. By concatenating the representations learned by LINE(1st) and LINE(2nd), the performance further improves, confirming that the two proximities are complementary to each other. Youtube Network. <ref type="table" target="#tab_5">Table 6</ref> reports the results on Youtube network, which is extremely sparse and the average degree is as low as 5. In most cases with different percentages of training data, LINE(1st) outperforms LINE(2nd), consistent with the results on the Flickr network. Due to the extreme sparsity, the performance of LINE(2nd) is even inferior to DeepWalk. By combining the representations learned by the LINE with both the first-and second-order proximity, the performance of LINE outperforms DeepWalk with either 128 or 256 dimension, showing that the two proximities are complementary to each other and able to address the problem of network sparsity. It is interesting to observe how DeepWalk tackles the network sparsity through truncated random walks, which enrich the neighbors or contexts of each vertex. The random walk approach acts like a depth-first search. Such an approach may quickly alleviate the sparsity of the neighborhood of nodes by bringing in indirect neighbors, but it may also introduce nodes that are long range away. A more reasonable way is to expand the neighborhood of each vertex using a breadth-first search strategy, i.e., recursively adding neighbors of neighbors. To verify this, we expand the neighborhood of the vertices whose degree are less than 1,000 by adding the neighbors of neighbors until the size of the extended neighborhood reaches 1,000 nodes. We find that adding more than 1,000 vertices does not further increase the performance.</p><p>The results in the brackets in <ref type="table" target="#tab_5">Table 6</ref> are obtained on this reconstructed network. The performance of GF, LINE(1st) and LINE(2nd) all improves, especially LINE(2nd). In the reconstructed network, the LINE(2nd) outperforms Deep-Walk in most cases. We can also see that the performance of LINE(1st+2nd) on the reconstructed network does not improve too much compared with those on the original network. This implies that the combination of first-order and second-order proximity on the original network has already captured most information and LINE(1st+2nd) approach is a quite effective and efficient way for network embedding, suitable for both dense and sparse networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Citation Network</head><p>We present the results on two citation networks, both of which are directed networks. Both the GF and LINE methods, which use first-order proximity, are not applicable for directed networks, and hence we only compare DeepWalk and LINE(2nd). We also evaluate the vertex embeddings through a multi-label classification task. We choose 7 popular conferences including AAAI, CIKM, ICML, KDD, NIPS, SIGIR, and WWW as the classification categories. Authors publishing in the conferences or papers published in the conferences are assumed to belong to the categories corresponding to the conferences. DBLP(AuthorCitation) Network. <ref type="table" target="#tab_6">Table 7</ref> reports the results on the DBLP(AuthorCitation) network. As this network is also very sparse, DeepWalk outperforms LINE(2nd). However, by reconstructing the network through recursively adding neighbors of neighbors for vertices with small degrees (smaller than 500), the performance of LINE(2nd) significantly increases and outperforms DeepWalk. The LINE model directly optimized by stochastic gradient descent, LINE(2nd), does not perform well as expected. DBLP(PaperCitation) Network. <ref type="table" target="#tab_7">Table 8</ref> reports the results on the DBLP(PaperCitation) network. The LINE(2nd) significantly outperforms DeepWalk. This is because the random walk on the paper citation network can only reach papers along the citing path (i.e., older papers) and cannot reach other references. Instead, the LINE(2nd) represents each paper with its references, which is obviously more reasonable. The performance of LINE(2nd) is further improved when the network is reconstructed by enriching the neighbors of vertices with small degrees (smaller than 200).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Network Layouts</head><p>An important application of network embedding is to create meaningful visualizations that layout a network on a      <ref type="figure">Figure 2</ref>: Visualization of the co-author network. The authors are mapped to the 2-D space using the t-SNE package with learned embeddings as input. Color of a node indicates the community of the author. Red: "data Mining," blue: "machine learning," green: "computer vision." two dimensional space. We visualize a co-author network extracted from the DBLP data. We select some conferences from three different research fields: WWW, KDD from "data mining," NIPS, ICML from "machine learning," and CVPR, ICCV from "computer vision." The co-author network is built from the papers published in these conferences. Authors with degree less than 3 are filtered out, and finally the network contains 18,561 authors and 207,074 edges. Laying out this co-author network is very challenging as the three research fields are very close to each other. We first map the co-author network into a low-dimensional space with different embedding approaches and then further map the low-dimensional vectors of the vertices to a 2-D space with the t-SNE package <ref type="bibr" target="#b20">[21]</ref>. <ref type="figure">Fig. 2</ref> compares the visualization results with different embedding approaches. The visualization using graph factorization is not very meaningful, in which the authors belonging to the same communities are not clustered together. The result of DeepWalk is much better. However, many authors belonging to different communities are clustered tightly into the center area, most of which are high degree vertices. This is because DeepWalk uses a random walk based approach to enrich the neighbors of the vertices, which brings in a lot of noise due to the randomness, especially for vertices with higher degrees. The LINE(2nd) performs quite well and generates meaningful layout of the network (nodes with same colors are distributed closer).  In this subsection, we formally analyze the performance of the above models w.r.t. the sparsity of networks. We use the social networks as examples. We first investigate how the sparsity of the networks affects the LINE(1st) and LINE(2nd). <ref type="figure" target="#fig_4">Fig. 3(a)</ref> shows the results w.r.t. the percentage of links on the Flickr network. We choose Flickr network as it is much denser than the Youtube network. We randomly select different percentages of links from the original network to construct networks with different levels of sparsity. We can see that in the beginning, when the network is very sparse, the LINE(1st) outperforms LINE(2nd). As we gradually increase the percentage of links, the LINE(2nd) begins to outperform the LINE(1st). This shows that the second-order proximity suffers when the network is extremely sparse, and it outperforms first-order proximity when there are sufficient nodes in the neighborhood of a node. <ref type="figure" target="#fig_4">Fig. 3(b)</ref> shows the performance w.r.t. the degrees of the vertices on both the original and reconstructed Youtube networks. We categorize the vertices into different groups according to their degrees including (0, 1], <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">30]</ref>, [31, +∞), and then evaluate the performance of vertices in different groups. Overall, the performance of different models increases when the degrees of the vertices increase. In the original network, the LINE(2nd) outperforms LINE(1st) except for the first group, which confirms that the second-order proximity does not work well for nodes with a low degree. In the reconstructed dense network, the performance of the LINE(1st) or LINE(2nd) improves, especially the LINE(2nd) that preserves the second-order proximity. We can also see that the LINE(2nd) model on the reconstructed network outperforms DeepWalk in all the groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance w.r.t. Network Sparsity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Sensitivity</head><p>Next, we investigate the performance w.r.t. the parameter dimension d and the converging performance of different models w.r.t the number of samples on the reconstructed Youtube network. <ref type="figure" target="#fig_0">Fig. 4(a)</ref> reports the performance of the LINE model w.r.t. the dimension d. We can see that the performance of the LINE(1st) or LINE(2nd) drops when the dimension becomes too large. <ref type="figure" target="#fig_0">Fig. 4(b)</ref> shows the results of the LINE and DeepWalk w.r.t. the number of samples during the optimization. The LINE(2nd) consistently outperforms LINE(1st) and DeepWalk, and both the LINE(1st) and LINE(2nd) converge much faster than DeepWalk.  Finally, we investigate the scalability of the LINE model optimized by the edge-sampling treatment and asynchronous stochastic gradient descent, which deploys multiple threads for optimization. <ref type="figure">Fig. 5(a)</ref> shows the speed up w.r.t. the number of threads on the Youtube data set. The speed up is quite close to linear. <ref type="figure">Fig. 5(b)</ref> shows that the classification performance remains stable when using multiple threads for model updating. The two figures together show that the inference algorithm of the LINE model is quite scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This paper presented a novel network embedding model called the "LINE," which can easily scale up to networks with millions of vertices and billions of edges. It has carefully designed objective functions that preserve both the first-order and second-order proximities, which are complementary to each other. An efficient and effective edge-sampling method is proposed for model inference, which solved the limitation of stochastic gradient descent on weighted edges without compromising the efficiency. Experimental results on various real-world networks prove the efficiency and effectiveness of LINE. In the future, we plan to investigate higher-order proximity beyond the first-order and second-order proximities in the network. Besides, we also plan to investigate the embedding of heterogeneous information networks, e.g., vertices with multiple types.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 4 .</head><label>4</label><figDesc>(Large-scale Information Network Embedding) Given a large network G = (V, E), the problem of Large-scale Information Network Embedding aims to represent each vertex v ∈ V into a low-dimensional space R d , i.e., learning a function fG : V → R d , where d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>22**) (51.41**) (51.92**) (52.20**) (52.40**) (52.59**) (52.78**) (52.70**) (53.02**) Significantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Degree of vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Performance w.r.t. network sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Sensitivity w.r.t. dimension and samples. Speed up v.s. #threads. Micro-F1 v.s. #threads. Performance w.r.t. # threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the real-world information networks.</figDesc><table><row><cell></cell><cell>Language Network</cell><cell cols="2">Social Network</cell><cell cols="2">Citation Network</cell></row><row><cell>Name</cell><cell>Wikipedia</cell><cell>Flickr</cell><cell>Youtube</cell><cell cols="2">DBLP(AuthorCitation) DBLP(PaperCitation)</cell></row><row><cell>Type</cell><cell>undirected,weighted</cell><cell cols="2">undirected,binary undirected,binary</cell><cell>dircted,weighted</cell><cell>directed,binary</cell></row><row><cell>|V|</cell><cell>1,985,098</cell><cell>1,715,256</cell><cell>1,138,499</cell><cell>524,061</cell><cell>781,109</cell></row><row><cell>|E|</cell><cell>1,000,924,086</cell><cell>22,613,981</cell><cell>2,990,443</cell><cell>20,580,238</cell><cell>4,191,677</cell></row><row><cell>Avg. degree</cell><cell>504.22</cell><cell>26.37</cell><cell>5.25</cell><cell>78.54</cell><cell>10.73</cell></row><row><cell>#Labels</cell><cell>7</cell><cell>5</cell><cell>47</cell><cell>7</cell><cell>7</cell></row><row><cell>#train</cell><cell>70,000</cell><cell>75,958</cell><cell>31,703</cell><cell>20,684</cell><cell>10,398</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of word analogy on Wikipedia data.</figDesc><table><row><cell>Algorithm</cell><cell cols="4">Semantic (%) Syntactic (%) Overall (%) Running time</cell></row><row><cell>GF</cell><cell>61.38</cell><cell>44.08</cell><cell>51.93</cell><cell>2.96h</cell></row><row><cell>DeepWalk</cell><cell>50.79</cell><cell>37.70</cell><cell>43.65</cell><cell>16.64h</cell></row><row><cell>SkipGram</cell><cell>69.14</cell><cell>57.94</cell><cell>63.02</cell><cell>2.82h</cell></row><row><cell>LINE-SGD(1st)</cell><cell>9.72</cell><cell>7.48</cell><cell>8.50</cell><cell>3.83h</cell></row><row><cell>LINE-SGD(2nd)</cell><cell>20.42</cell><cell>9.56</cell><cell>14.49</cell><cell>3.94h</cell></row><row><cell>LINE(1st)</cell><cell>58.08</cell><cell>49.42</cell><cell>53.35</cell><cell>2.44h</cell></row><row><cell>LINE(2nd)</cell><cell>73.79</cell><cell>59.72</cell><cell>66.10</cell><cell>2.55h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of Wikipedia page classification on Wikipedia data set. 81.04** 82.08** 82.58** 82.93** 83.16** 83.37** 83.52** 83.63** 83.74** Significantly outperforms GF at the: ** 0.01 and * 0.05 level, paired t-test.</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell>GF</cell><cell>79.63</cell><cell>80.51</cell><cell>80.94</cell><cell>81.18</cell><cell>81.38</cell><cell>81.54</cell><cell>81.63</cell><cell>81.71</cell><cell>81.78</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>78.89</cell><cell>79.92</cell><cell>80.41</cell><cell>80.69</cell><cell>80.92</cell><cell>81.08</cell><cell>81.21</cell><cell>81.35</cell><cell>81.42</cell></row><row><cell>Micro-F1</cell><cell>SkipGram LINE-SGD(1st)</cell><cell>79.84 76.03</cell><cell>80.82 77.05</cell><cell>81.28 77.57</cell><cell>81.57 77.85</cell><cell>81.71 78.08</cell><cell>81.87 78.25</cell><cell>81.98 78.39</cell><cell>82.05 78.44</cell><cell>82.09 78.49</cell></row><row><cell></cell><cell>LINE-SGD(2nd)</cell><cell>74.68</cell><cell>76.53</cell><cell>77.54</cell><cell>78.18</cell><cell>78.63</cell><cell>78.96</cell><cell>79.19</cell><cell>79.40</cell><cell>79.57</cell></row><row><cell></cell><cell>LINE(1st)</cell><cell>79.67</cell><cell>80.55</cell><cell>80.94</cell><cell>81.24</cell><cell>81.40</cell><cell>81.52</cell><cell>81.61</cell><cell>81.69</cell><cell>81.67</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>79.93</cell><cell>80.90</cell><cell>81.31</cell><cell>81.63</cell><cell>81.80</cell><cell>81.91</cell><cell>82.00</cell><cell>82.11</cell><cell>82.17</cell></row><row><cell cols="2">GF DeepWalk SkipGram LINE(1st+2nd) Macro-F1 LINE-SGD(1st)</cell><cell>79.49 78.78 79.74 75.85</cell><cell>80.39 79.78 80.71 76.90</cell><cell>80.82 80.30 81.15 77.40</cell><cell>81.08 80.56 81.46 77.71</cell><cell>81.26 80.82 81.63 77.94</cell><cell>81.40 80.97 81.78 78.12</cell><cell>81.52 81.11 81.88 78.24</cell><cell>81.61 81.24 81.98 78.29</cell><cell>81.68 81.32 82.01 78.36</cell></row><row><cell></cell><cell>LINE-SGD(2nd)</cell><cell>74.70</cell><cell>76.45</cell><cell>77.43</cell><cell>78.09</cell><cell>78.53</cell><cell>78.83</cell><cell>79.08</cell><cell>79.29</cell><cell>79.46</cell></row><row><cell></cell><cell>LINE(1st)</cell><cell>79.54</cell><cell>80.44</cell><cell>80.82</cell><cell>81.13</cell><cell>81.29</cell><cell>81.43</cell><cell>81.51</cell><cell>81.60</cell><cell>81.59</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>79.82</cell><cell>80.81</cell><cell>81.22</cell><cell>81.52</cell><cell>81.71</cell><cell>81.82</cell><cell>81.92</cell><cell>82.00</cell><cell>82.07</cell></row><row><cell></cell><cell cols="10">LINE(1st+2nd) 80.94** 81.99** 82.49** 82.83** 83.07** 83.29** 83.42** 83.55** 83.66**</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of most similar words using 1st-order and 2nd-order proximity.</figDesc><table><row><cell>Word</cell><cell>Similarity</cell><cell>Top similar words</cell></row><row><cell>good</cell><cell>1st 2nd</cell><cell>luck bad faith assume nice decent bad excellent lousy reasonable</cell></row><row><cell>information</cell><cell>1st 2nd</cell><cell>provide provides detailed facts verifiable infomation informaiton informations nonspammy animecons</cell></row><row><cell>graph</cell><cell>1st 2nd</cell><cell>graphs algebraic finite symmetric topology graphs subgraph matroid hypergraph undirected</cell></row><row><cell>learn</cell><cell>1st 2nd</cell><cell>teach learned inform educate how learned teach relearn learnt understand</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of multi-label classification on the Flickr network.</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell>GF</cell><cell>53.23</cell><cell>53.68</cell><cell>53.98</cell><cell>54.14</cell><cell>54.32</cell><cell>54.38</cell><cell>54.43</cell><cell>54.50</cell><cell>54.48</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>60.38</cell><cell>60.77</cell><cell>60.90</cell><cell>61.05</cell><cell>61.13</cell><cell>61.18</cell><cell>61.19</cell><cell>61.29</cell><cell>61.22</cell></row><row><cell>Micro-F1</cell><cell>DeepWalk(256dim)</cell><cell>60.41</cell><cell>61.09</cell><cell>61.35</cell><cell>61.52</cell><cell>61.69</cell><cell>61.76</cell><cell>61.80</cell><cell>61.91</cell><cell>61.83</cell></row><row><cell></cell><cell>LINE(1st)</cell><cell>63.27</cell><cell>63.69</cell><cell>63.82</cell><cell>63.92</cell><cell>63.96</cell><cell>64.03</cell><cell>64.06</cell><cell>64.17</cell><cell>64.10</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>62.83</cell><cell>63.24</cell><cell>63.34</cell><cell>63.44</cell><cell>63.55</cell><cell>63.55</cell><cell>63.59</cell><cell>63.66</cell><cell>63.69</cell></row><row><cell></cell><cell>LINE(1st+2nd)</cell><cell cols="7">63.20** 63.97** 64.25** 64.39** 64.53** 64.55** 64.61**</cell><cell>64.75**</cell><cell>64.74**</cell></row><row><cell></cell><cell>GF</cell><cell>48.66</cell><cell>48.73</cell><cell>48.84</cell><cell>48.91</cell><cell>49.03</cell><cell>49.03</cell><cell>49.07</cell><cell>49.08</cell><cell>49.02</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>58.60</cell><cell>58.93</cell><cell>59.04</cell><cell>59.18</cell><cell>59.26</cell><cell>59.29</cell><cell>59.28</cell><cell>59.39</cell><cell>59.30</cell></row><row><cell>Macro-F1</cell><cell>DeepWalk(256dim)</cell><cell>59.00</cell><cell>59.59</cell><cell>59.80</cell><cell>59.94</cell><cell>60.09</cell><cell>60.17</cell><cell>60.18</cell><cell>60.27</cell><cell>60.18</cell></row><row><cell></cell><cell>LINE(1st)</cell><cell>62.14</cell><cell>62.53</cell><cell>62.64</cell><cell>62.74</cell><cell>62.78</cell><cell>62.82</cell><cell>62.86</cell><cell>62.96</cell><cell>62.89</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>61.46</cell><cell>61.82</cell><cell>61.92</cell><cell>62.02</cell><cell>62.13</cell><cell>62.12</cell><cell>62.17</cell><cell>62.23</cell><cell>62.25</cell></row><row><cell></cell><cell>LINE(1st+2nd)</cell><cell cols="2">62.23** 62.95**</cell><cell cols="4">63.20** 63.35** 63.48** 63.48**</cell><cell cols="2">63.55** 63.69**</cell><cell>63.68**</cell></row><row><cell></cell><cell cols="9">Significantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of multi-label classification on the Youtube network. The results in the brackets are on the reconstructed network, which adds second-order neighbors (i.e., neighbors of neighbors) as neighbors for vertices with a low degree.</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>1%</cell><cell>2%</cell><cell>3%</cell><cell>4%</cell><cell>5%</cell><cell>6%</cell><cell>7%</cell><cell>8%</cell><cell>9%</cell><cell>10%</cell></row><row><cell></cell><cell>GF</cell><cell>25.43 (24.97)</cell><cell>26.16 (26.48)</cell><cell>26.60 (27.25)</cell><cell>26.91 (27.87)</cell><cell>27.32 (28.31)</cell><cell>27.61 (28.68)</cell><cell>27.88 (29.01)</cell><cell>28.13 (29.21)</cell><cell>28.30 (29.36)</cell><cell>28.51 (29.63)</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>39.68</cell><cell>41.78</cell><cell>42.78</cell><cell>43.55</cell><cell>43.96</cell><cell>44.31</cell><cell>44.61</cell><cell>44.89</cell><cell>45.06</cell><cell>45.23</cell></row><row><cell></cell><cell>DeepWalk(256dim)</cell><cell>39.94</cell><cell>42.17</cell><cell>43.19</cell><cell>44.05</cell><cell>44.47</cell><cell>44.84</cell><cell>45.17</cell><cell>45.43</cell><cell>45.65</cell><cell>45.81</cell></row><row><cell>Micro-F1</cell><cell>LINE(1st)</cell><cell>35.43 (36.47)</cell><cell>38.08 (38.87)</cell><cell>39.33 (40.01)</cell><cell>40.21 (40.85)</cell><cell>40.77 (41.33)</cell><cell>41.24 (41.73)</cell><cell>41.53 (42.05)</cell><cell>41.89 (42.34)</cell><cell>42.07 (42.57)</cell><cell>42.21 (42.73)</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>32.98 (36.78)</cell><cell>36.70 (40.37)</cell><cell>38.93 (42.10)</cell><cell>40.26 (43.25)</cell><cell>41.08 (43.90)</cell><cell>41.79 (44.44)</cell><cell>42.28 (44.83)</cell><cell>42.70 (45.18)</cell><cell>43.04 (45.50)</cell><cell>43.34 (45.67)</cell></row><row><cell></cell><cell>LINE(1st+2nd)</cell><cell>39.01* (40.20)</cell><cell>41.89 (42.70)</cell><cell cols="8">43.14 (43.94**) (44.71**) (45.19**) (45.55**) (45.87**) (46.15**) (46.33**) (46.43**) 44.04 44.62 45.06 45.34 45.69** 45.91** 46.08**</cell></row><row><cell></cell><cell>GF</cell><cell>7.38 (11.01)</cell><cell>8.44 (13.55)</cell><cell>9.35 (14.93)</cell><cell>9.80 (15.90)</cell><cell>10.38 (16.45)</cell><cell>10.79 (16.93)</cell><cell>11.21 (17.38)</cell><cell>11.55 (17.64)</cell><cell>11.81 (17.80)</cell><cell>12.08 (18.09)</cell></row><row><cell></cell><cell>DeepWalk</cell><cell>28.39</cell><cell>30.96</cell><cell>32.28</cell><cell>33.43</cell><cell>33.92</cell><cell>34.32</cell><cell>34.83</cell><cell>35.27</cell><cell>35.54</cell><cell>35.86</cell></row><row><cell></cell><cell>DeepWalk (256dim)</cell><cell>28.95</cell><cell>31.79</cell><cell>33.16</cell><cell>34.42</cell><cell>34.93</cell><cell>35.44</cell><cell>35.99</cell><cell>36.41</cell><cell>36.78</cell><cell>37.11</cell></row><row><cell>Macro-F1</cell><cell>LINE(1st)</cell><cell>28.74 (29.40)</cell><cell>31.24 (31.75)</cell><cell>32.26 (32.74)</cell><cell>33.05 (33.41)</cell><cell>33.30 (33.70)</cell><cell>33.60 (33.99)</cell><cell>33.86 (34.26)</cell><cell>34.18 (34.52)</cell><cell>34.33 (34.77)</cell><cell>34.44 (34.92)</cell></row><row><cell></cell><cell>LINE(2nd)</cell><cell>17.06 (22.18)</cell><cell>21.73 (27.25)</cell><cell>25.28 (29.87)</cell><cell>27.36 (31.88)</cell><cell>28.50 (32.86)</cell><cell>29.59 (33.73)</cell><cell>30.43 (34.50)</cell><cell>31.14 (35.15)</cell><cell>31.81 (35.76)</cell><cell>32.32 (36.19)</cell></row><row><cell></cell><cell>LINE(1st+2nd)</cell><cell cols="10">29.85 (29.24) (33.16**) (35.08**) (36.45**) (37.14**) (37.69**) (38.30**) (38.80**) (39.15**) (39.40**) 31.93 33.96 35.46** 36.25** 36.90** 37.48** 38.10** 38.46** 38.82**</cell></row><row><cell></cell><cell cols="9">Significantly outperforms DeepWalk at the: ** 0.01 and * 0.05 level, paired t-test.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results of multi-label classification on DBLP(AuthorCitation) network.</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results of multi-label classification on DBLP(PaperCitation) network.</figDesc><table><row><cell>Metric</cell><cell>Algorithm</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>Micro-F1</cell><cell>DeepWalk LINE(2nd)</cell><cell>52.83 58.42</cell><cell>53.80 59.58</cell><cell>54.24 60.29</cell><cell>54.75 60.78</cell><cell>55.07 60.94</cell><cell>55.13 61.20</cell><cell>55.48 61.39</cell><cell>55.42 61.39</cell><cell>55.90 61.79</cell></row><row><cell></cell><cell></cell><cell cols="9">(60.10**) (61.06**) (61.46**) (61.73**) (61.85**) (62.10**) (62.21**) (62.25**) (62.80**)</cell></row><row><cell>Macro-F1</cell><cell>DeepWalk LINE(2nd)</cell><cell>43.74 48.74</cell><cell>44.85 50.10</cell><cell>45.34 50.84</cell><cell>45.85 51.31</cell><cell>46.20 51.61</cell><cell>46.25 51.77</cell><cell>46.51 51.94</cell><cell>46.36 51.89</cell><cell>46.73 52.16</cell></row><row><cell></cell><cell></cell><cell>(50.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.csie.ntu.edu.tw/~cjlin/liblinear/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank the three anonymous reviewers for the helpful comments. The co-author Ming Zhang is supported by the National Natural Science Foundation of China (NSFC Grant No. 61472006); Qiaozhu Mei is supported by the National Science Foundation under grant numbers IIS-1054199 and CCF-1048168.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social Network Data Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory, 1930-1955</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in linguistic analysis</title>
		<editor>J. R. Firth</editor>
		<imprint>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The strength of weak ties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Granovetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of sociology</title>
		<imprint>
			<biblScope unit="page" from="1360" to="1380" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the sampling complexity of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Information network or social network?: the structure of the twitter follow graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the companion publication of the 23rd international conference on World wide web companion</title>
		<meeting>the companion publication of the 23rd international conference on World wide web companion</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM international conference on Web search and data mining</title>
		<meeting>the 7th ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
