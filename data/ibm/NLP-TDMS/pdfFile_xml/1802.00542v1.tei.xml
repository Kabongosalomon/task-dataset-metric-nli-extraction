<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExpNet: Landmark-Free, Deep, 3D Facial Expressions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
							<email>hassner@openu.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Open University of Israel</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
							<email>medioni@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Robotics and Intelligent Systems</orgName>
								<orgName type="institution">USC</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ExpNet: Landmark-Free, Deep, 3D Facial Expressions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a deep learning based method for estimating 3D facial expression coefficients. Unlike previous work, our process does not relay on facial landmark detection methods as a proxy step. Recent methods have shown that a CNN can be trained to regress accurate and discriminative 3D morphable model (3DMM) representations, directly from image intensities. By foregoing facial landmark detection, these methods were able to estimate shapes for occluded faces appearing in unprecedented in-the-wild viewing conditions. We build on those methods by showing that facial expressions can also be estimated by a robust, deep, landmark-free approach. Our ExpNet CNN is applied directly to the intensities of a face image and regresses a 29D vector of 3D expression coefficients. We propose a unique method for collecting data to train this network, leveraging on the robustness of deep networks to training label noise. We further offer a novel means of evaluating the accuracy of estimated expression coefficients: by measuring how well they capture facial emotions on the CK+ and EmotiW-17 emotion recognition benchmarks. We show that our ExpNet produces expression coefficients which better discriminate between facial emotions than those obtained using state of the art, facial landmark detection techniques. Moreover, this advantage grows as image scales drop, demonstrating that our ExpNet is more robust to scale changes than landmark detection methods. Finally, at the same level of accuracy, our ExpNet is orders of magnitude faster than its alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Successful methods for single view 3D face shape modeling were proposed nearly two decades ago <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b34">[34]</ref>. These methods, and the many that followed, often claimed high fidelity reconstructions and offered parameterizations for facial expressions besides the underlying 3D facial shape.</p><p>Despite their impressive results, they and others since <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b42">[42]</ref> suffered from prevailing problems when it came to processing face images taken under unconstrained viewing conditions. Many of these methods relied to some extent on facial landmark detection, performed either prior to reconstruction or concurrently, as part of the reconstruction process. By involving landmark detection, these methods are sensitive to face pose and, aside from a few recent exceptions (e.g., 3DDFA <ref type="bibr" target="#b49">[49]</ref>), could not operate well on faces viewed in extreme out of plane rotations (e.g., near profile). Scale and occlusions are also problems: Whether because landmarks are too small to accurately localize or altogether invisible due to occlusions, accurate detection and consequent 3D reconstruction is not handled well.</p><p>In addition to these problems, many methods applied <ref type="figure">Fig. 1</ref>: Deep 3D face modeling with expressions. We propose to regress 3DMM expression coefficients without facial landmark detection, directly from image intensities. We show this approach to be highly robust to extreme appearance variations, including out-of-plane head rotations (top row), scale changes (middle), and even ages (bottom).</p><p>iterative steps of analysis-by-synthesis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b35">[35]</ref>. These methods were not only computationally expensive, but also hard to distribute and run in parallel on dedicated hardware such as the ubiquitous graphical processing units (GPU). Very recently, some of these problems were addressed by two papers, which are both relevant to this work. First, Tran et al. <ref type="bibr" target="#b38">[38]</ref> proposed to use a deep CNN to estimate the 3D shape and texture of faces appearing in unconstrained images. Their CNN regressed 3D morphable face model (3DMM) parameters directly. To test the extent to which their estimates were robust and discriminative, they then used these 3DMM parameters as face representations in challenging, unconstrained face recognition benchmarks, including the Labeled Faces in the Wild (LFW) <ref type="bibr" target="#b20">[20]</ref> and the IARPA Janus Benchmark A (IJB-A) <ref type="bibr" target="#b24">[24]</ref>. By doing so, they showed that their estimated 3DMM parameters were nearly as discriminative as opaque deep features extracted by deep networks trained specifically for recognition.</p><p>Chang et al. <ref type="bibr" target="#b6">[7]</ref> extended this work by showing that 6 degrees of freedom (6DoF) pose can also be estimated using a similar deep, landmark free approach. Their proposed FacePoseNet (FPN) essentially performed face alignment in 3D, directly from image intensities and without the need for facial landmarks which are usually used for these purposes.</p><p>Our paper uses similar techniques to model 3D facial expressions. Specifically, we show how facial expressions can be modeled directly from image intensities using our proposed deep neural network: ExpNet. To our knowledge, this is the first time that a CNN is shown to estimate 3D expression coefficients directly, without requiring or involving facial landmark detection.</p><p>We provide a multitude of face reconstruction examples, visualizing our estimated expressions on faces appearing in challenging unconstrained conditions (see, e.g., <ref type="figure">Fig. 1</ref>). We know of few previous method who offered this many examples of their capabilities.</p><p>We go beyond previous work, however, by additionally offering quantitative comparisons of our facial expression estimates. To this end, we propose to measure how well different expression regression methods capture facial emotions on the Extended Cohn-Kanade (CK+) <ref type="bibr" target="#b27">[27]</ref> and EmotiW-17 benchmarks <ref type="bibr" target="#b9">[10]</ref>. Both benchmarks contain face images labeled for emotion categories, allowing us to focus on how well emotions are captured by our method and others. We show that not only does our deep approach provide more meaningful expression representations, it is more robust to scale changes than methods which rely on landmarks for this purpose. Finally, to promote reproduction of our results, our code and deep models are publicly available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expression Estimation</head><p>We first emphasize the distinction between the related, yet different tasks of emotion classification vs. expression regression. The former seeks to classify images or videos into discrete sets of facial emotion classes <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b27">[27]</ref> or action unites <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b47">[47]</ref>. This problem was often addressed by considering the locations of facial landmarks. In recent years a growing number of state of the art methods have instead adopted deep networks <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b48">[48]</ref>, applying them directly to image intensities rather than estimating landmark positions as a proxy step.</p><p>Methods for expression regression attempt to extract parameters for face deformations. These parameters are often expressed in the form of active appearance models (AAM) <ref type="bibr" target="#b27">[27]</ref> and Blendshape model coefficients <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>. In this work we focus on estimating 3D expression coefficients, using the same representation described by 3DDFA <ref type="bibr" target="#b49">[49]</ref>. Unlike 3DDFA, however, we completely decouple expression coefficient regression from facial landmark detection. Our tests demonstrate that by doing so, we obtain a method which is more robust to changing image scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Facial Landmark Detection</head><p>There has been a great deal of work dedicated to accurately detecting facial landmarks, and not only due to their role in expression estimation. Face landmark detection is a general problem which has applications in numerous face related systems. Landmark detectors are very often used to align face images by applying rigid <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b40">[40]</ref> and non-rigid transformations <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b49">[49]</ref> transformations in 2D and 3D <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>.</p><p>Generally speaking, landmark detectors can be divided into two broad categories: Regression based <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b41">[41]</ref> and Model based <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b49">[49]</ref> techniques. Regression based methods estimate landmark locations directly from facial appearance while model based methods explicitly model both the shape and appearance of landmarks. Regardless of the approach, landmark estimation can fail whenever faces are viewed in extreme out-of-plane rotations (far from frontal), low scale, or when the face bounding box differs significantly from the one used to develop the landmark detector.</p><p>To address the problem of varying 3D poses, the recent 3DDFA <ref type="bibr" target="#b49">[49]</ref>, related to our own, learns the parameters of a 3DMM representations using a CNN. Unlike us, however, they prescribe an iterative, analysis-by-synthesis approach. Also related to us is the recent CE-CLM <ref type="bibr" target="#b46">[46]</ref>. CE-CLM introduces a convolution expert network to capture very complex landmark appearance variations and thereby achieving state-of-the-art landmark detection accuracy.</p><p>The exact locations of facial landmarks were once considered subject-specific information which can be used for face recognition <ref type="bibr" target="#b10">[11]</ref>. Today, however, such attempts are all but abandoned. The reason for turning to other face representations may be due to the real-word imaging conditions typically assumed by modern face recognition systems <ref type="bibr" target="#b24">[24]</ref> where even state of the art landmark detection accuracy is insufficient to discriminate between individuals based solely on the locations of their detected facial landmarks. In other applications, however, facial landmarks prevail. This work follows recent attempts, most notably Chang et al. <ref type="bibr" target="#b6">[7]</ref>, by proposing landmark free alternatives for face understanding tasks. This effort is intended to allow for accurate expression estimation on images which defy landmark detection techniques, in similar spirit to the abandonment of landmarks as a means for representing identities. To our knowledge, such a direct, landmark free, deep approach to expression modeling was never previously attempted .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP, 3D EXPRESSION MODELING</head><p>We propose to estimate facial expression coefficients using a CNN applied directly to image intensities. A chief concern when training such deep networks is the availability of labeled training data. For our purposes, training labels are 29D real-valued vectors of expression coefficients. These labels do not have a natural interpretations that can easily be used by human operators to manually collect and label training data. We next explain how 3D shapes and their expressions are represented and how ample data may be collected to effectively train a deep network for our purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Representing 3D Faces and Expressions</head><p>We assume a standard 3DMM face representation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b32">[32]</ref>. Given an input face photo I, standard methods for estimating its 3DMM representation typically detect facial feature points and then use those as constraints when estimating the optimal 3DMM expression coefficients (see, for example, the recent 3DDFA method <ref type="bibr" target="#b49">[49]</ref>). Instead, we propose to estimate expression parameters by directly regressing 3DMM expression coefficients, decoupling shape and texture from pose and from expression. Specifically, we model a 3D face shape using the following, standard, linear 3DMM representation (for now, ignoring parameters representing facial texture and 6DoF pose):</p><formula xml:id="formula_0">S = s + Sα + Eη<label>(1)</label></formula><p>where s represents the average 3D face shape. The second term provides shape variations as a linear combination of shape coefficients α ∈ R s with S ∈ R 3n×s principal components. 3D expression deformations are provided as an additional linear combination of expression coefficients η ∈ R m and expression components E ∈ R 3n×m . Here, 3n represents the 3D coordinates for the n pixels in I. The numbers of components, s, for shape and for expression, m, provide the dimensionality of the 3DMM coefficients. Our representation uses the BFM 3DMM shape components <ref type="bibr" target="#b32">[32]</ref>, where s = 99 and the expression components defined by 3DDFA <ref type="bibr" target="#b49">[49]</ref>, with m = 29.</p><p>The vectors α and η control the intensity of deformations provided by the principal components. Given estimates for α and η, it is therefore possible to reconstruct the 3D face shape of the face appearing in the input image using Eq. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generating 3D Expression Data</head><p>To our knowledge, there is no publicly available data set containing sufficiently many face images labeled with their 29D expression coefficients. Presumably, one way of mitigating this problem is to use a 3D facial expressions database such as BU-4DFE <ref type="bibr" target="#b45">[45]</ref> as a training data set. BU-4DFE faces, however, are viewed under constrained conditions and this would therefore limit application of the network to constrained settings. Furthermore, BU-4DFE contains only 101 subjects and six facial expressions and can thus limit the range of expression coefficients our network predicts.</p><p>Another way of addressing the training data problem is by utilizing a face landmark detection benchmark. That is, taking the face images in existing landmark detection benchmarks and computing their expression coefficients using their ground truth landmark annotations in order to obtain 29D ground truth expression labels. Existing landmark detection benchmarks, however, are limited in their sizes: The number of images in the training and testing splits of the popular 300W landmark detection data set <ref type="bibr" target="#b36">[36]</ref>, for example, is 3,026. This is far too small to train a deep CNN to regress 29D real valued vectors.</p><p>Given the absence of sufficiently large and rich 3D expression training sets, we propose a simple method for generating ample examples of faces in thew wild, coupled with 29D expression coefficients labels. We begin by estimating 99D 3DMM coefficients for the 0.5 million face images in the CASIA WebFace collection <ref type="bibr" target="#b44">[44]</ref>. 3DMM shape parameters were estimated following the state of the art method of <ref type="bibr" target="#b38">[38]</ref>, giving us, for every CASIA image, an estimate of its shape coefficients, α. We assume that all images belonging to the same subject should have the same, single 3D shape. We therefore apply the shape coefficients pooling method of <ref type="bibr" target="#b38">[38]</ref> to average the 3DMM shape estimates for all images belonging to the same subject, thereby obtaining a single 3DMM shape estimate per subject.</p><p>Poses were additionally estimated for each image using FPN <ref type="bibr" target="#b6">[7]</ref>. We then use standard techniques <ref type="bibr" target="#b15">[15]</ref> to compute a projection matrix Π from the 6DoF provided by that method.</p><p>Given a projection matrix Π that maps from the recovered 3D shape, S , to the 2D points of an input image, we can solve the following optimization problem to get expression coefficients:</p><formula xml:id="formula_1">η = arg min η ||p − ΠS || 2 , subject to |η j | ≤ 3 δ Ej ,<label>(2)</label></formula><p>where α in S (Eq. 1) is estimated by <ref type="bibr" target="#b38">[38]</ref>. δ Ej is the standard deviation of the j-th principal components of the 3DMM expression; p is a set of 2D facial landmarks detected in the input image by a standard facial landmark detection method, in our experiments, CLNF <ref type="bibr" target="#b0">[1]</ref>. We solve for η in Eq. (2) via standard Gauss-Newton optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training ExpNet to Predict Expression Coefficients</head><p>We use the expression coefficients obtained from Eq. (2) as ground truth labels when training our ExpNet. In practice, ExpNet employs a ResNet-101 deep network architecture <ref type="bibr" target="#b18">[18]</ref>. We did not experiment with smaller network structures, and so a more compact network may well work just as well for our purposes. Our ExpNet is trained to regress a parametric function f ({W, b}, I) → η, where {W, b} represent the parametric filters and weights of the CNN. We use a standard 2 reconstruction loss between ExpNet predictions and its expression coefficients training labels.</p><p>ExpNet is trained using Stochastic Gradient Descent (SGD) with a mini-batch size of 144, momentum of 0.9, and weight decay of 5e-4. The network weights are updated with learning rate set to 1e-3. When the validation loss saturates, we decrease learning rates by an order of magnitude until the validation loss stops decreasing. No data augmentation is performed during training: that is, we use the plain images in the CASIA set since they are already roughly aligned <ref type="bibr" target="#b44">[44]</ref>. In order to make training easier, we removed the empirical mean from all the input faces.</p><p>We note that our approach is similar to the one used by Tran et al. <ref type="bibr" target="#b38">[38]</ref>, and in particular, we use the same network architecture used in their work to regress 3DMM shape and texture parameters. They, however, explicitly assume a unique shape representations for all images of the same subject. This assumption allowed them to better regularize their network, by presenting it with multiple images with varying nuisance but the same underlying label (i.e. shape coefficients do not vary within the images of a subject). Here, this is not the case and expression parameters vary from one image to the next, regardless of subject identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Estimating Expressions Coefficients with ExpNet</head><p>Existing methods for expression estimation often take an analysis-by-synthesis approach to optimizing facial landmark locations. Contrary to them, our expressions are obtained in a single forward pass of our CNN. To estimate an expression coefficients vector, η t , we evaluate I t f ({W, b}, I t ) for test image, I t . We preprocess test images using the face detector of Yang et al. <ref type="bibr" target="#b43">[43]</ref> and increasing its returned face bounding box by a scale of ×1.25 of its size. This scaling was manually determined to bring face bounding boxes to roughly the same size as the loose bounding boxes of CASIA faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>We evaluated our method both qualitatively and quantitatively. It is important to note that few previous methods for 3D expression estimation performed quantitative tests; instead, most offered only qualitative results. We provide an extensive number of figures demonstrating the quality of our expression estimation method (Sec. IV-B) In addition, we offer quantitative tests, designed to capture the extent to which our expressions reflect facial emotions (Sec. IV-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Tests</head><p>Benchmark settings. Aside from 3DDFA <ref type="bibr" target="#b49">[49]</ref>, we know of no previous method which directly estimates 29D expression coefficients vectors. Instead, previous work relied on facial landmark detectors and used their detected landmarks to estimate facial expressions. We therefore compare the expressions estimated by our ExpNet to those obtained from state of the art landmark detectors. Because no benchmark exists with ground truth expression coefficients, we compare these methods on the related task of facial emotion classification. Our underlying assumption here is that better expression estimation implies better emotion classification.</p><p>We use benchmarks containing face images labeled for discrete emotion classes. For each image we estimate its expression coefficients, either directly using our ExpNet and 3DDFA, or using detected landmarks by solving Eq. (2) as described in Sec. III-B. We then attempt to classify the emotions for test images using the exact same classification pipeline applied to these 29D expression representations.</p><p>Our tests utilize the Extended Cohn-Kanade (CK+) dataset <ref type="bibr" target="#b27">[27]</ref> and the Emotion Recognition in the Wild Challenge (EmotiW-17) dataset <ref type="bibr" target="#b9">[10]</ref>. The CK+ dataset is a constrained set, with frontal images taken in the lab, while the EmotiW-17 dataset contains highly challening video frames collected from 54 movie DVDs <ref type="bibr" target="#b8">[9]</ref>.</p><p>The CK+ dataset contains 327 face video clips labeled for seven emotion classes: anger (An), contempt (Co), disgust (Di), fear (Fe), happy (Ha), sadness (Sa), surprise (Su). From each clip, we take the peak frame (the end of video) the frame assigned with an emotion label and use it for classification. The EmotiW-17 dataset, on the other hand,  Following the protocol used by <ref type="bibr" target="#b27">[27]</ref>, we ran a leaveone clip-out test protocol to assess performance. We also evaluate the robustness of different methods to scale changes. Specifically, we tested all methods on multiple version of the CK+ and EmotiW-17 benchmarks, each version with all images scaled down to ×0.8, 0.6, 0.4, and 0.2 their sizes.</p><p>Emotion classification pipeline. The same simple classification method was used for all methods in all our tests. We preferred a simple classification method rather than a state of the art technique, in order to prevent obscuring the quality of the landmark detector / emotion estimation by using an elaborate classifier. We therefore use a simple kNN classifier with K = 5. It is important to note that the results obtained by all of the tested methods are far from the state of the art on this set; our goal is not to outperform state of the art emotion classification methods, but only to compare expression coefficient estimation techniques.</p><p>Baseline methods. We compare our approach to widely used, state-of-the-art face landmark detectors. These are DLIB <ref type="bibr" target="#b23">[23]</ref>, CLNF <ref type="bibr" target="#b0">[1]</ref>, OpenFace <ref type="bibr" target="#b1">[2]</ref>, CE-CLM <ref type="bibr" target="#b46">[46]</ref>, RCPR <ref type="bibr" target="#b5">[6]</ref>, and 3DDFA <ref type="bibr" target="#b49">[49]</ref>. Note that CLNF is the method used to produce our training labels.</p><p>Results. <ref type="figure">Fig. 2 and 3</ref> report the emotion classification confusion matrices on the original CK+ and EmotiW-17 datasets (unscaled) for our method <ref type="figure">(Fig. 2(c) and 3(c)</ref>), comparing it to the other methods, 3DDFA <ref type="figure">(Fig. 2(b)</ref>) and CE-CLM / CLNF <ref type="figure">(Fig. 2(a) / 3(a)</ref>).</p><p>On CK+, our expression coefficients were able to capture well surprise (Su), happy (Ha), and disgust (Di) emotions, all emotions which are well defined by facial expressions. On EmotiW-17, our method performed well on neutral (Ne), happy (Ha), sad (Sa), and angry (An), but less so on disgust (Di), fear (Fe), and surprise (Su). From our observations, these last emotions are visually similar to angry (An), which could explain why they challenged our system. On the whole, however, our representation was noticeably better at capturing all emotion classes than its baselines. <ref type="figure">Fig. 4</ref> reports emotion classification performances of all   <ref type="bibr" target="#b38">[38]</ref>. Expressions added using a number of baseline methods including our ExpNet. Our method is better able to model subtle expressions than 3DDFA. The top-performing landmark detector, CE-CLM <ref type="bibr" target="#b46">[46]</ref>, does not perform as well on these images. <ref type="figure">Fig. 6</ref>: Qualitative expression estimation on EmotiW-17. 3D head shapes estimated by a deep 3DMM fitting method <ref type="bibr" target="#b38">[38]</ref>. We add expressions using a number of baseline methods comparing them with our ExpNet. Our method and 3DDFA <ref type="bibr" target="#b49">[49]</ref> show consistent expression fitting across scales. Our method additionally models subtle expressions better than 3DDFA. The top-performing facial landmark detector, CLNF <ref type="bibr" target="#b0">[1]</ref>, does not perform as well on these images.</p><p>methods on scaled versions of the CK+ <ref type="figure">(Fig. 4(a)</ref>) and EmotiW-17 sets ( <ref type="figure">Fig. 4(b)</ref>). These results measure the sensitivity of different methods to the input image resolution: The x-axis reports the downsizing factor, proportional to the original scale. A scale of 1 therefore represents the original image sizes (640x490 for CK+; 720x576 for EmotiW-17), scale of 0.2 implies 128x98 for CK+ and 144x115 for EmotiW-17, and so fourth. <ref type="figure">Fig. 4</ref> clearly show our approach to be the most accurate in terms of emotion recognition accuracy. It is additionally far more robust to scale changes compared than the other landmark detection based methods. Note also the difference in emotion recognition between deep methodsours and <ref type="bibr" target="#b49">[49]</ref>-and landmark based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in</head><p>Importantly, our method outperforms CLNF <ref type="bibr" target="#b0">[1]</ref> by a wide margin in all tests. This result is significant, as CLNF was the method used to generate our expression labels in Sec. III-B. Our improved performance suggests that the network learned to generalize from its training data and thus performed better on a wider range of viewing conditions and challenges.</p><p>Runtime. Tab. I reports runtimes for the methods tested. All tests were performed on a machine with an NVIDIA, GeForce GTX TITAN X and an Intel Xeon CPU E5-2640 v3 @ 2.60GHz. The only exception was 3DDFA <ref type="bibr" target="#b49">[49]</ref>, which required a Windows system and was tested using an Intel Core i7-4820K CPU @ 3.70GHz with 8 CPUs.</p><p>We compare landmark based approaches with deep, direct method such as 3DDFA and our ExpNet. ExpNet is at least one order of magnitude faster than any of its alternatives. Note that, landmark based expression fitting methods generally follow a three-step process: (i) facial landmark detection, (ii) head pose estimation, and (iii) expression fitting. Their total processing time is therefore the sum of these steps. Although some landmark detection methods (e.g. DLIB) are extremely efficient (0.009s), they are still required to solve the optimization problem of Eq. (2), in order to translate these detections to an expression coefficients estimate. This process is much slower than our proposed method.</p><p>As for deep methods for expression estimation, the software package provided by 3DDFA <ref type="bibr" target="#b49">[49]</ref> does not allow testing on the GPU; in their paper, they report GPU runtime to be 0.076 seconds, which is similar to our runtime, which was measured on a GPU. Other facial landmarks detector based methods, including the code used to solve Eq. (2), are all intrinsically implemented on the CPU. Though they may conceivably be expedited significantly by porting them to the GPU, we are unaware of any such implementation. and also at our lowest resolution (scale 0.2). All the results in these figures use the same 3D shape provided by 3DMM-CNN <ref type="bibr" target="#b38">[38]</ref>. Additional, mid level facial details can possibly be added using, e.g., <ref type="bibr" target="#b39">[39]</ref>, but to emphasize expressions, rather than details, we used only course facial shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head><p>These figures visualize expressions estimated with our ExpNet compared with the recent deep method for joint estimation of shape and expression <ref type="bibr" target="#b49">[49]</ref>, and the top performing landmark detectors CE-CLM <ref type="bibr" target="#b46">[46]</ref>, and CLNF <ref type="bibr" target="#b0">[1]</ref>. For reference, we provide also the shape 3D face shape <ref type="bibr" target="#b38">[38]</ref> estimated before expressions were added.</p><p>Our expression estimates appear to be much better at capturing expression nuances: This is clear from the subtle <ref type="figure">Fig. 7</ref>: Expression estimation failures. Our method is less able to handle extreme facial expressions. Other methods, by comparison, appear to either exaggerate the expression (3DDFA) or are inconsistent across scales (CE-CLM).</p><p>expressions, fear and anger, rendered in <ref type="figure" target="#fig_1">Fig. 5</ref>. This is consistent with the improvement shown in the confusion matrices in <ref type="figure">Fig.2</ref>. 3DDFA appears inconsistent across the same expression (happy) and tends to either exaggerate the expression or underestimate it. Both CE-CLM and CLNF seem sensitive to input image resolutions: They both estimate different expressions for the same input image offered at different scales.</p><p>Finally, <ref type="figure" target="#fig_1">Fig. 5</ref> demonstrates a weaknesses of our ExpNet to strong intensity expressions such as surprise. 3DDFA, by comparison, produces somewhat over-exaggerated estimates on these images. Although CE-CLM produces visually suitable estimates, its predictions are inconsistent across scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We present a method for deep, 3D expression modeling and show it to be far more robust than than facial landmark detection methods widely used for this task. Our approach estimates expressions without the use of facial landmarks, suggesting that facial landmark detection methods may be redundant for this task. This conclusion is consistent with recent results demonstrating deep, landmark free 3D face shape estimation <ref type="bibr" target="#b6">[7]</ref> and 6DoF head alignment <ref type="bibr" target="#b38">[38]</ref>. The significance of these results is that by avoiding facial landmark detection, we can process face images obtained in extreme viewing condition which can be challenging for landmark detection methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :Fig. 3 :Fig. 4 :</head><label>234</label><figDesc>Confusion matrix for emotion recognition on the CK+ benchmark<ref type="bibr" target="#b27">[27]</ref>. Confusion distributions across emotion classes using the original input image resolution. Results provided for (a) the best performing landmark detector, CE-CLM<ref type="bibr" target="#b46">[46]</ref>,(b) the recent, deep 3DDFA [49], (c) our ExpNet. Confusion matrix for emotion recognition on the EmotiW-17 benchmark [10]. Confusion distributions across emotion classes using the original input image resolution. Results provided for (a) the best performing landmark detector, CLNF [1], (b) the recent, deep 3DDFA [49], (c) our ExpNet. Emotion recognition accuracy across scales. Results provided for (a) the CK+ and (b) EmotiW-17 benchmarks. Each curve corresponds to a different method. For each scale, the experiment resizes the input image accordingly. Lower scale values indicate lower resolutions. Original resolutions were 640×490 (CK+) and 720×576 (EmotiW-17).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative expression estimation on CK+. 3D head shapes estimated by a deep 3DMM fitting method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>and 6 provide qualitative renderings of the 3D expressions estimated on CK+ and EmotiW-17 images. Each result was obtained on the original, input image scale (scale 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Expression estimation runtime. Comparing a number of alternative methods to our ExpNet. Landmark based methods require several steps for landmark detection and then expression optimization; whereas deep methods solve for expression in a single step.</figDesc><table><row><cell>offers 383 face video clips labeled for 7 emotion classes:</cell></row><row><cell>anger (An), disgust (Di), fear (Fe), happy (Ha), neutral (Ne),</cell></row><row><cell>sadness (Sa), surprise (Su). We estimate 29D expression</cell></row><row><cell>representations for every frame and apply average pooling</cell></row><row><cell>of the per-frame estimates across all frames of each video.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available: github.com/fengju514/Expression-Net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Openface: an open source facial behavior analysis toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conf. on App. of Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fitting a 3D morphable model to edges: A comparison between hard and soft correspondences. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wuhrer</surname></persName>
		</author>
		<idno>abs/1602.01125</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face identification across different poses and illuminations with a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="192" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face recognition based on fitting a 3d morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FacePoseNet: Making a case for landmark-free face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D-aided face recognition robust to expression and pose variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From individual to group-level emotion recognition: Emotiw 5.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="524" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition using active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. on Inform. Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hello! My name is</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">-automatic naming of characters in TV video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Mach. Vision Conf</title>
		<meeting>British Mach. Vision Conf</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Benitez-Quiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5562" to="5570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Viewing real-world faces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<ptr target="Available:www.openu.ac.il/home/hassner/projects/poses" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face recognition using a unified 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>UMass, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multiresolution 3D morphable face model and fitting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rtsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark-A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotion recognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimodal Interaction</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning pose-aware models for pose-invariant face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose independent face recognition by localizing local binary patterns via deformation components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4477" to="4482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rapid synthesis of massive face sets for improved face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Automatic Face and Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="604" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Do We Really Need to Collect Millions of Faces for Effective Face Recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<ptr target="Availablewww.openu.ac.il/home/hassner/projects/augmented_faces" />
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advanced Video and Signal based Surveillance</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05053</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient, robust and accurate fitting of a 3D morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating 3D shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">300 faces in-the-wild challenge: Database and results. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Realtime conversion from a single 2d face image to a 3D text-driven emotive audio-visual avatar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1205" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3D morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05083</idno>
		<title level="m">Extreme 3D face reconstruction: Looking past occlusions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial landmark detection with tweaked convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Expression flow for 3D-aware face component transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A multi-scale cascade fully convolutional network face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="633" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<ptr target="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A high-resolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Convolutional experts constrained local model for facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Facial affect&quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gender and smile classification using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition Workshops</title>
		<meeting>Conf. Comput. Vision Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
