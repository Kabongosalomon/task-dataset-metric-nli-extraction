<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Perception-Oriented Network for Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-11">11 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
							<email>es:zheng_hui@aliyun.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Video &amp; Image Processing System (VIPS)</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>No.2, South Taibai Road</addrLine>
									<settlement>Xi&apos;an 710071</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<email>leejie@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Video &amp; Image Processing System (VIPS)</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>No.2, South Taibai Road</addrLine>
									<settlement>Xi&apos;an 710071</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
							<email>xbgao@mail.xidian.edu.cnxinbogao</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Video &amp; Image Processing System (VIPS)</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>No.2, South Taibai Road</addrLine>
									<settlement>Xi&apos;an 710071</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">The Chongqing Key Laboratory of Image Cognition</orgName>
								<orgName type="institution">Chongqing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>400065</postCode>
									<settlement>Chongqing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
							<email>wangxm@xidian.edu.cnxiumeiwang</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Video &amp; Image Processing System (VIPS)</orgName>
								<orgName type="department" key="dep2">School of Electronic Engineering</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<addrLine>No.2, South Taibai Road</addrLine>
									<settlement>Xi&apos;an 710071</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Perception-Oriented Network for Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-11">11 Sep 2020</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Journal of Information Sciences September 14, 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Perceptual image super-resolution</term>
					<term>progressive related works learning</term>
					<term>multi-scale hierarchical fusion * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, it has been demonstrated that deep neural networks can significantly improve the performance of single image super-resolution (SISR). Numerous studies have concentrated on raising the quantitative quality of super-resolved (SR) images. However, these methods that target PSNR maximization usually produce blurred images at large upscaling factor. The introduction of generative adversarial networks (GANs) can mitigate this issue and show impressive results with synthetic high-frequency textures. Nevertheless, these GAN-based approaches always have a tendency to add fake textures and even artifacts to make the SR image of visually higher-resolution. In this paper, we propose a novel perceptual image super-resolution method that progressively generates visually high-quality results by constructing a stage-wise network. Specifically, the first phase concentrates on minimizing pixel-wise error, and the second stage utilizes the features extracted by the previous stage to pursue results with better structural retention. The final stage employs fine structure features distilled by the second phase to produce more realistic results. In this way, we can maintain the pixel, and structural level information in the perceptual image as much as possible. It is useful to note that the proposed method can build three types of images in a feed-forward process. Also, we explore a new generator that adopts multi-scale hierarchical features fusion. Extensive experiments on benchmark datasets show that our approach is superior to the state-of-the-art methods. Code is available at https://github.com/Zheng222/PPON.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the emergence of deep learning for other fields of computer vision studies, the introduction of convolutional neural networks (CNNs) has dramatically advanced SR's performance. For instance, the pioneering work of the super-resolution convolution neural network (SRCNN) proposed by Dong et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> employed three convolutional layers to approximate the nonlinear mapping function from interpolated LR image to HR image and outperformed most conventional SR methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Various works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> that explore network architecture designs and training strategies have continuously improved SR performance in terms of quantitative quality such as peak signal-to-noise ratio (PSNR), root mean squared error (RMSE), and structural similarity (SSIM) <ref type="bibr" target="#b16">[17]</ref>. However, these PSNR-oriented approaches still suffer from blurry results at large upscaling factors, e.g., 4Ã—, particularly concerning the restoration of delicate texture details in the original HR image, distorted in the LR image.</p><p>In recent years, several perceptual-related methods have been exploited to boost visual quality under large upscaling factors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Specifically, the perceptual loss is proposed by Johnson et al. <ref type="bibr" target="#b17">[18]</ref>, which is a loss function that measures differences of the intermediate features of VGG19 <ref type="bibr" target="#b22">[23]</ref> when taking the ground-truth and generated images as inputs. Legig et al. <ref type="bibr" target="#b18">[19]</ref> extend this idea by adding an adversarial loss <ref type="bibr" target="#b23">[24]</ref> and Sajjadi et al. <ref type="bibr" target="#b19">[20]</ref> combine perceptual, adversarial and texture synthesis losses to produce sharper images with realistic textures. Wang et al. <ref type="bibr" target="#b24">[25]</ref> incorporate semantic segmentation maps into a CNN-based SR network to generate realistic and visually pleasing textures. Although these methods can produce sharper images, they typically contain artifacts that are readily observed.</p><p>Moreover, these approaches tend to improve visual quality without considering the substantial degradation of quantitative quality. Since the primary objective of the super-resolution task is to make the enlarged images resemble the ground-truth HR images as much as possible, it is necessary to maintain nature while guaranteeing the basic structural features that is related to pixelto-pixel losses e.g., mean squared error (MSE), mean absolute error (MAE). At present, the most common way is to pre-train a PSNR-oriented model and then fine-tune this pre-trained model, in company with a discriminator network and perceptual loss. Even though this strategy helps increase the stability of the training process, it still requires updating all parameters of the generator, which means an increase in training time.</p><p>In this paper, we propose a novel super-resolution method via the progressive perception-oriented network (PPON), which gradually generates images with pleasing visual quality. More specifically, inspired by <ref type="bibr" target="#b25">[26]</ref>, we propose a hierarchical feature fusion block (HFFB) as the basic block (shown in <ref type="figure" target="#fig_1">Figure 3(a)</ref>), which utilizes multiple dilated convolutions with different rates to exploit abundant multi-scale information. In order to ease the training of very deep networks, we assemble our basic blocks by using residual-in-residual fashion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref> named residual-in-residual fusion block (RRFB) as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(b). Our method adopts three reconstruction modules: a content reconstruction module (CRM), a structure reconstruction module (SRM), and a photo-realism reconstruction module (PRM). The CRM as showed in <ref type="figure">Figure 1</ref> mainly restores global information and minimizes pixel-by-pixel errors as previous PSNR-oriented approaches. The purpose of SRM is to maintain favorable structural information based on CRM's result using structural loss. Analogously, PRM estimates the residual between the real image and the output of SRM with adversarial and perceptual losses. The diagrammatic sketch of this procedure is given in <ref type="figure" target="#fig_0">Figure 2</ref>. Since the input of the perceptual features extraction module (PFEM) contains fruitful structure-related features and the generated perceptual image is built on the result of SRM, our PPON can synthesize a visually pleasing image that provides not only high-frequency components but also structural elements.</p><p>To achieve rapid training, we develop a step-by-step training mode, i.e., our basic model (illustrated in <ref type="figure">Figure 1</ref>) is trained first, then we freeze its parameters and train the sequential SFEM and SRM, and so on. The advantage is that when we train perception-related modules (PFEM and PRM), very few parameters need to be updated. It differs from previous algorithms that they require to optimize all parameters to produce photo-realistic results. Thus, it will reduce training time.</p><p>Overall, our contributions can be summarized as follows.</p><p>â€¢ We develop a progressive photo-realism reconstruction approach, which can synthesize images with high fidelity (PSNR) and compelling visual effects. Specifically, we develop three reconstruction modules for completing multiple tasks, i.e., the content, structure, and perception reconstructions of an image. More broadly, we can also generate three images with different types in a feed-forward process, which is instructive to satisfy various task's requirements.</p><p>â€¢ We design an effective training strategy according to the characteristic of our proposed progressive perception-oriented network (PPON), which is to fix the parameters of the previous training phase and utilize the features produced by this trained model to update a few parameters at the current stage. In this way, the training of the perception-oriented model is robust and fast.</p><p>â€¢ We also propose the basic model RFN mostly constructed by cascading residual-in-residual fusion blocks (RRFBs), which achieves state-of-the-art performance in terms of PSNR.</p><p>The rest of this paper is organized as follows. Section 2 provides a brief review of related SISR methods. Section 3 describes the proposed approach and loss functions in detail. In Section 4, we explain the experiments conducted for this work, experimental comparisons with other state-of-the-art methods, and model analysis. In Section 5, we conclude the study.    Each of them has 32 output channels for reducing block parameters. (b) RRFB is used in our primary and perception-oriented models and Î± is the residual scaling parameter <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we focus on deep neural network approaches to solve the SR problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep learning-based super-resolution</head><p>The pioneering work was done by Dong et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, who proposed SRCNN for the SISR task, which outperformed conventional algorithms. To further improve the accuracy, Kim et al. proposed two deep networks, i.e., VDSR <ref type="bibr" target="#b5">[6]</ref>, and DRCN <ref type="bibr" target="#b6">[7]</ref>, which apply global residual learning and recursive layer respectively to the SR problem. Tai et al. <ref type="bibr" target="#b8">[9]</ref> developed a deep recursive residual network (DRRN) to reduce the model size of the very deep network by using a parameter sharing mechanism. Another work designed by the authors is a very deep endto-end persistent memory network (MemNet) <ref type="bibr" target="#b11">[12]</ref> for image restoration task, which tackles the long-term dependency problem in the previous CNN architectures. The methods mentioned above need to take the interpolated LR images as inputs. It inevitably increases the computational complexity and often results in visible reconstruction artifacts <ref type="bibr" target="#b9">[10]</ref>.</p><p>To speed up the execution time of deep learning-based SR approaches, Shi et al. <ref type="bibr" target="#b7">[8]</ref> proposed an efficient sub-pixel convolutional neural network (ESPCN), which extracts features in the LR space and magnifies the spatial resolution at the end of the network by conducting an efficient sub-pixel convolution layer. Afterward, Dong et al. <ref type="bibr" target="#b4">[5]</ref> developed a fast SRCNN (FSRCNN), which employs the transposed convolution to upscale and aggregate the LR space features. However, these two methods fail to learn complicated mapping due to the limitation of the model capacity. EDSR <ref type="bibr" target="#b10">[11]</ref>, the winner solution of NTIRE2017 <ref type="bibr" target="#b26">[27]</ref>, was presented by Lim et al. . This work is much superior in performance to previous models. To alleviate the difficulty of SR tasks with large scaling factors such as 8Ã—, Lai et al. <ref type="bibr" target="#b9">[10]</ref> proposed the LapSRN, which progressively reconstructs the multiple SR images with different scales in one feed-forward network. Liu et al. <ref type="bibr" target="#b27">[28]</ref> used the phase congruency edge map to guide an end-to-end multiscale deep encoder and decoder network for SISR. Tong et al. <ref type="bibr" target="#b12">[13]</ref> presented a network for SR by employing dense skip connections, which demonstrated that the combination of features at different levels is helpful for improving SR performance. Recently, Zhang et al. <ref type="bibr" target="#b14">[15]</ref> extended this idea and proposed a residual dense network (RDN), where the kernel is residual dense block (RDB) that extracts abundant local features via dense connected convolutional layers. Furthermore, the authors proposed very deep residual channel attention networks (RCAN) <ref type="bibr" target="#b15">[16]</ref> that verified that the very deep network can availably improve SR performance and advantages of channel attention mechanisms. To leverage the execution speed and performance, IDN <ref type="bibr" target="#b13">[14]</ref> and CARN <ref type="bibr" target="#b28">[29]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Super-resolution considering naturalness</head><p>SRGAN <ref type="bibr" target="#b18">[19]</ref>, as a landmark work in perceptual-driven SR, was proposed by Ledig et al. . This approach is the first attempt to apply GAN <ref type="bibr" target="#b23">[24]</ref> framework to SR, where the generator is composed of residual blocks. To improve the naturalness of the images, perceptual and adversarial losses were utilized to train the model in SRGAN. Sajjadi et al. <ref type="bibr" target="#b19">[20]</ref> explored the local texture matching loss and further improved the visual quality of the composite images. Park et al. <ref type="bibr" target="#b29">[30]</ref> developed a GAN-based SISR method that produced realistic results by attaching an additional discriminator that works in the feature domain. Mechrez et al. <ref type="bibr" target="#b20">[21]</ref> defined the Contextual loss that measured the similarity between the generated image and a target image by comparing the statistical distribution of the feature space. Wang et al. <ref type="bibr" target="#b21">[22]</ref> enhanced SRGAN from three key aspects: network architecture, adversarial loss, and perceptual loss. A variant of Enhanced SRGAN (ESRGAN) won the first place in the PIRM2018-SR Challenge <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The proposed PSNR-oriented SR model</head><p>The single image super-resolution aims to estimate the SR image I SR from its LR counterpart I LR . An overall structure of the proposed basic model (RFN) is shown in <ref type="figure">Figure 1</ref>. This network mainly consists of two parts: content feature extraction module (CFEM) and reconstruction part, where the first part extracts content features for conventional image SR task (pursuing high PSNR value), and the second part naturally reconstructs I SR through the front features related to the image content. The first procedure could be expressed by</p><formula xml:id="formula_0">F c = H CFE I LR ,<label>(1)</label></formula><p>where H CFE (Â·) denotes content feature extractor, i.e., CFEM. Then, F c is sent to the content reconstruction module (CRM) H CR ,</p><formula xml:id="formula_1">I SR c = H CR (F c ) = H RFN I LR ,<label>(2)</label></formula><p>where H RFN (Â·) denotes the function of our RFN. The basic model is optimized with the MAE loss function, followed by the previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. Given a training set</p><formula xml:id="formula_2">I LR i , I HR i N i=1</formula><p>, where N is the number of training images, I HR i is the ground-truth high-resolution image of the low-resolution image I LR i , the loss function of our basic SR model is</p><formula xml:id="formula_3">L content (Î˜ c ) = 1 N N i=1 H RFN I LR i âˆ’ I HR i 1 ,<label>(3)</label></formula><p>where Î˜ c denotes the parameter set of our content-oriented branch (COBranch), i.e., RFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Progressive perception-oriented SR model</head><p>As depicted in <ref type="figure" target="#fig_0">Figure 2</ref>, based on the content features extracted by the CFEM, we design a SFEM to distill structure-related information for restoring images with SRM. This process can be expressed by</p><formula xml:id="formula_4">I SR s = H SR (F s ) + I SR c = H SR (H SFE (F c )) + I SR c ,<label>(4)</label></formula><p>where H SR (Â·) and H SFE (Â·) denote the functions of SRM and SFEM, respectively. To this end, we employ the multi-scale structural similarity index (MS-SSIM) and multi-scale L 1 as loss functions to optimize this branch. SSIM is defined as</p><formula xml:id="formula_5">SSIM (x, y) = 2Âµ x Âµ y + C 1 Âµ 2 x + Âµ 2 y + C 1 Â· 2Ïƒ xy + C 2 Ïƒ 2 x + Ïƒ 2 y + C 2 = l (x, y) Â· cs (x, y) ,<label>(5)</label></formula><p>where Âµ x , Âµ y are the mean, Ïƒ xy is the covariance of x and y, and C 1 , C 2 are constants. Given multiple scales through a process of M stages of downsampling, MS-SSIM is defined as</p><formula xml:id="formula_6">MS-SSIM(x, y) = l Î± M (x, y) Â· M j=1 cs Î²j j (x, y),<label>(6)</label></formula><p>where l M and cs j are the term we defined in Equation 5 at scale M and j, respectively. From <ref type="bibr" target="#b31">[32]</ref>, we set Î± = Î² M and Î² = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]. Therefore, the total loss function of our structure branch can be expressed by</p><formula xml:id="formula_7">L MS-SSIM = 1 N N i=1 1 âˆ’ MS-SSIM I HR i , H SOB F i c ,<label>(7)</label></formula><p>where H SOB (Â·) represents the cascade of SFEM and SRM (light red area in <ref type="figure">Figure 5</ref>). F i c denotes content features (see Equation 1) corresponding to i-th training sample in a batch. Thus, the total loss function of this branch can be formulated as follows</p><formula xml:id="formula_8">L structure (Î˜ s ) = L MS-L1 + Î»L MS-SSIM ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">L MS-L1 = M j=1</formula><p>Ï‰ j Â· l mae (x j , y j ) and Î» is a scalar value to balance two losses, Î˜ s denotes the parameter set of structure-oriented branch (SOBranch).</p><p>Here, we set M = 5, Ï‰ 1,2,...,5 = [1, 0.5, 0.25, 0.125, 0.125] through experience. Similarly, to obtain photorealistic images, we utilize structural-related features refined by SFEM and send them to our perception feature extraction module (PFEM). The merit of this practice is to avoid re-extracting features from the image domain. These extracted features contain abundant and superior quality structural information, which tremendously helps perceptual-oriented branch (POBranch, see in <ref type="figure">Figure 5</ref>) generate visually plausible SR images while maintaining the basic structure. Concretely, structural feature F s is entered in PFEM</p><formula xml:id="formula_10">I SR p = H PR (F p ) + I SR s = H PR (H PFE (F s )) + I SR s ,<label>(9)</label></formula><p>where H PR (Â·) and H PFE (Â·) indicate PRM and PFEM as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, respectively. For pursuing better visual effect, we adopt Relativistic GAN <ref type="bibr" target="#b32">[33]</ref> as in <ref type="bibr" target="#b21">[22]</ref>. Given a real image x r and a fake one x f , the relativistic discriminator intends to estimate the probability that x r is more realistic than x f . In standard GAN, the discriminator can be defined, in term of the non-transformed layer C (x), as D (x) = Ïƒ (C (x)), where Ïƒ is sigmoid function. The Relativistic average Discriminator (RaD, denoted by D Ra ) <ref type="bibr" target="#b32">[33]</ref> can be formulated as</p><formula xml:id="formula_11">D Ra (x r , x f ) = Ïƒ C (x) âˆ’ E x f [C (x f )] , if x is real. Here, E x f [C (Â·)]</formula><p>is the average of all fake data in a batch. The discriminator loss is defined by</p><formula xml:id="formula_12">L Ra D = âˆ’E xr [log (D Ra (x r , x f ))] âˆ’ E x f [log (1 âˆ’ D Ra (x f , x r ))] .<label>(10)</label></formula><p>The corresponding adversarial loss for generator is</p><formula xml:id="formula_13">L Ra G = âˆ’E xr [log (1 âˆ’ D Ra (x r , x f ))] âˆ’ E x f [log (D Ra (x f , x r ))] .<label>(11)</label></formula><p>where x f represents the generated images at the current perception-maximization stage, i.e., I SR p in equation 9. VGG loss that has been investigated in recent SR works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> for better visual quality is also introduced in this stage. We calculate the VGG loss based on the "conv5 4" layer of VGG19 <ref type="bibr" target="#b22">[23]</ref>,</p><formula xml:id="formula_14">L vgg = 1 V C i=1 Ï† i I HR âˆ’ Ï† i I SR p 1 ,<label>(12)</label></formula><p>where V and C indicate the tensor volume and channel number of the feature maps, respectively, and Ï† i denotes the i-th channel of the feature maps extracted from the hidden layer of VGG19 model. Therefore, the total loss for the perception stage is:</p><formula xml:id="formula_15">L perception (Î˜ p ) = L vgg + Î·L Ra G ,<label>(13)</label></formula><p>where Î· is the coefficients to balance these loss functions. And Î˜ p is the training parameters of POBranch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Residual-in-residual fusion block</head><p>We now give more details about our proposed RRFB structure (see <ref type="figure" target="#fig_1">Figure 3(b)</ref>), which consists of multiple hierarchical feature fusion blocks (HFFB) (see <ref type="figure" target="#fig_1">Figure 3</ref>(b)). Unlike the frequently-used residual block in SR, we intensify its representational ability by introducing the spatial pyramid of dilated convolutions <ref type="bibr" target="#b25">[26]</ref>. Specifically, we develop K n Ã— n dilated convolutional kernels simultaneously, each with a dilation rate of k, k = {1, . . . , K}. Due to these dilated convolutions preserve different receptive fields, we can aggregate them to obtain multi-scale features. As shown in <ref type="figure">Figure 4</ref>, single dilated convolution with a dilation rate of 3 (yellow block) looks sparse. The feature maps obtained using kernels of different dilation rates are hierarchically added to acquire an effective receptive field before concatenating them. A simple example is illustrated in <ref type="figure">Figure 4</ref>. For explaining this hierarchical feature fusion process clearly, the output of dilated convolution with a dilation rate of k is denoted by f k . In this way, concatenated multi-scale features H ms can be expressed by</p><formula xml:id="formula_16">H ms = [f 1 , f 1 + f 2 , . . . , f 1 + f 2 + Â· Â· Â· + f K ] .<label>(14)</label></formula><p>After collecting these multi-scale features, we fuse them through a 1 Ã— 1 convolution Conv 1Ã—1 , that is Conv 1Ã—1 (LReLU (F ms )). Finally, the local skip connection with residual scaling is utilized to complete our HFFB. </p><formula xml:id="formula_17">1 3 3 ï€½ ï‚´ r conv 1 2 3 3 3 3 ï€½ ï€½ ï‚´ ï‚´ ï€« r r conv conv 1 2 3 3 3 3 3 3 3 ï€½ ï€½ ï€½ ï‚´ ï‚´ ï‚´ ï€« ï€« r r r conv conv conv</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Training Details</head><p>We use the DIV2K dataset <ref type="bibr" target="#b26">[27]</ref>, which consists of 1,000 high-quality RGB images (800 training images, 100 validation images, and 100 test images) with 2K resolution. For increasing the diversity of training images, we also use the Flickr2K dataset <ref type="bibr" target="#b10">[11]</ref> consisting of 2,650 2K resolution images. In this way, we have 3,450 high-resolution images for training purposes. LR training images are obtained by downscaling HR with a scaling factor of 4Ã— images using bicubic interpolation function in MATLAB. HR image patches with a size of 192 Ã— 192 are randomly cropped from HR images as the input of our proposed model, and the mini-batch size is set to 25. Data augmentation is performed on the 3,450 training images, which are randomly horizontal flip and 90-degree rotation. For evaluation, we use six widely used benchmark datasets: Set5 <ref type="bibr" target="#b33">[34]</ref>, Set14 <ref type="bibr" target="#b34">[35]</ref>, BSD100 <ref type="bibr" target="#b35">[36]</ref>, Urban100 <ref type="bibr" target="#b36">[37]</ref>, Manga109 <ref type="bibr" target="#b37">[38]</ref>, and the PIRM dataset <ref type="bibr" target="#b30">[31]</ref>. The SR results are evaluated with PSNR, SSIM <ref type="bibr" target="#b16">[17]</ref>, learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b38">[39]</ref>, and perceptual index (PI) on Y (luminance) channel, in which PI is based on the non-reference image quality measures of Ma et al. <ref type="bibr" target="#b39">[40]</ref> and NIQE <ref type="bibr" target="#b40">[41]</ref>, i.e., PI = 1 2 ((10 âˆ’ Ma) + NIQE). The lower values of LPIPS and PI, the better.</p><p>As depicted in <ref type="figure">Figure 5</ref>, the training process is composed of three phases. First, we train the COBranch with Equation 3. The initial learning rate is set to 2 Ã— 10 âˆ’4 , which is decreased by 2 for every 1000 epochs (1.38 Ã— 10 5 iterations). And then, we fix the parameters of COBranch and only train the SOBranch through the loss function in Equation 8 with Î» = 1 Ã— 10 3 . This process is illustrated in the second row of <ref type="figure">Figure 5</ref>. During this stage, the learning rate is set to 1 Ã— 10 âˆ’4 and halved at every 250 epochs (3.45 Ã— 10 4 iterations). Similarly, we eventually only train the POBranch by Equation 13 with Î· = 5 Ã— 10 âˆ’3 . The learning rate scheme is the same as the second phase. All the stages are trained by ADAM optimizer <ref type="bibr" target="#b41">[42]</ref> with the momentum parameter Î² 1 = 0.9. We apply the PyTorch v1.1 framework to implement our model and train them using NVIDIA TITAN Xp GPUs.</p><p>We set the dilated convolutions number as K = 8 in the HFFB structure. All dilated convolutions have 3Ã—3 kernels and 32 filters, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a). In each RRFB, we set the HFFB number as 3. In COBranch, we apply 24 RRFBs. Moreover, only 2 RRFBs are employed in both SOBranch and POBranch. All standard convolutional layers have 64 filters, and their kernel sizes are set to 3Ã—3 expect for that at the end of HFFB, whose kernel size is 1Ã—1. The residual scaling parameter Î± = 0.2 and the negative slope of LReLU is set as 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model analysis</head><p>Model Parameters. We compare the trade-off between performance and model size in <ref type="figure" target="#fig_4">Figure 6</ref>. Among the nine models, RFN and RCAN show higher PSNR values than others. In particular, RFN scores the best performance in Set5. It should be pointed out that RFN uses fewer parameters than RCAN to achieve this performance. It does mean that RFN can better balance performance and model size.</p><p>Study of dilation convolution and hierarchical feature fusion. We remove the hierarchical feature fusion structure. Furthermore, in order to investigate the function of dilated convolution, we use ordinary convolutions. For validating quickly, only 1 RRFB is used in CFEM, and this network is called RFN mini. We conduct the training process with the DIV2K dataset, and the results are depicted in <ref type="table" target="#tab_2">Table 1</ref>. As the number of RRFB increases, the benefits will increase accumulatively (see in <ref type="table" target="#tab_3">Table 2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Progressive structure analysis</head><p>We observe that perceptual-driven SR results produced by GAN-based approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> often suffer from structural distortion, as illustrated in <ref type="figure">Figure 9</ref>. To alleviate this problem, we explicitly add structural information through our devised progressive architecture described in the main manuscript.  To make it easier to understand this progressive practice, we show an example in <ref type="figure" target="#fig_7">Figure 10</ref>. From this picture, we can note that the difference between SRc and SRp is mainly reflected in the sharper texture of SRp. Therefore, the remaining component is substantially the same. Please take into account this viewpoint, we naturally design the progressive topology structure, i.e., gradually adding high-frequency details.</p><p>To validate the feature maps extracted by the CFEM, SFEM, and PFEM have dependencies and relationships, we visualize the intermediate feature maps, as shown in <ref type="figure" target="#fig_6">Figure 8</ref>. From this picture, we can find that the feature maps distilled by three different extraction modules are similar. Thus, features extracted in the previous stage can be utilized in the current phase. In addition, feature maps in the third sub-figure contain more texture information, which is instructive to the reconstruction of visually high-quality images. To verify the necessity of using progressive structure, we remove CRM and SOBranch from PPON (i.e., changing to normal structure, similar to ESRGAN <ref type="bibr" target="#b21">[22]</ref>). We observe that PPON without CRM &amp; SOBranch cannot generate clear structural information, while PPON can better recover it. <ref type="table" target="#tab_4">Table 3</ref> suggests that our progressive structure can significantly improve the fidelity measured by PSNR and SSIM while improving perceptual quality. It indicates that fewer updatable parameters not    <ref type="table" target="#tab_4">Table 3</ref> with <ref type="table" target="#tab_5">Table 4</ref>, we observe model with GAN ("w/o CRM &amp; SOBranch") requires more memories and longer training time. However, the perceptual performance of the model with GAN dramatically boosts than RFN. It means GAN is necessary for our architecture. Few learnable model parameters (1.3M) complete task migration (i.e.from structure-aware to perceptual-aware) well in our work, while ESRGAN <ref type="bibr" target="#b21">[22]</ref> uses 16.7M to generate perceptual results. We explicitly decompose a task into three subtasks (content, structure, perception). This approach is similar to human painting, first sketching the lines, then adding details. Our topology structure can quickly achieve the migration of similar tasks and infer multiple HR SRGAN <ref type="bibr" target="#b18">[19]</ref> ENet <ref type="bibr" target="#b19">[20]</ref> CX <ref type="bibr" target="#b20">[21]</ref> SuperSR <ref type="bibr" target="#b21">[22]</ref> ESRGAN <ref type="bibr" target="#b21">[22]</ref> PPON(Ours) <ref type="figure">Figure 9</ref>: An example of the structure distortion. The image is from the BSD100 dataset <ref type="bibr" target="#b35">[36]</ref>. tasks according to the specific needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRc SRs SRp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Difference to the previous GAN-based methods</head><p>Unlike the previous perceptual SR methods ( e.g., SRGAN <ref type="bibr" target="#b18">[19]</ref>, EnhanceNet <ref type="bibr" target="#b19">[20]</ref>, CX <ref type="bibr" target="#b20">[21]</ref>, and ESRGAN <ref type="bibr" target="#b21">[22]</ref>), we employ the progressive strategy to gradually recover the fine-grained high-frequency details without sacrificing the structural information. As shown in <ref type="figure">Figure 11</ref>, we can obtain images with different perceptions by setting different values to Î±. Now, Equation 9 can be modified as follows:</p><formula xml:id="formula_18">I SR p = Î± Â· H PR (F p ) + I SR s = Î± Â· H PR (H PFE (F s )) + I SR s .<label>(15)</label></formula><p>We provide an example (see in <ref type="figure" target="#fig_0">Figure 12</ref>) to demonstrate the effectiveness of this user-controlled adjustment of SR results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparisons with state-of-the-art methods</head><p>We compare our RFN with 16 state-of-the-art methods: SRCNN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, FSR-CNN <ref type="bibr" target="#b4">[5]</ref>, VDSR <ref type="bibr" target="#b5">[6]</ref>, DRCN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b9">[10]</ref>, MemNet <ref type="bibr" target="#b11">[12]</ref>, IDN <ref type="bibr" target="#b13">[14]</ref>, EDSR <ref type="bibr" target="#b10">[11]</ref>,    SRMDNF <ref type="bibr" target="#b42">[43]</ref>, D-DBPN <ref type="bibr" target="#b43">[44]</ref>, RDN <ref type="bibr" target="#b14">[15]</ref>, MSRN <ref type="bibr" target="#b44">[45]</ref>, CARN <ref type="bibr" target="#b28">[29]</ref>, RCAN <ref type="bibr" target="#b15">[16]</ref>, SAN <ref type="bibr" target="#b46">[47]</ref>, and SRFBN <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_7">Table 5</ref> shows quantitative comparisons for Ã—4 SR. It can be seen that our RFN performs the best in terms of PSNR on all the datasets. The proposed S-RFN shows significant advantages of SSIM.</p><p>In <ref type="figure" target="#fig_1">Figure 13</ref>, we present visual comparisons on different datasets. For image "img 011", we observe that most of the compared methods cannot recover the lines and suffer from blurred artifacts. In contrast, our RFN can slightly alleviate this phenomenon and restore more details. <ref type="table" target="#tab_8">Table 6</ref> shows our quantitative evaluation results compared with 6 perceptualdriven state-of-the-arts approaches: SRGAN <ref type="bibr" target="#b18">[19]</ref>, ENet <ref type="bibr" target="#b19">[20]</ref>, CX <ref type="bibr" target="#b20">[21]</ref>, EPSR <ref type="bibr" target="#b47">[48]</ref>, NatSR <ref type="bibr" target="#b48">[49]</ref>, and ESRGAN <ref type="bibr" target="#b21">[22]</ref>. The proposed PPON achieves the best in terms of LPIPS and keep the presentable PSNR values. For image "86" in Figures 14, the result generated by S-RFN is blurred but has a elegant structure. Based on S-RFN, our PPON can synthesize realistic textures while retaining a delicate structure. It also validates the effectiveness of the proposed progressive architecture. We further apply our PPON to solve the noise image super-resolution. AWGN noises (noise level is set to 10) are added to clean low-resolution images. Quantitative results are shown in <ref type="table" target="#tab_9">Table 7</ref>. It is noted that we only fine-tune the COBranch by noise training images and maintain the SOBranch and POBranch. In this way, the produced structure-aware and perceptual-aware results are still steady as we can see that our RFN achieves the best PSNR performance, and S-RFN achieves the best SSIM performance, which is consistent with the results in <ref type="table" target="#tab_7">Table 5</ref>. Even if SOBranch does not retrain by noise-clean images pairs, S-RFN still obtains higher SSIM scores than RFN. It also suggests that the separability of PPON. We also show visual results in <ref type="figure" target="#fig_11">Figure 15</ref>. Obviously, RFN and S-RFN can generate sharper edges ("42049" from BSD100 and "img 032" from Urban100), and PPON can hallucinate some plausible details.</p><p>We further apply our PPON to upscale LR images with compression artifacts. Due to the previous image compression artifacts methods focusing on the     restoration of the Y channel (in YCbCr space), we only show our visual results in <ref type="figure" target="#fig_4">Figure 16</ref> (RGB JPEG compression artifacts reduction and super-resolution). From <ref type="figure" target="#fig_4">Figure 16</ref>, we can observe that our method can process the low-quality input well (clean edge, clean background). To probe into the influence of resolu- tion of the input images with JPEG compression, we feed JPEG compressed LR images with different spatial resolutions into our PPON, and then we explore memory occupation and inference on seven datasets (see in <ref type="table" target="#tab_11">Table 8</ref>). If the input resolution increased to twice, the memory and time consumption increased to less than 4 times. It suggests our model can run on large resolution image well, considering memory and speed.</p><p>In <ref type="figure" target="#fig_5">Figure 17</ref>, two qualitative results are showed to verify that the highresolution input image does gain better super-resolved images. For example, "img 091" with the spatial resolution 170 Ã— 256 is low quality, the generated images from RFN and S-RFN are similar, and PPON produces an image that is slightly better effect. When the input resolution is increasing to 340 Ã— 512, three results (RFN, S-RFN, and PPON) are of high quality. It demonstrates that our model can handle low-resolution images and high-resolution images: better quality input and better quality output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">The choice of main evaluation metric</head><p>We consider LPIPS 1 <ref type="bibr" target="#b38">[39]</ref> and PI 2 <ref type="bibr" target="#b30">[31]</ref> as our evaluation indices of perceptual image SR. As illustrated in <ref type="figure" target="#fig_6">Figure 18</ref>, we can see that the PI score of EPSR3 (2.2666) is even better than HR (2.3885), but EPSR3 shows unnatural and lacks proper texture and structure. When observing the results of ESRGAN and our PPON, their perception effect is superior to that of EPSR3, precisely in accordance with corresponding LPIPS values. From the results of S-RFN  and PPON, it can be demonstrated that both PI and LPIPS have the ability to distinguish a blurring image. From the images of EPSR3, SuperSR, and groundtruth (HR), we can distinctly know that the lower PI value does not mean better image quality. Compared with the image generated by ESRGAN <ref type="bibr" target="#b21">[22]</ref>, it is evident that the proposed PPON gets a better visual effect with more structure information, corresponding to the lower LPIPS value. Because the PI (nonreference measure) is not sensitive to deformation through the experiment and cannot reflect the similarity with ground-truth, we take LPIPS as our primary perceptual measure and PI as a secondary metric.</p><p>Besides, we performed a MOS (mean opinion score) test to validate the effectiveness of our PPON further. Specifically, we collect 16 raters to assign an integral score from 1 (bad quality) to 5 (excellent quality). To ensure the reliability of the results, we provide the raters with tests and original HR images simultaneously. The ground-truth images are set to 5, and the raters then score the test images based on it. The average MOS results are shown in <ref type="table" target="#tab_13">Table 9</ref>.  In ESRGAN <ref type="bibr" target="#b21">[22]</ref>, the authors mentioned that larger training patch sizes cost more training time and consume more computing resources. Thus, they used 192 Ã— 192 for PSNR-oriented methods and 128 Ã— 128 for perceptual-driven methods. In our main manuscript, we train the COBranch, SOBranch, and POBranch with 192 Ã— 192 image patches. Here, we further explore the influence of larger patches in the perceptual image generation stage.</p><p>It is important to note that the training perceptual-driven model requires more GPU memory and ore considerable computing resources than the PSNRoriented model since the VGG model and discriminator need to be loaded during the training of the former. Therefore, larger patches <ref type="bibr">(192 Ã— 192)</ref> are hard to be used in optimizing ESRGAN <ref type="bibr" target="#b21">[22]</ref> due to their large generator and discriminator to be updated. Thanks to our POBranch containing very few parameters, we employ 192 Ã— 192 training patches and achieve better results, as shown in <ref type="table" target="#tab_2">Table 10</ref>. Concerning the discriminators, we illustrate them in <ref type="figure">Figure 19</ref>. For a fair comparison with the ESRGAN <ref type="bibr" target="#b21">[22]</ref>, we retrain our POBranch with 128Ã—128 patches and provide the results in <ref type="table" target="#tab_2">Table 10</ref>.  <ref type="figure">Figure 19</ref>: The network structure of the discriminators. The output size is scaled down by stride 2, and the parameter of LReLU is 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a progressive perception-oriented network (PPON) for better perceptual image SR. Concretely, three branches are developed to learn the content, structure, and perceptual details, respectively. By exerting a stage-by-stage training scheme, we can steadily get promising results. It is worth mentioning that these three branches are not independent. A structureoriented branch can exploit the extracted features and output images of the content-oriented branch. Extensive experiments on both traditional SR and perceptual SR demonstrate the effectiveness of our proposed PPON.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of our progressive perception-oriented network (PPON). CFEM indicates content feature extraction module inFigure 1. CRM, SRM, and PRM represent content reconstruction module, structural reconstruction module, and photo-realism reconstruction module, respectively. SFEM denotes structural features extraction module and PFEM describes the perceptual features extraction part. In addition, âŠ• is the element-wise summation operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The basic blocks are proposed in this work. (a) We employ 8 dilated convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>were proposed by Hui et al. and Ahn et al. , respectively. More concretely, Hui et al. constructed a deep but compact network, which mainly exploited and fused different types of features. And Ahn et al. designed a cascading network architecture. The main idea is to add multiple cascading connections from each intermediary layer to others. Such connections help this model performing SISR accurately and efficiently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The diagrammatic sketch of multiple dilated convolutions addition. Taking the middle sub-figure as an example, conv r=2 3Ã—3 indicates 3 Ã— 3 dilated convolution with dilation rate of 2. Under the same conditions of receptive field, conv r=1 3Ã—3 + conv r=2 3Ã—3 is more dense than conv r=2 3Ã—3 . The training scheme for our PPON. The light green region (COBranch) in the first row is our basic model RFN. Light red and yellow areas represent SOBranch and POBranch mentioned in Section 3.2, respectively. The entire training process is split into 3 stages. The module with miniature lock means to freeze its parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>PSNR performance and number of parameters. The results are evaluated on Set5 dataset for a scaling factor of 4Ã—.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study of progressive structure. (a) w/o CRM &amp; SOBranch. (b) w/o SOBranch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The feature maps of CFEM, SFEM, and PFEM are visualized from left to right. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>A comparison of the visual effects between the three branch outputs. SRc, SRs, and SRp are outputs of the COBranch, SOBranch, and POBranch, respectively. The image is from the PIRM Val dataset<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>The inference architecture of our progressive perception-oriented network (PPON). The perception-distortion trade-off. In the first column, Î± = 0.0 directly denotes the outputs of SOBranch. Equally, Î± = 1.0 indicates the results (without any discount) of POBranch. Best viewed with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Visual comparisons for 4Ã— SR with RFN on Urban100 and Manga109 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Qualitative comparisons of perceptual-driven SR methods with our results at scaling factor of 4. Here, SuperSR is the variant of ESRGAN and it won the first place in the PIRM2018-SR Challenge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>Noise image super-resolution results with noise level Ïƒ = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 :</head><label>16</label><figDesc>JPEG compressed image super-resolution results with JPEG quality q = 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>A visual comparison with the state-of-the-art perceptual image SR algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>CRM CFEM COBranch SOBranch CFEM CRM SFEM SRM PFEM PRM SFEM SRM CRM CFEM POBranchCRM CFEM COBranch SOBranch CFEM CRM SFEM SRM PFEM PRM SFEM SRM CRM CFEM POBranch</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRp</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">Content Feature Extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Content Reconstruction</cell><cell>SRc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Module (CFEM)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Module (CRM)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LR</cell><cell></cell><cell></cell><cell cols="2">Conv</cell><cell></cell><cell cols="3">RRFB</cell><cell></cell><cell>RRFB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RRFB</cell><cell></cell><cell></cell><cell>Conv</cell><cell>Upsampler</cell><cell>Conv</cell><cell>Conv</cell><cell>SR</cell><cell>LR</cell><cell>CFEM</cell><cell>CRM</cell><cell>RRFB</cell><cell>RRFB SRM</cell><cell>RRFB</cell><cell>RRFB PRM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SFEM</cell><cell>PFEM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="30">Figure 1: The network architecture of our basic PSNR-oriented model (Residual Fusion Net-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="30">work, namely RFN). We use 24 RRFBs for our experiments.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64</cell><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell cols="2">128</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SR HR</cell><cell></cell><cell cols="2">3Ã—3 Conv</cell><cell cols="2">Leaky ReLU</cell><cell></cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell cols="2">3Ã—3 Conv</cell><cell cols="2">Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell cols="2">4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell>LR</cell><cell></cell><cell>Conv</cell><cell cols="2">RRFB</cell><cell>RRFB</cell><cell></cell><cell></cell><cell></cell><cell>RRFB</cell><cell></cell><cell>Conv</cell><cell></cell><cell cols="2">Upsampler</cell><cell cols="2">Conv</cell><cell></cell><cell>Conv</cell><cell cols="2">SR</cell><cell></cell><cell cols="2">64</cell><cell></cell><cell></cell><cell cols="2">64</cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2 128</cell><cell></cell><cell cols="4">128</cell><cell></cell><cell></cell><cell cols="2">s2 256</cell><cell>256</cell><cell>s2 512</cell><cell>512</cell><cell>s2 512</cell><cell>512</cell><cell>s2 512</cell><cell>512</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SR HR</cell><cell cols="2">3Ã—3 Conv</cell><cell cols="2">Leaky ReLU</cell><cell cols="2">4Ã—4 Conv</cell><cell cols="2">Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell cols="2">4Ã—4 Conv</cell><cell cols="2">Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell>s2</cell><cell>s2</cell><cell>s2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SR HR</cell><cell>3Ã—3 Conv</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell></cell><cell>4Ã—4 Conv</cell><cell cols="2">Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell cols="2">Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell></cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64</cell><cell></cell><cell>64</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell cols="3">512</cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="3">512</cell><cell></cell><cell></cell><cell></cell><cell cols="2">512</cell></row><row><cell>SR HR</cell><cell>3Ã—3 Conv</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell cols="2">Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell cols="2">3Ã—3 Conv</cell><cell cols="2">Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell cols="2">3Ã—3 Conv</cell><cell cols="2">Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Investigations of dilated convolution and hierarchical fusion. These models are trained 200k iterations with DIV2K training dataset.</figDesc><table><row><cell>Dilated convolution Hierarchical fusion</cell><cell>% % ! ! % ! % !</cell></row><row><cell cols="2">PSNR on Set5 (4Ã—) 31.68 31.69 31.63 31.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Investigations of dilated convolution. Above models are trained 300k iterations with DIV2K training dataset.</figDesc><table><row><cell>Method</cell><cell>N blocks</cell><cell>Set5</cell><cell cols="3">Set14 BSD100 Urban100</cell></row><row><cell>w/o dilation</cell><cell>2</cell><cell cols="2">32.05 28.51</cell><cell>27.52</cell><cell>25.91</cell></row><row><cell>RFN Mini</cell><cell>2</cell><cell cols="2">32.07 28.53</cell><cell>27.53</cell><cell>25.91</cell></row><row><cell>w/o dilation</cell><cell>4</cell><cell cols="2">32.18 28.63</cell><cell>27.59</cell><cell>26.16</cell></row><row><cell>RFN Mini</cell><cell>4</cell><cell cols="2">32.26 28.67</cell><cell>27.60</cell><cell>26.23</cell></row><row><cell>PIRM Val: 71</cell><cell>HR</cell><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>PPON</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of progressive structure (with GAN). PSNR, SSIM, and PI are evaluated on the Y channel while LPIPS are conducted on the RGB color space. PSNR / SSIM / LPIPS / PI) 25.61 / 0.6802 / 0.1287 / 2.2857 26.32 / 0.6981 / 0.1250 / 2.2282 26.20 / 0.6995 / 0.1194 / 2.2353 PIRM Test (PSNR / SSIM/ LPIPS / PI) 25.47 / 0.6667 / 0.1367 / 2.2055 26.16 / 0.6831 / 0.1309 / 2.1704 26.01 / 0.6831 / 0.1273 / 2.1511</figDesc><table><row><cell>Item</cell><cell>w/o CRM &amp; SOBranch</cell><cell>w/o SOBranch</cell><cell>PPON</cell></row><row><cell>Memory footprint (M)</cell><cell>11,599</cell><cell>5,373</cell><cell>5,357</cell></row><row><cell>Training time (sec/epoch)</cell><cell>347</cell><cell>176</cell><cell>183</cell></row><row><cell>PIRM Val (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of RFN and S-RFN (without GAN). All metrics are performed on the RGB color space.</figDesc><table><row><cell>Item</cell><cell>RFN</cell><cell>S-RFN</cell></row><row><cell>Memory footprint (M)</cell><cell>8,799</cell><cell>2,733</cell></row><row><cell>Training time (sec/epoch)</cell><cell>278</cell><cell>110</cell></row><row><cell>PIRM Val (PSNR / SSIM / LPIPS)</cell><cell cols="2">27.27 / 0.8961 / 0.2901 27.14 / 0.7741 / 0.2651</cell></row><row><cell cols="3">PIRM Test (PSNR / SSIM / LPIPS) 27.14 / 0.7571 / 0.3077 27.00 / 0.7637 / 0.2804</cell></row><row><cell>Set5 (PSNR / SSIM / LPIPS)</cell><cell cols="2">30.68 / 0.8714 / 0.1709 30.62 / 0.8737 / 0.1684</cell></row><row><cell>Set14 (PSNR / SSIM / LPIPS)</cell><cell cols="2">26.88 / 0.7543 / 0.2748 26.76 / 0.7595 / 0.2583</cell></row><row><cell>B100 (PSNR / SSIM / LPIPS)</cell><cell cols="2">26.52 / 0.7225 / 0.3620 26.40 / 0.7302 / 0.3377</cell></row><row><cell>Urban100 (PSNR / SSIM / LPIPS)</cell><cell cols="2">25.46 / 0.7940 / 0.1982 25.39 / 0.7982 / 0.1879</cell></row><row><cell>Manga109 (PSNR / SSIM / LPIPS)</cell><cell cols="2">29.71 / 0.8945 / 0.0984 29.62 / 0.8961 / 0.0939</cell></row><row><cell cols="3">only occupy less memory but also encourage faster training. "w/o CRM &amp; SO-</cell></row><row><cell cols="3">Branch" is a fundamental architecture without proposed progressive structure,</cell></row><row><cell cols="3">which consumes 11,599M memories. Once we turn to "w/o SOBranch", the</cell></row><row><cell cols="3">consumption of memory is reduced by 53.67%, and the training speed increased</cell></row><row><cell cols="3">by 97.16%. Thus, our progressive structure is useful when training model with</cell></row><row><cell cols="3">GAN. Comparing "w/o SOBranch" with PPON (LPIPS values), it naturally</cell></row><row><cell cols="3">demonstrated that SOBranch is beneficial to improve perceptual performance.</cell></row><row><cell cols="3">From Table 4, it can suggest that S-RFN occupies fewer memory footprints and</cell></row><row><cell cols="3">obtains faster training speed than RFN. Besides, the perceptual performance</cell></row><row><cell cols="3">(measured by LPIPS) of S-RFN is significantly improving than RFN evaluated</cell></row><row><cell>on seven test datasets. Combining</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>CRM CFEM COBranch SOBranch CFEM CRM SFEM SRM PFEM PRM SFEM SRM CRM CFEM POBranch</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="5">Content Feature Extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Content Reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Module (CFEM)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Module (CRM)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR</cell><cell></cell><cell>Conv</cell><cell cols="2">RRFB</cell><cell>RRFB</cell><cell></cell><cell></cell><cell></cell><cell>RRFB</cell><cell></cell><cell>Conv</cell><cell></cell><cell cols="2">Upsampler</cell><cell cols="2">Conv</cell><cell></cell><cell>Conv</cell><cell cols="2">SR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>64</cell><cell></cell><cell>64</cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell>256</cell><cell></cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SR HR</cell><cell>3Ã—3 Conv</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell></cell><cell cols="2">Real or Fake</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>64</cell><cell></cell><cell>64</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SR HR</cell><cell>3Ã—3 Conv</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell></cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation results in terms of PSNR and SSIM. Red and blue colors indicates the best and second best methods, respectively. Here, S-RFN is the combination of RFN and SOBranch. .9007 28.95 0.7901 27.83 0.7449 27.01 0.8135 31.59 0.9199 S-RFN(Ours) 32.66 0.9022 28.86 0.7946 27.74 0.7515 26.95 0.8169 31.51 0.9211</figDesc><table><row><cell>Method</cell><cell cols="2">Set5 PSNR SSIM</cell><cell cols="2">Set14 PSNR SSIM</cell><cell cols="2">B100 PSNR SSIM</cell><cell cols="2">Urban100 PSNR SSIM</cell><cell cols="2">Manga109 PSNR SSIM</cell></row><row><cell>Bicubic</cell><cell>28.42</cell><cell>0.8104</cell><cell>26.00</cell><cell>0.7027</cell><cell>25.96</cell><cell>0.6675</cell><cell>23.14</cell><cell>0.6577</cell><cell>24.89</cell><cell>0.7866</cell></row><row><cell>SRCNN [1]</cell><cell>30.48</cell><cell>0.8628</cell><cell>27.50</cell><cell>0.7513</cell><cell>26.90</cell><cell>0.7101</cell><cell>24.52</cell><cell>0.7221</cell><cell>27.58</cell><cell>0.8555</cell></row><row><cell>FSRCNN [5]</cell><cell>30.72</cell><cell>0.8660</cell><cell>27.61</cell><cell>0.7550</cell><cell>26.98</cell><cell>0.7150</cell><cell>24.62</cell><cell>0.7280</cell><cell>27.90</cell><cell>0.8610</cell></row><row><cell>VDSR [6]</cell><cell>31.35</cell><cell>0.8838</cell><cell>28.01</cell><cell>0.7674</cell><cell>27.29</cell><cell>0.7251</cell><cell>25.18</cell><cell>0.7524</cell><cell>28.87</cell><cell>0.8865</cell></row><row><cell>DRCN [7]</cell><cell>31.53</cell><cell>0.8854</cell><cell>28.02</cell><cell>0.7670</cell><cell>27.23</cell><cell>0.7233</cell><cell>25.14</cell><cell>0.7510</cell><cell>28.93</cell><cell>0.8854</cell></row><row><cell>LapSRN [10]</cell><cell>31.54</cell><cell>0.8852</cell><cell>28.09</cell><cell>0.7700</cell><cell>27.32</cell><cell>0.7275</cell><cell>25.21</cell><cell>0.7562</cell><cell>29.02</cell><cell>0.8900</cell></row><row><cell>MemNet [12]</cell><cell>31.74</cell><cell>0.8893</cell><cell>28.26</cell><cell>0.7723</cell><cell>27.40</cell><cell>0.7281</cell><cell>25.50</cell><cell>0.7630</cell><cell>29.42</cell><cell>0.8942</cell></row><row><cell>IDN [14]</cell><cell>31.82</cell><cell>0.8903</cell><cell>28.25</cell><cell>0.7730</cell><cell>27.41</cell><cell>0.7297</cell><cell>25.41</cell><cell>0.7632</cell><cell>29.41</cell><cell>0.8936</cell></row><row><cell>EDSR [11]</cell><cell>32.46</cell><cell>0.8968</cell><cell>28.80</cell><cell>0.7876</cell><cell>27.71</cell><cell>0.7420</cell><cell>26.64</cell><cell>0.8033</cell><cell>31.02</cell><cell>0.9148</cell></row><row><cell cols="2">SRMDNF [43] 31.96</cell><cell>0.8925</cell><cell>28.35</cell><cell>0.7772</cell><cell>27.49</cell><cell>0.7337</cell><cell>25.68</cell><cell>0.7731</cell><cell>30.09</cell><cell>0.9024</cell></row><row><cell>D-DBPN [44]</cell><cell>32.47</cell><cell>0.8980</cell><cell>28.82</cell><cell>0.7860</cell><cell>27.72</cell><cell>0.7400</cell><cell>26.38</cell><cell>0.7946</cell><cell>30.91</cell><cell>0.9137</cell></row><row><cell>RDN [15]</cell><cell>32.47</cell><cell>0.8990</cell><cell>28.81</cell><cell>0.7871</cell><cell>27.72</cell><cell>0.7419</cell><cell>26.61</cell><cell>0.8028</cell><cell>31.00</cell><cell>0.9151</cell></row><row><cell>MSRN [45]</cell><cell>32.07</cell><cell>0.8903</cell><cell>28.60</cell><cell>0.7751</cell><cell>27.52</cell><cell>0.7273</cell><cell>26.04</cell><cell>0.7896</cell><cell>30.17</cell><cell>0.9034</cell></row><row><cell>CARN [29]</cell><cell>32.13</cell><cell>0.8937</cell><cell>28.60</cell><cell>0.7806</cell><cell>27.58</cell><cell>0.7349</cell><cell>26.07</cell><cell>0.7837</cell><cell>30.47</cell><cell>0.9084</cell></row><row><cell>RCAN [16]</cell><cell>32.63</cell><cell>0.9002</cell><cell>28.87</cell><cell>0.7889</cell><cell>27.77</cell><cell>0.7436</cell><cell>26.82</cell><cell>0.8087</cell><cell>31.22</cell><cell>0.9173</cell></row><row><cell>SRFBN [46]</cell><cell>32.47</cell><cell>0.8983</cell><cell>28.81</cell><cell>0.7868</cell><cell>27.72</cell><cell>0.7409</cell><cell>26.60</cell><cell>0.8015</cell><cell>31.15</cell><cell>0.9160</cell></row><row><cell>SAN [47]</cell><cell>32.64</cell><cell>0.9003</cell><cell>28.92</cell><cell>0.7888</cell><cell>27.78</cell><cell>0.7436</cell><cell>26.79</cell><cell>0.8068</cell><cell>31.18</cell><cell>0.9169</cell></row><row><cell>RFN(Ours)</cell><cell cols="2">32.71 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on public benchmark datasets, PIRM Val, and PIRM Test for existing perceptual quality specific methods and our proposed PPON (Î± = 1.0). Red color indicates the best performance and blue color indicates the second best performance.</figDesc><table><row><cell>Dataset</cell><cell cols="9">Scores SRGAN [19] ENet [20] CX [21] EPSR2 [48] EPSR3 [48] NatSR [49] ESRGAN [22] PPON (Ours)</cell></row><row><cell></cell><cell>PSNR</cell><cell>29.43</cell><cell>28.57</cell><cell>29.12</cell><cell>31.24</cell><cell>29.59</cell><cell>31.00</cell><cell>30.47</cell><cell>30.84</cell></row><row><cell>Set5</cell><cell>SSIM PI</cell><cell>0.8356 3.3554</cell><cell>0.8103 2.9261</cell><cell>0.8323 3.2947</cell><cell>0.8650 4.1123</cell><cell>0.8415 3.2571</cell><cell>0.8617 4.1875</cell><cell>0.8518 3.7550</cell><cell>0.8561 3.4590</cell></row><row><cell></cell><cell>LPIPS</cell><cell>0.0837</cell><cell>0.1014</cell><cell>0.0806</cell><cell>0.0978</cell><cell>0.0889</cell><cell>0.0943</cell><cell>0.0748</cell><cell>0.0664</cell></row><row><cell></cell><cell>PSNR</cell><cell>26.12</cell><cell>25.77</cell><cell>26.06</cell><cell>27.77</cell><cell>26.36</cell><cell>27.53</cell><cell>26.28</cell><cell>26.97</cell></row><row><cell>Set14</cell><cell>SSIM PI</cell><cell>0.6958 2.8816</cell><cell>0.6782 3.0176</cell><cell>0.7001 2.7590</cell><cell>0.7440 3.0246</cell><cell>0.7097 2.6981</cell><cell>0.7356 3.1138</cell><cell>0.6984 2.9259</cell><cell>0.7194 2.7741</cell></row><row><cell></cell><cell>LPIPS</cell><cell>0.1488</cell><cell>0.1620</cell><cell>0.1452</cell><cell>0.1861</cell><cell>0.1576</cell><cell>0.1765</cell><cell>0.1329</cell><cell>0.1176</cell></row><row><cell></cell><cell>PSNR</cell><cell>25.18</cell><cell>24.94</cell><cell>24.59</cell><cell>26.28</cell><cell>25.19</cell><cell>26.45</cell><cell>25.32</cell><cell>25.74</cell></row><row><cell>B100</cell><cell>SSIM PI</cell><cell>0.6409 2.3513</cell><cell>0.6266 2.9078</cell><cell>0.6440 2.2501</cell><cell>0.6905 2.7458</cell><cell>0.6468 2.1990</cell><cell>0.6835 2.7746</cell><cell>0.6514 2.4789</cell><cell>0.6684 2.3775</cell></row><row><cell></cell><cell>LPIPS</cell><cell>0.1843</cell><cell>0.2013</cell><cell>0.1881</cell><cell>0.2474</cell><cell>0.2474</cell><cell>0.2115</cell><cell>0.1614</cell><cell>0.1597</cell></row><row><cell></cell><cell>PSNR</cell><cell>N/A</cell><cell>25.07</cell><cell>25.41</cell><cell>27.35</cell><cell>25.46</cell><cell>27.03</cell><cell>25.18</cell><cell>26.20</cell></row><row><cell>PIRM Val</cell><cell>SSIM PI</cell><cell>N/A N/A</cell><cell>0.6459 2.6876</cell><cell>0.6747 2.1310</cell><cell>0.7277 2.3880</cell><cell>0.6657 2.0688</cell><cell>0.7199 2.4758</cell><cell>0.6596 2.5550</cell><cell>0.6995 2.2353</cell></row><row><cell></cell><cell>LPIPS</cell><cell>N/A</cell><cell>0.1667</cell><cell>0.1447</cell><cell>0.1750</cell><cell>0.1869</cell><cell>0.1648</cell><cell>0.1443</cell><cell>0.1194</cell></row><row><cell></cell><cell>PSNR</cell><cell>N/A</cell><cell>24.95</cell><cell>25.31</cell><cell>27.04</cell><cell>25.35</cell><cell>26.95</cell><cell>25.04</cell><cell>26.01</cell></row><row><cell>PIRM Test</cell><cell>SSIM PI</cell><cell>N/A N/A</cell><cell>0.6306 2.7232</cell><cell>0.6636 2.1133</cell><cell>0.7068 2.2752</cell><cell>0.6535 2.0131</cell><cell>0.7090 2.3772</cell><cell>0.6454 2.4356</cell><cell>0.6831 2.1511</cell></row><row><cell></cell><cell>LPIPS</cell><cell>N/A</cell><cell>0.1776</cell><cell>0.1519</cell><cell>0.1739</cell><cell>0.1902</cell><cell>0.1712</cell><cell>0.1523</cell><cell>0.1273</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Quantitative results about noise image super-resolution. RNAN DN is the RGB image denoising version of RNAN. Similarly, RNAN SR is the RGB image super-resolution version of RNAN. Noise level Ïƒ = 10. The best results are highlighted.</figDesc><table><row><cell>Dataset</cell><cell>RNAN DN + RNAN SR [50] PSNR / SSIM</cell><cell>RFN PSNR / SSIM</cell><cell>S-RFN PSNR / SSIM</cell></row><row><cell>Set5 [34]</cell><cell>29.72 / 0.8693</cell><cell cols="2">30.17 / 0.8784 30.15 / 0.8790</cell></row><row><cell>Set14 [35]</cell><cell>27.30 / 0.7330</cell><cell cols="2">27.50 / 0.7395 27.48 / 0.7424</cell></row><row><cell>BSD100 [36]</cell><cell>26.49 / 0.6827</cell><cell cols="2">26.62 / 0.6877 26.60 / 0.6917</cell></row><row><cell>Urban100 [37]</cell><cell>24.88 / 0.7354</cell><cell cols="2">25.47 / 0.7581 25.45 / 0.7600</cell></row><row><cell>Manga109 [38]</cell><cell>28.41 / 0.8661</cell><cell cols="2">28.98 / 0.8802 28.96 / 0.8810</cell></row><row><cell>PIRM Val [31]</cell><cell>27.07 / 0.7154</cell><cell cols="2">27.20 / 0.7217 27.17 / 0.7253</cell></row><row><cell>PIRM Test [31]</cell><cell>27.04 / 0.7048</cell><cell cols="2">27.15 / 0.7103 27.13 / 0.7141</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Average resolution/time evaluated on seven datasets (JPEG LR Ã—4 SR).</figDesc><table><row><cell>Dataset</cell><cell cols="5">Input resolution (px, H Ã— W ) Memory (MB) Time (ms)</cell></row><row><cell>PIRM Test</cell><cell></cell><cell>121 Ã— 152 242 Ã— 305</cell><cell>1,171 4,087</cell><cell>206 745</cell><cell></cell></row><row><cell>PIRM Val</cell><cell></cell><cell>119 Ã— 155 239 Ã— 311</cell><cell>1,267 2,495</cell><cell>213 759</cell><cell></cell></row><row><cell>Set5</cell><cell></cell><cell>84 Ã— 72 168 Ã— 156</cell><cell>899 1,607</cell><cell>107 305</cell><cell></cell></row><row><cell>Set14</cell><cell></cell><cell>111 Ã— 122 222 Ã— 245</cell><cell>1399 2,089</cell><cell>163 577</cell><cell></cell></row><row><cell>B100</cell><cell></cell><cell>89 Ã— 111 178 Ã— 221</cell><cell>809 1,211</cell><cell>111 401</cell><cell></cell></row><row><cell>Urban100</cell><cell></cell><cell>199 Ã— 246 398 Ã— 492</cell><cell>2,047 6,583</cell><cell>501 2,028</cell><cell></cell></row><row><cell>Manga109</cell><cell></cell><cell>291 Ã— 205 584 Ã— 412</cell><cell>1,539 3,923</cell><cell>628 2,580</cell><cell></cell></row><row><cell>BSD100 (4Ã—): 42049</cell><cell>HR</cell><cell>Noisy (Ïƒ = 10) RNAN [50]</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row><row><cell>Urban100 (4Ã—): img 032</cell><cell>HR</cell><cell>Noisy (Ïƒ = 10) RNAN [50]</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>JPEG compressed image super-resolution results with JPEG quality q = 40 and different input resolutions. Here, two qualitative results from Urban100 and Manga109, respectively.</figDesc><table><row><cell>img 091: (170 Ã— 256)</cell><cell>LR (q = 40)</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row><row><cell>img 091: (340 Ã— 512)</cell><cell>LR (q = 40)</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row><row><cell>TennenSenshiG: (292 Ã— 206)</cell><cell>LR (q = 40)</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row><row><cell>TennenSenshiG: (585 Ã— 413)</cell><cell>LR (q = 40)</cell><cell>RFN</cell><cell>S-RFN</cell><cell>PPON</cell></row><row><cell>Figure 17: 296059 from BSD100</cell><cell cols="4">HR (âˆž / 0 / 2.3885) EPSR3 [48] (29.02 / 0.1911 /2.2666) (29.80 / 0.1703 /2.2913) (29.38 / 0.1333 / 2.3481) (31.40 / 0.3314 / 4.7222) (29.26 / 0.1305 / 2.5130) SRGAN [19] ENet [20] CX [21] EPSR2 [48] (28.96 / 0.1564 / 2.6015) (29.18 / 0.1432 / 2.8138) (28.57 / 0.1563 / 2.3492) (30.47 / 0.2046 /3.2575) SuperSR [22] ESRGAN [22] S-RFN (Ours) PPON (Ours)</cell></row><row><cell>(PSNR / LPIPS / PI)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison of CX, ESRGAN, S-RFN, and PPON.</figDesc><table><row><cell>PIRM Val</cell><cell>CX</cell><cell cols="3">ESRGAN S-RFN(Ours) PPON(Ours)</cell></row><row><cell>MOS</cell><cell>2.42</cell><cell>3.23</cell><cell>1.82</cell><cell>3.58</cell></row><row><cell>PSNR</cell><cell>25.41</cell><cell>25.18</cell><cell>28.63</cell><cell>26.20</cell></row><row><cell>SSIM</cell><cell>0.6747</cell><cell>0.6596</cell><cell>0.7913</cell><cell>0.6995</cell></row><row><cell cols="3">4.7. The influence of training patch size</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Quantitative evaluation of different perceptual-driven SR methods in LPIPS and PI. PPON 128 indicates the POBranch trained with 128 Ã— 128 image patches. The best and second best results are highlighted and underlined, respectively.</figDesc><table><row><cell>Method</cell><cell>PIRM Val LPIPS / PI</cell><cell>PIRM Test LPIPS/ PI</cell></row><row><cell>ESRGAN [22]</cell><cell>0.1443 / 2.5550</cell><cell>0.1523 / 2.4356</cell></row><row><cell>PPON 128 (Ours)</cell><cell>0.1241 / 2.3026</cell><cell>0.1321 / 2.2080</cell></row><row><cell>PPON (Ours)</cell><cell cols="2">0.1194 / 2.2736 0.1273 / 2.1770</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Discriminator for 128 Ã— 128 training patches in PPON 128.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRp</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRs</cell></row><row><cell cols="6">Content Feature Extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Content Reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SRc</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Module (CFEM)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Module (CRM)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LR</cell><cell>Conv LR</cell><cell cols="5">RRFB Conv Content Feature Extraction RRFB RRFB RRFB Module (CFEM)</cell><cell></cell><cell></cell><cell cols="2">RRFB RRFB</cell><cell></cell><cell>Conv Conv</cell><cell></cell><cell cols="7">Conv Conv Content Reconstruction Conv Conv Upsampler Module (CRM) Upsampler</cell><cell cols="2">SR</cell><cell cols="2">SR</cell><cell></cell><cell cols="2">LR</cell><cell>LR</cell><cell></cell><cell cols="5">CFEM CFEM CRM SRc</cell><cell cols="7">RRFB SFEM RRFB RRFB RRFB SRM SRs CRM RRFB SRM RRFB PRM RRFB PFEM SRp</cell><cell>RRFB PRM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SFEM</cell><cell></cell><cell></cell><cell cols="2">PFEM</cell></row><row><cell></cell><cell></cell><cell cols="2">64</cell><cell></cell><cell cols="2">64</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell></cell><cell>128</cell><cell></cell><cell></cell><cell cols="3">256</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell cols="3">512</cell><cell></cell><cell cols="3">512</cell><cell></cell><cell></cell><cell>512</cell><cell></cell><cell></cell></row><row><cell cols="2">HR</cell><cell cols="6">3Ã—3 Conv (a) SR Leaky ReLU 4Ã—4 Conv Batch Norm Leaky ReLU s2 64 64 s2 HR 3Ã—3 Conv Leaky ReLU 4Ã—4 Conv Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv 128 Batch Norm 3Ã—3 Conv</cell><cell cols="2">Leaky ReLU Batch Norm Leaky ReLU</cell><cell cols="2">4Ã—4 Conv s2 128 Batch Norm s2 4Ã—4 Conv Batch Norm</cell><cell>Leaky ReLU Leaky ReLU</cell><cell cols="2">3Ã—3 Conv 256 3Ã—3 Conv</cell><cell cols="3">Batch Norm Batch Norm Leaky ReLU Leaky ReLU</cell><cell cols="2">4Ã—4 Conv s2 256 s2 4Ã—4 Conv Batch Norm</cell><cell cols="2">Batch Norm Leaky ReLU</cell><cell>Leaky ReLU 512 3Ã—3 Conv</cell><cell cols="3">3Ã—3 Conv Batch Norm Leaky ReLU Batch Norm</cell><cell>Leaky ReLU 512 s2 4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>4Ã—4 Conv s2 Leaky ReLU</cell><cell cols="2">Batch Norm 512 Leaky ReLU 3Ã—3 Conv Batch Norm</cell><cell cols="2">3Ã—3 Conv Leaky ReLU</cell><cell>Batch Norm 512 s2 4Ã—4 Conv</cell><cell>Leaky ReLU Batch Norm</cell><cell>Leaky ReLU</cell><cell cols="3">4Ã—4 Conv s2 Dense (128) Batch Norm Leaky ReLU Leaky ReLU Dense (1)</cell><cell>Dense (128) Sigmoid</cell><cell>Leaky ReLU</cell><cell>Dense (1) Real or Fake</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell></cell><cell></cell><cell>64</cell><cell cols="2">64</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">128</cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell cols="2">256</cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell cols="3">512</cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell cols="2">512</cell><cell></cell><cell></cell><cell>512</cell><cell></cell></row><row><cell>SR HR</cell><cell></cell><cell>3Ã—3 Conv</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell cols="2">4Ã—4 Conv</cell><cell>Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell cols="2">Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell cols="2">Leaky ReLU</cell><cell>3Ã—3 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>4Ã—4 Conv</cell><cell>Batch Norm</cell><cell>Leaky ReLU</cell><cell>Dense (128)</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Sigmoid</cell><cell>Real or Fake</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CRM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CFEM</cell><cell></cell><cell></cell><cell cols="4">COBranch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CRM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SRM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CFEM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SFEM</cell><cell></cell><cell></cell><cell cols="3">SOBranch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CRM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SRM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">PRM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CFEM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SFEM</cell><cell></cell><cell></cell><cell></cell><cell cols="3">PFEM</cell><cell></cell><cell></cell><cell cols="4">POBranch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>s2 (b) Discriminator for 192 Ã— 192 training patches in PPON.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/richzhang/PerceptualSimilarity 2 https://github.com/roimehrez/PIRM2018</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Key Research and Development Program of China under Grants 2018AAA0102702, 2018AAA0103202, in part by the National Natural Science Foundation of China under Grant 61772402, 61671339, 61871308, and 61972305.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A phase congruency based patch evaluator for complexity reduction in multi-dictionary based single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="page" from="337" to="353" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piecewise linear regressionbased single image super-resolution via hadamard transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="page" from="315" to="330" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>HuszÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="624" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3147" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image super-resolution using dense skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4799" to="4807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast and accurate single image super-resolution via information distillation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="723" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single image superresolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhancenet</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4491" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Talmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<title level="m">Maintaining natural image statistics with the contextual loss</title>
		<imprint>
			<publisher>ACCV</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>CVPR Workshop</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image superresolution using multi-scale deep encoderdecoder with phase congruency edge map guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">473</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fast, accurate, and lightweight superresolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="252" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Srfeat: Single image superresolution with feature discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV Workshop</publisher>
		</imprint>
	</monogr>
	<note>pirm challenge on perceptual image super-resolution</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiscale structural similarity for image quality assessment</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
	<note>The Thrity-Seventh Asilomar Conference on Signals</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Line Alberi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>BMVC</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<title level="m">Sketch-based manga retrieval using manga109 dataset, Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning a single convolutional superresolution network for multiple degradations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3262" to="3271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep back-projection networks for super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1664" to="1673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multi-scale residual network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Feedback network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3867" to="3876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Analyzing perception-distortion tradeoff using enhanced perceptual super-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Madam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Natural and realistic single image super-resolution with explicit natural manifold discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="8122" to="8131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
