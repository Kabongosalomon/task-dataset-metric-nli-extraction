<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video Classification · Sequence Representation · Graph Neu- ral Network · Deep Convolutional Neural Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network(DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reflecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nearly 80% of the data on the Internet are images and videos. Research on analyzing, understanding and mining these multimedia data has drawn a significant amount of attention from both academia and industry. Labeling or classifying videos is one of the most important requirements, and challenges remained to be solved. Googles YouTube-8M team introduced a large multi-label video classification dataset which composed of nearly 8 million videos-500K hours of video-annotated with a vocabulary of 4800 visual entities <ref type="bibr" target="#b0">[1]</ref>. They used the stateof-the-art Inception-v3 network <ref type="bibr" target="#b1">[2]</ref> pre-trained on ImageNet to extract frame features at one-frame-per-second, providing reliable data supporting for large-scale video understanding. The key task is modeling the long sequence of frame features. The popular methods are LSTM (Long Short-Term Memory Networks) <ref type="bibr" target="#b2">[3]</ref>, GRU (Gated recurrent units) <ref type="bibr" target="#b3">[4]</ref>, DBoF(Deep Bag of Frame Pooling) <ref type="bibr" target="#b0">[1]</ref>, etc.</p><p>In this work, a novel deep convolutional graph based frame feature sequence modeling method is proposed, which aims to mine the complex relationship between video frames and shots, and perform hierarchical semantic abstraction across video. Evaluations are made based on the YouTube8M-2018 dataset, which containing about 5 million videos and 3862 labels. We use the provided frame level Inception-v3 features to training the model, the results show that our model out-performs the RNN based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Video feature sequence classification is essentially the the task of aggregating video features, that is, to aggregate N D-dimensional features into one Ddimensional feature by mining statistical relationships between these N features. The aggregated D -dimensional feature is a highly concentrated embedding, making the classifier easy to mapping the visual embedding space into the label semantic space. It is common using recurrent neural networks, such as LSTM (Long Short-Term Memory Networks) <ref type="bibr" target="#b2">[3]</ref>[5] <ref type="bibr" target="#b5">[6]</ref> and GRU (Gated recurrent units) <ref type="bibr" target="#b3">[4]</ref> <ref type="bibr" target="#b6">[7]</ref>, both are the state-of-the-art approaches for many sequence modeling tasks. However, the hidden state of RNN is dependent on previous steps, which prevent parallel computations. Moreover, LSTM or GRU use gate to solve RNN gradient vanish problem, but the sigmoid in the gates still cause gradient decay over layers in depth. It has been shown that LSTM has difficulties in converging when sequence length increase <ref type="bibr" target="#b7">[8]</ref>.There also exist end-to-end trainable order-less aggregation methods, such as DBoF(Deep Bag of Frame Pooling) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this paper, we propose a convolutional graph based video representation method for a sequence of video frame features. The main idea is that video is a hierarchical data structure, composed of events, scenes, shots, super-frames and frames. Additionally, the relations between frames, and the relations between shots are more complex than the order of a sequence. Consider an example sequence of cooking show as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, frames containing same targets are not continuous, and distributed in various timestamps, as well as the events and shots. We model the video frame sequence, shot, and event hierarchically by a deep convolution graph Network (DCGN). It gradually abstracts information from frame level to video level by convolution and information propagating through graph, and finally generates a global representation for further classification. The method is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Network with Deep Convolution</head><p>We use F={f D i , 0 i N } to denote video frame feature sequence, where D is the dimension of the frame feature and N is the number of video frames. We treat   <ref type="figure">Fig. 2</ref>. High-level illustration of our proposed method DCGN. This is an example with label of "cooking show", containing shots of chef cooking, host chatting, food, audience, etc. Rectangle with different color represent different shots or events. We use graph network to represent the relations between frames, shots, or events, that similar ones(nodes) have edge connected. The graph is gradually aggregated that represent frame, shot, event and video hierarchically. each frame, shot or event as a node of a graph, and the graph is densely connected (each pair of the nodes has an edge). Two nodes are connected weighted by their feature vector similarity. Let A be the adjacency matrix of F , and the elements is calculated by cosine similarity:</p><formula xml:id="formula_0">A(i, j) = D−1 d=0 (f [i][d] × f [j][d]) D−1 d=0 f [i][d] 2 × D−1 d=0 f [j][d] 2<label>(1)</label></formula><p>Graph neural network uses a differentiable aggregation function 2 to perform message passing. It is an end-to-end learning model, which can learn node and edge representations simultaneously.</p><formula xml:id="formula_1">h l = G(A, h l−1 , W l )<label>(2)</label></formula><p>where h l is the node representation(messages) after l-th iteration, W l is the parameters for the l-th iteration. G is the message propagation function. GCN <ref type="bibr" target="#b8">[9]</ref> is a popular graph model whose G is:</p><formula xml:id="formula_2">G = σ( DA Dh l−1 W l )<label>(3)</label></formula><p>where D is the diagonal node degree matrix that normalizing A such that all rows sum to one.</p><p>Graph pooling. In the equation 3 the adjacency matrix A is unchanged during iteration, so the topology of the graph is static. However, video frame sequence is a hierarchical structure, hence the graph topology should be abstracted to higher level gradually. We use two pooling methods to aggregate graph.</p><p>average pooling. This method applies the center of K consecutive nodes as the node of the next level graph:</p><formula xml:id="formula_3">p l [i][d] = K−1 k=0 h l−1 [i × K + k][d] K , d ∈ [0, D]<label>(4)</label></formula><p>where K is the pooling kernel size, p l is the l-th pooled graph node feature vector. After l-th iteration, the graph size is 1 K l of the original.</p><p>self-attention based pooling. This method performs a local self-attention to obtain a weight α for each feature of the local consecutive sequence, thereby obtaining a locally weighted and fused output of the feature sequence. Comparing to average pooling, it can better obtain the topology of the next layer graph, which is beneficial to the propagation of feature information. We formulate it as:</p><formula xml:id="formula_4">p l att [i] = α l [i] ⊗ h l−1 [i × K : (i + 1) × K], α l [i] = sof tmax(h l−1 [i × K : (i + 1) × K]W l att + b l )<label>(5)</label></formula><p>where K is the number of local features to perform self-attention, and W att and b are the parameters to learn.</p><p>Nodes convolution. Different from GCN <ref type="bibr" target="#b8">[9]</ref> which use fully connected layers to represent the nodes, in order to represent frame sequence hierarchically and maintain the local sequence order, we represent the nodes by convolution as follows:</p><formula xml:id="formula_5">c l [i] = h l−1 [i × K : (i + 1) × K]W l<label>(6)</label></formula><p>where c l is the l-th graph node embedding, and W l is the convolution kernel weights with size of K × D.</p><p>Pooling and convolution are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Node feature propagation. Now, we have obtained a new graph topology and new feature vector for each node. In order to obtain a more complete representation in higher level, we perform the message passing across the entire graph, so that the fused feature of each node is generated from the global perspective. We use the similar form as equation 3:</p><formula xml:id="formula_6">h l = σ( D l−1 A l−1 D l−1 c l−1 W l )<label>(7)</label></formula><p>where A l−1 is calculated using equation 1 in which f is replaced with p l−1 . <ref type="figure">Fig. 4</ref> shows the network described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shot segmentation aided graph pooling</head><p>Shot is a basic temporal unit, which is a series of interrelated consecutive pictures taken contiguously by a single camera and representing a continuous action in time and space. It is independent over video category, and can be obtained by unsupervised nonparametric way. Inspired by the kernel temporal segmentation (KTS) algorithm <ref type="bibr" target="#b9">[10]</ref>, the proposed method applies the deep convolutional neural network features to calculate the matrix of frame-to-frame similarities. The algorithm uses dynamic programming to minimize within segment kernel variances, and get the best shot boundaries under given number of shots, the object function is: min m;t0,...,tm−1 J m,n := L m,n + Cg(m, n)</p><p>where</p><formula xml:id="formula_8">L m,n = m i=0 v ti−1,ti , g(m, n) = m(log( n m ) + 1), v ti−1,ti = ti=1−1 t=ti f t − µ i 2 , u i = ti+1−1 t=tt i f t t i+1 − t i<label>(9)</label></formula><p>where m is the number of shots and g(m, n) is a penalty term. <ref type="figure">Fig. 5</ref> shows the segmented shots together with their positions in frame-to-frame similarity matrix. We apply this shot segmentation algorithm to the frame level graph modeling, and the formulation becomes:</p><formula xml:id="formula_9">p 1 [t][d] = st+1 k=st f [k][d] s t+1 − s t , d ∈ [0, D]<label>(10)</label></formula><formula xml:id="formula_10">c 1 [t] = f [s t : s t+1 ]W 1<label>(11)</label></formula><p>where t is the shot number, s t is frame index of the t-th shot boundary. <ref type="figure">Fig. 5</ref>. The left is a matrix of frame-to-frame similarities, which brighter(larger value) points indicate more similar frames. Local bright squares are treated as a shot. <ref type="figure">Fig. 6</ref> shows the network aided with shot segmentation: first, a fixed number m of shots are segmented, and a frame level convolution graph network is applied to get the shot level features H 1 . Then deep convolution graph network is applied to modeling higher-level features. Finally, the last level feature vectors are concatenated and inputted to a mixture of experts (MoE) <ref type="bibr" target="#b10">[11]</ref> model to get the classification score. The model is trained end-to-end. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we evaluate the proposed method with other alternatives, including average pooling, LSTM, GRU, DBoF. We train these models on YouTube-8m-2018 frame-level training set and evaluate on validation set. There are 3.9 million videos in the training set and 1.1 million videos in the validation set. The dataset is a multi-class task with 3862 classes in total, and we use cross entropy loss to train the model:</p><formula xml:id="formula_11">Loss = − C c=1 y c log(p c )<label>(12)</label></formula><p>where C is the number of classes, p c is the prediction for class c.</p><p>We use GAP and Hit@1 [1] <ref type="bibr" target="#b11">[12]</ref> as the evaluation metrics:</p><formula xml:id="formula_12">GAP = N i=1 p(i)∆r(i)<label>(13)</label></formula><p>where N is the number of final predictions, we set N =20 here, p(i) is the precision, and r(i) is the recall.</p><formula xml:id="formula_13">Hit@1 = 1 |V | v∈V ∨ e∈Gv I(rank v,e 1)<label>(14)</label></formula><p>where G v is the set of ground-truth entities for v. This is the fraction of test samples that contain at least one of the ground truth labels in the best prediction. The network architecture parameters and hyper-parameters for training these models are given in <ref type="table" target="#tab_1">Table 1</ref>.  <ref type="table" target="#tab_2">Table 2</ref> shows results for all methods. Simple averaging of features across all frames, perform poorly on this dataset. The order-less aggregation methods DBoF perform worse than the RNN based models. Our model outperforms all other models, and the self-attention based graph pooling got the best result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a novel sequence representation method DCGN for video classification. One layer in DCGN is composed of graph pooling, nodes convolution and nodes feature propagation. The problem of representing complex relationships between frames or shots is addressed by applying this graph based network hierarchically across the frame sequence. Based on the quantitative results in Section 4, the proposed method DCGN outperform the other alternatives such as LSTM and GRU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>cooking show video frame sequence. Frames with same color border contain similar targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Convolution and pooling (K=3) of N nodes with D-dimension, outputs new N 3 nodes with D-dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 Fig. 4 .</head><label>24</label><figDesc>Illustration of tow layer convolutional graph network. 15 input feature vectors are aggregated to 2 output feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Fig. 6 .</head><label>16</label><figDesc>Overall architecture. Shot-wise convolution and pooling performs at the frame level. The subsequent levels perform DCGN and obtain a video level feature H L . MoE classifier use H L as the input to provide the final label predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Training settings</figDesc><table><row><cell></cell><cell>Ours</cell><cell>Average Pooling</cell><cell>LSTM</cell><cell>GRU</cell><cell>DBoF</cell></row><row><cell>network parameters</cell><cell>layers: 5, filter size: 1024 (same for each layer)</cell><cell></cell><cell>cells: 1024, layers: 2</cell><cell>cells: 1024, layers: 2</cell><cell>cluster size: 8192, hidden size: 1024, pooling method: max</cell></row><row><cell>base learning rate</cell><cell></cell><cell></cell><cell>0.001</cell><cell></cell></row><row><cell>learning rate decay</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>learning rate decay examples</cell><cell></cell><cell></cell><cell>4M</cell><cell></cell></row><row><cell>mini batch size</cell><cell></cell><cell></cell><cell>1024</cell><cell></cell></row><row><cell>number of epochs</cell><cell></cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>number of mixtures in MoE</cell><cell></cell><cell></cell><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>evaluation results. All values in this table are averaged results and reported in percentage(%). Top-2 of each metrics are bolded.</figDesc><table><row><cell>methods</cell><cell>GAP</cell><cell cols="2">Hit@1 Loss</cell></row><row><cell>Average pooling</cell><cell>76.5</cell><cell>83.5</cell><cell>5.49</cell></row><row><cell>LSTM</cell><cell>83.6</cell><cell>87.2</cell><cell>4.12</cell></row><row><cell>GRU</cell><cell>83.9</cell><cell>87.9</cell><cell>4.03</cell></row><row><cell>DBoF</cell><cell>81.1</cell><cell>86.2</cell><cell>6.02</cell></row><row><cell>ours + average graph pooling</cell><cell>84.1</cell><cell>87.4</cell><cell>4.01</cell></row><row><cell cols="2">ours + self-attention graph pooling 84.5</cell><cell>87.7</cell><cell>3.98</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Youtube-8m: A large-scale video classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno>abs/1609.08675</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno>abs/1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Independently recurrent neural network (indrnn): Building A longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1803.04831</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="https://github.com/google/youtube-8m2018" />
		<title level="m">Google: Youtube-8m starter code</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
