<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep regularization and direct training of the inner layers of Neural Networks with Kernel Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-10">August 10, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">Ryan</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houman</forename><surname>Owhadi</surname></persName>
						</author>
						<title level="a" type="main">Deep regularization and direct training of the inner layers of Neural Networks with Kernel Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-10">August 10, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new regularization method for Artificial Neural Networks (ANNs) based on Kernel Flows (KFs). KFs were introduced in [8] as a method for kernel selection in regression/kriging based on the minimization of the loss of accuracy incurred by halving the number of interpolation points in random batches of the dataset. Writing f θ pxq "`f pnq θn˝f pn´1q θn´1˝¨¨¨˝f p1q θ1˘p xq for the functional representation of compositional structure of the ANN (where θ i are the weights and biases of the layer i), the inner layers outputs h piq pxq "`f piq θi˝f pi´1q θi´1˝¨¨¨˝f p1q θ1˘p xq define a hierarchy of feature maps and a hierarchy of kernels k piq px, x 1 q " expp´γ i }h piq pxqh piq px 1 q} 2 2 q. When combined with a batch of the dataset these kernels produce KF losses e piq 2 (defined as the L 2 regression error incurred by using a random half of the batch to predict the other half) depending on the parameters of the inner layers θ 1 , . . . , θ i (and γ i ). The proposed method simply consists in aggregating (as a weighted sum) a subset of these KF losses with a classical output loss (e.g. crossentropy). We test the proposed method on Convolutional Neural Networks (CNNs) and Wide Residual Networks (WRNs) without alteration of their structure nor their output classifier and report reduced test errors, decreased generalization gaps, and increased robustness to distribution shift without significant increase in computational complexity relative to standard CNN and WRN training (with Drop Out and Batch Normalization). We suspect that these results might be explained by the fact that while conventional training only employs a linear functional (a generalized moment) of the empirical distribution defined by the dataset and can be prone to trapping in the Neural Tangent Kernel regime (under over-parameterizations), the proposed loss function (defined as a nonlinear functional of the empirical distribution) effectively trains the underlying kernel defined by the CNN beyond regressing the data with that kernel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce a new regularization method for Artificial Neural Networks (ANNs) based on Kernel Flows (KFs). KFs were introduced in <ref type="bibr" target="#b6">[8]</ref> as a method for kernel selection in regression/kriging based on the minimization of the loss of accuracy incurred by halving the number of interpolation points in random batches of the dataset. Writing f θ pxq "`f pnq θn˝f pn´1q θn´1˝¨¨¨˝f p1q θ1˘p xq for the functional representation of compositional structure of the ANN (where θ i are the weights and biases of the layer i), the inner layers outputs h piq pxq "`f piq θi˝f pi´1q θi´1˝¨¨¨˝f p1q θ1˘p xq define a hierarchy of feature maps and a hierarchy of kernels k piq px, x 1 q " expp´γ i }h piq pxqh piq px 1 q} 2 2 q. When combined with a batch of the dataset these kernels produce KF losses e piq 2 (defined as the L 2 regression error incurred by using a random half of the batch to predict the other half) depending on the parameters of the inner layers θ 1 , . . . , θ i (and γ i ). The proposed method simply consists in aggregating (as a weighted sum) a subset of these KF losses with a classical output loss (e.g. crossentropy). We test the proposed method on Convolutional Neural Networks (CNNs) and Wide Residual Networks (WRNs) without alteration of their structure nor their output classifier and report reduced test errors, decreased generalization gaps, and increased robustness to distribution shift without significant increase in computational complexity relative to standard CNN and WRN training (with Drop Out and Batch Normalization). We suspect that these results might be explained by the fact that while conventional training only employs a linear functional (a generalized moment) of the empirical distribution defined by the dataset and can be prone to trapping in the Neural Tangent Kernel regime (under over-parameterizations), the proposed loss function (defined as a nonlinear functional of the empirical distribution) effectively trains the underlying kernel defined by the CNN beyond regressing the data with that kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">A reminder on Kernel Flows</head><p>Kernel Flows were introduced in [8] as a method for kernel selection/design in Kriging/Gaussian Process Regression (GPR). As a reminder on KFs consider the problem of approximating an unknown function u : mapping X to R based on the input/output dataset px i , y i q 1ďiďN (u : px i q " y i ). Any non-degenerate kernel Kpx, x 1 q can be used to approximate u : with the interpolant upxq " Kpx, XqKpX, Xq´1Y ,</p><p>(1.1) writing Y :" py 1 , . . . , y N q T , X :" px 1 , . . . , x N q, KpX, Xq for the NˆN Gram matrix Kpx i , x i q and Kpx, Xq for the N dimensional vector with entries Kpx, x i q. The kernel selection problem concerns the identification of a good kernel for performing this interpolation. The KF approach to this problem is to simply use the loss of accuracy incurred by removing half of the dataset as a loss of kernel selection. The application of this process to minibatches results in a loss that is doubly randomized by (1) the selection of the minibatch (2) the half sub-sampling of the minibatch. An iterated steepest descent minimization of this loss then results in stochastic gradient descent algorithm (where the minibatch and its half-subset are re-sampled at each step). Given a family of kernels K θ px, x 1 q parameterized by θ, the resulting algorithm can then be described as follows:</p><p>(1) Select random subvectors X b and Y b of X and Y (through uniform sampling without replacement in the index set t1, . . . , N u) (2) Select random subvectors X c and Y c of X b and Y b (by selecting, at random, uniformly and without replacement, half of the indices</p><formula xml:id="formula_0">defining X b ) (3) Let ρpθ, X b , Y b , X c , Y c q</formula><p>be the squared relative error (in the RKHS norm }¨} K θ defined by K θ ) between the interpolants u b and u c obtained from the two nested subsets of the dataset and the kernel K θ , i.e. 1</p><formula xml:id="formula_1">ρpθ, X b , Y b , X c , Y c q :" 1´Y c,T K θ pX c , X c q´1Y c Y f,T K θ pX b , X b q´1Y b .</formula><p>(1.2) (4) evolve θ in the gradient descent direction of ρ, i.e. θ Ð θ´δ∇ θ ρ (5) repeat.</p><p>Example. <ref type="figure">Fig. 1</ref> shows an application of the proposed approach to the selection of a kernel K F px, x 1 q " expp´γ}F pxq´F px 1 q} 2 q parameterised by a deformation F : R 2 Ñ R 2 of the input space (X " R 2 ). The dataset is the swissroll cheesecake (red points have labels`1 and blue points have labels´1), <ref type="figure">Fig. 1</ref> shows the deformed dataset F n pXq and the gradient´∇ F ρ averaged over 300 steps.</p><p>The l 2 -norm variant. In this paper we will consider the l 2 -norm variant of KF (introduced in <ref type="bibr" target="#b6">[8,</ref><ref type="bibr">Sec. 10]</ref>) in which the instantaneous loss ρ in (1.2) is replaced by the error (let }¨} 2 be the Euclidean l 2 norm) e 2 :" }Y b´uc pX b q} 2 2 of u c in predicting the labels Y b , i.e.</p><formula xml:id="formula_2">e 2 pθ, X b , Y b , X c , Y c q :" }Y b´K θ pX b , X c qK θ pX c , X c q´1Y c } 2 2 (1.3) 1 ρ :" }u b´uc } 2 K θ {}u b } 2 K θ , with u b pxq " K θ px, X b qK θ pX b , X b q´1Y b</formula><p>and u c pxq " K θ px, X c qK θ pX c , X c q´1Y c , and ρ admits <ref type="bibr" target="#b5">[7,</ref><ref type="bibr">Prop. 13</ref>.29] the representation (1.2) enabling its computation <ref type="figure">Figure 1</ref>: <ref type="bibr">[8,</ref>  <ref type="figure" target="#fig_1">Fig. 13</ref>], pF n px i qq 1ďiďN (dots) and 10pF n`300 pxq´F n pxqq{300 (arrows) for 5 different values of n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Kernel Flow regularization of Neural Networks</head><p>Write</p><formula xml:id="formula_3">f θ pxq "`f pnq θn˝f pn´1q θ n´1˝¨¨¨˝f p1q θ 1˘p xq (2.1)</formula><p>for the compositional structure of an artificial neural network (ANN) with input x and n layers f piq θ i pzq " φpW i z`b i q parameterized by the weights and biases θ i :" pW i , b i q, θ :" tθ 1 , . . . , θ n u. We will use ReLU for the non-linearity φ in our experiments. For i P t1, . . . , n´1u let h piq pxq be the output of the i-th (inner) layer, i.e.</p><formula xml:id="formula_4">h piq θ pxq :"`f piq θ i˝f pi´1q θ i´1˝¨¨¨˝f p1q θ 1˘p xq ,<label>(2.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>and let h θ pxq :" ph p1q θ pxq, . . . , h pn´1q θ pxqq be the pn´1q-ordered tuple representing all inner layer outputs. Let k γ p¨,¨q be a family of kernels parameterized by γ and let K γ,θ be the family of kernels parameterized by γ and θ defined by</p><formula xml:id="formula_5">K γ,θ px, x 1 q " k γ ph θ pxq, h θ px 1 qq . (2.3) Given the random mini-batch pX b , Y b q let L c-e pf θ pX b q, Y b q :" ř i L c-e pf θ pX b i q, Y b</formula><p>i q be the cross-entropy loss associated with that mini-batch. Given the (randomly sub-</p><formula xml:id="formula_6">sampled) half sub-batch pX c , Y c q, let L KF pγ, θ, X b , Y b , X c , Y c q be the loss function (with hyper-parameter λ ě 0) defined by L KF :" λ}Y b´K γ,θ pX b , X c qK γ,θ pX c , X c q´1Y c } 2 2`L c-e pf θ pX b q, Y b q . (2.4)</formula><p>Our proposed KF-regularization approach is then to train the parameters θ of the network f θ via the steepest descent pγ, θq Ð pγ, θq´δ∇ γ,θ L KF . Note that this algorithm (1) is randomized through both the sampling of the minibatch and its subsampling (2) adapts both θ and γ (since the KF term depends on both θ and γ) (3) simultaneously trains the accuracy of the output via the cross-entropy term and the generalization properties of the feature maps defined by the inner layers via the KF term. Furthermore while the cross-entropy term is a linear functional of the empirical distribution 1</p><formula xml:id="formula_7">N b ř i δ pX b i ,Y b i q</formula><p>defined by the mini-batch (writing N b for the number of indices contained in the minibatch), the KF term is non-linear. While K γ,θ may depend on the output of all the inner layers, in our numerical experiments we have restricted its dependence to the output of only one inner layer or used a weighted sum of such terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Numerical experiments</head><p>We will now use the proposed KF regularization method to train a simple Convolutional Neural Network (CNN) on MNIST and Wide Residual Networks (WRN) <ref type="bibr" target="#b13">[15]</ref> on fashion MNIST, CIFAR-10, and CIFAR-100. Our goal is to test the proposed approach and compare its performance with popular ones (Batch Normalization and Drop Out).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Kernel Flow regularization on MNIST</head><p>We consider a Convolutional Neural Network (CNN) with six convolutional layers and three fully connected layers, as charted in <ref type="table" target="#tab_1">Table 1</ref> (this CNN is a variant of a CNN presented in <ref type="bibr" target="#b1">[3]</ref> with code used from <ref type="bibr" target="#b0">[2]</ref>). Convolutional layers all have stride one in this network with the number of convolutional channels and the convolutional kernel size in the second and third columns from the left. "Valid" padding implies no 0-padding at the boundaries of the image while "same" 0-pads images to obtain convolutional outputs with the same sizes as the inputs. The "Max Pool" layers down sample their inputs by reducing each 2ˆ2 square to their maximum values. The "Average Pool" layer in the final convolutional layer takes a simple mean over each channel. The final three layers are fully connected each with outputs listed on the right column. All convolutional and dense layers include trainable biases. Using notations from the previous section, the outputs of the convolutional layers, which include ReLU and pooling, are h p1q pxq to h p6q pxq with output shapes described in the left column. The dense layers outputs are h p7q pxq to h p9q pxq. We do not pre-process the data and, when employed, the data augmentation step, in this context, passes the original MNIST image to the network with probability 1 3 , applies an elastic deformation <ref type="bibr" target="#b9">[11]</ref> with probability 1 3 , and a random small translation, rotation, and shear with probability 1 3 . The learning rate begins at 10´2 and smoothly exponentially decreases to 10´6 while training over 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Comparisons to Dropout</head><p>The first experiment we present is one comparing the use of our KF loss function with the use of dropout (DO) <ref type="bibr" target="#b10">[12]</ref>. We use Batch Normalization (BN) <ref type="bibr" target="#b2">[4]</ref>    <ref type="table" target="#tab_2">Table 2</ref>: A comparison of the average and standard deviation of testing errors each over 20 runs for networks. The first data column on the left shows networks trained and tested on original MNIST data. The middle is trained using data augmentation and uses original MNIST testing data. The right column shows the same data augmented trained network, but uses QMNIST testing data <ref type="bibr" target="#b12">[14]</ref>.</p><p>We present two KF experiments. The first one involves the following Gaussian kernel on the final convolutional layer h p6q pxq P R 300 :</p><formula xml:id="formula_8">K p6q γ 6 ,θ px, x 1 q " k p6q γ 6 ph p6q pxq, h p6q px 1 qq " e´γ 6 }h p6q pxq´h p6q px 1 q} 2 . (3.1)</formula><p>We optimize the loss function in (2.4) with kernel K p6q γ 6 over the parameters θ and γ 6 . The second experiment is a slight variant where we use both K p6q and K p3q γ 3 ,θ px, x 1 q " k p3q γ 3 ph p3q pxq, h p3q px 1 qq " e´γ 3 }aph p3q pxqq´aph p3q px 1 qq} 2 ,</p><formula xml:id="formula_9">(3.2)</formula><p>where a is a 12ˆ12 average pooling reducing each channel to a single point. Given the random mini-batch pX b , Y b q and the (randomly sub-sampled) half subbatch pX c , Y c q, we evolve θ and γ 6 in the steepest descent direction of the loss</p><formula xml:id="formula_10">L KF3,6 "λ p3q }Y b´K p3q γ3,θ pX b , X c qK p3q γ3,θ pX c , X c q´1Y c } 2 2 λ p6q }Y b´K p6q γ6,θ pX b , X c qK p6q γ6,θ pX c , X c q´1Y c } 2 2`Lc-e pf θ pX b q, Y b q (3.3)</formula><p>with respect to θ, γ 6 , and γ 3 . These two training methods are labeled KF 6 and KF 3, 6 respectively. The comparison between dropout and KF is made in  <ref type="bibr" target="#b12">[14]</ref>. These three regimes are presented in the data columns of table 2 from left to right. The difference between the original data augmented and QMNIST training errors quantifies the effect of distributional shift of the training data <ref type="bibr" target="#b8">[10]</ref>. This effect is observed to be reduced when using KF trained networks, which suggests some degree of robustness to distributional shift. The training and testing errors of single runs with the batch normalization only, dropout 0.25{0.25 and KF 3, 6 are plotted in <ref type="figure" target="#fig_0">figure  2</ref>. Observe that the generalization gap (the gap between the training and testing errors) decreases with the use of dropout and that decrease is even more pronounced with KF. Furthermore, contrary to dropout, KF does not reduce training accuracy. We observe similar findings on networks trained using data augmentation, albeit less pronounced. We finally examine the components of the KF 3, 6 loss function as in equation <ref type="bibr">(3.3)</ref>. The KF-loss at the 3rd layer, }Y b´K p3q γ 3 ,θ pX b , X c qK p3q γ 3 ,θ pX c , X c q´1Y c } 2 2 , and the 6th layer, }Y b´K p6q</p><formula xml:id="formula_11">γ 6 ,θ pX b , X c qK p6q γ 6 ,θ pX c , X c q´1Y c } 2 2 ,</formula><p>is computed for batch normalization, dropout, and KF training in <ref type="figure" target="#fig_1">figure 3</ref>. It can be seen that KF 6 reduces the 3rd layer KF-loss slightly compared to BN or DO 0.25{0.25, while significantly reducing the 6th layer KF-loss. Additionally, as expected, KF 3, 6 reduces both. We can further consider the ratio of mean inter-class and in-class distances within each batch of 3rd and 6th convolutional layer outputs. We see that KF 6 separates images based on class in the 6th layer outputs while KF 3, 6 does so on both the 3rd and 6th.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernel Flow regularization on Fashion MNIST and CIFAR</head><p>We now consider the Wide Residual Network (WRN) structure described in [15, <ref type="table" target="#tab_1">Table  1</ref>] with the addition of a dense layer. For convenience, we show this architecture in <ref type="table" target="#tab_4">Table 3</ref>. Note that there are four convolutional blocks, each with a certain number of residual layers, which are as described in <ref type="bibr">[15,</ref>  <ref type="figure">Fig. 1c,d</ref>  tween the blocks in dropout training, added to an identity mapping from the input of the layer. In our dropout experiments, we drop each neuron in the network with probability 0.3. Note that k and N are hyper-parameters of the WRN architecture governing width and depth respectively, and a network with such k, N is written WRN-k-N . In these presented WRN experiments, we use data augmentation where training images are randomly translated and horizontally flipped. In our implementations, we have modified the code from [1] (which uses TensorFlow). Batches consisting of 100 images are used in these experiments. In fashion MNIST and CIFAR-10, each half batch contains 5 random images from each of the 10 classes. Meanwhile in CIFAR-100, we require each class represented in the testing sub-batch to also be represented in the training sub-batch. We write the outputs of each of the four convolutional blocks as h p1q pxq, . . . , h p4q pxq. Again defining a as the average pooling operator, we have aph p1q pxqq P R 16 , aph p2q pxqq P R 16k , aph p3q pxqq P R 32k , and aph p4q pxqq " h p4q pxq P R 64k . We define corresponding RBF kernels K plq γ l px, x 1 q " k plq γ l ph plq pxq, h plq px 1 qq " e´γ l }aph plq pxqq´aph plq px 1 qq} 2 .</p><p>(3.4)</p><p>Given the random mini-batch pX b , Y b q and the (randomly sub-sampled) half subbatch pX c , Y c q, we evolve θ (and γ) in the steepest descent direction of the loss  <ref type="table" target="#tab_5">Table 4</ref>: A comparison of the median test errors over 5 runs for networks trained on augmented data on Fashion MNIST, CIFAR-10, CIFAR-10.1, and CIFAR-100. The second column to the right trains on augmented CIFAR-10 data but tests on CIFAR-10.1 data <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b11">13]</ref>.</p><formula xml:id="formula_12">L KF pBq " 4 ÿ l"1 λ plq }Y b´K plq γ l ,θ pX b , X c qK plq γ l ,θ pX c , X c q´1Y c } 2 2`Lc-e pf θ pX b q, Y b q . (3.5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Comparison to Dropout</head><p>The network architecture WRN-16-8 is used and median testing error over five runs is listed. We train with step exponentially decreasing learning rates over 200 epochs with identical hyperparameters as <ref type="bibr" target="#b13">[15]</ref>. We observe that the addition of KF improves testing error rates against training with BN and BN+DO. We also run a distributional shift experiment for CIFAR-10 using the data set CIFAR-10.1, <ref type="bibr" target="#b7">[9]</ref> which is sampled from <ref type="bibr" target="#b11">[13]</ref>.</p><p>As with the QMNIST experiment, we also observe improvements with the addition of KF. We finally compare the KF loss, L KF pBq, and ratios of inter-class and in-class Euclidean distances on the output of the final convolutional layers within each batch in <ref type="figure" target="#fig_2">figure 4</ref>. These statistics are plotted over runs of WRN trained with CIFAR-10 and CIFAR-100. We again observe reduced KF-losses and increased ratios of mean interclass and in-class distances on the final convolutional layer output h p4q when comparing between BN and BN+KF as well as BN+DO and BN+DO+KF. That is, KF reduces the distance (defined on the outputs of the inner layers) between images in the same class and increases that distance between images in distinct classes (thereby enhancing the separation). The opposite effect is observed with the addition of dropout in training, suggesting they have differing reasons for improving testing error rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concluding remarks</head><p>It has recently been found <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6]</ref> that, in the overparameterized regime, training Neural Networks (or models) f px, θq with gradient descent and cross-entropy or mean squared loss is essentially equivalent to interpolating the data with the Neural Tangent Kernel Kpx, x 1 q " ∇ θ f px, θ 0 q¨∇ θ f px 1 , θ 0 q, i.e., when combined with gradient descent, losses defined as linear functionals (generalized moments) of the empirical distribution simply interpolate the data with a kernel fixed at initialization (θ 0 ). Kernel Flows on the other hand use non-linear functionals of the empirical distribution designed to actually train the underlying kernel defined by the architecture of the Neural Network. We suspect that these observations could to some degree explain the results observed in this paper (decreased generalization gap, improved test accuracies and increased robustness to distributional shift).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Training and testing errors are plotted over single runs trained with original data using (1) batch normalization only (2) dropout 0.25{0.25 (3) KF 3, 6. Data augmented trained network errors are shown using (4) batch normalization only (5) dropout 0.25{0.25 (6) KF 3, 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Single run over each of BN only, DO 0.25{0.25, KF 6, and KF3, 6 training methods plotting (1) 3rd layer KF-loss (2) 6th layer KF-loss (3) ratio of mean interclass and in-class distances of 3rd layer outputs (4) ratio of mean inter-class and in-class distances of 6th layer outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Single run using WRN-16-8 with each of BN only, BN+KF, BN+DO, and BN+KF+DO plotting (1) CIFAR-10 KF-loss (2) CIFAR-100 KF-loss (3) CIFAR-10 ratio of mean inter-class and in-class distances h p4q (4) CIFAR-100 ratio of mean inter-class and in-class distances h p4q .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>in all the experiments, i.e. both DO and KF regularization, in this subsection. Our first dropout experiment uses dropping probabilities of 0.25 across all layers. Our second experiment uses dropping</figDesc><table><row><cell>Layer Type</cell><cell>Number of</cell><cell>Filter size</cell><cell>Padding</cell><cell>Output shape</cell></row><row><cell></cell><cell>filters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Input layer</cell><cell></cell><cell></cell><cell></cell><cell>28ˆ28ˆ1</cell></row><row><cell>Convolutional layer 1, ReLU</cell><cell>150</cell><cell>3ˆ3</cell><cell>Valid</cell><cell>26ˆ26ˆ150</cell></row><row><cell>Convolutional layer 2, ReLU</cell><cell>150</cell><cell>3ˆ3</cell><cell>Valid</cell><cell>24ˆ24ˆ150</cell></row><row><cell>Convolutional layer 3, ReLU</cell><cell>150</cell><cell>5ˆ5</cell><cell>Same</cell><cell>24ˆ24ˆ150</cell></row><row><cell>Max Pool</cell><cell></cell><cell>2ˆ2</cell><cell></cell><cell>12ˆ12ˆ150</cell></row><row><cell>Convolutional layer 4, ReLU</cell><cell>300</cell><cell>3ˆ3</cell><cell>Valid</cell><cell>10ˆ10ˆ300</cell></row><row><cell>Convolutional layer 5, ReLU</cell><cell>300</cell><cell>3ˆ3</cell><cell>Valid</cell><cell>8ˆ8ˆ300</cell></row><row><cell>Convolutional layer 6, ReLU</cell><cell>300</cell><cell>5ˆ5</cell><cell>Same</cell><cell>8ˆ8ˆ300</cell></row><row><cell>Max Pool</cell><cell></cell><cell>2ˆ2</cell><cell></cell><cell>4ˆ4ˆ300</cell></row><row><cell>Average Pool</cell><cell></cell><cell>4ˆ4</cell><cell></cell><cell>300</cell></row><row><cell>Dense layer 1, ReLU</cell><cell></cell><cell></cell><cell></cell><cell>1200</cell></row><row><cell>Dense layer 2, ReLU</cell><cell></cell><cell></cell><cell></cell><cell>300</cell></row><row><cell>Dense layer 3</cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row><row><cell>Softmax Output layer</cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Training</cell><cell cols="3">Original MNIST Data augmented QMNIST</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BN only</cell><cell>0.500˘0.054%</cell><cell>0.357˘0.029%</cell><cell>0.453˘0.025%</cell></row><row><cell cols="2">BN+DO 0.25{0.25 0.403˘0.048%</cell><cell>0.315˘0.033%</cell><cell>0.429˘0.012%</cell></row><row><cell>BN+DO 0.4{0.2</cell><cell>0.395˘0.036%</cell><cell>0.331˘0.031%</cell><cell>0.443˘0.018%</cell></row><row><cell>BN+KF 6</cell><cell>0.305˘0.028%</cell><cell>0.281˘0.026%</cell><cell>0.343˘0.012%</cell></row><row><cell>BN+KF 3, 6</cell><cell>0.307˘0.027%</cell><cell>0.276˘0.027%</cell><cell>0.356˘0.012%</cell></row></table><note>The architecture of the CNN used in KF regularization experiments is charted. Convolutional layers are divided with horizontal lines. The middle block shows layer specifics and the shapes of the outputs of each layer is on the right.probabilities of 0.4 over convolutional layers and 0.2 on the dense layers. We denote these as DO 0.25{0.25 and DO 0.4{0.2 respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>table 2</head><label>2</label><figDesc>Furthermore, this KF framework has another advantage of being flexible, allowing the control of generalization properties of multiple layers of the network simultaneously, as in KF 3, 6, which does so on the outputs of the max pooling convolutional layers.For each of the training methods we experiment with using original MNIST training and testing data, augmenting the MNIST training set and testing on the original data, and finally training on the augmented set, but testing on QMNIST, which is resampled MNIST test data</figDesc><table><row><cell>. KF 6 and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>] for BN and BN+DO training respectively. Each layer consists of two convolutional blocks, with dropout applied be-</figDesc><table><row><cell>Layer/Block name</cell><cell>Number of</cell><cell>Filter size</cell><cell>Number</cell><cell>of</cell><cell>Output shape</cell></row><row><cell></cell><cell>filters</cell><cell></cell><cell cols="2">residual layers</cell><cell></cell></row><row><cell>Input layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32ˆ32ˆ3</cell></row><row><cell>Convolutional block 1</cell><cell>16</cell><cell>3ˆ3</cell><cell>1</cell><cell></cell><cell>32ˆ32ˆ16</cell></row><row><cell>Convolutional block 2</cell><cell>16k</cell><cell>3ˆ3</cell><cell>N</cell><cell></cell><cell>32ˆ32ˆ16k</cell></row><row><cell>Convolutional block 3</cell><cell>32k</cell><cell>3ˆ3</cell><cell>N</cell><cell></cell><cell></cell></row><row><cell>Max Pool</cell><cell></cell><cell>2ˆ2</cell><cell></cell><cell></cell><cell>16ˆ16ˆ32k</cell></row><row><cell>Convolutional block 4</cell><cell>64k</cell><cell>3ˆ3</cell><cell>N</cell><cell></cell><cell></cell></row><row><cell>Max Pool</cell><cell></cell><cell>2ˆ2</cell><cell></cell><cell></cell><cell>8ˆ8ˆ64k</cell></row><row><cell>Average Pool</cell><cell></cell><cell>8ˆ8</cell><cell></cell><cell></cell><cell>64k</cell></row><row><cell>Dense layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64k</cell></row><row><cell>Softmax Output layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The architecture of the WRN used in KF regularization experiments with CIFAR input images. Convolutional blocks are divided with horizontal lines. The middle portion shows block specifics such as filter width and depth in each block and the shapes of the outputs of each layer is on the right. Note that max pooling occurs within the last residual layer of each block.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>compares the test errors obtained after training with only batch normalization (BN) with the incorporation of dropout (DO), KF, as well as a combination of all three.</figDesc><table><row><cell>Training Method</cell><cell>Fashion</cell><cell cols="2">CIFAR-10 CIFAR-</cell><cell>CIFAR-</cell></row><row><cell></cell><cell>MNIST</cell><cell></cell><cell>10.1</cell><cell>100</cell></row><row><cell>BN</cell><cell>4.95%</cell><cell>4.68%</cell><cell>10.85%</cell><cell>20.39%</cell></row><row><cell>BN+KF</cell><cell>4.90%</cell><cell>4.53%</cell><cell>10.55%</cell><cell>20.27%</cell></row><row><cell>BN+DO</cell><cell>4.82%</cell><cell>4.43%</cell><cell>10.50%</cell><cell>19.50%</cell></row><row><cell>BN+DO+KF</cell><cell>4.73%</cell><cell>4.22%</cell><cell>10.30%</cell><cell>19.20%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge support by the Air Force Office of Scientific Research under award number FA9550-18-1-0271 (Games for Computation and Learning), Beyond Limits (Learning Optimal Models) and NASA/JPL (Earth 2050).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow and deep learning without a PhD</title>
		<ptr target="https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd" />
		<imprint>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist" />
	</analytic>
	<monogr>
		<title level="j">How to choose CNN Architecture MNIST</title>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descen" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8570" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Operator-Adapted Wavelets, Fast Solvers, and Numerical Homogenization: From a Game Theoretic Approach to Numerical Approximation and Algorithm Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scovel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kernel Flows: From learning kernels from data into the abyss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Owhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcp.2019.03.040</idno>
		<ptr target="https://doi.org/10.1016/j.jcp.2019.03.040" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">389</biblScope>
			<biblScope unit="page" from="22" to="47" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00451</idno>
		<title level="m">Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do ImageNet classifiers generalize to ImageNet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Best practices for Convolutional Neural Networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis" />
		<imprint>
			<date type="published" when="2003-08" />
			<publisher>Institute of Electrical and Electronics Engineers, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.5555/2627435.2670313</idno>
		<ptr target="https://dl.acm.org/doi/abs/10.5555/2627435.2670313" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2008.128</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2008.128" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cold case: The lost MNIST digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9500-cold-case-the-lost-mnist-digits.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13443" to="13452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.30.87</idno>
		<ptr target="https://dx.doi.org/10.5244/C.30.87" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Edwin R. Hancock Richard C. Wilson and William A. P. Smith</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
