<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity with Cooperation: Ensemble Methods for Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LJK</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity with Cooperation: Ensemble Methods for Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot classification consists of learning a predictive model that is able to effectively adapt to a new class, given only a few annotated samples. To solve this challenging problem, meta-learning has become a popular paradigm that advocates the ability to "learn to adapt". Recent works have shown, however, that simple learning strategies without meta-learning could be competitive. In this paper, we go a step further and show that by addressing the fundamental high-variance issue of few-shot learning classifiers, it is possible to significantly outperform current metalearning techniques. Our approach consists of designing an ensemble of deep networks to leverage the variance of the classifiers, and introducing new strategies to encourage the networks to cooperate, while encouraging prediction diversity. Evaluation is conducted on the mini-ImageNet, tiered-ImageNet and CUB datasets, where we show that even a single network obtained by distillation yields state-of-theart results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks <ref type="bibr" target="#b16">[17]</ref> have become standard tools in computer vision to model images, leading to outstanding results in many visual recognition tasks such as classification <ref type="bibr" target="#b15">[16]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>, or semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. Massively annotated datasets such as ImageNet <ref type="bibr" target="#b27">[28]</ref> or COCO <ref type="bibr" target="#b17">[18]</ref> seem to have played a key role in this success. However, annotating a large corpus is expensive and not always feasible, depending on the task at hand. Improving the generalization capabilities of deep neural networks and removing the need for huge sets of annotations is thus of utmost importance.</p><p>While such a grand challenge may be addressed from different complementary points of views, e.g., large-scale unsupervised learning <ref type="bibr" target="#b3">[4]</ref>, self-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>, or by developing regularization techniques dedicated to  <ref type="figure">Figure 1</ref>: Illustration of the cooperation and diversity strategies on two networks. All networks receive the same image as input and compute corresponding class probabilities with softmax. Cooperation encourages the non-ground truth probabilities (in red) to be similar, after normalization, whereas diversity encourages orthogonality.</p><p>deep networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref>, we choose in this paper to focus on variance-reduction principles based on ensemble methods. Specifically, we are interested in few-shot classification, where a classifier is first trained from scratch on a mediumsized annotated corpus-that is, without leveraging external data or a pre-trained network, and then we evaluate its ability to adapt to new classes, for which only very few annotated samples are provided (typically 1 or 5). Unfortunately, simply fine-tuning a convolutional neural network on a new classification task with very few samples has been shown to provide poor results <ref type="bibr" target="#b8">[9]</ref>, which has motivated the community to develop dedicated approaches.</p><p>The dominant paradigm in few-shot learning builds upon meta-learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>, which is formulated as a principle to learn how to adapt to new learning problems. These approaches split a large annotated corpus into classification tasks, and the goal is to transfer knowledge across tasks in order to improve generalization. While the meta-learning principle seems appealing for few-shot learning, its empirical benefits have not been clearly established yet. There is indeed strong evidence <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23</ref>] that train-ing CNNs from scratch using meta-learning performs substantially worse than if CNN features are trained in a standard fashion-that is, by minimizing a classical loss function relying on corpus annotations; on the other hand, learning only the last layer with meta-learning has been found to produce better results <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>. Then, it was recently shown in <ref type="bibr" target="#b4">[5]</ref> that simple distance-based classifiers could achieve similar accuracy as meta-learning approaches.</p><p>Our paper goes a step further and shows that metalearning-free approaches can be improved and significantly outperform the current state of the art in few-shot learning. Our angle of attack consists of using ensemble methods to reduce the variance of few-shot learning classifiers, which is inevitably high given the small number of annotations. Given an initial medium-sized dataset (following the standard setting of few-shot learning), the most basic ensemble approach consists of first training several CNNs independently before freezing them and removing the last prediction layer. Then, given a new class (with few annotated samples), we build a mean centroid classifier for each network and estimate class probabilities-according to a basic probabilistic model-of test samples based on the distance to the centroids <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. The obtained probabilities are then averaged over networks, resulting in higher accuracy.</p><p>While we show that the basic ensemble method where networks are trained independently already performs well, we introduce penalty terms that allow the networks to cooperate during training, while encouraging enough diversity of predictions, as illustrated in <ref type="figure">Figure 1</ref>. The motivation for cooperation is that of easier learning and regularization, where individual networks from the ensemble can benefit from each other. The motivation for encouraging diversity is classical for ensemble methods <ref type="bibr" target="#b5">[6]</ref>, where a collection of weak learners making diverse predictions often performs better together than a single strong one. Whereas these two principles seem in contradiction with each other at first sight, we show that both principles are in fact useful and lead to significantly better results than the basic ensemble method. Finally, we also show that a single network trained by distillation <ref type="bibr" target="#b13">[14]</ref> to mimic the behavior of the ensemble also performs well, which brings a significant speed-up at test time. In summary, our contributions are three-fold:</p><p>• We introduce mechanisms to encourage cooperation and diversity for learning an ensemble of networks.</p><p>We study these two principles for few-shot learning and characterize the regimes where they are useful.</p><p>• We show that it is possible to significantly outperform current state-of-the-art techniques for few-shot classification without using meta-learning.</p><p>• As a minor contribution, we also show how to distill an ensemble into a single network with minor loss in accuracy, by using additional unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we discuss related work on few-shot learning, meta-learning, and ensemble methods.</p><p>Few-shot classification. Typical few-shot classification problems consist of two parts called meta-training and meta-testing <ref type="bibr" target="#b4">[5]</ref>. During the meta-training stage, one is given a large-enough annotated dataset, which is used to train a predictive model. During meta-testing, novel categories are provided along with few annotated examples, and we evaluate the capacity of the predictive model to retrain or adapt, and then generalize on these new classes.</p><p>Meta-learning approaches typically sample few-shot learning classification tasks from the meta-training dataset, and train a model such that it should generalize on a new task that has been left aside. For instance, in <ref type="bibr" target="#b8">[9]</ref> a "good network initialization" is learned such that a small number of gradient steps on a new problem is sufficient to obtain a good solution. In <ref type="bibr" target="#b23">[24]</ref>, the authors learn both the network initialization and an update rule (optimization model) represented by a Long-Term-Short-Memory network (LSTM). Inspired by few-shot learning strategies developed before deep learning approaches became popular <ref type="bibr" target="#b20">[21]</ref>, distancebased classifiers based on the distance to a centroid were also proposed, e.g., prototypical networks <ref type="bibr" target="#b30">[31]</ref>, or more sophisticated classifiers with attention <ref type="bibr" target="#b32">[33]</ref>. All these methods consider a classical backbone network, and train it from scratch using meta-learning.</p><p>Recently, these meta-learning were found to be suboptimal <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, better results were obtained by training the network on the classical classification task using the meta-training data in a first step, and then only fine-tuning with meta-learning in a second step <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Others such as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> simply freeze the network obtained in the first step, and train a simple prediction layer with meta-learning, which results in similar performance. Finally, the paper <ref type="bibr" target="#b4">[5]</ref> demonstrates that simple baselines without meta-learning-based on distancebased classifiers-work equally well. Our paper pushes such principles even further and shows that by appropriate variance-reduction techniques, these approaches can significantly outperform the current state of the art.</p><p>Ensemble methods. It is well known that ensemble methods reduce the variance of estimators and subsequently may improve the quality of prediction <ref type="bibr" target="#b9">[10]</ref>. To gain accuracy from averaging, various randomization or data augmentation techniques are typically used to encourage a high diversity of predictions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. While individual classifiers of the ensemble may perform poorly, the quality of the average prediction turns out to be sometimes surprisingly high.</p><p>Even though ensemble methods are costly at training time for neural networks, it was shown that a sin-gle network trained to mimic the behavior of the ensemble could perform almost equally well <ref type="bibr" target="#b13">[14]</ref>-a procedure called distillation-thus removing the overhead at test time. To improve the scalability of distillation in the context of highly-parallelized implementations, an online distillation procedure is proposed in <ref type="bibr" target="#b0">[1]</ref>. There, each network is encouraged to agree with the averaged predictions made by other networks of the ensemble, which results in more stable models. The objective of our work is however significantly different. The form of cooperation they encourage between networks is indeed targeted to scalability and stability (due to industrial constraints), but online distilled networks do not necessarily perform better than the basic ensemble strategy. Our goal, on the other hand, is to improve the quality of prediction and do better than basic ensembles.</p><p>To this end, we encourage cooperation in a different manner, by encouraging predictions between networks to match in terms of class probabilities conditioned on the prediction not being the ground truth label. While we show that such a strategy alone is useful in general when the number of networks is small, encouraging diversity becomes crucial when this number grows. Finally, we show that distillation can help to reduce the computational overhead at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we present our approach for few-shot classification, starting with preliminary components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mean-centroid classifiers</head><p>We now explain how to perform few-shot classification with a fixed feature extractor and a mean centroid classifier.</p><p>Few-shot classification with prototype classifier. During the meta-training stage, we are given a dataset D b with annotations, which we use to train a prediction function f θ represented by a CNN. Formally, after training the CNN on D b , we remove the final prediction layer and use the resulting vectorf θ (x) as a set of visual features for a given image x. The parameters θ represent the weights of the network, which are frozen after this training step.</p><p>During meta-testing, we are given a new dataset</p><formula xml:id="formula_0">D q = {x i , y i } nk i=1 ,</formula><p>where n is a number of new categories and k is the number of available examples for each class. The (x i , y i )'s represent image-label pairs. Then, we build a mean centroid classifier, leading to the class prototypes</p><formula xml:id="formula_1">c j = 1 k k i=1f θ (x i ), j = 1, ..., n.<label>(1)</label></formula><p>Finally, a test sample x is assigned to the nearest centroid's class. Simple mean-centroid classifiers have proven to be effective in the context of few-shot classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, which is confirmed in the following experiment.</p><p>Motivation for mean-centroid classifier. We report here an experiment showing that a more complex model than <ref type="bibr" target="#b0">(1)</ref> does not necessarily lead to better results for few-shot learning. Consider indeed a parametrized version of (1):</p><formula xml:id="formula_2">c j = nk i=1 α j if θ (x i ), j = 1, ..., n,<label>(2)</label></formula><p>where the weights α j i can be learned with gradient descent by maximizing the likelihood of the probabilistic model</p><formula xml:id="formula_3">p j (y = l|x) = exp(−d(f θ (x), c l )) n j=1 exp(−d(f θ (x), c j )<label>(3)</label></formula><p>where d(·, ·) is a distance function, such as Euclidian distance or negative cosine similarity. Since the coefficients are learned from data and not set arbitrarily to 1/k as in <ref type="formula" target="#formula_1">(1)</ref>, one would potentially expect this method to produce better classifiers if appropriately regularized. When we run the evaluation of the aforementioned classifiers on 1000 5shot learning tasks sampled from miniImagenet-test (see experimental section for details about this dataset), we get similar results on average: 77.28 ± 0.46% for (1) vs.</p><p>77.01 ± 0.50% for <ref type="formula" target="#formula_2">(2)</ref>, confirming that learning meaningful parameters in this very-low-sample regime is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning ensembles of deep networks</head><p>During meta-training, one needs to minimize the following loss function over a training set {x i , y i } m i=1 :</p><formula xml:id="formula_4">L(θ) = 1 m m i=1 (y i , σ(f θ (x i ))) + λ θ 2 2 ,<label>(4)</label></formula><p>where f θ is a CNN as before. The cost function (·, ·) is the cross-entropy between ground-truth labels and predicted class probabilities p = σ(f θ (x)), where σ is the normalized exponential function, and λ is a weight decay parameter.</p><p>When training an ensemble of K networks f θ k independently, one would solve (4) for each network separately. While these terms may look identical, solutions provided by deep neural networks will typically differ when trained with different initializations and random seeds, making ensemble methods appealing in this context.</p><p>In this paper, we are interested in ensemble of networks, but we also want to model relationships between its members; this may be achieved by considering a pairwise penalty function ψ, leading to the joint formulation: <ref type="bibr" target="#b4">(5)</ref> whereθ is the vector obtained by concatenating all the parameters θ j . By carefully designing the function ψ and setting up appropriately the parameter γ, it is possible to achieve desirable properties of the ensemble, such as diversity of predictions or collaboration during training.</p><formula xml:id="formula_5">L(θ) = K j=1 1 n n i=1 (y i , σ(f θj (x i ))) + λ θ j 2 2 + γ n(K − 1) n i=1 K j,l j =l ψ(y i , f θj (x i ), f θ l (x i )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encouraging diversity and cooperation</head><p>To reduce the high variance of few-shot learning classifiers, we use ensemble methods trained with a particular interaction function ψ, as in <ref type="bibr" target="#b4">(5)</ref>. Then, once the parameters θ j have been learned during meta-training, classification in meta-testing is performed by considering a collection of K mean-centroid classifiers associated to the basic probabilistic model presented in Eq. <ref type="bibr" target="#b2">(3)</ref>. Given a test image, the K class probabilities are averaged. Such a strategy was found to perform empirically better than a voting scheme.</p><p>As we show in the experimental section, the choice of pairwise relationship function ψ significantly influences the quality of the ensemble. Here, we describe three different strategies, which all provide benefits in different regimes, starting by a criterion encouraging diversity of predictions.</p><p>Diversity. One way to encourage diversity consists of introducing randomization in the learning procedure, e.g., by using data augmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> or various initializations. Here, we also evaluate the effect of an interaction function ψ that acts directly on the network predictions. Given an image x, two models parametrized by θ i and θ j respectively lead to class probabilities p i = σ(f θi (x)) and p j = σ(f θj (x)). During training, p i and p j are naturally encouraged to be close to the assignment vector e y in {0, 1} d with a single non-zero entry at position y, where y is the class label associated to x and d is the number of classes.</p><p>From <ref type="bibr" target="#b13">[14]</ref>, we know that even though only the largest entry of p i or p j is used to make predictions, other entriestypically not corresponding to the ground truth label ycarry important information about the network. It becomes then natural to consider the probabilitiesp i andp j conditioned on not being the ground truth label. Formally, these are obtained by setting to zero the entry y in p i and p j renormalizing the corresponding vectors such that they sum to one. Then, we consider the following diversity penalty</p><formula xml:id="formula_6">φ(p i ,p j ) = cos(p i ,p j ).<label>(6)</label></formula><p>When combined with the loss function, the resulting formulation encourages the networks to make the right prediction according to the ground-truth label, but then they are also encouraged to make different second-best, third-best, and so on, choice predictions (see <ref type="figure">Figure 1</ref>). This penalty turns out to be particularly effective when the number of networks is large, as shown in the experimental section. It typically worsens the performance of individual classifiers on average, but make the ensemble prediction more accurate.</p><p>Cooperation. Apparently opposite to the previous principle, encouraging the conditional probabilitiesp i to be similar-though with a different metric-may also improve the quality of prediction by allowing the networks to cooperate for better learning. Our experiments show that such a principle alone may be effective, but it appears to be mostly useful when the number of training networks is small, which suggests that there is a trade-off between cooperation and diversity that needs to be found. Specifically, our experiments show that using the negative cosine-in other words, the opposite of (6)-is ineffective. However, a penalty such as the symmetrized KLdivergence turned out to provide the desired effect:</p><formula xml:id="formula_7">φ(p i ,p j ) = 1 2 (KL(p i ||p j ) + KL(p j ||p i )).<label>(7)</label></formula><p>By using this penalty, we managed to obtain more stable and faster training, resulting in better performing individual networks, but also-perhaps surprisingly-a better ensemble. Unfortunately, we also observed that the gain of ensembling diminishes with the number of networks in the ensemble since the individual members become too similar.</p><p>Robustness and cooperation. Given experiments conducted with the two previous penalties, a trade-off between cooperation and diversity seems to correspond to two regimes (low vs. high number of networks). This motivated us to develop an approach designed to achieve the best trade-off. When considering the cooperation penalty <ref type="formula" target="#formula_7">(7)</ref>, we try to increase diversity of prediction by several additional means. i) We randomly drop some networks from the ensemble at each training iteration, which causes the networks to learn on different data streams and reduces the speed of knowledge propagation. ii) We introduce Dropout within each network to increase randomization. iii) We feed each network with a different (crop, color) transformation of the same image, which makes the ensemble more robust to input image transformations. Overall, this strategy was found to perform best in most scenarios (see <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ensemble distillation</head><p>As most ensemble methods, our ensemble strategy introduces a significant computional overhead at training time. To remove the overhead at test time, we use a variant of knowledge distillation <ref type="bibr" target="#b13">[14]</ref> to compress the ensemble into a single network f w . Given the meta-training dataset D b , we consider the following cost function on example (x, y):</p><formula xml:id="formula_8">(x, y) = (1 − α) ·ˆ (e y , σ(f w (x))) − α · T 2 ·ˆ 1 K K k=1 σ f θ k (x) T , σ fw(x) T ,<label>(8)</label></formula><p>where,ˆ is cross-entropy, e y is a one-hot embedding of the true label y. The second term performs distillation with parameter T (see <ref type="bibr" target="#b13">[14]</ref>). It encourages the single model f w to be similar to the average output of the ensemble. In our experiments, we are able to obtain a model with performance relatively close to that of the ensemble (see <ref type="bibr">Section 4)</ref>.</p><p>Modeling out-of-distribution behavior. When distillation is performed on the dataset D b , the network f w mimics the behavior of the ensemble on a specific data distribution. However, new categories are introduced at test time. Therefore, we also tried distillation by using additional unnannotated data, which yields slightly better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We now present experiments to study the effect of cooperation and diversity for ensemble methods, and start with experimental and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. We use mini-ImageNet <ref type="bibr" target="#b23">[24]</ref> and tiered-ImageNet <ref type="bibr" target="#b24">[25]</ref> which are derived from the original Ima-geNet <ref type="bibr" target="#b27">[28]</ref> dataset and Caltech-UCSD Birds (CUB) 200-2011 <ref type="bibr" target="#b33">[34]</ref>. Mini-ImageNet consists of 100 categories-64 for training, 16 for validation and 20 for testing-with 600 images each. Tiered-ImageNet is also a subset of Im-ageNet that includes 351 class for training, 97 for validation and 160 for testing which is 779,165 images in total. The splits are chosen such that the training classes are sufficiently different from the test ones, unlike in mini-ImageNet. The CUB dataset consists of 11,788 images of birds of more than 200 species. We adopt train, val, and test splits from <ref type="bibr" target="#b34">[35]</ref>, which were originally created by randomly splitting all 200 species in 100 for training, 50 for validation, and 50 for testing.</p><p>Evaluation. In few-shot classification, the test set is used to sample N 5-way classification problems, where only k examples of each category are provided for training and 15 for evaluation. We follow <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> and test our algorithms for k = 1 and 5 and N is set to 1 000. Each time, classes and corresponding train/test examples are sampled at random. For all our experiments we report the mean accuracy (in %) over 1 000 tasks and 95% confidence interval.</p><p>Implementation details. For all experiments, we use the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with an initial learning rate 10 −4 , which is decreased by a factor 10 once during training when no improvement in validation accuracy is observed for p consecutive epochs. For mini-ImageNet, we use p = 10, and 20 for the CUB dataset. When distilling an ensemble into one network, p is doubled. We use random crops and color augmentation during training as well as weight decay with parameter λ = 5 · 10 −4 . All experiments are conducted with the ResNet18 architecture <ref type="bibr" target="#b12">[13]</ref>, which allows us to train our ensembles of 20 networks on a single GPU. Input images are then re-scaled to the size 224 × 224, and organized in mini-batches of size 16. Validation accuracy is computed by running 5-shot evaluation on the validation set. During the meta-testing stage, we take central crops of size 224 × 224 from images and feed them to the feature extractor. No other preprocessing is used at test time. When building a mean centroid classifier, the distance d in <ref type="formula" target="#formula_3">(3)</ref> is computed as the negative cosine similarity <ref type="bibr" target="#b30">[31]</ref>, which is rescaled by a factor 10. For a fair comparison, we have also evaluated ensembles composed of ResNet18 <ref type="bibr" target="#b12">[13]</ref> with input image size 84 × 84 and WideResNet28 <ref type="bibr" target="#b36">[37]</ref> with input size 80 × 80. All details are reported in Appendix. For reproducibility purposes, our implementation will be made available at http://thoth.inrialpes.fr/ research/fewshot_ensemble/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ensembles for Few-Shot Classification</head><p>In this section, we study the effect of ensemble training with pairwise interaction terms that encourage cooperation or diversity. For that purpose, we analyze the link between the size of ensembles and their 1-and 5-shot classification performance on the mini-ImageNet and CUB datasets.</p><p>Details about the three strategies. When models are trained jointly, the data stream is shared across all networks and weight updates happen simultaneously. This is achieved by placing all models on the same GPU and optimizing the loss <ref type="bibr" target="#b4">(5)</ref>. When training a diverse ensemble, we use the cosine function <ref type="bibr" target="#b5">(6)</ref> and selected the parameter γ = 1 that performed best on the validation set among the tested values (10 i , for i = −2, . . . , 2) for n = 5 and n = 10 networks. Then, this value was kept for other values of n. To enforce cooperation between networks, we use the symmetrized KL function <ref type="bibr" target="#b6">(7)</ref> and selected the parameter γ = 10 in the same manner. Finally, the robust ensemble strategy is trained with the cooperation relationship penalty and the same parameter γ, but we use Dropout with probability 0.1 before the last layer; each of the network is dropped from the ensemble with probability 0.2 at every iteration; different networks receive different transformation of the same image, i.e. different random crops and color augmentation.</p><p>Results. <ref type="table">Table 1</ref> and <ref type="table">Table A1</ref> of Appendix summarize the few-shot classification accuracies of ensembles trained with our strategies and compare with basic ensembles. On the mini-ImageNet dataset, the results for 1-and 5-shot classification are consistent with each other. Training with cooperation allows smaller ensembles (n ≤ 5) to perform better, which leads to higher individual accuracy of the ensemble members, as seen in <ref type="figure">Figure 2</ref>. However, when n ≥ 10, cooperation is less effective, as opposed to the diversity strategy, which benefits from larger n. As we can see from <ref type="figure">Figure 2</ref>, individual members of the ensemble become worse, but the ensemble accuracy improves substantially. Finally, the robust strategy seems to perform best for all values of n in almost all settings. The situation for the CUB dataset is similar, although we notice that robust ensembles perform similarly as the diversity strategy for n = 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distilling an ensemble</head><p>We distill robust ensembles of all sizes to study knowledge transferability with growing ensemble size. To do so, we use the meta-training dataset and optimize the loss (8) with parameters T = 10 and α = 0.8. For the strategy using external data, we randomly add at each iteration 8 images (without annotations) from the COCO <ref type="bibr" target="#b17">[18]</ref> dataset to the 16 annotated samples from the meta-training data. Those images contribute only to the distillation part of the loss <ref type="bibr" target="#b7">(8)</ref>. <ref type="table">Table 1</ref> and <ref type="table">Table A1</ref> of Appendix display model accuracies for mini-ImageNet and CUB datasets respectively. For 5-shot classification on mini-ImageNet, the difference between ensemble and its distilled version is rather low (around 1%), while adding extra non-annotated data helps reducing this gap. Surprisingly, 1-shot classification accuracy is slightly higher for distilled models than for their corresponding full ensembles. On the CUB dataset, distilled models stop improving after n = 5, even though the performance of full ensembles keeps growing. This seems to indicate that the capacity of the single network may have been reached, which suggests using a more complex architecture here. Consistently with such hypothesis, adding extra data is not as helpful as for mini-ImageNet, most likely because data distributions of COCO and CUB are more different.</p><p>In <ref type="table" target="#tab_1">Tables 2, 3</ref>, we also compare the performance of our distilled networks with other baselines from the literature, including current state-of-the-art meta-learning approaches, showing that our approach does significantly better on the mini-ImageNet <ref type="bibr" target="#b23">[24]</ref> and tiered-ImageNet <ref type="bibr" target="#b24">[25]</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Study of relationship penalties</head><p>There are many possible ways to model relationship between the members of an ensemble. In this subsection, we study and discuss such particular choices.</p><p>Input to relationship function. As noted by <ref type="bibr" target="#b13">[14]</ref>, class probabilities obtained by the softmax layer of a network seem to carry a lot of information and are useful for distillation. However, after meta-training, such probabilities are often close to binary vectors with a dominant value associated to the ground-truth label. To make small values more noticeable, distillation uses a parameter T , as in <ref type="bibr" target="#b7">(8)</ref>. Given such a class probability computed by a network, we experimented such a strategy consisting of introducing new probabilitiesp = σ(p/T ), where the contributions of non groundtruth values are emphasized. When used within our diversity <ref type="bibr" target="#b5">(6)</ref> or cooperation <ref type="formula" target="#formula_7">(7)</ref> penalties, we however did not see any improvement over the basic ensemble method. Instead, we found that computing the class probabilities conditioned on not being the ground truth label, as explained in Section 3.3, would perform much better. This is illustrated on the following experiment with two network ensembles of size n = 5. We enforce similarity on the full probability vectors in the first one, computed with softmax at T = 10 following <ref type="bibr" target="#b0">[1]</ref>, and with conditionally non-ground-truth probabilities for the second one as defined in Section 3.3. When using the cooperation training formulation, the second strategy turns out to perform about 1% better than the first one (79.79 % vs 80.60%), when tested on MiniImageNet. Similar observations have been made using the diversity criterion. In comparison, the basic ensemble method without interactions achieves about 80%.  Choice of relationship function. In principle, any similarity measure could be used to design a penalty encour-Method Input size Network 5-shot 1-shot TADAM <ref type="bibr" target="#b21">[22]</ref> 84 <ref type="table">Table 3</ref>: Comparison of distilled ensembles to other methods on 1-and 5-shot tiered-ImageNet <ref type="bibr" target="#b24">[25]</ref>. To evaluate our methods we performed 5 000 independent experiments on tiered-ImageNet-test and report the average accuracy with 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot</head><p>aging cooperation. Here, we show that in fact, selecting the right criterion for comparing probability vectors (cosine similarity, L2 distance, symmetrized KL divergence), is crucial depending on the desired effect (cooperation or diversity). In <ref type="table">Table 4</ref>, we perform such a comparison for an ensemble with n = 5 networks on the MiniImageNet dataset for a 5−shot classification task, when plugging the above function in the formulation 5, with a specific sign. The parameter γ for each experiment is chosen such that the performance on the validation set is maximized. When looking for diversity, the cosine similarity performs slightly better than negative L2 distance, although the Purpose (Sign)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L2</head><p>-cos KL sim Cooperation (+) 80.14 ± 0.43 80.29 ± 0.44 80.72 ± 0.42 Diversity <ref type="bibr">(-)</ref> 80.54 ± 0.44 80.82 ± 0.42 79.81 ± 0.43 <ref type="table">Table 4</ref>: Evaluating different relationship criteria on mini-Imagenet 5-shot The first row indicates which function was used as a relationship criteria, the first column indicates for which purpose the function is used and the corresponding sign. To evaluate our methods, we performed 1 000 independent experiments on CUB-test and report the average accuracy with 95% confidence intervals. All ensembles are trained on mini-ImageNet-train.</p><p>accuracies are within error bars. Using negative KL sim with various γ was either not distinguishable from independent training or was hurting the performance for larger values of γ (not reported on the table). As for cooperation, positive KL sim gives better results than L2 distance or negative cosine similarity. We believe that this behavior is due to important difference in the way these functions compare small values in probability vectors. While negative cosine or L2 losses would penalize heavily the largest difference, KL sim concentrates on values that are close to 0 in one vector and are greater in the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance under domain shift</head><p>Finally, we evaluate the performance of ensemble methods under domain shift. We proceed by meta-training the models on the mini-ImageNet training set and evaluate the model on the CUB-test set. The following setting was first proposed by <ref type="bibr" target="#b4">[5]</ref> and aims at evaluating the performance of algorithms to adapt when the difference between training and testing distributions is large. To compare to the results reported in the original work, we adopt their CUB test split. <ref type="table">Table 5</ref> compares our results to the ones listed in <ref type="bibr" target="#b4">[5]</ref>. We can see that neither the full robust ensemble nor its distilled version are able to do better than training a linear classifier on top of a frozen network. Yet, it does significantly better than distance-based approaches (denoted by cosine classifier in the table). However, if a diverse ensemble is used, it achieves the best accuracy. This is not surprising and highlights the importance of having diverse models when ensembling weak classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we show that distance-based classifiers for few-shot learning suffer from high variance, which can be significantly reduced by using an ensemble of classifiers. Unlike traditional ensembling paradigms where diversity of predictions is encouraged by various randomization and data augmentation techniques, we show that encouraging the networks to cooperate during training is also important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>mini-ImageNet → CUB MatchingNet <ref type="bibr" target="#b32">[33]</ref> 53.07 ± 0.74 MAML <ref type="bibr" target="#b8">[9]</ref> 51.34 ± 0.72 ProtoNet <ref type="bibr" target="#b30">[31]</ref> 62.02 ± 0.70 Linear Classifier <ref type="bibr" target="#b4">[5]</ref> 65.57 ± 0.70 Cosine Classifier <ref type="bibr" target="#b4">[5]</ref> 62.04 ± 0.76  64.23 ± 0.58 <ref type="bibr">Robust 20 Full (ours)</ref> 65.04 ± 0.57 <ref type="bibr">Diverse 20 Full (ours)</ref> 66.17 ± 0.55 <ref type="table">Table 5</ref>: 5-shot classification accuracy under domain shift. The last two models are full ensembles and should not be directly compared with the rest of the table. We performed 1 000 independent experiments on CUB-test from <ref type="bibr" target="#b4">[5]</ref> and report the average and confidence interval here. All ensembles are trained on mini-ImageNet.</p><p>The overall performance of a single network obtained by distillation (with no computational overhead at test time) leads to state-of-the-art performance for few shot learning, without relying on the meta-learning paradigm. While such a result may sound negative for meta-learning approaches, it may simply mean that a lot of work remains to be done in this area to truly learn how to learn or to adapt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>In this section, we elaborate on training, testing and distillation details of the proposed ensemble methods for different datasets, network architectures and input image resolution.</p><p>Training ResNet18 on 84x84 images on mini-ImageNet. For all experiments, we use ResNet18 with input image size 84x84, train with the Adam optimizer with an initial learning rate 3 · 10 −4 , which is decreased by a factor 10 once during training when no improvement in validation accuracy is observed for p consecutive epochs. We use p = 20 for training individual models, p = 30 for training ensembles and when distilling the model. When distilling an ensemble into one network, p is doubled. We use random crops and color augmentation during training as well as weight decay with parameter λ = 5 · 10 −4 . At training time we use random crop, color transformation and adding random noise as data augmentation. During the meta-testing stage, we take central crops of size 224 × 224 from images and feed them to the feature extractor. No other preprocessing is used at test time. The parameters used in distillation are the same as in Section 4.3 of the paper.</p><p>Training WideResNet28 on 80x80 images on mini-ImageNet. For all experiments, we use WideResNet28 with input image size 80x80, train with the Adam optimizer with an initial learning rate 1 · 10 −4 , which is decreased by a factor 10 once during training when no improvement in validation accuracy is observed for p consecutive epochs. We use p = 20 for training individual models, p = 30 for training ensembles and when distilling the model. When distilling an ensemble into one network, p is doubled. We use random crops and color augmentation during training as well as weight decay with parameter λ = 5 · 10 −4 . We also set a dropout rate inside convolutional blocks to be 0.5 as described in. At training time we use random crop and color transformation only as data augmentation. During the metatesting stage, we take central crops of size 80 × 80 from images and feed them to the feature extractor. No other preprocessing is used at test time. The parameters used in distillation are the same as in Section 4.3 of the paper. Here, the maximal ensemble size we evaluated is 10 and not 20 due to memory limitations on available GPUs. Therefore, to construct an ensemble of size 20 we merge two ensembles of size 10, that were trained independently.</p><p>Training ResNet18 on 224x224 images on tiered-ImageNet For all experiments, we use ResNet18 with input image size 224x224, train with the Adam optimizer with an initial learning rate 3 · 10 −4 , which is decreased by a factor 10 once during training when no improvement in validation accuracy is observed for p consecutive epochs. We use p = 20 for training individual models, ensembles and for distilation. We use random crops and color augmentation during training as well as weight decay with parameter λ = 1 · 10 −4 . At training time we use random crop and color transformation. During the meta-testing stage, we take central crops of size 224 × 224 from images and feed them to the feature extractor. No other preprocessing is used at test time. The parameters used in distillation are the same as in Section 4.3 of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Results</head><p>In this section we report and analyze the performance of different ensemble types depending on their size for different network architectures and input image resolutions.</p><p>Few-shot Classification with ResNet18 on 224x224 images on CUB. The results for 1-and 5-shot classification on CUB are presented in <ref type="table">Table A1</ref>. Training details and Figure summary of the results are discussed in Experimental section of the paper.</p><p>Few-shot Classification with ResNet18 on 84x84 images on mini-ImageNet. The results for 1-and 5-shot classification on MiniImageNet are presented in <ref type="table">Table A3</ref> and summarized in <ref type="figure">Figure A1</ref>. We can see that Cooperation training is the most successful here for all ensemble sizes &lt; 20 and other training strategies that introduce diversity tend to perform worse. This happens because single networks are far from overfitting the training set (as opposed to the case with 224x224 input size) and forcing diversity acts as harmful regularization. In contrary, cooperation training enforces useful learning signal and helps ensemble members achieve higher accuracy. Only for n = 20 where diversity matters more, robust ensembles perform the best.</p><p>Few-shot Classification with WideResNet28 on 80x80 images on mini-ImageNet. Results for 1-and 5-shot classification on MiniImageNet are presented in <ref type="table" target="#tab_1">Table A2</ref> and summarized in <ref type="figure">Figure A1</ref>. In this case we can see again that Diverse training does not help since the networks do not memorize the training set. Robust ensembles outperform other training regimes emphasizing the importance of the proposed solution that generalizes across architectures.  <ref type="table">Table A1</ref>: Few-shot classification accuracy on CUB. The first column gives the type of ensemble and the top row indicates the number of networks in an ensemble. Here, dist means that an ensemble was distilled into a single network, and '++' indicates that extra unannotated images were used for distillation. We performed 1000 independent experiments on CUB-test and report the average with 95% confidence interval. All networks are trained on CUB-train set.  <ref type="table" target="#tab_1">Table A2</ref>: Few-shot classification accuracy on MiniImageNet, using ResNet18 and 84x84 image size. The first column gives the strategy, the top row indicates the number N of networks in an ensemble. Here, dist means that an ensemble was distilled into a single network, and '++' indicates that extra unannotated images were used for distillation. We performed 1 000 independent experiments on MiniImageNet-test and report the average with 95% confidence interval. All networks are trained on MiniImageNet-train set.  <ref type="table">Table A3</ref>: Few-shot classification accuracy on MiniImageNet, using WideResNet28 and 80x80 image size. The first column gives the strategy, the top row indicates the number N of networks in an ensemble. Here, dist means that an ensemble was distilled into a single network, and '++' indicates that extra unannotated images were used for distillation. We performed 1 000 independent experiments on MiniImageNet-test and report the average with 95% confidence interval. All networks are trained on MiniImageNet-train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-shot</head><p>(a) ResNet18 with 84x84 input (b) WideResNet28 with 80x80 input <ref type="figure">Figure A1</ref>: Dependency of ensemble accuracy on network architecture and input size for different ensemble strategies (one for each color) and various numbers of networks on MiniImageNet 5-shots classification. Solid lines give the ensemble accuracy after aggregating predictions. The average performance of single models from the ensemble is plotted with a dashed line. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( a )Figure 2 :</head><label>a2</label><figDesc>MiniImageNet 5-shots (b) CUB 5-shots Accuracies of different ensemble strategies (one for each color) for various numbers of networks. Solid lines give the ensemble accuracy after aggregating predictions. The average performance of single models from the ensemble is plotted with a dashed line. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>± 0.46 78.27 ± 0.45 79.38 ± 0.43 80.02 ± 0.43 80.30 ± 0.43 80.57 ± 0.42 Diversity 77.28 ± 0.46 78.34 ± 0.46 79.18 ± 0.43 79.89 ± 0.43 80.82 ± 0.42 81.18 ± 0.42 Cooperation 77.28 ± 0.46 78.67 ± 0.46 80.20 ± 0.42 80.60 ± 0.43 80.72 ± 0.42 80.80 ± 0.42 Robust 77.28 ± 0.46 78.71 ± 0.45 80.26 ± 0.43 81.00 ± 0.42 81.22 ± 0.43 81.59 ± 0.42 Distilled Ensembles Robust-dist − 79.44 ± 0.44 79.84 ± 0.44 80.01 ± 0.42 80.25 ± 0.44 80.63 ± 0.42 Robust-dist++ − 79.16 ± 0.46 80.00 ± 0.44 80.25 ± 0.42 80.35 ± 0.44 81.19 ± 0.43 ± 0.62 60.04 ± 0.60 60.83 ± 0.63 61.34 ± 0.61 61.93 ± 0.61 62.06 ± 0.61 Diversity 58.71 ± 0.63 59.95 ± 0.61 61.27 ± 0.62 61.43 ± 0.61 62.23 ± 0.61 62.47 ± 0.62 Cooperation 58.71 ± 0.62 60.20 ± 0.61 61.46 ± 0.61 61.61 ± 0.61 62.06 ± 0.61 62.12 ± 0.62 Robust 58.71 ± 0.62 60.91 ± 0.62 62.36 ± 0.60 62.70 ± 0.61 62.97 ± 0.62 63.95 ± 0.61 Distilled Ensembles Robust-dist − 62.33 ± 0.62 62.64 ± 0.60 63.14 ± 0.61 63.01 ± 0.62 63.06 ± 0.61 Robust-dist ++ − 62.07 ± 0.62 62.81 ± 0.60 63.39 ± 0.61 63.20 ± 0.62 63.73 ± 0.</figDesc><table><row><cell cols="2">Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Independent</cell><cell></cell><cell cols="3">77.28 1-shot</cell><cell></cell><cell></cell></row><row><cell cols="2">Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell cols="3">Independent 58.71 Method Input size Network</cell><cell>5-shot</cell><cell>1-shot</cell><cell></cell><cell></cell></row><row><cell>TADAM [22]</cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>62 Table 1: Few-shot classification accuracy on mini-ImageNet. The first column gives the strategy, the top row indicates the number N of networks in an ensemble. Here, dist means that an ensemble was distilled into a single network, and '++' indicates that extra unannotated images were used for distillation. We performed 1 000 independent experiments on mini-ImageNet-test and report the average with 95% confidence interval. All networks are trained on mini-ImageNet-train set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of distilled ensembles to other methods on 1-and 5-shot miniImageNet. The two last columns display the accuracy on 1-and 5-shot learning tasks. To evaluate our methods we performed 1 000 independent experiments on MiniImageNet-test and report the average and 95% confidence interval. Here, '++' means that extra non-annotated images were used to perform distillation. The last model is a full ensemble and should not be directly compared to the rest of the table.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 0.49 81.34 ± 0.46 82.57 ± 0.46 83.16 ± 0.45 83.80 ± 0.45 83.95 ± 0.46 Diversity 79.47 ± 0.49 81.09 ± 0.45 82.23 ± 0.46 82.91 ± 0.46 84.30 ± 0.44 85.20 ± 0.43 Cooperation 79.47 ± 0.49 81.69 ± 0.46 82.95 ± 0.47 83.43 ± 0.47 84.01 ± 0.44 84.26 ± 0.44 Robust 79.47 ± 0.49 82.90 ± 0.46 83.36 ± 0.46 83.62 ± 0.45 84.47 ± 0.46 84.62 ± 0.44 Distilled Ensembles Robust-dist − 82.72 ± 0.47 82.95 ± 0.46 83.27 ± 0.46 83.61 ± 0.46 83.57 ± 0.45 Robust-dist++ − 82.53 ± 0.48 83.04 ± 0.45 83.37 ± 0.46 83.22 ± 0.46 83.21 ± 0.44 ± 0.73 66.60 ± 0.72 67.64 ± 0.71 68.07 ± 0.70 68.93 ± 0.70 69.64 ± 0.69 Diversity 64.25 ± 0.73 65.99 ± 0.71 66.71 ± 0.72 68.19 ± 0.71 69.35 ± 0.70 70.07 ± 0.70 Cooperation 64.25 ± 0.73 67.21 ± 0.71 67.93 ± 0.70 68.22 ± 0.70 68.69 ± 0.70 68.80 ± 0.68 Robust 64.25 ± 0.73 67.33 ± 0.71 68.01 ± 0.72 68.53 ± 0.70 68.59 ± 0.70 69.47 ± 0.69 Distilled Ensembles Robust-dist − 67.47 ± 0.71 67.29 ± 0.72 68.09 ± 0.70 68.71 ± 0.71 68.77 ± 0.71 Robust-dist++ − 67.01 ± 0.74 67.62 ± 0.72 68.68 ± 0.71 68.38 ± 0.70 68.68 ± 0.69</figDesc><table><row><cell>Full Ensemble</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Independent</cell><cell cols="3">79.47 1-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Independent</cell><cell>64.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>± 0.51 73.24 ± 0.49 74.29 ± 0.48 74.89 ± 0.47 75.69 ± 0.47 75.93 ± 0.47 Diversity 70.59 ± 0.51 72.35 ± 0.47 73.44 ± 0.49 74.81 ± 0.48 75.47 ± 0.48 76.36 ± 0.47 Cooperation 70.59 ± 0.51 74.04 ± 0.47 74.81 ± 0.47 76.37 ± 0.48 76.73 ± 0.48 76.50 ± 0.47 Robust 70.59 ± 0.51 72.92 ± 0.50 73.09 ± 0.43 75.69 ± 0.42 76.71 ± 0.47 76.90 ± 0.48 Distilled Ensembles Robust-dist − 73.04 ± 0.50 73.58 ± 0.49 74.35 ± 0.48 74.69 ± 0.49 75.24 ± 0.49 Robust-dist++ − 73.50 ± 0.49 74.17 ± 0.49 74.84 ± 0.49 75.12 ± 0.44 75.62 ± 0.48 ± 0.64 55.72 ± 0.60 56.85 ± 0.64 57.90 ± 0.63 58.21 ± 0.63 58.56 ± 0.61 Diversity 53.31 ± 0.64 54.61 ± 0.62 55.90 ± 0.62 57.06 ± 0.63 57.49 ± 0.62 58.93 ± 0.64 Cooperation 53.31 ± 0.64 55.80 ± 0.64 57.13 ± 0.63 58.18 ± 0.64 58.63 ± 0.63 58.73 ± 0.62 Robust 53.31 ± 0.64 55.95 ± 0.62 56.27 ± 0.64 58.51 ± 0.65 59.38 ± 0.65 59.48 ± 0.65 Distilled Ensembles Robust-dist − 56.84 ± 0.64 56.58 ± 0.65 57.13 ± 0.63 57.41 ± 0.65 58.11 ± 0.64 Robust-dist ++ − 56.53 ± 0.62 57.03 ± 0.64 57.48 ± 0.65 58.05 ± 0.63 58.67 ± 0.65</figDesc><table><row><cell>Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Independent</cell><cell cols="3">70.59 1-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>Independent</cell><cell>53.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>± 0.45 78.78 ± 0.45 79.26 ± 0.43 79.91 ± 0.44 80.12 ± 0.43 Diversity 77.54 ± 0.45 77.88 ± 0.45 79.15 ± 0.44 79.79 ± 0.44 80.18 ± 0.44 Cooperation 77.54 ± 0.45 78.96 ± 0.46 80.06 ± 0.44 80.58 ± 0.45 80.87 ± 0.43 Robust 77.54 ± 0.45 78.99 ± 0.45 80.12 ± 0.43 80.91 ± 0.43 81.72 ± 0.44 Distilled Ensembles Robust-dist − 79.44 ± 0.44 79.84 ± 0.44 80.01 ± 0.42 80.85 ± 0.43 Robust-dist++ − 79.16 ± 0.46 80.00 ± 0.44 80.25 ± 0.42 81.11 ± 0.43 ± 0.63 60.07 ± 0.62 60.58 ± 0.61 61.24 ± 0.63 62.05 ± 0.61 Diversity 59.02 ± 0.63 58.87 ± 0.62 60.63 ± 0.61 61.30 ± 0.62 62.28 ± 0.61 Cooperation 59.02 ± 0.63 60.22 ± 0.62 61.03 ± 0.61 62.07 ± 0.61 62.42 ± 0.61 Robust 59.02 ± 0.63 60.92 ± 0.62 62.03 ± 0.62 62.78 ± 0.61 63.39 ± 0.62 Distilled Ensembles Robust-dist − 61.07 ± 0.62 61.57 ± 0.61 62.24 ± 0.61 62.80 ± 0.62 Robust-dist ++ − 61.37 ± 0.62 62.01 ± 0.60 62.45 ± 0.62 63.25 ± 0.62</figDesc><table><row><cell>Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>Independent</cell><cell cols="2">77.54 1-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ensemble type</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>5</cell><cell>10</cell></row><row><cell>Independent</cell><cell>59.02</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the ERC grant number 714381 (SOLARIS project), the ERC advanced grant AL-LEGRO and grants from Amazon and Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A kernel perspective for regularizing deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00363</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Heuristics of instability and stabilization in model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2350" to="2383" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on multiple classifier systems</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distance-based image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2624" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodríguez López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="105" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
		<title level="m">Spectral norm regularization for improving the generalizability of deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
