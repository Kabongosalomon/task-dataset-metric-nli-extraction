<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Multi-modality Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xpeng</forename><surname>Motors</surname></persName>
						</author>
						<title level="a" type="main">Discriminative Multi-modality Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision is often used as a complementary modality for audio speech recognition (ASR), especially in the noisy environment where performance of solo audio modality significantly deteriorates. After combining visual modality, ASR is upgraded to the multi-modality speech recognition (MSR). In this paper, we propose a two-stage speech recognition model. In the first stage, the target voice is separated from background noises with help from the corresponding visual information of lip movements, making the model 'listen' clearly. At the second stage, the audio modality combines visual modality again to better understand the speech by a MSR sub-network, further improving the recognition rate. There are some other key contributions: we introduce a pseudo-3D residual convolution (P3D)-based visual front-end to extract more discriminative features; we upgrade the temporal convolution block from 1D ResNet with the temporal convolutional network (TCN), which is more suitable for the temporal tasks; the MSR sub-network is built on the top of Element-wise-Attention Gated Recurrent Unit (EleAtt-GRU), which is more effective than Transformer in long sequences. We conducted extensive experiments on the LRS3-TED and the LRW datasets. Our twostage model (audio enhanced multi-modality speech recognition, AE-MSR) consistently achieves the state-of-the-art performance by a significant margin, which demonstrates the necessity and effectiveness of AE-MSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2005.05592v2 [cs.CV] 13 May 2020</head><p>• 1DRN-TM-seq2seq: an AE-MSR network, where the audio enhancement (AE) sub-network (1DRN-AE) uses 1D ResNet as temporal convolution unit and outputs enhanced audio modality. The MSR sub-network of this network is TM-seq2seq.</p><p>• TCN-TM-seq2seq: an AE-MSR network, where the AE sub-network (TCN-AE) uses the temporal convolutional network (TCN) as temporal convolution unit. The MSR sub-network is TM-seq2seq.</p><p>• 1DRN-EG-seq2seq: an AE-MSR network, where the AE sub-network is 1DRN-AE and the MSR subnetwork is EG-seq2seq.</p><p>• TCN-EG-seq2seq: an AE-MSR network, where the AE sub-network is TCN-AE and the MSR subnetwork is EG-seq2seq.</p><p>In this section, all the models above use only audio modality at MSR stage. As shown in columns under VA in <ref type="table">Table 3</ref>, our AE networks can benefit other speech recognition models, for example at SNR of -5 dB, the WER is reduced from 87.9% of TM-seq2seq to 50.2% of 1DRN-TM-seq2seq and 49.5% of TCN-TM-seq2seq. The enhancement</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the book The Listening Eye: A Simple Introduction to the Art of Lip-reading <ref type="bibr" target="#b16">[17]</ref>, Clegg mentions that "When you are deaf you live inside a well-corked glass bottle. You see the entrancing outside world, but it does not reach you. After learning to lip read, you are still inside the bottle, but the cork has come out and the outside world slowly but surely comes in to you." Lip reading is an approach for people with hearing impairments to communicate with the world, so that they can interpret what other say by looking at the movements of lips <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref>. Lip reading is a dif-  <ref type="figure">Figure 1</ref>: Overview of the audio enhanced multi-modality speech recognition network (AE-MSR). Mag: magnitude. Different from other MSR methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36]</ref> with only single visual awareness, we firstly filter the voices of speakers and background noises with help from visual awareness. Then we combine visual awareness again for MSR to benefit speech recognition.</p><p>ficult skill for human to grasp and requires intensive training <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>. Lip reading is also an inexact art, because different characters may exhibit the similar lip movements (e.g. 'b' and 'p') <ref type="bibr" target="#b1">[2]</ref>. Consequently, several machine lip reading models are proposed to discriminate such subtle difference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref>. However they still suffer difficulties on extracting spatio-temporal features from the video.</p><p>Automatic lip reading becomes achievable due to rapid development of deep neural network in computer vision <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, and with help from large scale training datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. In addition to serving as a powerful solution to hearing impairment, lip reading can also contribute to audio speech recognition (ASR) in adversary environments, such as in high noise level where hu-man speaking is inaudible. Multi-modality (video and audio) is more effective than single modality (video or audio) in terms of both robustness and accuracy. Multi-modality (audio-visual) speech recognition (MSR) is one of the main extended applications of multi-modality. But similar to ASR, there is a significant deterioration in performance for MSR in noisy environment as well <ref type="bibr" target="#b1">[2]</ref>. Compared to audio modality operating in a clean voice environment, the one in noisy environment shows less gain because of upgrading from ASR to MSR. <ref type="bibr" target="#b1">[2]</ref> demonstrates that the noisy level of audio modality directly impacts the performance gain of MSR compared to single modality.</p><p>The goal of this paper is to introduce a two-stage speech recognition method with double visual-modality awareness. In the first stage, we reconstruct the audio signal which only contains the target speaker's voice with the guiding visual information (lip movements). In the second stage, the enhanced audio modality is combined with the visual modality again to yield better speech recognition. Compared to typical MSR methods with single time of visual modality awareness, our method is more advantageous in terms of robustness and accuracy.</p><p>We propose a deep neural network model named audioenhanced multi-modality speech recognition (AE-MSR) with double visual awareness to implement the method. The AE-MSR model consists of two sub-networks, the audio enhancement (AE) and MSR. Before being fed into the MSR sub-network, audio modality is enhanced with help from the first visual awareness in the AE sub-network. After enhancement, audio stream and revisited visual stream are then fed into the MSR sub-network to make speech predictions.The techniques we incorporated into AE-MSR include pseudo 3D residual convolution (P3D), temporal convolutional network (TCN), and element-wise attention gated recurrent unit (EleAtt-GRU). Ablation study shown in the paper demonstrates the effectiveness of each of the above sub-modules and the combination of them. The MSR subnetwork is also built on top of EleAtt-GRU.</p><p>The intuition of our AE-MSR is as follows. Typically, a deep learning-based MSR uses symmetric encoders for both audio and video. Though visual encoder is trained in an e2e fashion, we conduct experiments to show this is not the optimal way to leverage the visual information. The reason might be that the intrinsic architecture of the typical MSR implicitly suggests equal importance of audio and video. However we tell from various experiments that audio is still much more reliable to recognize speech, even in a noisy environment. Therefore, we re-design the architecture to embed this bias between video and audio as a prior.</p><p>Overall, the contributions of this paper are:</p><p>• We propose a two-stage double visual awareness MSR model, where the first visual awareness is applied to remove the audio noise.</p><p>• We introduce the P3D as visual front-end to extract more discriminative visual features and EleAtt-GRU to adaptively encode the spatio-temporal information in AE and MSR sub-networks, benefiting performance of both networks.</p><p>• We upgrade the temporal convolution block of 1D ResNet to a TCN one in AE sub-network for establishing temporal connections.</p><p>• Extensive experiments demonstrate that AE-MSR surpasses state-of-the-art MSR model <ref type="bibr" target="#b1">[2]</ref> both in audio clean and noisy environments on the Lip Reading Sentences 3 (LRS3-TED) dataset <ref type="bibr" target="#b4">[5]</ref>. The word classification model we build based on P3D also outperforms the word-level state-of-the-art <ref type="bibr" target="#b41">[42]</ref> on the Lip Reading in the Wild (LRW) dataset <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>In this section, we introduce some related works about audio enhancement (AE) driven by visual information and multi-modality speech recognition (MSR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio enhancement</head><p>A few researchers have demonstrated that the target audio signal can be separated from other speakers' voices and background noises, e.g. Gabbay et al. <ref type="bibr" target="#b22">[23]</ref> introduce a trained silent-video-to-speech model previously proposed by <ref type="bibr" target="#b20">[21]</ref> to generate speech predictions as a mask on the noisy audio signal which is then discarded in the pipeline of audio enhancement. Gabbay et al. <ref type="bibr" target="#b23">[24]</ref> also use the convolution neural networks (CNNs) to encode multi-modality features. The embedding vectors of audio and vision are concatenated before audio decoder and fed into transposed convolution of audio decoder to produce enhanced melscale spectrograms. Hou et al. <ref type="bibr" target="#b28">[29]</ref> build a visual driven AE network on the top of CNNs and fully connected (FC) layers to generate enhanced speech and reconstructed lip image frames. Afouras et al. <ref type="bibr" target="#b2">[3]</ref> use 1D ResNet as temporal convolution unit to process audio and visual features individually. Then the multi-modality features are concatenated and encoded into a mask by another 1D-ResNet-based encoder to remove noisy components in the audio signal. In their latest article, they propose a new approach that replaces the multi-modality feature encoder with Bi-LSTM [6].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modality speech recognition</head><p>Vision is often used as a complementary modality for audio speech recognition (ASR), especially in noisy environments. After combining visual modality, ASR is upgraded  to the multi-modality speech recognition (MSR). Reciprocally, MSR is also an upgrade to the lip reading and benefits people with hearing impairments to recognize speech by generating meaningful text.</p><p>In the field of deep learning, research on lip reading has longer history than MSR <ref type="bibr" target="#b49">[50]</ref>. Assael et al. <ref type="bibr" target="#b6">[7]</ref> propose Lip-Net, an end-to-end model on top of spatio-temporal convolutions, LSTM <ref type="bibr" target="#b27">[28]</ref> and connectionist temporal classification (CTC) loss on variable-length sequence of video frames. Stafylakis et al. <ref type="bibr" target="#b41">[42]</ref> introduce the state-of-theart word-level classification lip reading network on LRW dataset <ref type="bibr" target="#b14">[15]</ref>. The network consists of spatio-temporal convolution, residual network and Bi-LSTM.</p><p>On the basis of lip reading, MSR is developed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref>. Various MSR methods often use encoder-to-decoder (enc2dec) mechanism which is inspired by machine translation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>. Chung et al. <ref type="bibr" target="#b13">[14]</ref> use a dual sequence-to-sequence model with enc2dec mechanism. Visual features and audio features are encoded separately by LSTM units. Then multi-modality features are combined and decoded into characters. Afouras et al. <ref type="bibr" target="#b1">[2]</ref> introduce a sequence-to-sequence model of encoder-to-decoder mechanism. The encoder and decoder of the model are built based on the transformer <ref type="bibr" target="#b45">[46]</ref> attention architecture. In encoder stage, each modality feature is encoded with self-attention individually. After multi-head attention in decoder stage, the context vectors produced by multiple modalities are concatenated and fed to the feed forward layers to produce probable characters. However, their state-of-the-art method suffer in noisy scenarios. In noisy environments, the performance dramatically decreases, this is the main reason why we propose the method of AE-MSR. In this paper, we qualitatively evaluate performance of the AE-MSR model for speech recognition in the noisy environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architectures</head><p>In this section, we describe the double visual awareness multi-modality speech recognition (AE-MSR) network. It first learns to filter magnitude spectrogram from the voices of other speakers or background noises with help from the information of visual modality (Watch once to listen clearly). The subsequent MSR then revisits the visual modality and combines it with filtered audio magnitude spectrogram (Watch again to understand precisely). The model architecture is shown in detail in <ref type="figure" target="#fig_0">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Watch once to listen clearly</head><p>Audio features. We use Short Time Fourier Transform (STFT) to extract magnitude spectrogram from the waveform signal at a sample rate of 16kHz. To align with the video frame rate at 25fps, we set the STFT window length to 40ms and hop length to 10ms, corresponding to 75% overlap. We multiply the resulting magnitude by a mel-spaced filter to compute the audio feature of mel-scale magnitude with mel-frequency bins of 80 between 0 to 8 kHz.</p><p>Visual features. We produce image frames by cropping original video frames to 112 × 112 pixel patches and choose mouth patch as region of interest (ROI). To extract video features, we build a 3D CNN (C3D) <ref type="bibr" target="#b44">[45]</ref> -P3D <ref type="bibr" target="#b36">[37]</ref> network to produce a more powerful visual spatio-temporal representation instead of using C3D plus 2D ResNet <ref type="bibr" target="#b26">[27]</ref> which is mentioned in many other lipreading papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>C3D is a beneficial method to capture spatio-temporal features of videos and widely adopted <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6]</ref>. Multi-layer C3D can achieve even better performances in temporal tasks than a single layer one, however they are both computationally expensive and memory demanding. We use P3D to replace part of the C3D layers to alleviate this situation. The three block versions of P3D are shown in <ref type="figure" target="#fig_5">Figure 5</ref>, P3D ResNet is implemented by separating N × N × N convolutions into 1 × 3 × 3 convolution filters on spatial domain and 3 × 1 × 1 convolution filters on temporal domain to extract spatial-temporal features. P3D ResNet achieves superior performances over 2D ResNet in different temporal tasks <ref type="bibr" target="#b36">[37]</ref>. We implement a 50-layer P3D network by cyclically mixing the three blocks in the order of P3D-A, P3D-B, P3D-C.</p><p>The visual front-end is built on a 3D convolution layer with 64 filters of kernel size 5 × 7 × 7, followed by batch normalization (BN), ReLU activation and max-pooling layers. And then the max-pooling is followed by a 50-layer P3D ResNet that gradually decreases the spatial dimensions with depth while maintaining the temporal dimension. For an input of T × H × W frames, the output of the subnetwork is a T × 512 tensor (in the final stage, the feature is average-pooled in spatial dimension and processed as a 512dimensional vector representing each video frame). The visual feature and corresponding magnitude spectrogram are then fed into audio enhancement sub-network.</p><p>Audio enhancement with the first visual awareness. Noise-free audio signal achieves satisfactory performance on audio speech recognition (ASR) and multi-modality speech recognition (MSR). However there is a significant deterioration in recognition performance in noisy environments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Architecture of the audio enhancement subnetwork is illustrated in <ref type="figure" target="#fig_0">Figure 2a</ref>, where the visual features are fed into a temporal convolution network (video stream). The video stream consists of N v temporal convolution blocks, outputting video feature vectors. We introduce two versions of temporal convolution blocks, one is the temporal convolutional network (TCN) proposed by <ref type="bibr" target="#b8">[9]</ref> and the other is 1D ResNet block proposed by <ref type="bibr" target="#b5">[6]</ref>.</p><p>Architectures of temporal convolution blocks are shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the residual block of TCN consists of two dilated causal convolution layers, each layer followed by a weight normalization (WN) <ref type="bibr" target="#b38">[39]</ref> layer and a rectified linear unit (ReLU) <ref type="bibr" target="#b33">[34]</ref> layer. A spatial dropout <ref type="bibr" target="#b40">[41]</ref> layer is added after ReLU layer for regularization <ref type="bibr" target="#b8">[9]</ref>. Identity skip connection are added after the second dilated causal convolution layer. By combining causal convolution and dilated convolution, TCN guarantees no leakage from the future to the past and effectively expands the receptive field to maintain a longer memory size <ref type="bibr" target="#b8">[9]</ref>. The 1D ResNet block is based on 1D temporal convolution layer, followed by a batch normalization (BN) layer. Residual connection is added after ReLU activation layer.</p><p>Two of the intermediate temporal convolution blocks containing transposed convolution layers up-sample the video features by 4 to match the temporal dimension of the audio feature vectors (4T ). Similarly, the noisy magnitude spectrograms are proposed by a residual network (audio stream) which consists of N a temporal convolution blocks, outputting audio feature vectors. Then the audio feature vectors and the video feature vectors are fused in a fusion layer by simply concatenating over the channel dimension. The fused multi-modality vector is then fed into a one-layer EleAtt-GRU encoder followed by 2 fully connected layers with a Sigmoid as activation to produce a target enhancement mask (values range from 0 to 1). EleAtt-GRU is demonstrated more effective than other RNN variants in spatio-temporal tasks and its detail is introduced in section 3.2. The enhanced magnitude is produced by multiplying the original magnitude spectrogram with the target enhancement mask element-wise. Architecture details of the audio enhancement sub-network are given in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Watch again to understand precisely</head><p>Multi-modality speech recognition with the second visual awareness. Visual information can help enhance audio modality by separating target audio signal from noisy background. After audio enhancement by visual awareness, the multi-modality speech recognition (MSR) is implemented by combining enhanced audio and the revisited visual representation to benefit the performance of speech recognition further.</p><p>We use encoder-to-decoder (enc2dec) mechanism in the MSR sub-network. Instead of using transformer <ref type="bibr" target="#b45">[46]</ref>, which demonstrates decent performance on lip reading <ref type="bibr" target="#b3">[4]</ref> and MSR <ref type="bibr" target="#b1">[2]</ref>, our network is basically built on a RNN variant model named gated recurrent unit with element-wiseattention (EleAtt-GRU) <ref type="bibr" target="#b48">[49]</ref>. Although transformer is a powerful model emerging in machine translation <ref type="bibr" target="#b45">[46]</ref> and lip reading <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, it builds character relationships within limited length, less effective with long sequences than RNN. EleAtt-GRU can alleviate this situation, because it is equipped with an element-wise-attention gate (EleAttG) that empowers an RNN neuron to have the attentive capacity. EleAttG is designed to modulate the input adap-tively by assigning different importance levels, i.e., attention, to each element or dimension of the input. Illustration of EleAttG for a GRU block is shown in <ref type="figure" target="#fig_6">Figure 6</ref>. In a GRU block/layer, all neurons share the same EleAttG, which reduces the cost of computation and number of parameters.</p><p>Architecture of the AE-MSR network is shown in <ref type="figure" target="#fig_0">Figure 2</ref>, a sequence-to-sequence MSR network is built based on EleAtt-GRU. The encoder is a two-layer EleAtt-GRU for both modalities. The enhanced audio magnitude is fed into an encoder layer between two 1D-ResNet blocks with stride 2 that down-sample the temporal dimension by 4 to match the temporal dimension of video features (T ). The 1D-ResNet layer are followed by another encoder layer, outputting the audio modality encoder context. The video features extracted by C3D-P3D network are fed into the video encoder to output video encoder context. In the decoder stage, video context and audio context are decoded separately by independent decoder layer. Generated context vectors of both modalities are concatenated over the channel dimensions and propagated to another decoder layer to produce character probabilities. The number of unit of EleAtt-GRU in both encoder and decoder is 128. The decoder outputs character probabilities which are directly matched to the ground truth labels and trained with a cross-entropy loss and the whole output sequence is trained with sequence-tosequence (seq2seq) loss <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The proposed network is trained and evaluated on LRW <ref type="bibr" target="#b14">[15]</ref> and LRS3-TED <ref type="bibr" target="#b4">[5]</ref> datasets. LRW is a very large-scale lip reading dataset in the wild from British television broadcasts, including news and talk shows. LRW consists of up to 1000 utterances of 500 different words, spoken by more than 1000 speakers. We use LRW dataset to pre-train the P3D spatio-temporal front-end based on a word-level classification network of lip reading.</p><p>LRS3-TED is the largest available dataset in the field of lip reading (visual speech recognition). It consists of face tracks from over 400 hours of TED and TEDx videos, and organized into three sets: pre-train, train-val and test. We train the audio enhancement (AE) sub-network and the multi-modality speech recognition (MSR) sub-network on the LRS3-TED dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metric</head><p>For the word-level lip reading experiment, the train, validation and test sets are provided with the LRW dataset. We report word accuracy for classification in 500 word classes of LRW. For sentence-level recognition experiments, we report the Word Error Rate (WER). WER is defined as W ER = (S + D + I)/N , where S is the number of sub-stitution, D is the number of deletions, I is the number of insertions to get from the reference to the hypothesis, and N is the number of words in the reference <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training strategy</head><p>Visual front-end. The visual front-end of C3D-P3D is pre-trained on a word-level classification network of lip reading with LRW dataset for 500 word classes and we adopt a two-step training strategy. In the first step, image frames are fed into a 3D convolution, which is followed by a 50-layer P3D, and the back-end is based on one dense layer. In the second step, to improve the effectiveness of the model we replace the dense layer with two layers of Bi-LSTM, followed by a linear and a SoftMax layer. We use cross entropy loss to train the word classification tasks. With the visual front-end frozen, we extract and save video features, as well as magnitude spectrograms for both original audio and the mix-noise one.</p><p>Noisy samples. In order to train our model so that it can be resistant to background noise or speakers, we follow the noise mix method of <ref type="bibr" target="#b1">[2]</ref>, the babble noise with SNR from -10 dB to 10 dB is added to audio stream with probability p n = 0.25 and the babble noise samples are synthesized by mixing the signals of 30 different audio samples from LRS3-TED dataset.</p><p>AE and MSR sub-networks. The AE sub-network is firstly trained on multi-modality of mixed noises with temporal convolution block of TCN and 1D ResNet separately. The AE sub-network is trained by minimizing the L 1 loss between the predicted magnitude spectrogram and the ground truth. Simultaneously, the multi-modality speech recognition (MSR) sub-network is trained with video features and clean magnitude spectrogram as inputs. The MSR sub-network is also trained when only single modality (audio or visual) is available. For MSR sub-network, we use a sequence-to-sequence (seq2seq) loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>AE-MSR. We freeze the AE sub-network and train the AE-MSR network. To demonstrate the benefit of our model, we reproduce and evaluate the state-of-the-art multimodality speech recognition model provided by <ref type="bibr" target="#b1">[2]</ref> at different noise levels. The training begins with one-word samples, and then the length of the training sequence gradually grows. This is a cumulative method that not only improves the convergence rate on the training set, but also reduces overfitting significantly. Output size of decoder is set to 41, accounting for the 26 characters in the alphabet, the 10 digits, and tokens for [PAD], [EOS], [BOS] and <ref type="bibr">[SPACE]</ref>. We also use teacher forcing method <ref type="bibr" target="#b1">[2]</ref>, in which the ground truth of the previous decoding step serves as input to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details. The implementation of the network is based on the TensorFlow library [1] and trained</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Word accuracy</p><p>Chung and Zisserman <ref type="bibr" target="#b13">[14]</ref> 76.2% Stafylakis and Tzimiropoulos <ref type="bibr" target="#b41">[42]</ref> 83.0% Petridis and Stafylakis <ref type="bibr" target="#b35">[36]</ref> 82.0% Ours 84.8%  on a single Tesla P100 GPU with 16GB memory. We use the ADAM optimiser to train the network with dropout and label smoothing. An initial learning rate is set to 10 −4 , and decreased by a factor of 2 every time if the training error did not improve, the final learning rate is 5×10 −6 . Training of the entire network takes approximately 15 days, including the training of the audio enhancement sub-network on both of the two temporal convolution blocks and the MSR subnetwork separately and the subsequent joint training. Please see our code 1 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">P3D-based visual front-end and EleAtt-GRUbased enc2dec</head><p>P3D-based visual front-end. We perform lip reading experiments on both word-level and sentence-level. In section 4.3, we introduce a word-level lip reading network on the LRW dataset to classify 500 word classes to train the visual front-end of C3D-P3D. Result of this word-level lip reading network is shown in <ref type="table" target="#tab_2">Table 1</ref>, where we report word accuracy as evaluation metric and our result surpasses the state-of-the-art <ref type="bibr" target="#b41">[42]</ref> on the LRW dataset. It demonstrates</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AV</head><p>VA VAV <ref type="table" target="#tab_2">Met TM-s2s EG-s2s 1D-TM-s2s T-TM-s2s 1D-EG-s2s T-EG-s2s 1D-TM-s2s T-TM-s2s 1D-EG-s2s T-EG-s2s   clean  8.0% 6.8%  --------10</ref> 33   that visual front-end network of C3D-P3D is more advantageous in extracting video feature representations than the C3D-2D-ResNet one used by <ref type="bibr" target="#b1">[2]</ref>. EleAtt-GRU-based enc2dec. Results in both of Column V and A in <ref type="table" target="#tab_3">Table 2</ref> demonstrate that EleAtt-GRUbased enc2dec is more beneficial in speech recognition than the Transformer-based enc2dec. As shown in <ref type="table" target="#tab_3">Table 2</ref> Column V, our multi-modality speech recognition (EG-seq2seq) network (illustrated in <ref type="figure" target="#fig_0">Figure 2b</ref>) with only visual modality reduces word error rate (WER) by 2.1% compared to the previous state-of-the-art (TM-seq2seq) <ref type="bibr" target="#b1">[2]</ref> WER of 59.9% on the LRS3 dataset without using language model in decoder. Furthermore, we also evaluate the EleAtt-GRUbased enc2dec model in ASR at different noise levels. As shown in <ref type="table" target="#tab_3">Table 2</ref> Column A, EG-seq2seq exceeds the stateof-the-art (TM-seq2seq) model on ASR at all noise levels (-10 dB to 10 dB) without extra language model. <ref type="table" target="#tab_3">Table 2</ref> Column A also shows that neither EG-seq2seq or TM-seq2seq works any more with only audio modality at -10 dB SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNR dB</head><p>Results in the columns under AV in <ref type="table">Table 3</ref> demonstrate the speech recognition accuracy improvement after adding the visual awareness once at the MSR stage, especially in noisy environments. Even when the audio is clean, visual modality can still play a helping role, for example the WER is reduced from 7.2% for audio modality only to 6.8% for multi-modality. EG-seq2seq outperforms the state-of-theart (TM-seq2seq) model on MSR at different noise levels. It again demonstrates the superiority of EleAtt-GRU-based enc2dec in speech recognition. However, we notice that under very noisy conditions, audio modality can negatively impact the MSR because of its highly polluted input, when comparing lip reading (V in <ref type="table" target="#tab_3">Table 2</ref>) with MSR (AV in Table 3) at -10 dB SNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Audio enhancement (AE) with the first visual awareness</head><p>In order to demonstrate the enhancement effectiveness of our AE models so that it can benefit not only our speech recognition models but also other speech recognition models. Compared with MSR in the Section 5.1, here we apply visual awareness at audio enhancement stage, instead of at MSR. We compare and analyze the results of following networks at different noise levels:  <ref type="table">Table 4</ref>: Energy errors between original noise-free audio magnitudes and enhanced magnitudes produced by different audio enhancement models.</p><p>gain is also clearly illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. Moreover, by comparing the result of columns under AV and VA in <ref type="table">Table 3</ref>, with the same number of visual awareness, our audio enhancement approach shows more benefit in speech recognition than the multi-modality with single visual awareness in noisy environments. Magnitudes produced by the two AE models are shown in <ref type="figure" target="#fig_8">Figure 8</ref>. We also introduce an energy error function to measure the effect of audio enhancement models as follow:</p><formula xml:id="formula_0">∆M = M − M o 2 M o 2<label>(1)</label></formula><p>where M is the magnitudes of noisy audio or enhanced audio, M o is the original audio without mixing noises, ∆M is the deviation results between M and M o . We chose 10,000 noise-free samples that are added to babble noises with SNR of -5 dB, 0 dB and 5 dB separately to compare the enhancement performance between 1DRN-AE and TCN-AE networks. We average the ∆M results among samples at each SNR-level. Results in <ref type="table">Table 4</ref> show the beneficial performance of TCN-AE.</p><p>In <ref type="table">Table 5</ref>, we list some of the many examples where the single modality (video or audio alone) fails to predict the correct sentences, but these sentences are correctly deciphered by applying both modalities. It also shows that, in some noisy environment the multi-modality also fails to produce the right sentence, however the enhanced audio modality predict successfully. Experimental results of speech recognition in <ref type="table">Table 3</ref>.2 also demonstrate that TCN-EG-seq2seq is more advantageous than 1DRN-EG-seq2seq in audio modality enhancement due to the TCN temporal convolution unit, which has a longer-term memory and larger receptive field by combining causal convolution and dilated convolution that more beneficial in temporal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Multi-modality speech recognition with the second visual awareness</head><p>After audio enhancement with the first visual awareness, we implement multi-modality speech recognition with the second visual awareness. By comparing the results in columns under VA and VAV in <ref type="table">Table 3</ref>, MSR with double Each method in this diagram equivalent to the one with same name in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>visual awareness leads to a further improvement compared to any single visual awareness method (e.g. AV, VA and V). For example, the WER of 1DRN-EG-seq2seq is reduced from 36.6% to 28.5% when combining the visual awareness again for speech recognition after audio enhancement, and the TCN-EG-seq2seq model reduces the WER even more. It demonstrates the performance gain because of the second visual awareness in MSR. Our AE-MSR network shows significant advantage in terms of performance after combining visual awareness twice, once for audio enhancement and the other for MSR. In <ref type="table">Table 5</ref> we list some examples that the multi-modality model (AV) and the AE model (VA) fail to predict the correct sentences, but the AE-MSR model deciphers the words successfully in some noisy environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce a two-stage speech recognition model named double visual awareness multi-modality speech recognition (AE-MSR) network, which consists of the audio enhancement (AE) sub-network and the multimodality speech recognition (MSR) sub-network. By extensive experiments, we demonstrate the necessity and effectiveness of double visual awareness for MSR, and our method leads to a significant performance gain on MSR especially in noisy environments. Furth er, our models in this paper outperform the state-of-the-art ones on the LRS3-TED and the LRW datasets by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Blocks of the Pseudo-3D (P3D) network</head><p>P3D ResNet is implemented by separating N × N × N convolutions into 1 × 3 × 3 convolution filters on spatial domain and 3×1×1 convolution filters on temporal domain to extract spatial-temporal features <ref type="bibr" target="#b36">[37]</ref>. The three versions of P3D blocks are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of an EleAtt-GRU block</head><p>The details of the EleAtt-GRU <ref type="bibr" target="#b48">[49]</ref> building block used by our models are outlined in <ref type="figure" target="#fig_6">Figure 6</ref>. Each GRU block has (e.g., N ) GRU neurons. Yellow boxes -the units of the original GRU with the output dimension of N . Blue circle -element-wise operation and the brown circle denotes vector addition operation. Red box -EleAttG with an output dimension of D, which is the same as the dimension of the input x t .  follows:</p><formula xml:id="formula_1">x t = a t x t r t = σ (W xrxt + W hr h t−1 + b r ) z t = σ (W xzxt + W hz h t−1 + b z ) h t = tanh (W xhxt + W hh (r t h t−1 ) + b h ) h t = z t h t−1 + (1 − z t ) h t</formula><p>where σ denotes the activation function of Sigmoid. The attention response of an EleAttG is the vector a t with the same dimension as the input x t of GRU. a t modulates x t to generatex t . r t and z t denote the reset gate and update gate of GRU. h t and h t−1 are the output vectors of the current hidden state and the previous hidden state. W αβ denotes the weight matrix related with α and β, where α ∈ {x, h} and β ∈ {r, z, h}. b γ is the bias vector, where γ ∈ {r, z, h} <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Examples of AE and AE-MSR speech recognition results.</head><p>Examples of AE and AE-MSR speech recognition results are illustrated in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Enhancement examples of the 1DRN-AE and the TCN-AE models</head><p>Enhancement examples of our audio enhancement subnetworks are illustrated in <ref type="figure" target="#fig_8">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Examples of mouth crop</head><p>We produce image frames by cropping original video frames to 112 × 112 pixel patches and choose mouth patch as region of interest (ROI). As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, facial landmarks are extracted by the Dlib <ref type="bibr" target="#b29">[30]</ref> toolkit and the mouth ROI inside the red squares are achieved by 4 (red points) specified out of 68 facial landmarks (green points).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transcription</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∆M% WER % GT</head><p>We can prevent the worst case scenario -V</p><p>We can put and worst case scenario -34 A</p><p>We can prevent the worst case tcario -8 AV</p><p>We can prevent the worst case scenario -0</p><p>Noisy <ref type="formula">(5 dB</ref>  <ref type="table">Table 5</ref>: Examples of recognition results by our models. GT: Ground truth; V: visual modality only; A: audio modality only; AV: multi-modality with single visual modality awareness; VA: enhanced audio modality by single visual awareness for ASR; VAV: multi-modality by double visual awareness for multi-modality speech recognition (MSR); 1DRN, TCN: the temporal convolutional unit is 1D ResNet or TCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Architecture details of the audio enhancement networks</head><p>Architecture details of the audio enhancement subnetwork are given in <ref type="table">Table 6</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the multi-modality speech recognition network with double visual awareness (AE-MSR). The AE-MSR network consists of two sub-networks: a) the audio enhancement (AE) network. The network receives image frames and audio signals as inputs, outputting the enhanced magnitude spectrograms that the noisy spectrograms are filtered. V: visual features; A: enhanced audio magnitude. b) the multi-modality speech recognition (MSR) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Temporal convolution blocks. a) TCN ResNet block. US: Up-sample; AP: Average Pooling [27]. b) The 1D ResNet block. DS: Depthwise separable [13]; BN: Batch Normalization. The non-upsample convolution layers are all depthwise separable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Word error rates (WER) of both audio speech recognition (ASR) with single visual modality awareness and multimodality speech recognition (MSR) with double visual modality awareness on the LRS3-TED dataset. Met: method. TM-s2s: TM-seq2seq; EG-s2s: EG-seq2seq; 1D-TM-s2s: an AE-MSR model, which consists of 1DRN-AE and TM-seq2seq; T-TM-s2s: an AE-MSR model, which consists of TCN-AE and TM-seq2seq; 1D-EG-s2s: an AE-MSR model, which consists of 1DRN-AE and EG-seq2seq; T-EG-s2s: an AE-MSR model, which consists of TCN-AE and EG-seq2seq. AV: multi-modality with single visual modality awareness; VA: enhanced audio modality by single visual awareness for ASR; VAV: multi-modality by double visual awareness for multi-modality speech recognition (MSR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Word error rate (WER) on different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Bottleneck building blocks of the Pseudo-3D (P3D) ResNet network. P3D ResNet is produced by interleaving P3D-A, P3D-B and P3D-C in turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>An Element-wise-Attention Gate (EleAttG) of GRU block. Corresponding computations of an EleAtt-GRU are as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Examples of mouth crop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>(a) Clean audio magnitude (b) Noisy audio magnitude (c) Enhanced audio magnitude by 1DRN-AE (d) Enhanced audio magnitude by TCN-AE Enhancement effects of the 1D-ResNet-based audio enhancement (1DRN-AE) model and the TCN-based audio enhancement (TCN-AE) model: a) clean audio utterance; b) we obtain this noisy utterance by adding babble noise to the 100 central audio frames; c) the enhanced audio utterance by 1DRN-AE; d) the enhanced audio utterance by TCN-AE; c) and d) show the effect of audio enhancement when compared to b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Word accuracy of different word-level classification networks on the LRW dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="5">Google [11] TM-seq2seq [2] EG-seq2seq</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">WER %</cell><cell></cell><cell></cell></row><row><cell>SNR dB</cell><cell>M</cell><cell>A</cell><cell>A</cell><cell>V</cell><cell>A</cell><cell>V</cell></row><row><cell>clean</cell><cell></cell><cell>10.4</cell><cell>9.0</cell><cell>59.9</cell><cell cols="2">7.2 57.8</cell></row><row><cell>10</cell><cell></cell><cell>-</cell><cell>35.9</cell><cell>-</cell><cell>35.5</cell><cell>-</cell></row><row><cell>5</cell><cell></cell><cell>-</cell><cell>49.0</cell><cell>-</cell><cell>42.6</cell><cell>-</cell></row><row><cell>0</cell><cell></cell><cell>70.3</cell><cell>60.5</cell><cell>-</cell><cell>58.2</cell><cell>-</cell></row><row><cell>-5</cell><cell></cell><cell>-</cell><cell>87.9</cell><cell>-</cell><cell>86.1</cell><cell>-</cell></row><row><cell>-10</cell><cell></cell><cell>-</cell><cell>100.0</cell><cell>-</cell><cell cols="2">100.0% -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Word error rates (WER) of both single modality speech recognition and multi-modality speech recognition (MSR) on the LRS3-TED dataset. M: modality. A: audio modality only; V: visual modality only.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/JackSyu/ Discriminative-Multi-modality-Speech-Recognition</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martn</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04121</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep lip reading: A comparison of models and an online application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06053</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Joon Son Chung, and Andrew Zisserman</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<title level="m">Joon Son Chung, and Andrew Zisserman. Lrs3-ted: A large-scale dataset for visual speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">My lips are concealed: Audio-visual speech enhancement through obstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04975</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lipnet: End-to-end sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01599</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">State-of-the-art speech recognition with sequence-tosequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4774" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Lip reading in the wild. Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The listening eye: a simple introduction to the art of lip-reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dorothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clegg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953" />
			<pubPlace>Methuen &amp; Company</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for speech perception and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2421" to="2424" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An audio-visual corpus for multimodal automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Czyzewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bozena</forename><surname>Kostek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bratoszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Kotus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Szykulski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="192" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perceptual dominance during lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Randolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marylu</forename><surname>Easton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved speech reconstruction from silent video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavi</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Confusions among visually perceived consonants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cletus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="796" to="804" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tavi Halperin, and Shmuel Peleg. Seeing through noise: Visually driven speaker separation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Gabbay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08789</idno>
		<title level="m">Asaph Shamir, and Shmuel Peleg. Visual speech enhancement</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Audio-visual speech enhancement using multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Cheng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syu-Siang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Hui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiu-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Profile view lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitiz</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">429</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5588</biblScope>
			<biblScope unit="page">746</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic lipreading to enhance speech recognition (speech reading)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>David Petajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingehuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<idno type="arXiv">arXiv:1703.04105</idno>
		<title level="m">Themos Stafylakis and Georgios Tzimiropoulos. Combining residual networks with lstms for lipreading</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Phoneme perception in lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carroll G</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech and Hearing Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="212" to="222" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lrw-1000: A naturally-distributed largescale benchmark for lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01939</idno>
		<title level="m">Zhanning Gao, and Nanning Zheng. Eleatt-rnn: Adding attentiveness to neurons in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A review of recent advances in visual speech decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="590" to="605" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
