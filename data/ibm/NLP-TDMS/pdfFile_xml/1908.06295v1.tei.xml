<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Design</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient endto-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train. Our code is publicly available in our project page 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have shown significant success in image and pattern recognition, video analysis, and natural language processing <ref type="bibr" target="#b17">[18]</ref>. Extending this success from 2D to 3D domain has been receiving great interests. Promising results have been demonstrated for the long-standing problem of scene understanding. Previously a 3D scene is often represented using structured representations such as volumes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>, multiple images <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref>, hierarchical data structures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref>. However, such representations usually face great challenges from memory consumption, imprecise representation, or lack of scalability for tasks such as classification and segmentation. <ref type="figure">Figure 1</ref>. The accuracy of point cloud classification of different methods over time and epochs. While being accurate, some methods are quite costly to train. We address this problem by Shell-Conv, a simple yet effective convolutional operator based on concentric shell statistics. In both equal-time and equal-epoch comparisons, our method performs the best. It can achieve over 80% accuracy within two minutes, and reach 90% on the test dataset after only 15 minutes of training.</p><p>Recently, directly consuming point clouds using neural networks has shown great promises <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20]</ref>. Point-Net <ref type="bibr" target="#b24">[25]</ref> pioneers this direction by learning with a symmetric function to make the network robust to point order ambiguity. Many subsequent works extend this direction by designing convolution that better captures local features of a point cloud. While such efforts lead to improved scene understanding performance, there is often a trade-off between network complexity, training speed, and accuracy. For example, the follow-up work PointNet++ <ref type="bibr" target="#b26">[27]</ref> segments point cloud into smaller clusters and applies PointNet locally in a hierarchical manner. While achieving better result, the network is more complicated with reduced speed. Pointwise convolution <ref type="bibr" target="#b11">[12]</ref> is simple to implement but inaccurate. Spi-derCNN <ref type="bibr" target="#b41">[42]</ref> extends traditional convolution on 2D images to 3D point clouds by parameterizing a family of convolution filters. Although high accuracy is achieved, more time is taken for training. PointCNN <ref type="bibr" target="#b19">[20]</ref> achieves the state-ofthe-art accuracy via learning a local convolution order but its training is slow to converge. In general, designing a convolution for point cloud that can strike a good balance between such performance factors is a challenging problem.</p><p>Based on these observations, we propose a novel approach to consume point clouds directly in a very simple neural network which is able to achieve the state-of-the-art accuracy with very fast training speed, as shown in <ref type="figure">Figure 1</ref>. Our idea is to split a local point neighborhood such that point neighboring and convolution with points can be performed efficiently. To achieve this, at each point, we query the point neighborhood and partition it with a set of concentric spheres, resulting in concentric spherical shells. In each shell, the representative features can be extracted based on the statistics of the points inside. By using ShellConv as the core convolution operator, an efficient neural network called ShellNet can be constructed to solve 3D scene understanding tasks such as object classification, object part segmentation, and semantic scene segmentation.</p><p>In general, the main contributions of this work are:</p><p>• ShellConv, a simple yet effective convolution operator for orderless point cloud. The convolution is defined on a domain that can be partitioned by concentric spherical shells, simultaneously allowing efficient neighbor point query and resolving point order ambiguity by defining a convolution order from the inner to the outer shells;</p><p>• ShellNet, an efficient neural network architecture based on ShellConv for learning with 3D point clouds directly without having any point order ambiguity;</p><p>• Applications of ShellNet on object classification, object part segmentation, and semantic scene segmentation that achieves the state-of-the-art accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recent advances in computer vision witness the growing availability of 3D scene datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>, leading to deep learning techniques to tackle the long-standing problem of scene understanding, particularly object classification, object part and scene segmentation. In this section, we review the state-of-the-art research in deep learning with 3D data, and then focus on techniques that enable feature learning on point clouds for scene understanding tasks.</p><p>Early deep learning with 3D data uses regular representations such as volumes <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref> and multi-view images <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref> for feature learning to solve object classification and semantic segmentation. Unfortunately, volume representation is very limited due to large memory footprints. Multi-view image representation does not have this issue, but it stores depth information implicitly, which makes it challenging to learn view-independent features.</p><p>Recently, deep learning in 3D focuses toward point clouds, which is more compact and intuitive compared to volumes. As point cloud is mathematically a set, using point cloud with deep neural networks requires fundamental changes to the core operator: convolution. Defining ef-ficient convolution for point clouds has since been a challenging, but an important task. Inspired from learning with volumes, Hua et al. <ref type="bibr" target="#b11">[12]</ref> perform on-the-fly voxelization at each point of the point cloud based on nearest point queries. <ref type="bibr">Le et al. [17]</ref> propose to apply convolution on a regular grid with each cell containing point features that are resampled to a fixed size. Tatarchenko et al. <ref type="bibr" target="#b32">[33]</ref> perform convolution on the local tangent planes. Xie et al. <ref type="bibr" target="#b40">[41]</ref> generalize shape context to convolution for point cloud. Liu et al. <ref type="bibr" target="#b21">[22]</ref> use a sequence model to summarize local features with multiple scales. Such techniques lead to straightforward implementations of convolutional neural network for point clouds. However, extra computations are required for the explicit data representation, making the learning inefficient.</p><p>Instead of voxelization, it is possible to make neural network operate directly on point clouds. Qi et al. <ref type="bibr" target="#b24">[25]</ref> propose PointNet, a pioneering network that learns global per-point features by optimizing a symmetric function to achieve point order invariance. The drawback of PointNet is that each point feature is learnt globally, i.e., no features from local regions are considered. Recent methods in point cloud learning are focused on designing convolution operators that can capture such local features.</p><p>In this trend, PointNet++ <ref type="bibr" target="#b26">[27]</ref> supports local features by a hierarchy of PointNet, and relies on a heuristic point grouping to build the hierarchy. Li et al. <ref type="bibr" target="#b19">[20]</ref> propose to learn a transformation matrix to turn the point cloud to a latent canonical representation, which can be further processed with standard convolutions. Xu et al. <ref type="bibr" target="#b41">[42]</ref> propose to parameterize convolution kernels with a step function and Taylor polynomials. Wang et al. <ref type="bibr" target="#b37">[38]</ref> propose a similar network structure to PointNet by optimizing weights between a point and its neighbors and using them for convolution. Shen et al. <ref type="bibr" target="#b29">[30]</ref> also improve a PointNet-like network by kernel correlation and graph pooling. Huang et al. <ref type="bibr" target="#b12">[13]</ref> learn the local structure particularly for semantic segmentation by applying traditional learning algorithms from recurrent neural networks. Ben-Shabat et al. <ref type="bibr" target="#b3">[4]</ref> use a grid of spherical Gaussians with Fisher vectors to describe points. Such great efforts lead to networks with very high accuracies, but the efficiency of the learning is often overlooked (see <ref type="figure">Figure 1</ref>). This motivates us to focus on efficiency for local features learning in this work.</p><p>Beyond learning on unstructured point clouds, there have been some notable extension works, such as learning with hierarchical structures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, learning with selforganizing network <ref type="bibr" target="#b18">[19]</ref>, learning to map a 3D point cloud to a 2D grid <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8]</ref>, addressing large-scale point cloud segmentation <ref type="bibr" target="#b14">[15]</ref>, handling non-uniform point cloud <ref type="bibr" target="#b10">[11]</ref>, and employing spectral analysis <ref type="bibr" target="#b44">[45]</ref>. Such ideas are orthogonal to our method, and adding them on top of our proposed convolution could be an interesting future research. Following the inner to the outer order, a standard 1D convolution can be performed to yield the output features (d). Thicker dot means less points but each has higher dimensional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The ShellConv Operator</head><p>To achieve an efficient neural network for point cloud, the first task is to define a convolution that is able to directly consume a point cloud. Our problem statement is given a set of points as input, define a convolution that can efficiently output a feature vector to describe the input point set.</p><p>There are two main issues when defining this convolution. First, the input point set has to be defined. It can be the entire point cloud, or a subset of the point cloud. The former case seeks a global feature vector that describes the entire point cloud; the latter seeks a local feature vector for each point set that can be further combined when needed. Second, one has to seamlessly take care of the point order ambiguity in a set and the density of the points in the point cloud. PointNet <ref type="bibr" target="#b24">[25]</ref> opted to learn global features, but it has been shown by recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> that local features can lead to more representative features, resulting in better performance. We are motivated by these works and define a convolution to obtain features for a local point set. To keep our convolution simple but efficient, we propose an intuitive approach to addresses the challenges, below.</p><p>Convolution. We show the main idea of our convolution in <ref type="figure" target="#fig_0">Figure 2</ref>. The common strategy in a traditional CNN architecture is to decrease the spatial resolution of the input and output more feature channels at deeper layers. We also support this strategy in our convolution by combining point sampling into the convolution, outputting sparser point sets at deeper layers. Different from previous works that stack many layers to increase receptive field, our method can obtain a larger receptive field without increasing the number of layers. Particularly, from the input point set, a set of repre-  <ref type="figure" target="#fig_0">Figure 2</ref> (a)). Each representative point and its neighbor points define a point set for convolution ( <ref type="figure" target="#fig_0">Figure 2 (b)</ref>).</p><formula xml:id="formula_0">Algorithm 1 ShellConv Operator. Input: 1: p, Ω p , {F prev (q) : q ∈ Ω p } * Representative point,</formula><p>Let us now focus on a single representative point p and its neighbors q ∈ Ω p , where Ω p is the set of neighbors determined by a nearest neighbor query. By definition, the convolution at p is</p><formula xml:id="formula_1">F (p) (n) = q∈Ω (n) p w(q) (n) F (q) (n−1)<label>(1)</label></formula><p>where F denotes the input feature of the point set for a par- <ref type="figure">Figure 3</ref>. ShellNet architecture. For classification, we apply three layers of ShellConv before the fully connected classifier. For semantic segmentation, we follow a U-net <ref type="bibr" target="#b28">[29]</ref> architecture. The encoder is in green and the decoder is in yellow. Point downsampling and upsampling is also included in our convolution, depending on its use. N0 &gt; N1 &gt; N2 denotes the number of points in the input and after being sampled in each convolution, and C &lt; C0 &lt; C1 &lt; C2 denotes the output feature channels at each point. S0 &gt; S1 &gt; S2 denotes the number of shells in each ShellConv operator that is analogous to the convolution kernel size. Given a fixed shell size, when the point cloud is downsampled, the number of shells also decreases. 1 × S0 × C0 denotes a convolution that convolutes an input features using kernel (1, S0) and output C0 feature channels.</p><p>ticular channel, w is the weight of the convolution. We use superscript (n) to denote the data or parameters of layer n. Note that F (p) and F (q) denote the features of point p and q. They are disregarded of the order of p and q in the point cloud because we simply treat the point cloud as a mathematical set. The only issue with this convolution here is how to define the weight function. The weights have to be suitable for training, i.e., w has to be discretized into a fixedsize vector of trainable parameters. Defining w for each point is not practical because the points are not ordered.</p><p>To address this issue, our observation here is that we can exploit the partitioning of the neighborhood into regions such that w is well defined and the output can be computed efficiently. Particularly, to facilitate neighbor queries, we use a set of multi-scale concentric spheres to define the regions <ref type="figure" target="#fig_0">(Figure 2(c)</ref>). The region between two spheres forms a spherical shell. The union of the concentric spherical shells yields the domain Ω p . Therefore, we can define our convolution as</p><formula xml:id="formula_2">F (p) (n) = S∈Ω (n) p w (n) S F (S) (n−1)<label>(2)</label></formula><p>Note that as the shells are naturally ordered (from the inner most to the outermost), there is no ambiguity among the shells and the convolution is well defined, with weight w S for each shell. What remains ambiguous is the order of the points in the shells. To address this problem, we propose a statistical approach to aggregate features of the points in each shell such that it yields an order-invariant output. Particularly, we choose to represent the features by only the maximum value in each feature channel:</p><formula xml:id="formula_3">F (S) = maxpool({F (q) : q ∈ Ω S })<label>(3)</label></formula><p>where Ω S denotes a shell S. Theoretically, the maximum value is a crude approximation to the underlying distribution, but because each point often has tens or hundreds of feature channels, the information from many points in the shell can still be represented. The detailed steps of Shell-Conv is presented in Algorithm 1.</p><p>Spherical Shells Construction. We use a simple heuristics approach to establish the spherical shells as follows.</p><p>We first compute the distance between the neighbor points to the representative point at the center. We then sort the distances, and distribute points to shells based on their distances to the center, from inner to outer. We assign a fixed number of points to each shell, i.e., n points per shell in our implementation. Particularly, we first grow a sphere from the center until n points falls inside the sphere. This is the innermost shell. After that, the sphere continues to grow to collect another n points that forms the second shell, and so on. We found that this approach of shells construction provides a good stratification of point distributions in the shells. It is also easy to implement and has low overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ShellNet</head><p>We now proceed to design a convolutional neural network for point cloud feature learning. We draw inspirations from typical 2D convolutional neural networks and build an architecture named ShellNet which uses ShellConv in place of traditional 2D convolution (see <ref type="figure">Figure 3</ref>). This architecture can be used for multiple scene understanding tasks. Particularly, the classification and segmentation networks both share the encoder part, and only differ in the part after that. Since ShellConv is permutation invariant for input points, ShellNet is able to consume point sets directly.</p><p>Our network for point cloud deep learning has three layers. In the classification stage, we pass all the input points through three ShellConv operators. The points are gradually subsampled into less representative points denoted as N 0 &gt; N 1 &gt; N 2 respectively, while the output feature channels increases layer by layer, denoted as C 0 &lt; C 1 &lt; C 2 respectively. In <ref type="figure">Figure 3</ref>, N i represented as blue dots with thicker shape that indicates a higher dimension. This design is similar to a typical 2D convolutional neural network: the number of representative points decreases while the number of output channels increases. After three layers of Shell-Conv, we obtain a matrix of size N 2 × C 2 , where N 2 is the final number of representative points extracted from the input point cloud with each one contains a high dimensional feature vector of size C 2 . This matrix is fed into the mlp module size of (256, 128) to produce the probability map for object classification. Finally, we obtain a 128 × k cls matrix with k cls indicates the number of classes. The specific parameter settings are discussed in Section 5. <ref type="bibr" target="#b0">1</ref> The segmentation network follows U-net <ref type="bibr" target="#b28">[29]</ref>, an encoder-decoder architecture with skip connections. The deconvolution part starts with the set of N 2 points from the encoder, passing through the ShellConv operators until the point cloud reaches the original resolution. The deconvolution layers gradually output more points but less feature channels. Skip connections retain features from earlier layers and concatenate them to the output features of the deconvolution layers. Such strategy is shown to be highly effective for dense semantic segmentation on images <ref type="bibr" target="#b28">[29]</ref>, which we adopts here for point clouds. Note that we use ShellConv for both convolution and deconvolution. The output N × C is also fed into an mlp to produce the probability map for segmentation, where we obtain a 64 × k seg matrix with k seg indicates the number of segment labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we perform the experiments with three typical point cloud learning tasks: object classification, part segmentation, and semantic segmentation. We evaluate our method under different settings to justify the results. In general, our method achieves the state-of-the-art performance for both accuracy and speed in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parameter Setting</head><p>ShellNet has three encoding layers, each of which contains a ShellConv. The parameters are N i , S i , and C i that denote the number of representative points, the number of shells, and output channels in each layer respectively. From the first to third layers, N i is set to 512, 128, 32, S i is set to 4, 2, 1, and C i is set to 128, 256, and 512 for i = 0, 1, 2 respectively. C is set to 64 at the last convolution for segmentation. We define the number of points contained in each shell as shell size, which is set to 16 for classification and 8 for segmentation. So the number of neighbors for each representative point is S i × 16 and S i × 8, which is equal to 64, 32, and 16 for the three layers of classification, and 32, 16, 8 for segmentation, respectively. During training, we use a batch size of 32 for classification and 16 for segmentation. The optimization is done with an Adam optimizer with initial learning rate set to 0.001. Our network is implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref> and run on a NVIDIA GTX 1080 GPU for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object Classification</head><p>The classification is tested on ModelNet40 <ref type="bibr" target="#b39">[40]</ref> which is composed of 40 object classes and has 9, 843 models for training and 2, 468 models for testing. We use the point cloud data of ModelNet40 provided by Qi et al. <ref type="bibr" target="#b24">[25]</ref> as input, where 1024 points are roughly uniformly sampled from each mesh. Only the geometric coordinates (x, y, z) of the sampled points are used in the experiment. We follow the train-test split from PointNet <ref type="bibr" target="#b24">[25]</ref>. The data is augmented by randomly perturbing the point locations. The comparison results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As we can see, our results have achieved the state-of-theart. While ShellNet with shell size (ss) of 16 is the default setting for classification, other ss are also tested. When decreasing ss to 8, the receptive fields become smaller and less overlapped and accuracy also decreases slightly but still around 91.0%. When ss increases, receptive field is enlarged so that more spatial context information is captured. ShellNet achieves 93.1% accuracy with ss is 32. However, this does not mean the larger the better, since too large receptive field can also wash out the high frequency fine structure of the features <ref type="bibr" target="#b23">[24]</ref>. We can see that when ss is set to 64, the accuracy drops to 92.8%. To balance between speed and accuracy, we set ss to 16 for object classification. <ref type="figure">Figure 1</ref> provides an accuracy plot under equal-time and equal-epoch setting. As can be seen, our method outperformed all tested methods, being the fastest and most accurate towards convergence. Compared to PointCNN <ref type="bibr" target="#b19">[20]</ref>, one of the fastest method in this experiment, we use a much simpler network architecture. To turn the point cloud into a latent canonical representation, their X-Conv operator requires to learn a transformation matrix while our method only requires a statistical computation to aggregate features. This allows our convolution to be more intuitive and easy to implement but able to achieve high performance. We also provide per-class accuracy in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Segmentation</head><p>Segmentation aims to predict the label for each point, which can also be seen as a dense pointwise classification problem. In this subsection, both object part segmentation and semantic scene segmentation are performed. We use ShapeNet dataset <ref type="bibr" target="#b43">[44]</ref> for part segmentation, which contains 16, 880 models (14, 006 models for training and 2, 874 models for testing) in 16 categories, each annotated with 2 to 6 parts and there are 50 different parts in total. For semantic segmentation, we use ScanNet <ref type="bibr" target="#b6">[7]</ref> and S3DIS dataset <ref type="bibr" target="#b1">[2]</ref> for indoor scenes, and Semantic3D <ref type="bibr" target="#b8">[9]</ref> for outdoor scenes. ScanNet consists of 1513 RGB-D reconstructed indoor scenes annotated in 20 categories. S3DIS contains 3D scans from Matterport scanners in 6 indoor areas including 271 rooms with each point is annotated with one of the semantic labels from 13 categories. Semantic3D is an online large-scale, outdoor LIDAR benchmark dataset comprising more than 4 billion annotated points with 8 classes. We follow PointCNN <ref type="bibr" target="#b19">[20]</ref> to prepare the datasets. Object Part Segmentation. Our results are reported in <ref type="table">Table 2</ref>. Per-class accuracies can be found in the supplementary document. It can be seen that our method outperforms most of the state-of-the-art techniques. Qualitative comparisons between our prediction and the ground truth are shown Method ShapeNet ScanNet S3DIS Semantic3D mpIoU OA mIoU mIoU SyncCNN <ref type="bibr" target="#b44">[45]</ref> 82.0 ---SpiderCNN <ref type="bibr" target="#b41">[42]</ref> 81.7 ---SplatNet <ref type="bibr" target="#b30">[31]</ref> 83.7 ---SO-Net <ref type="bibr" target="#b18">[19]</ref> 81.0 ---SGPN <ref type="bibr" target="#b36">[37]</ref> 82.8 -50.4 -PCNN <ref type="bibr" target="#b2">[3]</ref> 81.8 ---KCNet <ref type="bibr" target="#b29">[30]</ref> 82.2 ---KdNet <ref type="bibr" target="#b13">[14]</ref> 77.4 ---3DmFV-Net <ref type="bibr" target="#b3">[4]</ref> 81.0 ---DGCNN <ref type="bibr" target="#b37">[38]</ref> 82.3 -56.1 -RSNet <ref type="bibr" target="#b12">[13]</ref> 81.4 -56.5 -PointNet <ref type="bibr" target="#b24">[25]</ref> 80.4 73.9 47.6 -PointNet++ <ref type="bibr" target="#b26">[27]</ref> 81.9 84.5 --PointCNN <ref type="bibr" target="#b19">[20]</ref> 84.  <ref type="table">Table 2</ref>. Comparisons of segmentation tasks. Object part segmentation is performed on ShapeNet dataset <ref type="bibr" target="#b5">[6]</ref>, and semantic segmentation is performed on ScanNet <ref type="bibr" target="#b6">[7]</ref>, S3DIS dataset <ref type="bibr" target="#b1">[2]</ref>, and Semantic3D <ref type="bibr" target="#b8">[9]</ref> respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Ground truth <ref type="figure">Figure 4</ref>. Object part segmentation with the ShapeNet dataset. The example objects are a chair, lamp, skateboard, airplane, and a car. Overall, our method produces accurate predictions.</p><p>in <ref type="figure">Figure 4</ref>. It can be seen that ShellNet method can run robustly on many objects. Noted that our method only trains 20 hours to achieve such accuracy.</p><p>Indoor Semantic Scene Segmentation. The mIoU accuracies of indoor benchmarks ScanNet <ref type="bibr" target="#b6">[7]</ref> and S3DIS <ref type="bibr" target="#b1">[2]</ref> are shown in <ref type="table">Table 2</ref>. ShellNet ranks 1st on ScanNet and ranks 1st on S3DIS. For the latter, we also list the per-class scores (mIoU) in the supplementary document. The qualitative results are presented in <ref type="figure" target="#fig_2">Figure 5</ref>. We can see some misclassification are between wall, caseboard, and window as these categories are quite similar in pure geometries, and need other features like color or normal vectors to improve.</p><p>Outdoor Semantic Scene Segmentation. The Seman-tic3D <ref type="bibr" target="#b8">[9]</ref> is more challenging as it is a real-world dataset  of strongly varying point density. For a fair comparison, we excluded results without a publication. ShellNet performs well on this dataset with accuracy ranked 2nd ( <ref type="table">Table 2)</ref>. Per-class accuracy can be found in the supplementary document. The qualitative results are presented in <ref type="figure" target="#fig_3">Figure 6</ref>. Note that our method only takes the 3D coordinates as input while previous methods such as <ref type="bibr" target="#b14">[15]</ref> also used color or postprocess with CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Network Efficiency</head><p>We measure network complexity by the number of trainable parameters, floating point operations (FLOPs), and running time to analyze the network efficiency. With batch size 16, point cloud size 1024 from the ModelNet40 dataset, the statistics are reported in <ref type="table" target="#tab_3">Table 3</ref>. For all three metrics, ShellNet is better than existing methods. While being much less complex in time and space, ShellNet can still converge to the state-of-the-art accuracy very efficiently as shown in the plot in <ref type="figure">Figure 1</ref>.</p><p>The improvement in speed and memory of our work comes from the effective use of mlp and 1D convolution in our network. Particularly, on top of the proposed system-atic approach based on concentric shells for point grouping, which naturally handles multi-scale features, we only need a single mlp to learn point features in a shell, and a 1D convolution to relate features among the shells <ref type="figure" target="#fig_0">(Figure 2</ref>). This simplicity greatly reduces the number of trainable parameters and computation.</p><p>In ShellNet, the receptive field is directly controlled by the shell size. Thus, we can further analyse the performance of ShellNet with different shell sizes <ref type="figure" target="#fig_4">(Figure 7)</ref>. In equaltime comparison, ShellNet with shell size 16 performs best, achieving high accuracy in very short time. When shell size is 64, it performs slightly worse. In equal-epoch setting, using shell size 8 is not as good as the others because of smaller receptive fields. Having a receptive field that balances between size and speed yields the best convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Neighboring Point Sampling</head><p>Let the network in <ref type="figure">Figure 3</ref> as the baseline (Setting A), we conduct a series of experiments to verify the effectiveness of our network architecture and justify how neighbor points can be sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Params</head><p>FLOPs   Here we compare four settings. Particularly, Setting A is the default configuration of our classification experiment, in which random sampling is used to obtain neighbor points and each concentric shell contains a fixed number of points. Setting B, C, D are obtained by varying the neighbor sampling strategies. In Setting B, we change neighbor sampling to farthest point sampling. In Setting C, we divide a local region into equidistant shells, leading to shells that contain a dynamic number of points. In Setting D, we search nearest neighbors in the feature space instead of the 3D coordinate space. The results are shown in <ref type="table">Table 4</ref>. It shows that accuracies are similar across variants, and Setting A is the most efficient for the classification task. For segmentation, we also conduct the same experiment and found that Setting B works best in this case. The reason is that farthest point sampling in Setting B results in more uniform point distribution that can cover more geometry details, leading to more accurate segmentation.</p><p>Our method is without limitation. Particularly, we found that while our method can work with sparse and partial data, more investigations into its robustness is required. Here we provide an example of object part segmentation to demonstrate the robustness of ShellConv in <ref type="figure">Figure 8</ref>. The mpIoU accuracies for the original, sparse and partial segmentation are 82.4%, 80.2%, 72.6% respectively. For partial data, the boundary points are less accurate.  <ref type="table">Table 4</ref>. Experiments with neighbor point sampling. Setting (A) is the default strategy. Setting (B), (C), (D) are modified from (A) based on point sampling type, shell size, and neighbor query features. As can be seen, setting (B) -furthest point sampling, (C) -equidistant shells, (D) -latent features for neighborhood construction, produces similar accuracy but training and inference time becomes slower. <ref type="figure">Figure 8</ref>. Part segmentation on sparse and partial point clouds. For partial data, points on the boundaries appear to be less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced a novel approach for deep learning with 3D point clouds based on concentric spherical shells constructed from local point sets. We designed a new convolution operator named, ShellConv, which supports convolution of a point set efficiently based on shells and their statistics. This structure not only solves the convolution order problem naturally but also allows larger and more overlapped receptive field without increasing the number of network layers. Based on ShellConv, we build simple yet effective neural network that achieves the state-of-the-art results on object classification and segmentation tasks with pure point cloud inputs.</p><p>Together with recent advances in deep learning with point cloud data, our work leads to several potential future research. With the fast capability of local feature learning, it would be interesting to see how object detection and semantic instance segmentation can benefit from our work. It is also interesting to extend this work for learning with meshes. Finally, it would be of great interest to apply our approach to build autoencoders for point cloud generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>ShellConv operator. (a) For an input point cloud with/without associated features, representative points (red dots) are randomly sampled. The nearest neighbors are then chosen to form a point set centered at the representative points. The point sets are distributed across a series of concentric spherical shells (b) and the statistics of each shell is summarized by a maxpooling over all points in the shell, the features of which are lifted by an mlp to a higher dimension. The maxpooled features are indicated as squares with different colors (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>point set, and previous layer features of point set. Output: F p * Convolutional features of p. 2: {q} ← {q − p : ∀q ∈ Ω p } * Neighbor point q is localised with p as the center. 3: {F local (q)} ← {mlp(q)} * Individually lift each point q to a higher dimensional space. 4: {F (q)} ← {[F prev (q), F local (q)} * Concatenate the local and previous layer features. 5: {S} ← {S : q ∈ Ω S } * Determine which shell q belongs to according the distances from q to center p. 6: {F (S)} ← {maxpool({F (q) : q ∈ Ω S }) : ∀S} * Get fixed-size feature of each shell by a maxpool over all points in the shell. 7: F p ← conv({F (S)}) * Perform a 1D convolution with all shell features from inner to outer. 8: return F p sentative points are randomly sampled (red dots in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Semantic segmentation for indoor scenes in the S3DIS dataset<ref type="bibr" target="#b1">[2]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Semantic segmentation for outdoor scenes in the Semantic3D dataset<ref type="bibr" target="#b8">[9]</ref>. Left: colored point clouds (for visualization only). Right: our segmentation. Note that the ground truth of the test set is not publicly available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>The accuracy of point cloud classification versus time and epochs with different shell sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Core Operator</cell><cell>input</cell><cell>OA</cell></row><row><cell>FPNN [21]</cell><cell>1D Conv.</cell><cell>P</cell><cell>87.5</cell></row><row><cell>Vol. CNN [26]</cell><cell>3D Conv.</cell><cell>V</cell><cell>89.9</cell></row><row><cell>O-CNN [35]</cell><cell>Sparse 3D Conv.</cell><cell>O</cell><cell>90.6</cell></row><row><cell>Pointwise [12]</cell><cell>Point Conv.</cell><cell>P</cell><cell>86.1</cell></row><row><cell>PointNet [25]</cell><cell>Point MLP</cell><cell>P</cell><cell>89.2</cell></row><row><cell>PointNet++ [27]</cell><cell>Multiscale Point MLP</cell><cell>P+N</cell><cell>90.7</cell></row><row><cell>PointCNN [20]</cell><cell>X-Conv</cell><cell>P</cell><cell>92.2</cell></row><row><cell>ShellNet (ss=8)</cell><cell>ShellConv</cell><cell>P</cell><cell>91.0</cell></row><row><cell>ShellNet (ss=16)</cell><cell>ShellConv</cell><cell>P</cell><cell>93.1</cell></row><row><cell>ShellNet (ss=32)</cell><cell>ShellConv</cell><cell>P</cell><cell>93.1</cell></row><row><cell>ShellNet (ss=64)</cell><cell>ShellConv</cell><cell>P</cell><cell>92.8</cell></row></table><note>. Comparisons of classification accuracy (overall accuracy %) on ModelNet40 [40] with input type denoted as O (Octrees), V (Voxels), P (Points) and N (Normals). Performance of ShellNet is tested with different shell size (ss)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Trainable parameters, FLOPs and running time comparisons. Compared to previous methods, ShellNet is lightweight and fast while being accurate. Reducing the receptive field (small RF) by setting a smaller shell size can make the computation even faster as neighbor query becomes cheaper.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://hkust-vgd.github.io/shellnet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors acknowledge support from the SUTD Digital Manufacturing and Design Centre funded by the Singapore National Research Foundation, and an internal grant from HKUST (R9429).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shapenet: An information-rich 3d model repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic3d.net: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubor</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS annals of the photogrammetry, remote sensing and spatial information sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere-Pau</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Àlvar</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix Järemo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointgrid: A deep network for 3d shape understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9204" to="9214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How receptive field parameters affect neural learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omohundro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R. P. Lippmann, J. E. Moody, and D. S. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="757" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SPLATNet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive o-cnn: A patch-based deep representation of 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
