<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Classification with FineCoarse Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">G</forename><surname>Bors</surname></persName>
							<email>adrian.bors@york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Classification with FineCoarse Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A rich representation of the information in video data can be realized by means of frequency analysis. Fine motion details from the boundaries of moving regions are characterized by high frequencies in the spatio-temporal domain. Meanwhile, lower frequencies are encoded with coarse information containing substantial redundancy, which causes low efficiency for those video models that take as input raw RGB frames. In this work, we propose a Motion Band-pass Module (MBPM) for separating the fine-grained information from coarse information in raw video data. By representing the coarse information with low resolution, we can increase the efficiency of video data processing. By embedding the MBPM into a two-pathway CNN architecture, we define a FineCoarse network. The efficiency of the FineCoarse network is determined by avoiding the redundancy in the feature space processed by the two pathways: one operates on downsampled features of low-resolution data, while the other operates on the fine-grained motion information captured by the MBPM. The proposed FineCoarse network outperforms many recent video processing models on Kinetics400, UCF101 and HMDB51. Furthermore, our approach achieves the state-of-the-art with 57.0% top-1 accuracy on Something-Something V1.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video sequences are one of the drivers for information representation, and their ubiquitousness has increased exponentially during recent years. Applications, such as autonomous driving technology, drones and robots are driving the demand for new video processing methods.</p><p>An effective way to extend the usage of Convolution Neural Networks (CNNs) from the image to the video domain is by expanding the convolution kernels from 2D to 3D <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. Since the progress made by I3D <ref type="bibr" target="#b2">[3]</ref>, the main research effort in video processing and classification has been directed towards designing new 3D architectures. However, 3D CNN video models are more computationally MBPM <ref type="figure">Figure 1</ref>: Motion Band-pass Module (MBPM) distills the fine-grained motion features from the raw video sequence. For every three consecutive RGB frames, the MBPM generates a one-frame output, which substantially reduces redundancy in the feature space.</p><p>intensive than 2D CNN image models. Some recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> increase the efficiency of 3D CNNs by reducing the redundancy in the model parameters. Another reason for the heavy computation required by video models is that natural video data contains substantial redundancy in the spatio-temporal dimensions. In video data, fine-grained information describing motion happening in the boundary regions is crucial for video classification while the coarse information, such as smooth background textures sharing information between neighboring locations, contains substantial redundancy. For efficient processing, we can decompose a video into fine and coarse components. Subsequently, we separately process the fine component and the coarse component: high-complexity processing for the fine-grained information and low-complexity processing for the coarse information.</p><p>This study proposes a lightweight and end-to-end trainable motion feature extraction method called Motion Bandpass Module (MBPM), which can distill the video mo-tion information conveyed within a specific frequency bandwidth in the frequency domain. As shown in <ref type="figure">Figure 1</ref>, by applying the MBPM to the video sequence (3 segments), the number of representative frames is reduced from 9 to 3 whilst retaining the essential motion information. Our experiments demonstrate that by simply replacing the RGB frame input with the motion representation extracted by our MBPM, the performance of existing video models can be boosted. Secondly, we design a two-pathway multi-scale architecture, called the FineCoarse Network, whose processing pipeline is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. One pathway, called Fine, contains an MBPM, responsible for distilling and processing fine-grained motion information. The other pathway, called Coarse, is devised to process the coarse information encoded with global smooth video structures. In order to fuse the information from different pathways, we design the lateral connection module (LCM), which is set up between the Fine and Coarse pathways. We demonstrate that the LCM is the key factor to the overall model optimization success during the experiments.</p><p>Compared with the frame summarization approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45]</ref>, MBPM retains the strict temporal order of the frame sequences, which is considered essential for long-term temporal relationship modeling. Compared with optical flowbased motion representation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>, the motion representation captured by MBPM has smaller temporal size (e.g. for every 3 RGB frames, the MBPM encodes only one-frame). Meanwhile, Octave Convolution <ref type="bibr" target="#b4">[5]</ref>, bL-Net <ref type="bibr" target="#b3">[4]</ref> and SlowFast networks <ref type="bibr" target="#b11">[12]</ref> only reduce the input redundancy in either the spatial or temporal dimensions. Instead, the proposed FineCoarse network reduces the redundancy in the joint spatio-temporal space.</p><p>Our contributions can be summarized as follows:</p><p>• A novel Motion Band-pass Module (MBPM) is proposed for motion information distillation. The volume of the new motion cue extracted by the MBPM is 3× smaller than the original video, reducing temporal redundancy significantly. • We design a two-pathway FineCoarse network that separately processes the fine-grained motion information and coarse-grained information. By separating the fine-grained information using MBPM, we can safely downsample the coarse-video information to further reduce redundancy. • Extensive experiments demonstrate the superiority of the proposed FineCoarse network over a wide range of models on four standard video benchmarks, i.e. Kinetics400 <ref type="bibr" target="#b2">[3]</ref>, Something-Something V1 <ref type="bibr" target="#b14">[15]</ref>, UCF101 <ref type="bibr" target="#b35">[36]</ref>, HMDB51 <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Spatio-temporal Networks. Early works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7]</ref> attempt to extend the success of 2D CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref> in the image domain to the video domain. Representatively, the two-stream model <ref type="bibr" target="#b33">[34]</ref> and its variants <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b13">14]</ref> utilize optical flow as an auxiliary input modality for effective temporal modeling. The more recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, given the progress in GPU performance, tend to exploit the computationally intensive 3D convolution. Meanwhile, some studies focus on improving the 3D CNN architecture from the perspective of efficiency, such as P3D <ref type="bibr" target="#b31">[32]</ref>, R(2+1)D <ref type="bibr" target="#b42">[43]</ref>, S3D <ref type="bibr" target="#b48">[49]</ref>, TSM <ref type="bibr" target="#b26">[27]</ref>, CSN <ref type="bibr" target="#b41">[42]</ref>, X3D <ref type="bibr" target="#b10">[11]</ref>. Our study is complementary to these methods. Our FineCoarse architecture can benefit from the efficiency of these 3D CNN methods by simply adopting them as the backbone of our model. Motion Representation. Optical flow as a short-term motion representation has been widely used in many video applications. However, the optical flow estimation in largescale video datasets is inefficient. Some recent works use deep learning to improve the optical flow estimation quality, such as Flow Nets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20]</ref>. Some other methods aim to explore new end-to-end trainable motion cues, such as OFF <ref type="bibr" target="#b36">[37]</ref>, TVNet <ref type="bibr" target="#b8">[9]</ref>, EMV <ref type="bibr" target="#b51">[52]</ref>, Flow-of-Flow <ref type="bibr" target="#b30">[31]</ref>, Dynamic Image <ref type="bibr" target="#b1">[2]</ref> and PA <ref type="bibr" target="#b52">[53]</ref>. Different from these approaches, the proposed motion representation module is more like a basic component of our 3D architecture. Low Information Redundancy. In the deep learning era, bL-Net <ref type="bibr" target="#b3">[4]</ref> adopts a downsampling strategy that operates on the block level aiming to reduce the spatial redundancy of its feature maps, and then uses two branches to separately process the feature maps with different resolutions. Instead of operating on the block level, Octave Convolution <ref type="bibr" target="#b4">[5]</ref> directly replaces the convolutions in existing architectures for reducing feature redundancy. The spatial information is decomposed into low-frequency and high-frequency components using the Octave Convolution, and then each data component is processed separately. For video classification, the bLV-Net <ref type="bibr" target="#b9">[10]</ref> extends the idea of bL-Net <ref type="bibr" target="#b3">[4]</ref> to the temporal dimension. SlowFast networks <ref type="bibr" target="#b11">[12]</ref> introduce two pathways and decompose the input into the slow and fast components along the temporal dimension for efficient temporal modeling. Different from the previous methods, our FineCoarse networks, described in Section 4, decompose the video input into fine and coarse components by introducing a predefined filter named MBPM, described in Section 3. Moreover, FineCoarse networks reduce the feature redundancy in the joint spatio-temporal space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion Band-pass Module (MBPM)</head><p>Firstly, we introduce a 3D band-pass filter, which can distill the video motion information conveyed within a specific frequency bandwidth in the frequency domain. A video clip of T frames can be defined as a function with three arguments, I (t) (x, y), where x, y indicate the spatial dimensions, while t is the temporal dimension. The function value I (t) (x, y) corresponds to the pixel value at position (x, y) in t-th frame of an arbitrary channel in the video. When considering the multi-channel case, we repeat the same procedure for each color channel, which is omitted here for simplifying the notations. The output Γ of the 3D band-pass filter is given by :</p><formula xml:id="formula_0">Γ(x, y, t) = ∂ 2 ∂t 2 I (t) (x, y) * LoG σ (x, y) , ≈ t−1≤i≤t+1 h(i) · [I (i) (x, y) * LoG σ (x, y)], h(i) = 2 3 if i = t, − 1 3 otherwise,</formula><p>(1) for t = 1, . . . , T and * represents the convolution operation.</p><formula xml:id="formula_1">LoG σ (x, y) = 2 G σ (x, y) = − e − x 2 +y 2 2σ 2 πσ 4 1 − x 2 + y 2 2σ 2 .</formula><p>(2) LoG σ (x, y) is a two-dimensional Laplacian of Gaussian with the scale parameter σ. We approximate the second derivative with respect to t by convolving with a discrete kernel of temporal weights [− 1 3 , 2 3 , − 1 3 ], which is literally function h(i) from (1).</p><p>From equations (1) and <ref type="bibr" target="#b1">(2)</ref> we can observe that the 3D filtering is fully-differentiable. In order to make the 3D band-pass filtering compatible with CNNs, we approximate it with two sequential channel-wise 1 convolutional layers <ref type="bibr" target="#b32">[33]</ref>, as shown in <ref type="figure">Figure 1</ref>. We name the discrete approximation Motion Band-pass Module (MBPM) which can be expressed in an engineering form as follows:</p><formula xml:id="formula_2">Γ ≈ MBPM(I) = H 3×1×1 s (LoG 1×k×k σ (I)),<label>(3)</label></formula><p>where LoG 1×k×k σ is referred to as a spatial channel-wise convolutional layer <ref type="bibr" target="#b32">[33]</ref> with a k × k kernel, each channel of which is initialized with a Laplacian of Gaussian distribution with scale σ. The sum of kernel values is normalized to 1. Meanwhile, H 3×1×1 s is referred to as a temporal channelwise convolutional layer with a stride s. In each channel, the kernel value of H 3×1×1</p><formula xml:id="formula_3">s is initialized to [− 1 3 , 2 3 , − 1 3 ]</formula><p>, which is a high-pass filter. In order to let the kernel parameters of MBPM fine-tune on the video datasets, we include the MBPM in the CNN training process to form an endto-end training pipeline, which is optimized with the video classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FineCoarse Networks</head><p>The FineCoarse network contains two pathways, Fine and Coarse, operating in parallel on two distinct video data components, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. While the Fine pathway operates on the fine-grained motion representation extracted by the MBPM, the Coarse pathway operates on the downsampled version of raw RGB frames. The Fine and Coarse pathways are bridged by multiple lateral connection modules ( see Section 4.2) for information fusion. The Temporal Shift Module (TSM) <ref type="bibr" target="#b26">[27]</ref> can enable 2D CNNs with local temporal modeling ability, at no extra computational cost, by shifting part of the channels along the temporal dimension. Thus, we utilize TSM ResNet <ref type="bibr" target="#b26">[27]</ref> as the backbones of the two pathways in the proposed FineCoarse network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine and Coarse pathways</head><p>Fine pathway. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the Fine pathway is designed to learn fine-grained features. In the Fine pathway, the raw RGB frames are first processed by an MBPM to extract the critical motion information located at the boundaries of moving objects or regions of significant spatiotemporal movement change. Then, the MBPM output is considered the input to the backbone CNN to learn the finegrained motion features. For the MBPM in the Fine pathway, the convolutional stride s of H 3×1×1 s from equation <ref type="formula" target="#formula_2">(3)</ref> is set to 3, which means that for every three consecutive frames, the MBPM generates a one-frame output, as shown in <ref type="figure">Figure 1</ref>. The MBPM output preserves the temporal order of the videos while significantly reducing the redundant temporal information. We intend to utilize larger spatial input sizes to the Fine pathway to extract more distinct finegrained textures. Coarse pathway. The Coarse pathway focuses on processing coarse-grained information, representing the characteristics of large regions of movement, such as the movement happening in the smooth-textured background regions. In the Coarse pathway, we first perform 3D downsample pooling on the raw RGB frames to produce the coarse-grained information and reduce redundancy. Then, the downsampled features are considered the input to the backbone CNN to learn deep coarse-grained features. Particularly, the 3D downsample pooling's temporal stride is set to 3, which means the temporal size of the downsampled video is <ref type="bibr" target="#b0">1</ref> 3 of the original video. In the spatial dimensions, we perform bilinear downsampling to reduce the redundant spatial information shared in neighboring locations. We explore the effect of various spatial input sizes of the Coarse pathway in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Lateral Connection Module (LCM)</head><p>Lateral connections <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> are commonly used for merging the features from different levels of CNNs in order to learn robust multi-scale feature representations. In the FineCoarse network, we design a novel lateral connection module (LCM) which has an MBPM embedded. The LCMs established between the two pathways provide a mechanism for information exchange, enabling an optimal fusion of video representation information. The fusion direction of our LCM reverses back and forth, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, unlike the unidirectional lateral connections in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref> whose information fusion direction is fixed, always fusing the information from one pathway to the other. We denote the two inputs of the LCM from the i-th residual blocks in the Fine and Coarse pathways, as x i f and x i c , respectively. For simplifying the notation, we assume that x i f and x i c are of the same size. When their sizes are different, we adopt bilinear interpolation to match them in size. The output y i f and y i c for the Fine and Coarse, respectively, is</p><p>given by</p><formula xml:id="formula_4">y i f = BN(MBPM(x i c )) + x i f if mod(i, 2) = 0, x i f otherwise, y i c = x i c if mod(i, 2) = 0, BN(Wx i f ) + x i c otherwise, i = 1, 2, ..., B<label>(4)</label></formula><p>where B denotes the number of residual blocks in the ResNet backbone. W is the weight for the linear transformation, implemented as a 1×1×1 convolution. BN indicates Batch Normalization <ref type="bibr" target="#b20">[21]</ref> of which the weights are initialized to zero. For the MBPM in an LCM, the convolutional stride s of H 3×1×1 s from equation <ref type="formula" target="#formula_2">(3)</ref> is set to 1, maintaining the same temporal size. By default, we place an LCM between the two pathways right after each pair of residual blocks. By embedding an MBPM into an LCM, we can further distill the fine-grained information from the coarse pathway's features during the information fusion process. This design gives the best performance in our practices. More details about exploring the design of LCM can be found in Appendix A from the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first introduce the datasets and implementation details. Then we conduct ablation studies to investigate the efficiency and effectiveness of our proposed methods. Finally, we compare our methods with the stateof-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Implementation Details</head><p>Datasets. We evaluate our approach on Something-Something V1 <ref type="bibr" target="#b14">[15]</ref>, Kinetics400 <ref type="bibr" target="#b2">[3]</ref>, UCF101 <ref type="bibr" target="#b35">[36]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>. Most of the videos in Kinetics400, UCF101 and HMDB51 can be accurately classified by only considering their background scene information, and the temporal relationships within frames are not very important. In Something-Something V1 <ref type="bibr" target="#b14">[15]</ref>, many action categories are symmetrical (e.g. "Pulling something from left to right" and "Pulling something from right to left"). Discriminating these symmetric actions requires models with strong temporal modeling ability. Since Something-Something is widely used for evaluating temporal modeling efficiency, we mainly evaluate FineCoarse networks on this dataset. Training &amp; Testing. Our models are pretrained on Ima-geNet <ref type="bibr" target="#b5">[6]</ref>, unless specified otherwise. For training, we utilize the dense sampling strategy <ref type="bibr" target="#b46">[47]</ref> for Kinetics400. As for other datasets, we utilize the sparse sampling strategy as shown in <ref type="figure" target="#fig_0">Figure 2</ref> where a video is equally divided into N segments, and we randomly sample 3 consecutive frames in each segment to constitute a video clip of length T =3N . Unless specified otherwise, a default video clip is composed Predict ion t rained 3 x 3 t rained 7 x 7 t rained 9 x 9 t rained 11 x 11 unt rained 3 x 3 unt rained 7 x 7 unt rained 9 x 9 unt rained 11 x 11   <ref type="bibr" target="#b45">[46]</ref> 87.0% TV-L1 Flow † <ref type="bibr" target="#b50">[51]</ref> 88.5% Dynamic Image † <ref type="bibr" target="#b1">[2]</ref> 86.2% FlowNet2.0 † <ref type="bibr" target="#b19">[20]</ref> 87.3% TVNet † <ref type="bibr" target="#b8">[9]</ref> 88.6% PA <ref type="bibr" target="#b52">[53]</ref> 89.5%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MBPM (ours) 90.3%</head><p>of T =24 RGB frames. During the test, we sample 1 clip with T frames from the video for efficient inference, which is used in all our ablation studies. When pursuing high accuracy, we consider sampling multiple clips&amp;crops from the video and averaging the prediction scores of multiple spacetime "views " (spatial crops×temporal clips) used in <ref type="bibr" target="#b11">[12]</ref>. More training details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Studies for MBPM</head><p>Following the practice in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref>, we conduct the ablation studies on UCF101 (split 1), commonly used to evaluate motion patterns, to study the settings and effectiveness of the MBPM. The model we use in this subsection has the same structure as the Fine pathway from <ref type="figure" target="#fig_0">Figure 2</ref>, where an MBPM is embedded right in front of the backbone network (TSM ResNet50 <ref type="bibr" target="#b26">[27]</ref>). In <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_2">Figure 3</ref>, we report the accuracy with the efficient inference protocol (center crop×1 clip) with 224 2 spatial size. Instantiations and Settings. In the MBPM, the scale parameter σ from equation <ref type="formula">(2)</ref> and the kernel size of the spatial channel-wise convolution LoG 1×k×k σ have a significant impact on the performance. We vary the scale parameter σ ∈ (0, 2] and the kernel size for searching for the appropriate settings. Meanwhile, in order to highlight the importance of the training for the MBPM, we compare the performance of trained MBPMs with the untrained MBPMs whose kernel weights are not optimized with the classification loss. The results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We summarize two things: 1) the optimal value of σ of the MBPM changes as its kernel size changes, and the MBPM with σ = 1.1 and a spatial kernel size 9×9 gives the best performance within the searching range. 2) optimizing the parameters of MBPM with the video classification loss generally produces higher prediction accuracy. In the following experiments, we set the MBPM in the Fine pathway as trainable with the scale parameter σ = 1.1 and the kernel size 9×9, unless specified otherwise. Effectiveness of the MBPM. We draw an apple-to-apple comparison between the proposed MBPM and other motion representation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. The comparison results are shown in <ref type="table" target="#tab_0">Table 1</ref>. We reimplement some of these methods with the same backbone network, ResNet50 when the results provided by the authors are not directly comparable. The motion representations produced by these methods are used as inputs to ResNet50. The prediction scores are obtained by the average consensus of eight temporal segments <ref type="bibr" target="#b45">[46]</ref>. More detail about the implementations can be found in Appendix C. Note that our proposed MBPM achieves 1.8% higher video prediction accuracy than the optical flow <ref type="bibr" target="#b50">[51]</ref> and consistently outperforms the listed CNNbased motion representation methods, which indicates that the proposed MBPM can effectively extract motion cues from videos. Visualization analysis. We present the visualization results of some video sequences and their corresponding MBPM outputs in <ref type="figure" target="#fig_3">Figure 4</ref>. The extracted representations are stable in cases of jittering and other minor camera movements. <ref type="figure" target="#fig_3">Figure 4</ref> shows that the MBPM not only suppresses the stationary information and the background movement, but also highlights the boundaries of moving objects, which are of vital importance for action discrimination. For example, in the "spinning poi" video, from <ref type="figure" target="#fig_3">Figure 4</ref>(1), MBPM highlights the poi's movement rather than the movement of the background or the performer. More visualization results and visual comparison with other motion representation methods are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies for FineCoarse Networks</head><p>We conduct the ablation studies on Something-Something V1 <ref type="bibr" target="#b14">[15]</ref> to evaluate the temporal modeling effi-  ciency of the FineCoarse architecture with different configurations. We apply the efficient inference protocol (center crop×1 clip) during testing. FineCoarse vs. Coarse+Fine. In order to evaluate the effectiveness of the proposed FineCoarse architecture, we compare FineCoarse with the simple fusion (Coarse+Fine), which mimics the two-stream model <ref type="bibr" target="#b33">[34]</ref> by averaging the predictions of two pathways trained separately. <ref type="table" target="#tab_1">Table 2a</ref> shows that the individual Fine pathway provides substantially higher top-1 accuracy than the individual Coarse pathway (48.0% vs. 46.5%). Not surprisingly, the fusion of two individual pathways (Coarse+Fine) generates higher top-1 accuracy (50.3%) than the individual pathways, which indicates that the features learned by the Coarse pathway and by the Fine pathway are complementary. Surprisingly, the FineCoarse model has 51.6% top-1 accuracy, which is 1.3% better than the fusion, Coarse+Fine. The high-performance gain strongly demonstrates the advantages of the proposed FineCoarse architecture. Fusion strategies applied at the end of the Fine and Coarse pathways also influence the performance of FineCoarse networks. <ref type="table" target="#tab_1">Table 2c</ref> shows the results of various fusion strategies. We observe that the average fusion gives the best result among the listed methods, while the concatenation fusion is second only to the averaging. Besides, placing the average fusion layer after the fully-connected layer is better than placing it before. Effectiveness of the LCM. We can set a maximum of up to 16 LCMs in the FineCoarse architecture (i.e. 3 in res2, 4 in res3, 6 in res4, 3 in res5 2 ), when using ResNet50 <ref type="bibr" target="#b17">[18]</ref> as the backbone. For the LCMs in stage res2, res3 and res4, we set the spatial kernel size of MBPM as 7×7, and the scale parameter σ = 0.9. As for stage res5, whose feature size is relatively small, the kernel size is therefore set to 3×3. In the preliminary work, we have verified that injecting LCMs to any stage of the network could increase the accuracy, more details of which can be found in Appendix E. We find that FineCoarse without LCMs produces a lower accuracy than the simple ensemble of Coarse+Fine (49.6% vs. 50.3%). From <ref type="table" target="#tab_1">Table 2b</ref>, we can observe that the model performance improves gradually as the number of LCMs increases. The model with 8 LCMs produces 50.7% top-1 accuracy, and the model with 16 LCMs produces 51.6% top-1 accuracy. The substantial performance gains demonstrate the importance of using LCMs for the FineCoarse network. Spatial-temporal input sizes. In the FineCoarse network, the Fine pathway takes as input the MBPM output, which has the same spatial size as the raw video clip while the temporal size is a third of the length of the raw video clip. Meanwhile, the Coarse pathway takes as input the downsampled version of the raw video clip. Larger spatiotemporal inputs to the Fine pathway could better capture the fine-grained motion, while smaller inputs to the Coarse pathway would reduce the computational cost. <ref type="table" target="#tab_2">Table 3</ref> shows the effect of different input sizes to the two pathways. Reducing the spatial size of the input to the Coarse pathway within an appropriate range does not significantly impact the performance but reduces the computational cost. Meanwhile, by increasing the spatial input size of the Fine pathway, the models have higher accuracy. With the same temporal size 8 for the inputs, the spatial size combination   slightly better top-1 accuracy (+0.1%) than the combination of 224 2 + 224 2 but saves 5.2 GFLOPs in computational cost. We also attempt to reduce the temporal input size of the Coarse pathway. However, this would result in a significant performance drop. One possible explanation is that because of the effect of the 3D downsampling in the Coarse pathway, the input's temporal size is only a third of the raw video clip. An even smaller temporal size might fail to preserve the correct temporal order of the video, which is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons with the State-of-the-Art</head><p>We compare FineCoarse with current state-of-the-art methods on four datasets. In the FineCoarse networks, the Coarse and Fine pathways' spatial input size is set to 160 2 and 256 2 , respectively. For pursuing high accuracy, we attempt to use longer clip lengths and a deeper backbone network, ResNet101. Results on Something-Something V1. <ref type="table" target="#tab_3">Table 4</ref> summarizes the comprehensive comparison, including the inference protocols, corresponding computational costs (FLOPs) and the prediction accuracy. Our method surpasses all other methods by a good margin. For example, the multiclip accuracy of FineCoarse 24f with TSM ResNet50 is 7.2% higher than NL I3D GCN 32f <ref type="bibr" target="#b47">[48]</ref> while requiring 5× fewer GFLOPs. Among the models based on ResNet50, FineCoarse 48f has the highest top-1 accuracy (54.3%), which surpasses the second-best, TEA 16f <ref type="bibr" target="#b25">[26]</ref>, by a margin of +2%. Furthermore, our signal-clip FineCoarse 24f has higher accuracy (51.7%) than most other multi-clip models, requiring only 60 GFLOPs, which demonstrates the high efficiency and strong temporal modeling ability of our model. By adopting a deeper backbone (TSM ResNet101), FineCoarse 48f has 54.9% top-1 accuracy, higher than any single model. By averaging the prediction results of FineCoarse, Coarse and Fine that are trained independently, we achieve the state-of-the-art top-1/5 accuracy (57.0%/83.7%). Results on Kinetics400, UCF101 and HMDB51. <ref type="table" target="#tab_4">Table 5</ref> shows the comparison results on Kinetics400. FineCoarse 72f achieves 77.3%/93.2% top-1/5 accuracy, which is better than the 3D CNN-based architecture, I3D <ref type="bibr" target="#b2">[3]</ref>, by a big margin of +6.2%/3.9%. Compared with SlowFast 4×16 , which is also a multi-pathway model, FineCoarse 72f achieves +1.7% higher top-1 accuracy. FineCoarse 72f is 2.7% better than Oct-I3D <ref type="bibr" target="#b4">[5]</ref>, which is a method relying on reducing feature redundancy, for top-1 accuracy. The high-efficiency of our method is reflected in the fact that the proposed FineCoarse network can process more video frames requiring lower computational complexity, compared with methods that have similar accuracy results to ours. For example, FineCoarse 72f processes ∼2× more video frames, requiring 3.6× fewer FLOPs, when compared with SmallBigNet <ref type="bibr" target="#b24">[25]</ref>. <ref type="table" target="#tab_5">Table 6</ref> shows the results on two small-scale datasets, UCF101 and HMDB51. We pretrain our model on Kinetics400 to avoid the overfitting problem. The accuracy of our method is obtained by the inference protocol (3 crops×2 clips). FineCoarse with TSM ResNet50 outperforms most other methods except for I3D RGB+Flow , which uses additional optical flow input modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we develop a novel video representation learning method called Motion Band-pass Module (MBPM), which can capture non-trivial motion cues. We design an efficient and effective spatio-temporal architecture called FineCoarse, which separately processes finegrained and coarse-grained information of video data, to reduce the redundancy in the feature space. We hope that our proposed principles can provide valuable guidance for designing temporal modeling and other areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Designs of Lateral Connection Module (LCM)</head><p>This Appendix is relevant to Section 4.2 from the main paper. In the following, we test various designs for the Lateral Connection Module (LCM). Various LCM designs are illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. We implement the unidirectional connection designs, LCM-A and LCM-B, which are shown in the first two diagrams from <ref type="figure" target="#fig_4">Figure 5</ref>. The unidirectional connection design is adopted in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref> for merging the features from different levels of CNNs. Meanwhile, we implement the proposed bidirectional designs, LCM-C and LCM-D, which are shown in the third and fourth diagrams from <ref type="figure" target="#fig_4">Figure 5</ref>. <ref type="table" target="#tab_6">Table 7</ref> shows the performance of various LCMs on Something-Something V1 <ref type="bibr" target="#b14">[15]</ref>. The bidirectional design LCM-C has higher accuracy than the unidirectional designs LCM-A and LCM-B. Among the listed designs, LCM-D, detailed in Section 4.2 of the main paper, which reverses the information fusion direction back and forth, provides the highest accuracy. We also compare LCM-D with LCM-E, which does not contain an MBPM. As a result, LCM-E shows lower accuracy than LCM-D, which demonstrates the importance of MBPM for the LCM. We use LCM-D as the default LCM in the rest of the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More training details</head><p>These additional explanations are useful for Section 5.1 from the main paper. For training, we use Stochastic Gradient Descent (SGD) with momentum 0.9 and cosine learning rate schedule to train all the models. In order to prevent overfitting, we add a dropout layer before the classification layer of each pathway in the FineCoarse networks. Following the experimental settings in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref>, the learning rate and weight decay parameters for the fully-connected layers are set to 5× higher than the convolutional layers. Meanwhile, we only apply L2 regularization to the weights in the convolutional and full-connected layers to avoid overfitting. For Kinetics400 <ref type="bibr" target="#b2">[3]</ref>, the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.08, 512 (8 samples per GPU), 100, 2e-4 and 0.5, respectively. For Something-Something V1 <ref type="bibr" target="#b14">[15]</ref>, these hyperparameters are set to 0.12, 256, 50, 8e-4 and 0.8, respectively. For Kinet-ics400 and Something-Something V1, we use linear warmup <ref type="bibr" target="#b29">[30]</ref> for the first 10 epochs and 7 epochs respectively to overcome early optimization difficulty. When training the models on two small-scale datasets, UCF101 <ref type="bibr" target="#b35">[36]</ref> and HMDB51 <ref type="bibr" target="#b23">[24]</ref>, we finetune the Kinetics400 models on them and freeze all of the batch normalization <ref type="bibr" target="#b20">[21]</ref> layers except for the first one to avoid overfitting, following the settings in TSN <ref type="bibr" target="#b45">[46]</ref>. For UCF101 and HMDB51, the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.001, 64 (4 samples per GPU), 10, 1e-4 and 0.8, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details of Effectiveness of the MBPM</head><p>These additional explanations are useful for Section 5.2 from the main paper. We provide the implementation details for the comparative experiments of MBPM with other mainstream motion representation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. We follow the experimental settings on PA <ref type="bibr" target="#b52">[53]</ref> for fair comparison. The backbone networks for all the methods are ResNet50 <ref type="bibr" target="#b17">[18]</ref>. We use the computer code provided by the original authors for these methods to generate the network inputs. For any kind of motion representation, we divide the representation of a video into 8 segments and randomly select one frame of the representation for each segment. Following the practices in TSN <ref type="bibr" target="#b45">[46]</ref> and PA <ref type="bibr" target="#b52">[53]</ref>, the output activations of 8 segments are averaged as the final prediction score. In our reimplementation, Dynamic Image <ref type="bibr" target="#b1">[2]</ref> generates one dynamic image for every 6 consecutive RGB frames, which consumes the same number of RGB frames as PA <ref type="bibr" target="#b52">[53]</ref>. Our MBPM generates one representative frame for every 3 consecutive RGB frames. As for TVNet <ref type="bibr" target="#b8">[9]</ref> and TV-L1 Flow <ref type="bibr" target="#b50">[51]</ref>, a one-frame input to the backbone network is formed by stacking 5 frames of the estimated flow along the channel dimension, which totally consumes 6 RGB frames. All the models are pretrained on ImageNet. We use the same hyperparameters to train all the models: the initial learning rate, batch size, total epochs, weight decay and dropout ratio are set to 0.01, 64 (4 samples per GPU), 80, 1e-4 and 0.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization examples</head><p>These additional explanations and results are useful for Section 5.2 from the main paper. For better observation, we use the optical flow visualization approach used in <ref type="bibr" target="#b0">[1]</ref> to visualize the motion representations. From the visualization examples shown in <ref type="figure">Figure 6</ref>, we can observe that TVNet <ref type="bibr" target="#b8">[9]</ref>, PA <ref type="bibr" target="#b52">[53]</ref> and MBPM are absorbed in capturing motion boundaries. In comparison with both TVNet <ref type="bibr" target="#b8">[9]</ref> and PA <ref type="bibr" target="#b52">[53]</ref>, the motion boundaries captured by our MBPM are  <ref type="figure">Figure 6</ref>: Comparison in the visualization between different motion representations on the UCF101. TV-L1 Flow <ref type="bibr" target="#b50">[51]</ref> evaluates the movement in every spatial positions, while TVNet <ref type="bibr" target="#b8">[9]</ref>, PA <ref type="bibr" target="#b52">[53]</ref> and our MBPM capture the outline of the moving objects. Compared with TVNet and PA, the motion representation generated by the proposed MBPM contains less noise caused by jittering or minor camera movements. smoother and contain less noise, which could be the reason for the higher performance by MBPM. <ref type="figure">Figures 9-11</ref> display the motion representation extracted by the MBPM for eight different sequences from various video datasets used for the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Which stage to add LCMs?</head><p>These additional explanations and results are useful for Section 5.3 from the main paper. ResNet50 contains four processing stages, named res2, res3, res4, res5, respectively. We evaluate whether or not inserting LCMs to a stage is helpful for the performance in a progressive manner. <ref type="table" target="#tab_7">Table 8</ref> shows the results on Something-Something V1. These results show that adding LCMs to all processing stages is helpful for performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Kernel visualization</head><p>In <ref type="figure" target="#fig_5">Figure 7</ref>, we visualize the kernel of the spatial convolution LoG 1×k×k σ of MBPM in the Fine pathway. The kernel weights do not change their values much after training, which suggests that untrained MBPM has already learned some common features before training. In <ref type="figure">Figure 8</ref>, we visualize the first channel of the 64 filters in the first layers of the FineCoarse network and the baseline (TSM ResNet50). We can observe that the Fine and Coarse pathways' filters have quite distinct shapes in their kernels, suggesting that the Fine and Coarse pathways learned different types of features after training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The FineCoarse network is made up of two parallel pathways : Fine and Coarse. 'lc' indicates lateral connection. The Coarse pathway takes as input the downsampled RGB frames, while the Fine pathway takes as input the output of the MBPM. A fusion strategy, such as concatenation is applied at the end of the two pathways. The final prediction is obtained by averaging the prediction scores across multiple frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study results of various scale parameters σ and kernel sizes k × k of the spatial channel-wise convolution in MBPM. Dash lines indicate trained kernels, while solid lines indicate the untrained kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Example video sequences and their MBPM outputs. The four video clips (1)-(4) are randomly picked from Kinetics, Something-Something, UCF101 and HMDB51, respectively. crucial for temporal relationship modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Designs of LCM. Bilinear interpolation is used for resizing the feature maps when x i c and x i f do not have the same spatial size. i refers to the index of the residual block, mentioned in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the spatial channel-wise convolution LoG 1×k×k σ of MBPM in the Fine pathway before and after training on Kinetics400. The 9×9 channel-wise convolution is initialized with a Laplacian of Gaussian with the scale parameter σ = 1.1. In the visualization, red indicates positive values, while blue indicates negative values. The brighter the color, the larger the absolute value indicated. Best viewed in color and zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :Figure 10 :Figure 11 :</head><label>891011</label><figDesc>Visualization of the first channels of the 64 conv1 filters of FineCoarse network after training on Kinetics400. All the 64 filters have a size of 7 × 7. From left to right, in (a), (b) and (c), we respectively present the trained conv1 filters in the Fine pathway, Coarse pathway and TSM ResNet50. In the visualization, red indicates positive values, while blue indicates negative values. The brighter the color, the larger the absolute value indicated. Note that the kernels of the 64 filters in the Fine pathway have a similar line-like shape, while those for the filters in the Coarse pathway are more like larger blobs. The conv1 in TSM ResNet50 (baseline) contains both types of filters from the Fine and Coarse pathways. Best viewed in color and zoomed in. "Playing violin" from UCF101 "Swing baseball" from HMDB51 Examples of video sequences and the corresponding motion representations extracted by MBPM. "Playing poker" from Kinetics400 "Moving something away from something" from Something-Something V1 "Apply eye makeup" from UCF101 "Draw sword" from HMDB51 Examples of video sequences and the corresponding motion representations extracted by MBPM. "Playing basketball" from Kinetics400 "Holding something next to something" from Something-Something V1 Examples of video sequences and the corresponding motion representations extracted by MBPM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness of the MBPM: comparison of the MBPM with other popular motion representation methods on the UCF101 (split 1). † denotes our reimplementation.</figDesc><table><row><cell>Motion Representation Method</cell><cell>Accuracy</cell></row><row><cell>RGB (baseline)</cell><cell>87.1%</cell></row><row><cell>RGB Diff †</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation Studies for FineCoarse Networks on Something-Something V1. We show top-1 and top-5 prediction accuracy (%), as well as computational complexity measured in GFLOPs for a single crop &amp; single clip.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-1 Top-5 GFLOPs</cell><cell cols="3"># LCM Top-1 Top-5 GFLOPs</cell><cell cols="2">Fusion Method Position Top-1 Top-5</cell></row><row><cell>Coarse</cell><cell>46.5 75.3</cell><cell>32.8</cell><cell>0</cell><cell>49.6 78.9</cell><cell>65.7</cell><cell>average</cell><cell>before fc 50.9 79.8</cell></row><row><cell>Fine</cell><cell>48.0 76.8</cell><cell>32.8</cell><cell>4</cell><cell>50.2 79.2</cell><cell>65.8</cell><cell>average</cell><cell>after fc 51.6 80.5</cell></row><row><cell cols="2">Coarse+Fine 50.3 79.0</cell><cell>65.7</cell><cell>8</cell><cell>50.7 79.7</cell><cell>65.8</cell><cell>max</cell><cell>after fc 50.1 78.7</cell></row><row><cell>FineCoarse</cell><cell>51.6 80.5</cell><cell>65.9</cell><cell>16</cell><cell>51.6 80.5</cell><cell>65.9</cell><cell>concatenation</cell><cell>before fc 51.3 80.2</cell></row><row><cell cols="3">(a) Complementarity of Coarse and Fine.</cell><cell cols="3">(b) Effect of LCMs. The FineCoarse</cell><cell cols="2">(c) Fusion Strategies. The fully-connected</cell></row><row><cell cols="3">+ refers to averaging the Coarse and Fine</cell><cell cols="3">model with more LCMs has higher ac-</cell><cell cols="2">layers of the two pathways share the param-</cell></row><row><cell cols="3">pathways which are trained separately.</cell><cell>curacy.</cell><cell></cell><cell></cell><cell>eters.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the spatio-temporal input size (width 2 ×time) of the two pathways in FineCoarse network.</figDesc><table><row><cell>Input size for Coarse</cell><cell>Input size for Fine</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell><cell>GFLOPs</cell></row><row><cell>224 2 × 8</cell><cell>224 2 × 8</cell><cell>51.6</cell><cell>80.5</cell><cell>65.9</cell></row><row><cell>192 2 × 8</cell><cell>224 2 × 8</cell><cell>51.5</cell><cell>79.9</cell><cell>58.0</cell></row><row><cell>160 2 × 8</cell><cell>224 2 × 8</cell><cell>51.3</cell><cell>80.1</cell><cell>50.5</cell></row><row><cell>128 2 × 8</cell><cell>224 2 × 8</cell><cell>50.7</cell><cell>79.2</cell><cell>44.4</cell></row><row><cell>224 2 × 8</cell><cell>256 2 × 8</cell><cell>51.8</cell><cell>80.5</cell><cell>77.1</cell></row><row><cell>192 2 × 8</cell><cell>256 2 × 8</cell><cell>51.7</cell><cell>80.2</cell><cell>68.3</cell></row><row><cell>160 2 × 8</cell><cell>256 2 × 8</cell><cell>51.7</cell><cell>80.5</cell><cell>60.7</cell></row><row><cell>128 2 × 8</cell><cell>256 2 × 8</cell><cell>51.3</cell><cell>79.4</cell><cell>54.6</cell></row><row><cell>160 2 × 6</cell><cell>256 2 × 8</cell><cell>49.6</cell><cell>78.3</cell><cell>55.5</cell></row><row><cell>224 2 × 4</cell><cell>224 2 × 8</cell><cell>48.7</cell><cell>77.1</cell><cell>49.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison results on Something-Something V1. "N/A" indicates the numbers are not available. + refers to averaging the predictions from different models.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>Backbone</cell><cell>Frames×Crops×Clips</cell><cell>FLOPs</cell><cell cols="2">Top-1 (%) Top-5 (%)</cell></row><row><cell>NL I3D GCN [48]</cell><cell>RGB</cell><cell>3D R50</cell><cell>32×3×2</cell><cell>303G×3×2</cell><cell>46.1</cell><cell>76.8</cell></row><row><cell>TSN [46]</cell><cell>RGB</cell><cell>R50</cell><cell>8×1×1</cell><cell>33G×1×1</cell><cell>19.7</cell><cell>46.6</cell></row><row><cell>TSM [27]</cell><cell>RGB</cell><cell>R50</cell><cell>16×1×1</cell><cell>65G×1×1</cell><cell>47.2</cell><cell>77.1</cell></row><row><cell>TSMEn [27]</cell><cell>RGB</cell><cell>R50</cell><cell>(16+8)×1×1</cell><cell>98G×1×1</cell><cell>49.7</cell><cell>78.5</cell></row><row><cell>TSM [27]</cell><cell>RGB+Flow</cell><cell>R50</cell><cell>(16+96)×1×1</cell><cell>N/A</cell><cell>49.7</cell><cell>78.5</cell></row><row><cell>TRN [54]</cell><cell cols="2">RGB+Flow BNInception</cell><cell>(8+48)×1×1</cell><cell>N/A</cell><cell>42.0</cell><cell>-</cell></row><row><cell>ECOEnLite [55]</cell><cell cols="2">RGB+Flow Inc+3D R18</cell><cell>(92+552)×1×1</cell><cell>N/A</cell><cell>49.5</cell><cell>-</cell></row><row><cell>TEA [26]</cell><cell>RGB</cell><cell>R50</cell><cell>16×3×10</cell><cell>70G×3×10</cell><cell>52.3</cell><cell>81.9</cell></row><row><cell>SmallBig [25]</cell><cell>RGB</cell><cell>R50</cell><cell>16×3×2</cell><cell>105G×3×2</cell><cell>50.0</cell><cell>79.8</cell></row><row><cell>bLVNet-TAM [10]</cell><cell>RGB</cell><cell>bLR50</cell><cell>8×1×2</cell><cell>12G×1×2</cell><cell>46.4</cell><cell>76.6</cell></row><row><cell>PAN Full [53]</cell><cell>RGB</cell><cell>TSM R50</cell><cell>40×1×2</cell><cell>67.7G×1×2</cell><cell>50.5</cell><cell>79.2</cell></row><row><cell>PANEn [53]</cell><cell>RGB</cell><cell>TSM R50</cell><cell>(40+40)×1×2</cell><cell>134G×1×2</cell><cell>53.4</cell><cell>81.1</cell></row><row><cell>FineCoarse</cell><cell>RGB</cell><cell>TSM R50</cell><cell>24×1×1</cell><cell>60G×1×1</cell><cell>51.7</cell><cell>80.5</cell></row><row><cell>FineCoarse</cell><cell>RGB</cell><cell>TSM R50</cell><cell>24×3×2</cell><cell>60G×3×2</cell><cell>53.3</cell><cell>82.0</cell></row><row><cell>FineCoarse</cell><cell>RGB</cell><cell>TSM R50</cell><cell>48×3×2</cell><cell>121G×3×2</cell><cell>54.3</cell><cell>82.0</cell></row><row><cell>FineCoarseEn</cell><cell>RGB</cell><cell>TSM R50</cell><cell>(24+48)×3×2</cell><cell>181G×3×2</cell><cell>55.8</cell><cell>83.3</cell></row><row><cell>PANEn [53]</cell><cell>RGB</cell><cell>TSM R101</cell><cell>(40+40)×1×2</cell><cell>251G×1×2</cell><cell>55.3</cell><cell>82.8</cell></row><row><cell>ir-CSN [42]</cell><cell>RGB</cell><cell>3D R101</cell><cell>32×1×10</cell><cell>73G×1×10</cell><cell>48.4</cell><cell>-</cell></row><row><cell>ir-CSN [42]</cell><cell>RGB</cell><cell>3D R152</cell><cell>32×1×10</cell><cell>96G×1×10</cell><cell>49.3</cell><cell>-</cell></row><row><cell>FineCoarse</cell><cell>RGB</cell><cell>TSM R101</cell><cell>48×3×2</cell><cell>231G×3×2</cell><cell>54.9</cell><cell>81.7</cell></row><row><cell>Fine</cell><cell>RGB</cell><cell>TSM R101</cell><cell>48×3×2</cell><cell>125G×3×2</cell><cell>52.1</cell><cell>80.0</cell></row><row><cell>Coarse</cell><cell>RGB</cell><cell>TSM R101</cell><cell>48×3×2</cell><cell>125G×3×2</cell><cell>50.4</cell><cell>79.2</cell></row><row><cell>FineCoarse+Fine+Coarse</cell><cell>RGB</cell><cell>TSM R101</cell><cell>(48+48+48)×1×2</cell><cell>481G×3×2</cell><cell>57.0</cell><cell>83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison results on Kinetics400. We report the inference cost of multiple "views" (spatial crops × temporal clips).</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Frames × views</cell><cell>FLOPs</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell cols="2">bLVNet-TAM [10] bLR50</cell><cell>16×9</cell><cell cols="3">561G 72.0 90.6</cell></row><row><cell>TSM [27]</cell><cell>R50</cell><cell cols="3">16×30 2580G 74.7</cell><cell>-</cell></row><row><cell>TEINet [29]</cell><cell>R50</cell><cell cols="4">16×30 2580G 76.2 92.5</cell></row><row><cell>TEA [26]</cell><cell>R50</cell><cell cols="4">16×30 2100G 76.1 92.5</cell></row><row><cell>STM [22]</cell><cell>R50</cell><cell cols="4">16×30 2010G 73.7 91.6</cell></row><row><cell>X3D-XL [11]</cell><cell>-</cell><cell cols="4">16×30 1452G 79.1 93.9</cell></row><row><cell cols="2">SlowFast4×16 [12] 3D R50</cell><cell cols="4">32×30 1083G 75.6 92.1</cell></row><row><cell>CorrNet [44]</cell><cell>3D R50</cell><cell cols="3">32×10 1150G 77.2</cell><cell>-</cell></row><row><cell>R(2+1)D [43]</cell><cell>R34</cell><cell cols="4">32×10 1520G 72.0 91.4</cell></row><row><cell>ip-CSN [42]</cell><cell cols="5">3D R101 32×30 2490G 76.8 92.5</cell></row><row><cell>SmallBigNet [25]</cell><cell>R101</cell><cell cols="4">32×12 6552G 77.4 93.3</cell></row><row><cell>PAN Full</cell><cell cols="2">TSM R50 40×2</cell><cell cols="3">176G 74.4 91.6</cell></row><row><cell>TSN [46]</cell><cell>BNInc.</cell><cell>25×10</cell><cell cols="3">530G 69.1 88.7</cell></row><row><cell>I3DRGB [3]</cell><cell cols="5">Inc. V1 64×N/A N/A 71.1 89.3</cell></row><row><cell>Oct-I3D [5]</cell><cell>R50</cell><cell cols="3">N/A×N/A N/A 74.6</cell><cell>-</cell></row><row><cell>NL I3D [47]</cell><cell cols="5">3D R101 128×30 10770G 77.7 93.3</cell></row><row><cell>FineCoarse</cell><cell cols="5">TSM R50 48×10 1210G 76.8 92.4</cell></row><row><cell>FineCoarse</cell><cell cols="5">TSM R50 72×10 1820G 77.3 93.2</cell></row></table><note>of 160 2 + 256 2 for the Coarse and Fine, respectively, has</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison results on HMDB51 and UCF101. We report the mean class accuracy over the three official splits.</figDesc><table><row><cell>Method</cell><cell cols="3">Backbone HMDB51 UCF101</cell></row><row><cell>StNet [17]</cell><cell>R50</cell><cell>-</cell><cell>93.5</cell></row><row><cell>TSM [27]</cell><cell>R50</cell><cell>73.5</cell><cell>95.9</cell></row><row><cell>STM [22]</cell><cell>R50</cell><cell>72.2</cell><cell>96.2</cell></row><row><cell>TEA [26]</cell><cell>R50</cell><cell>73.3</cell><cell>96.9</cell></row><row><cell cols="2">DI Four-Stream [2] ResNeXt101</cell><cell>72.5</cell><cell>95.5</cell></row><row><cell>TVNet [9]</cell><cell>BNInception</cell><cell>71.0</cell><cell>94.5</cell></row><row><cell cols="2">TSN RGB+Flow [46] BNInception</cell><cell>68.5</cell><cell>94.0</cell></row><row><cell>I3D RGB+Flow [3]</cell><cell>3D Inception</cell><cell>80.7</cell><cell>98.0</cell></row><row><cell>PAN Full [53]</cell><cell>TSM R50</cell><cell>77.0</cell><cell>96.5</cell></row><row><cell>FineCoarse</cell><cell>TSM R50</cell><cell>77.6</cell><cell>97.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results of FineCoarse network with various LCM designs on Something-Something V1. We set 16 LCMs between two pathways for producing the obvious accuracy gaps between different designs.</figDesc><table><row><cell>Design</cell><cell>Top-1 (%)</cell><cell>Top-5 (%)</cell></row><row><cell>LCM-A</cell><cell>50.9</cell><cell>79.8</cell></row><row><cell>LCM-B</cell><cell>50.9</cell><cell>79.7</cell></row><row><cell>LCM-C</cell><cell>51.5</cell><cell>80.2</cell></row><row><cell>LCM-D</cell><cell>51.6</cell><cell>80.5</cell></row><row><cell>LCM-E</cell><cell>51.3</cell><cell>79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Processing stages where we consider inserting LCMs. For each stage, we only set one LCM after its first residual block.</figDesc><table><row><cell>Stages</cell><cell cols="3"># LCM Top-1 (%) Top-5 (%)</cell></row><row><cell>res2</cell><cell>1</cell><cell>49.8</cell><cell>79.1</cell></row><row><cell>res2+res3</cell><cell>2</cell><cell>50.1</cell><cell>78.7</cell></row><row><cell>res2+res3+res4</cell><cell>3</cell><cell>50.2</cell><cell>79.0</cell></row><row><cell>res2+res3+res4+res5</cell><cell>4</cell><cell>50.2</cell><cell>79.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Also referred to as "depth-wise". We use the term "channel-wise" to avoid confusions with the network depth.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">ResNet50 contains four stages, named res2, res3, res4, res5, respectively. These stages are composed of 3, 4, 6, 3 residual blocks, respectively.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action recognition with dynamic image networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2799" to="2813" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Big-little net: An efficient multi-scale feature representation for visual and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mallinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03848</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3435" to="3444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Longterm recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2264" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5842" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artif</title>
		<meeting>AAAI Conference on Artif</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8401" to="8408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR 37</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference Mach. Learn. (ICML)</title>
		<meeting>Int. Conference Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artif. Intel</title>
		<meeting>AAAI Conference on Artif. Intel</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conference Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9945" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Information Process. Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Int. Conference Learn. Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1390" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference Mach. Learn. (ICML)</title>
		<meeting>Int. Conference Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conference Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conference Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video representation learning using discriminative pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy tradeoffs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference Computer Vision Pattern Recog. (CVPR)</title>
		<meeting>IEEE Conference Computer Vision Pattern Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Pattern Recog. Symp</title>
		<meeting>Joint Pattern Recog. Symp</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4713</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Real-time action recognition with deeply transferred motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2326" to="2339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pan: Persistent appearance network with an efficient motion cue for fast action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conference Multimedia</title>
		<meeting>ACM Int. Conference Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference Computer Vision (ECCV)</title>
		<meeting>European Conference Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
