<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unconstrained Scene Generation with Locally Conditioned Radiance Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
						</author>
						<title level="a" type="main">Unconstrained Scene Generation with Locally Conditioned Radiance Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Scenes sampled from our learned prior, rendered from freely moving camera paths containing various rotation, translation, and forward/backward motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We tackle the challenge of learning a distribution over complex, realistic, indoor scenes. In this paper, we introduce Generative Scene Networks (GSN), which learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera. Our model can be used as a prior to generate new scenes, or to complete a scene given only sparse 2D observations. Recent work has shown that generative models of radiance fields can capture properties such as multiview consistency and view-dependent lighting. However, these models are specialized for constrained viewing of single objects, such as cars or faces. Due to the size and complexity of realistic indoor environments, existing models lack the representational capacity to adequately capture them. Our decomposition scheme scales to larger and more complex scenes while preserving details and diversity, and the learned prior enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. When compared to existing models, GSN produces quantitatively higher-quality scene renderings across several different scene datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spatial understanding entails the ability to infer the geometry and appearance of a scene when observed from any viewpoint or orientation given sparse or incomplete observations. Although a wide range of geometry and learningbased approaches for 3D view synthesis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62]</ref> can interpolate between observed views of a scene, they cannot extrapolate to infer unobserved parts of the scene. The fundamental limitation of these models is their inability to learn a prior over scenes. As a result, learning-based approaches have limited performance in the extrapolation regime, whether it be inpainting disocclusions or synthesizing views beyond the boundaries of the given observations. For example, the popular NeRF <ref type="bibr" target="#b33">[34]</ref> approach represents a scene via its radiance field, enabling continuous view interpolation given a densely captured scene. However, since NeRF does not learn a scene prior, it cannot extrapolate views. On the other hand, conditional auto-encoder models for view synthesis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b53">54]</ref> are able to extrapolate views of simple objects from multiple viewpoints and orientations. Yet, they overfit to viewpoints seen during † Work done as part of an internship at Apple. arXiv:2104.00670v1 [cs.CV] 1 Apr 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Multiple Scenes/Objects Generative Latent Radiance Field Scene Level Camera Placement</head><p>NeRF <ref type="bibr" target="#b33">[34]</ref> -Sphere/Wide-baseline NSVF <ref type="bibr" target="#b29">[30]</ref> 3D Sphere/Wide-baseline PixelNerf <ref type="bibr" target="#b57">[58]</ref> N×2D Sphere/Wide-baseline GRAF <ref type="bibr" target="#b48">[49]</ref> 1D Sphere HoloGAN <ref type="bibr" target="#b36">[37]</ref> 1D Sphere PlatonicGAN <ref type="bibr" target="#b15">[16]</ref> 1D Sphere ENR <ref type="bibr" target="#b10">[11]</ref> 3D Sphere GTM-SM <ref type="bibr" target="#b12">[13]</ref> 1D Free moving ISS <ref type="bibr" target="#b40">[41]</ref> 2D Free moving GSN (ours) 2D Free moving training (see <ref type="bibr" target="#b6">[7]</ref> for a detailed analysis). Moreover, conditional auto-encoders tend to favor point estimates (e.g. the distribution mean) and produce blurry renderings when extrapolating far from observed views as a result.</p><p>A learned prior for scenes may be used for unconditional or conditional inference. A compelling use case for unconditional inference is to generate realistic scenes and freely move through them in the absence of input observations, relying on the prior distribution over scenes (see <ref type="figure">Fig. 1</ref> for examples of trajectories of a freely moving camera on scenes sampled from our model). Likewise, conditional inference lends itself to different types of problems. For instance, plausible scene completions may be sampled by inverting scene observations back to the learned scene prior <ref type="bibr" target="#b56">[57]</ref>. A generative model for scenes would be a practical tool for tackling a wide range of problems in machine learning and computer vision, including model-based reinforcement learning <ref type="bibr" target="#b14">[15]</ref>, SLAM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, content creation <ref type="bibr" target="#b23">[24]</ref>, and adaptation for AR/VR or immersive 3D photography.</p><p>In this paper we introduce Generative Scene Networks (GSN), a generative model of scenes that allows view synthesis of a freely moving camera in an open environment. Our contributions are the following. We: (i) introduce the first generative model for unconstrained scene-level radiance fields; (ii) demonstrate that decomposing a latent code into a grid of locally conditioned radiance fields results in an expressive and robust scene representation, which outperforming strong baselines; (iii) infer observations from arbitrary cameras given a sparse set of observations by inverting GSN (i.e., fill in the scene); and (iv) show that GSN can be trained on multiple scenes to learn a rich scene prior, while rendered trajectories are smooth and consistent, maintaining scene coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>An extensive body of literature has tackled view synthesis of simple synthetic objects <ref type="bibr" target="#b3">[4]</ref> against a homogeneous background <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63]</ref>. Relevant models typically rely on predicting 2D/3D flow fields to transform pixels in the input view to pixels in the predicted view. Moreover, in <ref type="bibr" target="#b49">[50]</ref> a continuous representation is introduced in the form of the weights of a parametric vector field, which is used to infer appearance of 3D points expressed in a world coordinate system. At inference time the model is optimized to find latent codes that are predictive of the weights. The above approaches rely on the existence of training data of the form input-transformation-output tuples, where at training time the model has access to two views (input and output) in a viewing sphere and the corresponding 3D transformation. Sidestepping this requirement <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37]</ref> showed that similar models can be trained in an adversarial setting without access to input-transformation-output tuples.</p><p>Recent approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> have been shown to encode a continuous radiance field in the parameters of a neural network by fitting it to a collection of high resolution views of realistic scenes. The encoded radiance field is rendered using the classical volume rendering equation <ref type="bibr" target="#b20">[21]</ref> and a reconstruction loss is used to optimize the model parameters. While successful at modelling the radiance field at high resolutions, these approaches require optimizing a new model for every scene (where optimization usually takes days on commodity hardware). Thus, the fundamental limitation of these works is that they are unable to represent multiple scenes within the same model. As a result, these models cannot learn a prior distribution over multiple scenes. Some newer works have explore generative radiance field models in the adversarial setting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref>. However, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49]</ref> are restricted to modeling single objects where    <ref type="figure">Figure 2</ref>: Architecture of the GSN generator. We sample a latent code z ∼ p z that is fed to our global generator g producing a local latent grid W. This local latent grid W conceptually represents a latent scene "floorplan" and is used for locally conditioning a radiance field f from which images are rendered via volumetric rendering. For a given point p expressed in a global coordinate system to be rendered, we sample W at the location (i, j), given by p resulting in w ij . In turn f takes as input p which results from expressing p relative to the local coordinate system of w ij . a camera is positioned on a viewing sphere oriented towards the object, and they use a 1D latent representation, which prevents them from efficiently scaling to full scenes (c.f. § 4 for experimental evidence). Concurrent to our work, Niemeyer et al. <ref type="bibr" target="#b37">[38]</ref> model a scene with multiple entities but report results on single-object datasets and simple synthetic scenes with 2-5 geometrical shapes in the CLEVR dataset <ref type="bibr" target="#b19">[20]</ref>. The object-level problem may be solved by learning a pixel-aligned representation for conditioning the radiance field <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. However, <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref> are designed as conditional auto-encoders as opposed to a generative model, thereby ignoring the fundamentally stochastic nature of the view synthesis problem. In <ref type="bibr" target="#b27">[28]</ref> a model is trained on long trajectories of nature scenarios. However, due to their iterative refinement approach the model has no persistent scene representation. While allowing for perpetual generation, the lack of a persistent scene representation limits its applicability for other downstream tasks that may require loop closure.</p><formula xml:id="formula_0">= " &gt; A A A C B H i c b V B N S 8 N A E N 3 4 W e t X 1 G M v i 0 X w V J I q 6 r H g x W M V + w F N L Z v t p l 2 6 2 Y T d i V h C D 1 7 8 K 1 4 8 K O L V H + H N f + O m z U F b H w w 8 3 p t h Z p 4 f C 6 7 B c b 6 t p e W V 1 b X 1 w k Z x c 2 t 7 Z 9 f e 2 2 / q K F G U N W g k I t X 2 i W a C S 9 Y A D o K 1 Y 8 V I 6 A v W 8 k e X m d + 6 Z 0 r z S N 7 C O G b d k A w k D z g l Y K S e X f K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>o = " &gt; A A A B 8 X i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K k k R 9 V j w 4 r F C v 7 A N Z b O d t E s 3 m 7 A 7 E U v o v / D i Q R G v / h t v / h u 3 b Q 7 a + m D g 8 d 4 M M / O C R A q D r v v t r K 1 v b G 5 t F 3 a K u 3 v 7 B 4 e l o + O W i V P N o c l j G e t O w A x I o a C J</head><formula xml:id="formula_1">A i V 0 E g 0 s C i S 0 g / H t z G 8 / g j Y i V g 2 c J O B H b K h E K D h D K z 3 0 E J 4 w C L P G t F 8 q u x V 3 D r p K v J y U S Y 5 6 v / T V G 8 Q 8 j U A h l 8 y Y r u c m 6 G d M o + A S p s V e a i B h f M y G 0 L V U s Q i M n 8 0 v n t J z q w x o G G t b C u l c / T 2 R s c i Y S R T Y z o j h y C x 7 M / E / r 5 t i e O N n Q i U p g u K L R W E q K c Z 0 9 j 4 d C A 0 c 5 c Q S x r W w t 1 I + Y p p x t C E V b Q j e 8</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s u r p F W t e F c V 7 / 6 y X K v m c R T I K T k j F 8 Q j 1 6 R G 7 k i d N A k n i j y T V / L m G O f F e X c + F q 1 r T j 5 z Q v 7 A + f w B 7 U G R C Q = = &lt; / l a t e x i t &gt; T &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q P s / I e C N U u x 0 R 8 c t L Q J w E N z v 9 5 0 = " &gt; A A A C A H i c b V B N S 8 N A E N 3 U r 1 q / o h 4 8 e F k s g q e S F F G</head><formula xml:id="formula_2">P h R 7 0 W N F + Q B v K Z r N t l 2 6 y Y X c i l p C L f 8 W L B 0 W 8 + j O 8 + W / c t j l o 6 4 O B x 3 s z z M z z Y 8 E 1 O M 6 3 V V h Z X V v f K G 6 W t r Z 3 d v f s / Y O W l o m i r E m l k K r j E 8 0 E j 1 g T O A j W i R U j o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>g c O W l P I o T Y B G d L x o k A o P E 0 z R w w B W j I C a G E K q 4 u R X T E V G E g s m s Z E J w F 1 9 e J q 1 q x b 2 o O L f n 5 V o 1 j 6 O I j t E J O k M u u k Q 1 d I M a q I k o y t A z e k V v 1 p P 1 Y r 1 b H / P W g p X P H K I / s D 5 / A K r m l m Y = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Coord Sys</head><p>Approaches tackling view synthesis for a freely moving camera in a scene offer the capability to fully explore a scene <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref>. Irrespective of the output image resolution, this is strictly a more complex problem than a camera moving on a sphere oriented towards a single object <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b2">3]</ref>. For scenes with freely moving cameras the model needs to learn to represent a full scene that consists of a multitude of objects rather than a single object. In this setup, it is often useful to equip models with a memory mechanism to aggregate the set of incoming observations. In particular, it has been useful to employ a dictionary-based memory to represent an environment where keys are camera poses and values are latent observation representations <ref type="bibr" target="#b12">[13]</ref>. Furthermore, the memory may be extended to a 2D feature map that represents at top-down view of the environment <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref>. At inference time, given a query viewpoint the model queries the memory and predicts the expected observation. This dependency on stored observations significantly limits these models' ability to cope with unseen viewpoints. GSN can perform a similar task by matching observations to scenes in its latent space, but with the added benefit that the learned scene prior allows for extrapolation beyond the given observations. A summarized comparison of GSN with the most relevant related work is shown in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As in traditional GANs <ref type="bibr" target="#b13">[14]</ref>, our model is composed of a generator G θ and a discriminator D φ , with latent codes z ∼ p z , where p z denotes the prior distribution. The generator G θ is composed of two subnetworks G = g θg · f θ f : a global generator g θg and a locally conditioned radiance field network f θ f (in the rest of the text and figures we drop the subscripts for clarity). Unlike standard 2D GANs, which often only require samples z ∼ p z as input, GSN also takes a camera pose T ∈ SE(3) from an empirical pose distribution p T , as well as camera intrinsic parameters K ∈ R 3 × 3 . These additional inputs allow for explicit control over the viewpoint and field of view of the output image, which is rendered via a radiance field. <ref type="figure">Fig. 2</ref> outlines the GSN network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global Generator</head><p>In GRAF <ref type="bibr" target="#b48">[49]</ref> and π-GAN     densely populated scenes. Simply increasing the capacity of a single radiance field network has diminishing returns with respect to render quality <ref type="bibr" target="#b42">[43]</ref>. It is more effective to distribute a scene among several smaller networks, such that each can specialize on representing a local region <ref type="bibr" target="#b42">[43]</ref>. Inspired by this insight we propose the addition of a global generator that learns to decompose large scenes into many smaller, more manageable pieces.</p><formula xml:id="formula_3">I g = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U C P v l i l t 1 5 y C r x M t J B X L U + + W v 3 i B m a Y T S M E G 1 7 n p u Y v y M K s O Z w G m p l 2 p M K B v T I X Y t l T R C 7 W f z Q 6 f k z C o D E s b K l j R k r v 6 e y G i k 9 S Q K b G d E z U g v e z P x P 6 + b m v D G z 7 h M U o O S L R a F q S A m J r O v y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 m 5 I N w V t + e Z W 0 L q r e V d V r X F Z q t 3 k c R T i B U z g H D 6 6 h B v d Q h y Y w Q H i G V 3 h</formula><p>The global generator g maps from the global latent code z ∈ R d , which represents an entire scene, to a 2D grid of local latent codes W ∈ R c×s×s , each of which represent a small portion of the larger scene (see <ref type="figure" target="#fig_7">Fig. 3a</ref>). Conceptually, the 2D grid of local latent codes can be interpreted as a latent floorplan representation of a scene, where each code is used to locally condition a radiance field network. We opt for a 2D representation produced with a convolutional generator for computational efficiency, but our framework can be easily extended to a 3D grid <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Locally Conditioned Radiance Field</head><p>To render an imageX ∈ R 3×w×h given a grid of latent codes W ∈ R c×s×s , and a camera with pose T = [R, t] ∈ SE(3) and intrinsics K ∈ R 3×3 , we model scenes with a locally conditioned radiance field <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_4">f : R 5 × R c → [0, 1] × R 3 .</formula><p>To predict radiance for a 3D point p and 2D camera direction d, f is locally conditioned on a vector w ij ∈ R c sampled at a discrete location (i, j) from the grid of latent codes W using bi-linear sampling <ref type="bibr" target="#b18">[19]</ref>. The location (i, j) is naturally given by the projection of p on the zx-plane (assuming the y-axis points up).</p><p>Radiance fields are usually defined over R 3 considering a global coordinate system <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3]</ref> (i.e. a coordinate system that spans the whole scene/object). This has been successful since these works do not learn a decomposition of scenes into multiple radiance fields. To decompose the scene into a grid of independent radiance fields, we enable our model to perform spatial sharing of local latent codes; the same latent code w ij can be used to represent the same scene part irrespective of its position (i, j) on the grid (see <ref type="figure" target="#fig_9">Fig. 6</ref> for empirical results). In order to achieve this spatial sharing, it is not sufficient to learn a grid of local latent codes, it is also necessary to decompose the global coordinate system into multiple local coordinate systems (one for each local latent w ij ). The reason for this is to prevent f from relying on the absolute coordinate value of input points to predict radiance. As a result, the input to f is p , which results from expressing p relative to the local coordinate system of its corresponding local latent w ij and applying positional encoding <ref type="bibr" target="#b33">[34]</ref>.</p><p>Finally, the locally conditioned radiance field f outputs two variables: the occupancy σ ∈ [0, ∞] and the appearance a ∈ R 3 (see <ref type="figure" target="#fig_7">Fig. 3b</ref>). To renderX given f we leverage Eq. 1 for implicit volumetric rendering <ref type="bibr" target="#b33">[34]</ref> and densely evaluate f on points uniformly sampled along ray directions r, where each pixel inX corresponds to a ray and the color c ∈ R 3 of pixel/ray r is obtained by approximating the integral in Eq. 1. For computational efficiency, we use Eq. 1 to render a feature map as in <ref type="bibr" target="#b37">[38]</ref>. The rendered feature map is then is upsampled with a small convolutional refinement network to achieve the desired output resolution, resulting in final rendered outputX.</p><formula xml:id="formula_5">c(r, W) = u f un T r(u)σ (r(u), w ij ) a (r(u), d, w ij ) du T r(u) = exp − u un σ(r(u), w ij )du<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sampling Camera Poses</head><p>Unlike standard 2D generative models which have no explicit concept of viewpoint, radiance fields require a camera pose for rendering an image. Therefore, camera poses T = [R|t] ∈ SE(3) need to be sampled from pose distribution p T in addition to the latent code z ∼ p z , which is challenging for the case of realistic scenes and a freely moving camera. GRAF <ref type="bibr" target="#b48">[49]</ref> and π-GAN <ref type="bibr" target="#b2">[3]</ref> avoid this issue by training on datasets containing objects placed at the origin, where the camera is constrained to move on a viewing sphere around the object and oriented towards the origin. <ref type="bibr" target="#b0">1</ref> For GSN, sampling camera poses becomes more challenging due to i) the absence of such constraints (i.e. need to sample T ∈ SE(3)), ii) and the possibility of sampling invalid locations, such as inside walls or other solid objects that sporadically populate the scene.</p><p>To overcome the issue of sampling invalid locations we perform stochastic weighted sampling over a an empirical pose distribution p T composed by a set of candidate poses, where each pose is weighted by the occupancy (i.e., the σ value predicted by the model) at that location. When sampling the candidate poses, we query the generator at each candidate location to retrieve the corresponding occupancy. Then the occupancy values are normalized with a softmin to produce sampling weights for a multinomial distribution. This sampling strategy reduces the likelihood of sampling invalid camera locations while retaining stochasticity required for scene exploration and sample diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discriminator</head><p>Our model adopts the convolutional discriminator architecture from StyleGAN2 <ref type="bibr" target="#b24">[25]</ref>.The discriminator takes as input an image, concatenated with corresponding depth map normalized between [0, 1] and predicts whether the input comes from the true distribution or the generated distribution. We found it critical to add a small decoder C to the discriminator and to enforce a reconstruction penalty on real images, similar to the self-supervised discriminator proposed by Lui et al. <ref type="bibr" target="#b28">[29]</ref>. The additional regularization term restricts the discriminator from overfitting, while encouraging it to learn relevant features about the input that will provide a useful training signal for the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>Let X = {X} : X ∈ R 4×w×h denote the set of i.i.d RGB-D samples obtained by recording camera trajectories on the true distribution of scenes p S . Generator G, discriminator D, and decoder C are parameterized by θ, φ, and ω, respectively. We train our model by optimizing the nonsaturating GAN loss <ref type="bibr" target="#b13">[14]</ref> with R1 gradient penalty <ref type="bibr" target="#b31">[32]</ref>, as well as the discriminator reconstruction objective <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_6">L(θ, φ, ω) = E z∼pz,T∼p T [h(D φ (G θ (z, T)))] + E X∼p S [h(−D φ (X)) + λ R1 |∇D φ (X)| 2 + λ Recon |X − C ω (D φ (X))| 2 ],</formula><p>where h(u) = − log(1 + exp(−u)).</p><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we report both quantitative and qualitative experimental results on different settings. First, we compare the generative performance of GSN with recent state-of-theart approaches. Second, we provide extensive ablation experiments that show the quantitative improvement obtained by the individual components of our model. Finally, we report results on view synthesis by inverting GSN and query-ing the model to predict views of a scene given a set of input observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generation Performance</head><p>We evaluate the generative performance of our model on three datasets: i) the VizDoom environment <ref type="bibr" target="#b25">[26]</ref>, a synthetic computer-generated world, ii) the Replica dataset <ref type="bibr" target="#b51">[52]</ref> containing 18 scans of realistic scenes that we render using the Habitat environment <ref type="bibr" target="#b47">[48]</ref>, and iii) the Active Vision Dataset (AVD) <ref type="bibr" target="#b0">[1]</ref> consisting of 20k images with noisy depth measurements from 9 real world scenes. <ref type="bibr" target="#b1">2</ref> Images are resized to 64 × 64 resolution for all generation experiments. To generate data for the VizDoom and Replica experiments we render 100 sequences of 100 steps each, where an interactive agent explores a scene collecting RGB and depth observations as well as camera poses. Sequences for AVD are defined by an agent performing a random walk through the scenes according to rules defined in <ref type="bibr" target="#b40">[41]</ref>. At training time we sample sub-sequences and express camera poses relative the middle frame of the sub-sequence. This normalization enforces an egocentric coordinate system whose origin is placed at the center of W (see supplementary material for details).</p><p>We compare GSN to two recent approaches for generative modelling of radiance fields: GRAF <ref type="bibr" target="#b48">[49]</ref> and π-GAN <ref type="bibr" target="#b2">[3]</ref>. For fair comparison all models use the same base architecture and training pipeline, with two main differences between models. The first difference is that GSN uses locally conditioned radiance fields, while GRAF and π-GAN use global conditioning. The second difference is the type of layer used in the radiance field generator network: GRAF utilizes linear layers, π-GAN employs modulated sine layers, and GSN uses modulated linear layers. Quantitative performance is evaluated with the Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">[17]</ref> and SwAV-FID <ref type="bibr" target="#b34">[35]</ref> metrics, which measure the distance between the distributions of real and generated images in pretrained image embedding spaces. We sample 5,000 real and 5,000 generated images when calculating either of these metrics.</p><p>Tab. 2 shows the results of our generative modelling comparison. Despite our best attempts to tune π-GAN's hyperparameters, we find that it struggles to learn detailed depth in this setting, which significantly impedes render quality and leads to early collapse of the model. GSN achieves much better performance than GRAF or π-GAN across all datasets, obtaining an improvement of 10-14 absolute points on FID. We attribute GSN's higher performance to the increased expressiveness afforded by the locally conditioned radiance field, and not the specific layer type used (compare GRAF in Tab. 2 to GSN + global latents in Tab. 3). As a measure of qualitative results we show scenes sampled from GSN trained on the Replica dataset <ref type="figure">Figure 4</ref>: Random trajectories through scenes generated by GSN. Models are trained on VizDoom <ref type="bibr" target="#b25">[26]</ref> (left), Replica <ref type="bibr" target="#b51">[52]</ref> (right) at 64 × 64 resolution. We omit qualitative results for AVD <ref type="bibr" target="#b0">[1]</ref> due to unclear licensing terms regarding reproduction of figures for this dataset.</p><p>VizDoom <ref type="bibr" target="#b25">[26]</ref> Replica <ref type="bibr" target="#b51">[52]</ref> AVD <ref type="bibr" target="#b0">[1]</ref> FID  <ref type="table">Table 2</ref>: Generative performance of state-of-the-art approaches for generative modelling of radiance fields on 3 scene-level datasets: Vizdoom <ref type="bibr" target="#b25">[26]</ref>, Replica <ref type="bibr" target="#b51">[52]</ref> and Active Vision (AVD) <ref type="bibr" target="#b0">[1]</ref>, according to FID <ref type="bibr" target="#b16">[17]</ref> and SwAV-FID <ref type="bibr" target="#b34">[35]</ref> metrics.</p><formula xml:id="formula_7">↓ SwAV-FID ↓ FID ↓ SwAV-FID ↓ FID ↓ SwAV-FID ↓ GRAF [</formula><p>at 128 × 128 resolution in <ref type="figure">Fig. 1</ref>, and on the VizDoom, Replica, and AVD datasets at 64 × 64 resolution in <ref type="figure">Fig. 4</ref>.</p><p>Latent Space Interpolation To confirm that GSN is learning a useful scene prior we demonstrate in <ref type="figure" target="#fig_8">Fig. 5</ref> some examples of interpolation in the global latent space. The model aligns both geometry and appearance features such that traversing the latent space transitions smoothly between scenes without producing unrealistic off-manifold samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation</head><p>To further analyze our contributions, we report extensive ablation experiments where we show how our design choices affect the generative performance of GSN. For ablation, we perform our analysis on the Replica dataset <ref type="bibr" target="#b51">[52]</ref> at 64 ×64 resolution and analyze the following factors: (i) the choice of latent code representation (global vs local); (ii) the effects of global and local coordinate systems on the learned representation; (iii) the effect of sampled trajectory length; (iv) the depth resolution needed in order to successfully learn a scene prior; (v) the regularization applied to the discriminator.</p><p>How useful are locally conditioned radiance fields? We compare the effect of replacing our local latent codes W ∈ R c×s×s with a global latent code w ∈ R c that globally conditions the radiance field f θ f . Generative performance can be improved by decomposing the scene into many inde-pendent locally conditioned radiance fields (Tab. 3).  <ref type="table">Table 3</ref>: Decomposition of the global latent code into a local latent grid has a drastic impact in performance.</p><p>What are the benefits of a local coordinate system? Enabling feature sharing in local coordinate systems to (e.g. a latent w ij can be used to represent the same scene part irrespective of its position (i, j) on the grid). To empirically validate this hypothesis we train GSN with both global and local coordinate systems. We then sample from the prior obtaining 5k latent grids W for each model. Next, we perform a simple rigid re-arrangement of latent codes in W by applying a 2D rotation at different angles and measuring FID by using the resulting grids to predict a radiance field. A local coordinate system is significantly more robust to rearranging local latent codes than a global one ( <ref type="figure" target="#fig_9">Fig. 6</ref>; see supplementary material for qualitative results).</p><p>How long do camera trajectories need to be? The length of trajectories that collect the camera poses affects the representation of large scenes. Given that we normal-  ize camera poses w.r.t. the middle step in a trajectory, if the trajectories are too short the model will only explore small local neighbourhoods and will not be able to deal with long displacements in camera poses during test time. Models trained on short trajectories fail to generalize to long trajectories during evaluation <ref type="figure" target="#fig_10">(Fig. 7)</ref>. However, models trained with long trajectories do not struggle when evaluated on short trajectories, as evidenced by the stability of the FID when evaluated on short trajectories.</p><p>How much depth information is required during training? The amount of depth information used during training affects GSN. To test the sensitivity of GSN to depth information, we down-sample both real and generated depth maps to the target resolution, then up-sample back to the original resolution before concatenating with the corresponding image and feeding them to the discriminator. Without depth information GSN fails to train (Tab. 4). However, we demonstrate that the depth resolution can be reduced to a single pixel without finding statistically significant reduction in the generated image quality. This is a significant result, as it enables GSN to be trained in settings with sparse depth information in future work. We speculate that in early stages of training the added depth information guides the model to learn some aspect of depth, after which it can learn without additional supervision. Does D φ need to be regularized? Different forms of regularizing the discriminator affect the quality of generated samples. We discover that D φ greatly benefits from regularization (Tab. 5). In particular, data augmentation <ref type="bibr" target="#b60">[61]</ref> and the discriminator reconstruction penalty <ref type="bibr" target="#b28">[29]</ref> are both crucial; training to rapidly diverge without either of these components. The R1 gradient penalty <ref type="bibr" target="#b31">[32]</ref> offers an addi-   <ref type="table">Table 5</ref>: Regularizing D φ shows great benefits for GSN, especially when using a reconstruction penalty and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">View Synthesis</head><p>We now turn to the task of view synthesis, where we show how GSN performs in comparison with two approaches for scene-level view synthesis of free moving cameras: Generative Temporal models with Spatial Memory (GTM-SM) <ref type="bibr" target="#b12">[13]</ref> and Incremental Scene Synthesis (ISS) <ref type="bibr" target="#b40">[41]</ref>. Taking the definition in <ref type="bibr" target="#b40">[41]</ref>, the view synthesis task is defined as follows: for a given step t in a sequence we want the model to predict the target t + 5 views T = {(X, T) i } i=t:t+5 conditioned on the source t − 5 views S = {(X, T) i } i=t−5:t along the camera trajectory. Note that this view synthesis problem is unrelated to video prediction, since the scenes are static and camera poses for source and target views are given. To tackle this task both GTM-SM <ref type="bibr" target="#b12">[13]</ref> and ISS <ref type="bibr" target="#b40">[41]</ref> rely on auto-encoder architectures augmented with memory mechanisms that directly adapt to the conditional nature of the task. For GSN to deal with this task we follow standard practices for GAN inversion <ref type="bibr" target="#b56">[57]</ref> (see supplementary material for details on the encoder architecture and inversion algorithm). We invert source views S into the prior and use the resulting latent to locally condition the radiance field and render observations using the camera poses of the target views T .  Following <ref type="bibr" target="#b40">[41]</ref> we report results on the Vizdoom <ref type="bibr" target="#b25">[26]</ref> and AVD <ref type="bibr" target="#b0">[1]</ref> datasets in Tab. 6. <ref type="bibr" target="#b2">3</ref> We report two different aspects of view synthesis: the capacity to fit the source views or Memorization (e.g. reconstruction), and the ability to predict the target views or Hallucination (e.g. scene completion) using L1 and SSIM metrics. <ref type="bibr" target="#b3">4</ref> GSN outperforms both GTM-SM <ref type="bibr" target="#b12">[13]</ref> and ISS <ref type="bibr" target="#b40">[41]</ref> for nearly all tasks (Tab. 6), even though it was not trained to explicitly learn a mapping from S to T (see supplementary material for qualitative results). We attribute this success to the powerful scene prior learned by GSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have made great strides towards generative modeling of unconstrained, complex and realistic indoor scenes. We introduced GSN, a generative model that decomposes a scene into many local radiance fields, which can be rendered by a free moving camera. Our decomposition scheme scales efficiently to big scenes while preserving details and distribution coverage. We show that GSN can be trained on multiple scenes to learn a rich scene prior, while rendered trajectories on scenes sampled from the prior are smooth and consistent, maintaining scene coherence. The prior distribution learned by GSN can also be used to infer observations from arbitrary cameras given a sparse set of observations. A multitude of avenues for future work are now enabled by GSN, from improving the rendering performance and training on large-scale datasets to exploring the wide range of down-stream tasks that benefit from a learned scene prior like model-based reinforcement learning <ref type="bibr" target="#b26">[27]</ref>, SLAM <ref type="bibr" target="#b5">[6]</ref> or 3D completion for immersive photography <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Architectures and Training Details</head><p>In this section we summarize the model architectures, hyperparameter settings, and other training details used for producing the GSN models presented in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Mapping Network</head><p>The mapping network maps the global latent code z to an intermediate non-linear latent space <ref type="bibr" target="#b23">[24]</ref>. All models in our experiments, including those that do not use the global generator, use a mapping network which consists of a normalization step followed by three linear layers with LeakyReLU activations <ref type="bibr" target="#b30">[31]</ref>, as shown in Tab. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation</head><p>Output Shape  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Global Generator</head><p>The purpose of the global generator (Tab. 8) is to map from a single global latent code z to a 2D grid of local latent codes W which represent the spatial layout of the scene. The global generator is composed of successive modulated convolutional layers <ref type="bibr" target="#b24">[25]</ref> that are conditioned on the global latent code. Following StyleGAN <ref type="bibr" target="#b23">[24]</ref>, the model learns a constant input for the first layer. The first layers in every pair of modulated convolutional layers thereafter upsamples the feature map resolution by 2×, which is implemented as a transposed convolution with a stride of 2, followed by bilinear filtering <ref type="bibr" target="#b58">[59]</ref>.</p><p>The output resolution of the global generator (which is also the spatial resolution of W) is a hyperparameter which effectively controls the size of the spatial region represented by each individual local latent code. We set the global generator output resolution to 32 × 32 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation</head><p>Output Shape </p><formula xml:id="formula_8">Constant Input - 256 × 4 × 4 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 8 × 8 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 8 × 8 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 16 × 16 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 16 × 16 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 32 × 32 ModulatedConv (3 × 3) LeakyReLU (0.2) 256 × 32 × 32 ModulatedConv (3 × 3) - 32 × 32 × 32</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Local Generator</head><p>The local generator is composed of a locally conditioned radiance field network which maps coordinates and view direction to appearance a and occupancy σ, a volumetric rendering step which accumulates along sampled rays to convert a and σ values to feature vectors, and a refinement network which upsamples feature maps to higher resolution RGB images.</p><p>The locally conditioned radiance field network ( <ref type="figure" target="#fig_11">Fig. 8</ref>) mimics the architecture of the the original NeRF network <ref type="bibr" target="#b33">[34]</ref>. To condition the network such that it can represent many different radiance fields we swap out the fixed linear layers for modulated linear layers similar to those used in CIPS <ref type="bibr" target="#b1">[2]</ref>, where each modulated linear layer is conditioned on w ij . Each modulated linear layer has 128 channels.</p><p>When performing volumetric rendering we threshold occupancy values σ with a softplus as in D-NeRF <ref type="bibr" target="#b39">[40]</ref> as opposed to the standard ReLU, as we find it leads to more stable training. For all experiments we sample 64 samples per ray. When generating 64 × 64 images we sample the radiance field network to produce feature maps at 32 × 32 resolution, and when generating 128 × 128 resolution images we sample feature maps at 64 × 64 resolution.</p><p>Once volumetric rendering has been performed we upsample the resulting feature map with refinement blocks <ref type="figure" target="#fig_12">(Fig. 10</ref>) until the desired resolution is achieved, then apply a sigmoid to bound the final output, as in GIRAFFE <ref type="bibr" target="#b37">[38]</ref>. In general, we found that sampling higher resolution feature maps directly from the radiance field produced higher quality results compared to sampling at low resolution and applying many refinement blocks, but the computational cost is significantly higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Discriminator</head><p>The discriminator is based on the architecture used in StyleGAN2 <ref type="bibr" target="#b24">[25]</ref> (Tab. 9), including residual blocks ( <ref type="figure" target="#fig_12">Fig. 10</ref>) and minibatch standard deviation layer <ref type="bibr" target="#b22">[23]</ref>. When including depth information as input to the discriminator we normalize it to [0, 1]. In the case that the radiance field network is sampled at a resolution lower than the final output resolution (such as when using the refinement network), then resulting depth maps will have lower resolution than real examples. To prevent the discriminator from using this difference in detail to differentiate real and fake samples we downsample all real depth maps to match the resolution of the generated depth maps, then upsample them both back to full resolution.</p><p>The decoder (Tab. 10) takes as input 4 × 4 resolution feature maps from the discriminator (before the minibatch standard deviation layer), and applies successive transposed convolutions with a stride of 2 and bilinear filtering <ref type="bibr" target="#b58">[59]</ref> to upsample the input until the original resolution is recovered.  <ref type="figure">Figure 9</ref>: Refinement block. Feature maps pass through the left path, while RGB images pass through the right path. The input to the right path is not applied for the first refinement block after the volumetric rendering step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Sampling Camera Poses</head><p>We use poses from camera trajectories in the training set as candidate poses during sampling, as real camera poses better reflect the true distribution of occupable locations compared to uniformly sampling over the entire scene region. Sampled camera poses are normalized and expressed    relative to the camera pose in the middle of the trajectory. This normalization enforces an egocentric coordinate system whose origin is placed at the center of W. Note that despite working with trajectories of multiple camera poses, we still only sample a single camera pose per generated scene during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. Training Details</head><p>We use the RMSprop <ref type="bibr" target="#b17">[18]</ref> optimizer with α = 0.99, = 10 −8 , and a learning rate of 0.002 for both the generator and discriminator. Following StyleGAN <ref type="bibr" target="#b23">[24]</ref>, we set the learning rate of the mapping network 100× less than the rest of the network for improved training stability. Equalized learning rate <ref type="bibr" target="#b22">[23]</ref> is used for all learnable parameters, and an exponential moving average of the generator weights <ref type="bibr" target="#b22">[23]</ref> with a decay of 0.999 is used to stabilize test-time performance. Differentiable data augmentations <ref type="bibr" target="#b60">[61]</ref> such as random translation, colour jitter, and Cutout <ref type="bibr" target="#b9">[10]</ref> are applied to all inputs to the discriminator in order to combat overfitting. To save compute, the R1 gradient penalty is applied using a lazy regularization strategy <ref type="bibr" target="#b24">[25]</ref> by applying it every 16 iterations. We set λ R1 to 0.01 and λ Recon to 1000 for all experiments.</p><p>All 64 × 64 resolution models used for the generation performance evaluation (GSN and otherwise) were trained for 500k iterations with a batch size of 32. Training takes 4 days on two NVIDIA A100 GPUs with 40GB of memory each. Mixed precision training is applied to the generator for a small reduction in memory cost and training time. We do not apply mixed precision training to the discriminator as training stability decreases in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inverting GSN for View Synthesis</head><p>In order for GSN to deal with the view synthesis problem we follow common practices for GAN inversion <ref type="bibr" target="#b56">[57]</ref>, adopting a hybrid inversion approach where we first train an encoder E θ E : R 3×w×h × SE(3) −→ R c×s×s×s on {(X, T, W) i } i=1:n tuples sampled from a trained GSN (trained on the training set of the same dataset). The goal of this encoder is to predict an initial grid of latent codes W 0 given a set of posed views. Our encoder is conceptually similar to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref> where viewsX are first processed with a backbone (UNet <ref type="bibr" target="#b46">[47]</ref> with a ResNet-50 encoder in our case) and the resulting feature maps are back-projected using camera poses T into a shared feature volume V ∈ R c×s×s×s . Finally, we perform average pooling over the height dimension of V to get W 0 ∈ R c×s×s . We train the encoder by minimizing the following reconstruction loss:</p><formula xml:id="formula_9">L(X, T, W; θ E ) = W − E θ E (X, T) 2 + (3) + X − f (E θ E (X, T), T) 2 ,<label>(4)</label></formula><p>where the first term encourages the reconstruction of the local latent grid, and the second term encourages samples from locally conditioned radiance field f to match the original input views.</p><p>At inference time, given a trained encoder E θ E , we feed the source views S = {(X, T) i } i=t−5:t through our encoder to predict an initialization latent code grid W 0 that we then optimize via SGD for 1000 iterations to getŴ. Given that scenes do not share a canonical orientation, we predict W 0 at multiple rotation angles of {T i } i=t−5:t about the y−axis to find the generator's preferred orientation and use this orientation during optimization (note that relative transformations between camera poses do not change with this global transformation). We define the preferred orientation as the one that minimizes an auto-encoding LPIPS loss <ref type="bibr" target="#b59">[60]</ref>. The optimization process is performed by freezing the weights of f θ f and computing a reconstruction loss w.r.t. S. We then useŴ in the locally conditioned radiance field and render observations using the camera poses of S to produceŜ (i.e. to auto-encode source views), while also rendering from the camera poses of the target views T to produceT (i.e. to predict unseen parts of the scene). Future work will explore in depth how to improve the quality and efficiency of the inversion approach for GSN-based models where the generative model tends to prefer a certain orientation. In <ref type="figure">Fig. 11</ref> we show qualitative results for view synthesis on Vizdoom on held out sequences not seen during training. We can see how GSN learns a robust prior that is able to fill in the scene with plausible completions (e.g. row 5 and 8), even if those completions do not strictly minimize the L1 reconstruction loss.</p><p>In addition, <ref type="figure">Fig. 12</ref> shows qualitative view synthesis results on the Replica dataset <ref type="bibr" target="#b51">[52]</ref>, showing the applicability of GSN for view synthesis on realistic data. In this experiment we follow the settings described for Vizdoom in terms of S and T . In <ref type="figure">Fig. 12</ref> the top 3 rows show results on data from the training set (e.g. scenes that were observed during training) and the bottom 3 rows show test set results (e.g. results on unseen scenes). We can see how GSN successfully uses the prior learned from training data to find a plausible scene completion for unseen scenes that respects the global scene geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results on Local vs. Global Coordinate Systems</head><p>In this section we demonstrate the robustness of GSN w.r.t. re-arrangement of the latent codes in W. In order to do so we sample different scenes from our learned prior and apply a rigid transformation to their corresponding W (a 2D rotation). In principle, this rotation should amount to a rotation of the scene represented by W that does not change the radiance field prediction. To qualitatively evaluate this effect we sample different scenes and rotate their corresponding W by {0, 90, 180, 270} degrees while (i) rotating the camera by the same amount about the y−axis so that the rendered image should remain constant and (ii) keeping the camera fixed so that the scene should rotate. In <ref type="figure" target="#fig_7">Fig. 13-14</ref> we show the result of the setting in (i) for a local and global coordinate system respectively. In these results we see how a local coordinate system is drastically more robust to rearrangements of the latent codes than a global coordinate system. In addition, we show results for the setting in (ii) in <ref type="figure" target="#fig_8">Fig. 15-16</ref> for local and global coordinate systems respectively. In this case, we see how given a fixed camera, a rotation of W amounts to rotating the scene. In this case we <ref type="figure">Figure 11</ref>: Qualitative view synthesis results on Vizdoom sequences not seen during training. Given source views S we invert GSN to obtain a local latent code gridŴ, which is then use both to reconstruct S, denoted asŜ, and also to predict target views T (given their camera poses) which are denoted asT . Each row corresponds to a different set of source views S. Frames highlighted in green are input to GSN, frames highlighted in blue are predictions. <ref type="figure">Figure 12</ref>: Qualitative view synthesis results on Replica. Given source views S we invert GSN to obtain a local latent code gridŴ, which is then use both to reconstruct S, denoted asŜ, and also to predict target views T (given their camera poses) which are denoted asT . Each row corresponds to a different set of source views S (top 3 rows are scenes from the training set, bottom 3 rows are scenes in a heldout test set). Frames highlighted in green are input to GSN, frames highlighted in blue are predictions.</p><p>can also see how a local coordinate system results in higher rendering quality compared to that of a global coordinate system, which suffers from degradation as the rotation angle increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scene Editing</head><p>A nice property of the local latent grid produced by GSN is that it can be used to perform scene editing by directly altering W. This property allows us a degree of manual control for scene synthesis beyond what we get from ran-  domly sampling the generator. While the low resolution of W used in current models currently limits us to high level scene modifications, training with larger local latent grids could allow for more fine-grained control over scenes, such as rearrangement of furniture.</p><p>We find that, as with most image composition operations, the results of scene editing appear most convincing when the inputs are well aligned in terms of appearance and geometry. We demonstrate editing operations by manipulating the codes from single scenes, since we don't need to worry about matching appearance and geometry, but multi-ple scenes could be combined if they were similar enough. In <ref type="figure" target="#fig_10">Fig. 17-18</ref> we manipulate the local latent codes by mirroring them along the horizontal axis to produce unique scenes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>&lt;</head><label></label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " w E I r j K b W C O e W t z Z L b 1 q T X J F k J P w = " &gt; A A A C B X i c b V C 7 T s N A E D y H V w g v A y U U J y I E V W Q H B J S R a C g D I g 8 p N t H 5 c k 5 O O Z + t u z U i s t L Q 8 C s 0 F C B E y z / Q 8 T d c H g U k j L T S a G Z X u z t B I r g G x / m 2 c g u L S 8 s r + d X C 2 v r G 5 p a 9 v V P X c a o o q 9 F Y x K o Z E M 0 E l 6 w G H A R r J o q R K B C s E f Q v R 3 7 j n i n N Y 3 k L g 4 T 5 E e l K H n J K w E h t e 9 8 D 9 g B B m C X D I + x x i b 2 I Q C 8 I s p v h 3 U n b L j o l Z w w 8 T 9 w p K a I p q m 3 7 y + v E N I 2 Y B C q I 1 i 3 X S c D P i A J O B R s W v F S z h N A + 6 b K W o Z J E T P v Z + I s h P j R K B 4 e x M i U B j 9 X f E x m J t B 5 E g e k c 3 a h n v Z H 4 n 9 d K I b z w M y 6 T F J i k k 0 V h K j D E e B Q J 7 n D F K I i B I Y Q q b m 7 F t E c U o W C C K 5 g Q 3 N m X 5 0 m 9 X H L P S s 7 1 a b F S n s a R R 3 v o A B 0 j F 5 2 j C r p C V V R D F D 2 i Z / S K 3 q w n 6 8 V 6 t z 4 m r T l r O r O L / s D 6 / A E i l 5 h I &lt; / l a t e x i t &gt; p 0 2 R 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 b I e B C x J h C / V b T Y o g z 1 O L I F / P k g = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I n k p S R D 0 W e v H g o a L 9 g D a U z W b b L t 1 k w + 5 E D L E H / 4 o X D 4 p 4 9 W 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J n n x 4 J r c J x v a 2 l 5 Z X V t v b B R 3 N z a 3 t m 1 9 / a b W i a K s g a V Q q q 2 T z Q T P G I N 4 C B Y O 1 a M h L 5 g L X 9 U m / i t e 6 Y 0 l 9 E d p D H z Q j K I e J 9 T A k b q 2 Y d d Y A + Q X U t K B K 5 J q Q J 8 m + p x z y 4 5 Z W c K v E j c n J R Q j n r P / u o G k i Y h i 4 A K o n X H d W L w M q K A U 8 H G x W 6 i W U z o i A x Y x 9 C I h E x 7 2 f T + M T 4 x S o D 7 U p m K A E / V 3 x M Z C b V O Q 9 9 0 h g S G e t 6 b i P 9 5 n Q T 6 l 1 7 G o z g B F t H Z o n 4 i M E g 8 C Q M H X D E K I j W E U M X N r Z g O i S I U T G R F E 4 I 7 / / I i a V b K 7 n n Z u T k r V S t 5 H A V 0 h I 7 R K X L R B a q i K 1 R H D U T R I 3 p G r + j N e r J e r H f r Y 9 a 6 Z O U z B + g P r M 8 f 4 j O V 9 g = = &lt; / l a t e x i t &gt; Local Coord Sys &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w E f 4 r f e 3 1 B l P a s R 6 l E p W V I 1 n / i Y = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K 9 g P S U D b b T b t 0 s x t 2 J 2 I N / R l e P C j i 1 V / j z X / j p s 1 B W x 8 M P N 6 b Y W Z e m A h u w H W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 o G 5 V q y l p U C a W 7 I T F M c M l a w E G w b q I Z i U P B O u H 4 J v c 7 D 0 w b r u Q 9 T B I W x G Q o e c Q p A S v 5 P W C P E E b Z 0 7 T S r 9 b c u j s D X i Z e Q W q o Q L N f / e o N F E 1 j J o E K Y o z v u Q k E G d H A q W D T S i 8 1 L C F 0 T I b M t 1 S S m J k g m 5 0 8 x S d W G e B I a V s S 8 E z 9 P Z G R 2 J h J H N r O m M D I L H q 5 + J / n p x B d B R m X S Q p M 0 v m i K B U Y F M 7 / x w O u G Q U x s Y R Q z e 2 t m I 6 I J h R s S n k I 3 u L L y 6 R 9 V v c u 6 u 7 d e a 1 x X c R R R k f o G J 0 i D 1 2 i B r p F T d R C F C n 0 j F 7 R m w P O i / P u f M x b S 0 4 x c 4 j + w P n 8 A W Q F k V I = &lt; / l a t e x i t &gt; z &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M X x K w + B 6 O d Y b n N A z N z 7 D 3 H h w P Y U = " &gt; A A A B 8 X i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K 9 g P b U D b b S b t 0 s w m 7 E 7 G E / g s v H h T x 6 r / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q D g 0 e y 1 i 3 A 2 Z A C g U N F C i h n W h g U S C h F Y x u p n 7 r E b Q R s b r H c Q J + x A Z K h I I z t N J D F + E J g z B r T X r l i l t 1 Z 6 D L x M t J h e S o 9 8 p f 3 X 7 M 0 w g U c s m M 6 X h u g n 7 G N A o u Y V L q p g Y S x k d s A B 1 L F Y v A + N n s 4 g k 9 s U q f h r G 2 p Z D O 1 N 8 T G Y u M G U e B 7 Y w Y D s 2 i N x X / 8 z o p h l d + J l S S I i g + X x S m k m J M p + / T v t D A U Y 4 t Y V w L e y v l Q 6 Y Z R x t S y Y b g L b 6 8 T J p n V e + i 6 t 6 d V 2 r X e R x F c k S O y S n x y C W p k V t S J w 3 C i S L P 5 J W 8 O c Z 5 c d 6 d j 3 l r w c l n D s k f O J 8 / 9 k 6 R G w = = &lt; / l a t e x i t &gt; W &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 7 u 0 O T H X 7 R 2 5 b J Q n D A v r 4 G e M u n k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>A P Y A f p P E E e 1 x i L y Q w 9 P 3 0 Z n J 3 0 r P L T s W Z A i 8 S N y d l l K P e s 7 + 8 f k S T k E m g g m j d c Z 0 Y u i l R w K l g k 6 K X a B Y T O i I D 1 j F U k p D p b j p 9 Y o K P j N L H Q a R M S c B T 9 f d E S k K t x 6 F v O r M b 9 b y X i f 9 5 n Q S C i 2 7 K Z Z w A k 3 S 2 K E g E h g h n i e A + V 4 y C G B t C q O L m V k y H R B E K J r e i C c G d f 3 m R N K s V 9 6 z i X J + W a 9 U 8 j g I q o U N 0 j F x 0 j m r o C t V R A 1 H 0 i J 7 R K 3 q z n q w X 6 9 3 6 m L U u W f n M A f o D 6 / M H u 0 2 Y F w = = &lt; / l a t e x i t &gt; p 2 R 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 9 Z I R z J Q I y v o K 0 5 I s v Q Y 7 Y y p p 7 E = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 L B b B U 0 m K q M e C F 4 8 V 7 A e 0 I W y 2 m 3 b t Z h N 2 J 2 o N + S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u w X G + r Z X V t f W N z d J W e X t n d 6 9 i 7 x + 0 d Z w q y l o 0 F r H q B k Q z w S V r A Q f B u o l i J A o E 6 w T j q 6 n f u W d K 8 1 j e w i R h X k S G k o e c E j C S b 1 f 6 w B 4 h C L O H 3 M / 4 X e 7 b V a f m z I C X i V u Q K i r Q 9 O 2 v / i C m a c Q k U E G 0 7 r l O A l 5 G F H A q W F 7 u p 5 o l h I 7 J k P U M l S R i 2 s t m h + f 4 x C g D H M b K l A Q 8 U 3 9 P Z C T S e h I F p j M i M N K L 3 l T 8 z + u l E F 5 6 G Z d J C k z S + a I w F R h i P E 0 B D 7 h i F M T E E E I V N 7 d i O i K K U D B Z l U 0 I 7 u L L y 6 R d r 7 n n N e f m r N q o F 3 G U 0 B E 6 R q f I R R e o g a 5 R E 7 U Q R S l 6 R q / o z X q y X q x 3 6 2 P e u m I V M 4 f o D 6 z P H 7 S J k 7 g = &lt; / l a t e x i t &gt; w ij Volume Rendering &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r I b o L L R L 4 a r G W X 3 G r I H 6 I v y 8 F O M = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G N B B I 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o p e N U M W y y W M S q E 1 C N g k t s G m 4 E d h K F N A o E t o P x 7 c x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p M e y X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s I b P+ M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 V 1 W v c V m p 3 e V x F O E E T u E c P L i G G t x D H Z r A A O E Z X u H N e X R en H f n Y 9 F a c P K Z Y / g D 5 / M H z 1 S M 9 A = = &lt; / l a t e x i t &gt; g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 0 6 w q s K l u L j e l L K A S v i S b U T 2 v T o = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G N B B I 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g p r 6 x u b W 8 X t 0 s 7 u 3 v 5 B + f C o p e N U M W y y W M S q E 1 C N g k t s G m 4 E d h K F N A o E t o P x 7 c x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p E f b L F b f q z k F W i Z e T C u S o 9 8 t f v U H M 0 g i l Y Y J q 3 f X c x P g Z V Y Y z g d N S L 9 W Y U D a m Q + x a K m m E 2 s / m h 0 7 J m V U G J I y V L W n I X P 0 9 k d F I 6 0 k U 2 M 6 I m p F e 9 m b i f 1 4 3 N e G N n 3 G Z p A Y l W y w K U 0 F M T G Z f k w F X y I y Y W E K Z 4 v Z W w k Z U U W Z s N i U b g r f 8 8 i p p X V S 9 q 6 r X u K z U 7 v I 4 i n A C p 3 A O H l x D D e 6 h D k 1 g g P A M r / D m P D o v z r v z s W g t O P n M M f y B 8 / k D z d C M 8 w = = &lt; / l a t e x i t &gt; f &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O f o r s n 3 q X l 3 p F l Z U 0 Y i y b 1 E T W W Y = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I H q Q k I t p j w Y v H C v Y D m l A 2 2 0 2 7 d D c J u x O x x B z 8 K 1 4 8 K O L V v + H N f + O 2 z U F b H w w 8 3 p t h Z l 6 Q C K 7 B c b 6 t p e W V 1 b X 1 0 k Z 5 c 2 t 7 Z 9 f e 2 2 / p O F W U N W k s Y t U J i G a C R 6 w J H A T r J I o R G Q j W D k b X E 7 9 9 z 5 T m c X Q H 4 4 T 5 k g w i H n J K w E g 9 + 9 D L P M 0 H k p x h D 9 g D B G F G c i / v 2 R W n 6 k y B F 4 l b k A o q 0 O j Z X 1 4 / p q l k E V B B t O 6 6 T g J + R h R w K l h e 9 l L N E k J H Z M C 6 h k Z E M u 1 n 0 / t z f G K U P g 5 j Z S o C P F V / T 2 R E a j 2 W g e m U B I Z 6 3 p u I / 3 n d F M K a n / E o S Y F F d L Y o T A W G G E / C w H 2 u G A U x N o R Q x c 2 t m A 6 J I h R M Z G U T g j v / 8 i J p n V f d y 6 p 7 e 1 G p 1 4 o 4 S u g I H a N T 5 K I r V E c 3 q I G a i K J H 9 I x e 0 Z v 1 Z L 1 Y 7 9 b H r H X J K m Y O 0 B 9 Y n z 8 s 7 J Y u &lt; / l a t e x i t &gt; { , a} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 7 u 0 O T H X 7 R 2 5 b J Q n D A v r 4 G e M u n k = " &gt; A A A C B H i c b V B N S 8 N A E N 3 4 W e t X 1 G M v i 0 X w V J I q 6 r H g x W M V + w F N L Z v t p l 2 6 2 Y T d i V h C D 1 7 8 K 1 4 8 K O L V H + H N f + O m z U F b H w w 8 3 p t h Z p 4 f C 6 7 B c b 6 t p e W V 1 b X 1 w k Z x c 2 t 7 Z 9 f e 2 2 / q K F G U N W g k I t X 2 i W a C S 9 Y A D o K 1 Y 8 V I 6 A v W 8 k e X m d + 6 Z 0 r z S N 7 C O G b d k A w k D z g l Y K S e X f K A P Y A f p P E E e 1 x i L y Q w 9 P 3 0 Z n J 3 0 r P L T s W Z A i 8 S N y d l l K P e s 7 + 8 f k S T k E m g g m j d c Z 0 Y u i l R w K l g k 6 K X a B Y T O i I D 1 j F U k p D p b j p 9 Y o K P j N L H Q a R M S c B T 9 f d E S k K t x 6 F v O r M b 9 b y X i f 9 5 n Q S C i 2 7 K Z Z w A k 3 S 2 K E g E h g h n i e A + V 4 y C G B t C q O L m V k y H R B E K J r e i C c G d f 3 m R N K s V 9 6 z i X J + W a 9 U 8 j g I q o U N 0 j F x 0 j m r o C t V R A 1 H 0 i J 7 R K 3 q z n q w X 6 9 3 6 m L U u W f n M A f o D 6 / M H u 0 2 Y F w = = &lt; / l a t e x i t &gt; p 2 R 3 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y K f 9 V h 5 j B 6 J 2 q k H N E u Q 2 c M 2 n U T 0 = " &gt; A A A B 7 H i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q Q c p u E f V Y 8 O K x g t s W 2 q V k 0 2 w b m 8 0 u S V Y o S 3 + D F w + K e P U H e f P f m L Z 7 0 N Y H A 4 / 3 Z p i Z F y S C a + M 4 3 6 i w t r 6 x u V X c L u 3 s 7 u 0 f l A + P W j p O F W U e j U W s O g H R T H D J P M O N Y J 1 E M R I F g r W D 8 e 3 M b z 8 x p X k s H 8 w k Y X 5 E h p K H n B J j J a / K L x 7 P + + W K U 3 P m w K v E z U k F c j T 7 5 a / e I K Z p x K S h g m j d d Z 3 E + B l R h l P B p q V e q l l C 6 J g M W d d S S S K m / W x + 7 B S f W W W A w 1 j Z k g b P 1 d 8 T G Y m 0 n k S B 7 Y y I G e l l b y b + 5 3 V T E 9 7 4 G Z d J a p i k i 0 V h K r C J 8 e x z P O C K U S M m l h C q u L 0 V 0 x F R h B q b T 8 m G 4 C 6 / v E p a 9 Z p 7 V X P u L y u N e h 5 H E U 7 g F K r g w j U 0 4 A 6 a 4 A E F D s / w C m 9 I o h f 0 j j 4 W r Q W U z x z D H 6 D P H 8 D G j f E = &lt; / l a t e x i t &gt; (i, j) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 a F t 7 4 A h q O h 8 5 7 8 7 7 H p E 2 E l j 9 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S 9 Y 2 x / X p 3 7 7 g S n N Z X Q P k 5 h 5 I R l G f M A p A S P 1 7 a M e s E d I r 4 X 0 i c B 1 K V W A 7 y Y 6 6 9 t l p + L M g J e J m 5 M y y t H o 2 1 + 9 Q N I k Z B F Q Q b T u u k 4 M X k o U c C p Y V u o l m s W E j s m Q d Q 2 N S M i 0 l 8 4 e y P C p U Q I 8 k M p U B H i m / p 5 I S a j 1 J P R N Z 0 h g p B e 9 q f i f 1 0 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b2">[3]</ref> a 1D global latent code z ∼ p z is used to condition an MLP which parameterizes a single radiance field. Although such 1D global latent code can be effective when representing individual object categories, such as cars or faces, it does not scale well to large, z Conv . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Global generator &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j g 6 b M I F O H d W 1 n B S r a P M 6 + / s 6 g p Q = " &gt; A A A C E 3 i c b V D L S s N A F J 3 U d 3 1 F X b o Z L K K I l K T 4 W g p u X K p Y W 2 h i m U w n O n Q y C T M 3 Y g n 5 B z f + i h s X i r h 1 4 8 6 / c V I j q P X A w J l z 7 u X e e 4 J E c A 2 O 8 2 F V x s Y n J q e m Z 6 q z c / M L i / b S 8 o W O U 0 V Z k 8 Y i V u 2 A a C a 4 Z E 3 g I F g 7 U Y x E g W C t o H 9 U + K 0 b p j S P 5 T k M E u Z H 5 E r y k F M C R u r a W x 0 P 2 C 0 E Y Z b k G 9 v 4 + 9 P L f e x x i b 2 I w H U Q Z G f 5 5 W 7 X r j l 1 Z w g 8 S t y S 1 F C J k 6 7 9 7 v V i m k Z M A h V E 6 4 7 r J O B n R A G n g u V V L 9 U s I b R P r l j H U E k i p v 1 s e F O O 1 4 3 S w 2 G s z J O A h + r P j o x E W g + i w F Q W O + q / X i H + 5 3 V S C A / 8 j M s k B S b p 1 6 A w F R h i X A S E e 1 w x C m J g C K G K m 1 0 x v S a K U D A x V k 0 I 7 t + T R 8 l F o + 7 u 1 d 3 T n d p h o 4 x j G q 2 i N b S J X L S P D t E x O k F N R N E d e k B P 6 N m 6 t x 6 t F + v 1 q 7 R i l T 0 r 6 B e s t 0 / p 9 J 4 g &lt; / l a t e x i t &gt; [p 0 , d] 2 R 5 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c t a j A + E w 7 9 A F F 9 g q 5 h D F o r I 4 a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>z H p 0 X 5 9 3 5 W L Q W n H z m G P 7 A + f w B z O m M 8 A = = &lt; / l a t e x i t &gt; f Linear &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 f s 7 i 6 K 5 6 f A v Y Z 5 y 8 M V 8 J e Y l q m I = " &gt; A A A B 7 H i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K I q M e i F 4 8 V T F t o Q 9 l s N + 3 S z S b s v g g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b R 1 Y G G b e s O 9 N m E p h 0 H W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q m S T T j P s s k Y n u h N R w K R T 3 U a D k n V R z G o e S t 8 P x 3 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r J 7 w 0 S N P 1 q z a 2 7 c 5 B V 4 h W k B g W a / e q X z b E s 5 g q Z p M Z 0 P T f F I K c a B Z N 8 W u l l h q e U j e m Q d y 1 V N O Y m y O f L T s m Z V Q Y k S r R 9 C s l c / Z 3 I a W z M J A 7 t Z E x x Z J a 9 m f i f 1 8 0 w u g l y o d I M u W K L j 6 J M E k z I 7 H I y E J o z l B N L K N P C 7 k r Y i G r K 0 P Z T s S V 4 y y e v k t Z F 3 b u q e w + X t c Z t U U c Z T u A U z s G D a 2 j A P T T B B w Y C n u E V 3 h z l v D j v z s d i t O Q U m W P 4 A + f z B / N g j s g = &lt; / l a t e x i t &gt; . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b z c J Z N x u v y e r P n n i j w i d i n E z i g o = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I H q Q k I u q x 6 M V j B f s B T S i b 7 a Z d u p u E 3 Y l Y Y g 7 + F S 8 e F P H q 3 / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y 9 I B N f g O N / W w u L S 8 s p q a a 2 8 v r G 5 t W 3 v 7 D Z 1 n C r K G j Q W s W o H R D P B I 9 Y A D o K 1 E 8 W I D A R r B c P r s d + 6 Z 0 r z O L q D U c J 8 S f o R D z k l Y K S u v e 9 l n u Z 9 S U 6 w B + w B g j A j u Z d 3 7 Y p T d S b A 8 8 Q t S A U V q H f t L 6 8 X 0 1 S y C K g g W n d c J w E / I w o 4 F S w v e 6 l m C a F D 0 m c d Q y M i m f a z y f 0 5 P j J K D 4 e x M h U B n q i / J z I i t R 7 J w H R K A g M 9 6 4 3 F / 7 x O C u G l n / E o S Y F F d L o o T A W G G I / D w D 2 u G A U x M o R Q x c 2 t m A 6 I I h R M Z G U T g j v 7 8 j x p n l b d 8 6 p 7 e 1 a p X R V x l N A B O k T H y E U X q I Z u U B 0 1 E E W P 6 B m 9 o j f r y X q x 3 q 2 P a e u C V c z s o T + w P n 8 A L + 6 W O A = = &lt; / l a t e x i t &gt; { , a} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 9 Z I R z J Q I y v o K 0 5 I s v Q Y 7 Y y p p 7 E = " &gt; A A A B + H i c b V B N S 8 N A E N 3 4 W e t H o x 6 9 L B b B U 0 m K q M e C F 4 8 V 7 A e 0 I W y 2 m 3 b t Z h N 2 J 2 o N + S V e P C j i 1 Z / i z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u w X G + r Z X V t f W N z d J W e X t n d 6 9 i 7 x + 0 d Z w q y l o 0 F r H q B k Q z w S V r A Q f B u o l i J A o E 6 w T j q 6 n f u W d K 8 1 j e w i R h X k S G k o e c E j C S b 1 f 6 w B 4 h C L O H 3 M / 4 X e 7 b V a f m z I C X i V u Q K i r Q 9 O 2 v / i C m a c Q k U E G 0 7 r l O A l 5 G F H A q W F 7 u p 5 o l h I 7 J k P U M l S R i 2 s t m h + f 4 x C g D H M b K l A Q 8 U 3 9 P Z C T S e h I F p j M i M N K L 3 l T 8 z + u l E F 5 6 G Z d J C k z S + a I w F R h i P E 0 B D 7 h i F M T E E E I V N 7 d i O i K K U D B Z l U 0 I 7 u L L y 6 R d r 7 n n N e f m r N q o F 3 G U 0 B E 6 R q f I R R e o g a 5 R E 7 U Q R S l 6 R q / o z X q y X q x 3 6 2 P e u m I V M 4 f o D 6 z P H 7 S J k 7 g = &lt; / l a t e x i t &gt; w ij (b) Locally conditioned radiance field network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>(a) Architecture of global generator g. We use a mapping network, modulated convolutional blocks, and a learned constant input as in StyleGAN2<ref type="bibr" target="#b24">[25]</ref>. (b) Architecture of the locally conditioned radiance field network f . Latent code w ij , sampled from W, is used to modulate linear layers, similar to CIPS<ref type="bibr" target="#b1">[2]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Two example latent interpolations between global latent codes z. Scenes transition smoothly by aligning geometry features such as walls (top) and appearance features such as the picture frame and doorway (bottom). Views are rendered from a fixed camera pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Robustness of generation quality w.r.t. to rotations of the local latent grid W. Using a local coordinate system for each local radiance field limits degradation due to rigid re-arrangement of the local latent codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Effect of trajectory length (x axis) on FID scores (y axis) when evaluating short 4-step trajectories (red) vs. 40-step trajectories (blue) during training. Models trained only with short trajectories achieve good local render quality, but cannot move far from the origin without encountering quality degradation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b z c J Z N x u v y e r P n n i j w i d i n E z i g o = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I H q Q k I u q x 6 M V j B f s B T S i b 7 a Z d u p u E 3 Y l Y Y g 7 + F S 8 e F P H q 3 / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y 9 I B N f g O N / W w u L S 8 s p q a a 2 8 v r G 5 t W 3 v 7 D Z 1 n C r K G j Q W s W o H R D P B I 9 Y A D o K 1 E 8 W I D A R r B c P r s d + 6 Z 0 r z O L q D U c J 8 S f o R D z k l Y K S u v e 9 l n u Z 9 S U 6 w B + w B g j A j u Z d 3 7 Y p T d S b A 8 8 Q t S A U V q H f t L 6 8 X 0 1 S y C K g g W n d c J w E / I w o 4 F S w v e 6 l m C a F D 0 m c d Q y M i m f a z y f 0 5 P j J K D 4 e x M h U B n q i / J z I i t R 7 J w H R K A g M 9 6 4 3 F / 7 x O C u G l n / E o S Y F F d L o o T A W G G I / D w D 2 u G A U x M o R Q x c 2 t m A 6 I I h R M Z G U T g j v 7 8 j x p n l b d 8 6 p 7 e 1 a p X R V x l N A B O k T H y E U X q I Z u U B 0 1 E E W P 6 B m 9 o j f r y X q x 3 q 2 P a e u C V c z s o T + w P n 8 A L + 6 W O A = = &lt; / l a t e x i t &gt; { , a} &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b z c J Z N x u v y e r P n n i j w i d i n E z i g o = " &gt; A A A B / 3 i c b V B N S 8 N A E N 3 4 W e t X V P D i Z b E I H q Q k I u q x 6 M V j B f s B T S i b 7 a Z d u p u E 3 Y l Y Y g 7 + F S 8 e F P H q 3 / D m v 3 H b 5 q C t D w Y e 7 8 0 w M y 9 I B N f g O N / W w u L S 8 s p q a a 2 8 v r G 5 t W 3 v 7 D Z 1 n C r K G j Q W s W o H R D P B I 9 Y A D o K 1 E 8 W I D A R r B c P r s d + 6 Z 0 r z O L q D U c J 8 S f o R D z k l Y K S u v e 9 l n u Z 9 S U 6 w B + w B g j A j u Z d 3 7 Y p T d S b A 8 8 Q t S A U V q H f t L 6 8 X 0 1 S y C K g g W n d c J w E / I w o 4 F S w v e 6 l m C a F D 0 m c d Q y M i m f a z y f 0 5 P j J K D 4 e x M h U B n q i / J z I i t R 7 J w H R K A g M 9 6 4 3 F / 7 x O C u G l n / E o S Y F F d L o o T A W G G I / D w D 2 u G A U x M o R Q x c 2 t m A 6 I I h R M Z G U T g j v 7 8 j x p n l b d 8 6 p 7 e 1 a p X R V x l N A B O k T H y E U X q I Z u U B 0 1 E E W P 6 B m 9 o j f r y X q x 3 q 2 P a e u C V c z s o T + w P n 8 A L + 6 W O A = = &lt; / l a t e x i t &gt; { , a} Modulated Linear Locally conditioned radiance field network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Residual block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Change in generation output as local latent codes are rotated with a local coordinate system for two different scenes. (Top) Rendered image. (Middle) Residual w.r.t. 0 degree rotation. (Bottom) Visualization of W. Each column corresponds to a rotation of the camera and W in {0, 90, 180, 270}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Change in generation output as local latent codes are rotated with a global coordinate system. (Top) Rendered image. (Middle) Residual w.r.t. 0 degree rotation. (Bottom) Visualization of W. Each column corresponds to a rotation of the camera and W in {0, 90, 180, 270}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Change in generation output for a fixed camera as local latent codes are rotated with a local coordinate system for two different scenes. (Top) Rendered image. (Bottom) Visualization of W. Each column corresponds to a rotation of W in {0, 90, 180, 270}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Change in generation output for a fixed camera as local latent codes are rotated with a global coordinate system for two different scenes. (Top) Rendered image. (Bottom) Visualization of W. Each column corresponds to a rotation of W in {0, 90, 180, 270}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Panoramas and corresponding local latent codes for scenes produced by GSN. Mirroring the local latent code from a single room (top row) produces a new room (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Panoramas and corresponding local latent codes for scenes produced by GSN. Mirroring the local latent code from a single room (top row) produces a new room (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of contributions and comparison with relevant related work. (Multiple Scene/Objects): Ability to model multiple scenes/objects in the same network. (Generative) Whether the model is generative (e.g. allows for free sampling). (Latent) Latent code spatial dim. (Radiance Field) Whether the model predicts a radiance field. (Scene level) Results demonstrated in scene-level environments. (Camera Placement) What camera motion is permitted?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Ours) 37.21 ± 1.17 4.56 ± 0.19 41.75 ± 1.33 4.14 ± 0.02 51.11 ± 1.37 6.59 ± 0.03</figDesc><table><row><cell>49]</cell><cell>47.50 ± 2.13</cell><cell>5.44 ± 0.43</cell><cell>65.37 ± 1.64</cell><cell>5.76 ± 0.14</cell><cell>62.59 ± 2.41</cell><cell>6.95 ± 0.15</cell></row><row><cell>π-GAN[3]</cell><cell cols="4">143.55 ± 4.81 15.26 ± 0.15 166.55 ± 3.61 13.17 ± 0.20</cell><cell>98.76 ± 1.49</cell><cell>9.54 ± 0.29</cell></row><row><cell>GSN (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Generative performance of GSN models trained</cell></row><row><cell cols="3">with different depth resolutions. Remarkably, we show that</cell></row><row><cell cols="3">we can down-sample depth resolution to a single pixel with-</cell></row><row><cell cols="3">out degrading the quality of the generative model.</cell></row><row><cell cols="3">tional improvement of training stability helping adversarial</cell></row><row><cell>learning to converge.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>FID ↓</cell><cell>SwAV-FID ↓</cell></row><row><cell>GSN (full model)</cell><cell>41.75 ± 1.33</cell><cell>4.14 ± 0.02</cell></row><row><cell>-R1 gradient penalty</cell><cell>52.9 ± 19.9</cell><cell>4.56 ± 1.50</cell></row><row><cell>-Reconstruction penalty</cell><cell>274.3 ± 41.4</cell><cell>23.58 ± 4.11</cell></row><row><cell>-Data augmentation</cell><cell cols="2">412.25 ± 14.34 35.71 ± 9.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Results of view synthesis on Vizdoom [26] (a) and</cell></row><row><cell>AVD [1] (b). GSN improves view synthesis quality as a</cell></row><row><cell>result of modeling a powerful prior over scenes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Mapping network architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Global generator architecture.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Discriminator architecture.</figDesc><table><row><cell></cell><cell>Activation</cell><cell>Output Shape</cell></row><row><cell>Input Feature Map</cell><cell>-</cell><cell>512 × 4 × 4</cell></row><row><cell cols="2">ConvTranspose (3 × 3) LeakyReLU (0.2)</cell><cell>256 × 8 × 8</cell></row><row><cell cols="3">ConvTranspose (3 × 3) LeakyReLU (0.2) 128 × 16 × 16</cell></row><row><cell cols="2">ConvTranspose (3 × 3) LeakyReLU (0.2)</cell><cell>64 × 32 × 32</cell></row><row><cell cols="2">ConvTranspose (3 × 3) LeakyReLU (0.2)</cell><cell>32 × 64 × 64</cell></row><row><cell>Conv (3 × 3)</cell><cell>-</cell><cell>4 × 64 × 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Decoder architecture</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Camera poses T on a sphere oriented towards the origin are constrained to SO(3) as opposed to free cameras in SE(3).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We will release code for reproducibility.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">There is no code or data release for<ref type="bibr" target="#b40">[41]</ref>. In private communication with the authors of<ref type="bibr" target="#b40">[41]</ref> we discussed the data splits and settings used for generating trajectories and follow them as closely as possible. We thank them for their help.<ref type="bibr" target="#b3">4</ref> We note that while these metrics are a good proxy for reconstruction quality, they do not tell a complete picture in the hallucination setting due to the stochastic nature of the view synthesis problem.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A dataset for developing and benchmarking active vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Ammirato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Košecká</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1378" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image generators with conditionally-independent pixel synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Ivan Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Demochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korzhenkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13775</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Eric R Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pi-Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00926</idno>
		<title level="m">Periodic implicit generative adversarial networks for 3d-aware image synthesis</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object goal navigation using goal-oriented semantic exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj Prakashchand</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural topological slam for visual navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular neural image based rendering with continuous view control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4090" to="4100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extreme view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Troccoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7781" to="7790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural point cloud rendering via multi-plane projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7830" to="7839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07630</idno>
		<title level="m">Equivariant neural rendering</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural scene representation and rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avraham</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6394</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative temporal models with spatial memory for partially observed environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09401</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10122</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">World models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Escaping plato&apos;s cave: 3d shape from adversarial rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9984" to="9993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian P Von</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH computer graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05375</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Jaśkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computational Intelligence and Games (CIG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to simulate dynamic environments with gamegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Seung Wook Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1231" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Infinite nature: Perpetual view generation of natural scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09855</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kunpeng Song, and Ahmed Elgammal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards faster and stabilized GAN training for high-fidelity few-shot image synthesis. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyaw Zaw</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11571</idno>
		<title level="m">Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>page 3. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Local light field fusion: Practical view synthesis with prescriptive sampling guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><forename type="middle">Khademi</forename><surname>Ortiz-Cayon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08934</idno>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On self-supervised image representations for GAN evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Atlas: Endto-end 3d scene reconstruction from posed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tarrence Van As</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10432,2020.14</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hologan: Unsupervised learning of 3d representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7588" to="7597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12100</idno>
		<title level="m">Giraffe: Representing scenes as compositional generative neural feature fields</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformable bottleneck networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7648" to="7657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo-Martin</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brualla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12948</idno>
		<title level="m">Deformable neural radiance fields</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incremental scene synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Planche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Kosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1668" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative appearance flow: A hybrid approach for outdoor view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hunter</forename><surname>Muhammad Usman Rafique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rebain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12490</idno>
		<title level="m">Decomposed radiance fields</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08860</idno>
		<title level="m">Shape-conditioned radiance fields from a single view</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Free view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07233</idno>
		<title level="m">Stable view synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Habitat: A platform for embodied ai research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9339" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02442</idno>
		<title level="m">Generative radiance fields for 3d-aware image synthesis</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene representation networks: Continuous 3d-structure-aware neural scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The replica dataset: A digital replica of indoor spaces</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-view to novel view: Synthesizing novel views with self-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Shao-Hua Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Grf: Learning a general radiance field for 3d scene representation and rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Trevithick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04595,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13090</idno>
		<title level="m">Ibrnet: Learning multi-view image-based rendering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05278</idno>
		<title level="m">GAN inversion: A survey</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vickie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02190</idno>
		<title level="m">Neural radiance fields from one or few images</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Making convolutional networks shiftinvariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Stereo magnification: Learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">View synthesis by appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
