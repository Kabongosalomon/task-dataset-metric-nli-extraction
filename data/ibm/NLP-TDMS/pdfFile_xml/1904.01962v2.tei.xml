<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rep the Set: Neural Networks for Learning Set Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Skianis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole PolytechniqueÉcole Polytechnique &amp; AUEBÉ cole PolytechniqueÉcole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole PolytechniqueÉcole Polytechnique &amp; AUEBÉ cole PolytechniqueÉcole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratis</forename><surname>Limnios</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole PolytechniqueÉcole Polytechnique &amp; AUEBÉ cole PolytechniqueÉcole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgianniś</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole PolytechniqueÉcole Polytechnique &amp; AUEBÉ cole PolytechniqueÉcole Polytechnique &amp; AUEB</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rep the Set: Neural Networks for Learning Set Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In several domains, data objects can be decomposed into sets of simpler objects. It is then natural to represent each object as the set of its components or parts. Many conventional machine learning algorithms are unable to process this kind of representations, since sets may vary in cardinality and elements lack a meaningful ordering. In this paper, we present a new neural network architecture, called RepSet, that can handle examples that are represented as sets of vectors. The proposed model computes the correspondences between an input set and some hidden sets by solving a series of network flow problems. This representation is then fed to a standard neural network architecture to produce the output. The architecture allows end-to-end gradient-based learning. We demonstrate RepSet on classification tasks, including text categorization, and graph classification, and we show that the proposed neural network achieves performance better or comparable to state-of-the-art algorithms. Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, Michalis Vazirgiannis 1 1.76 0.40 0.97 2 2.24 1.86 −0.97 3 0.95 −0.15 −0.10 4 0.41 0.14 1.45 5 0.52 0.08 1.62 6 2.14 1.72 −1.05 7 1.55 −0.45 0.88 8 −0.34 −1.26 0.24 9 1.08 −0.21 −0.09</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a variety of domains, complex data objects can be expressed as compositions of other, simpler objects. These simpler objects naturally correspond to the parts or components of the complex objects. For instance, in natural language processing, documents may be represented by sets of word embeddings. Likewise, in graph mining, a graph may be viewed as a set of vectors where these vectors correspond to the embeddings of its nodes. In computer vision, images may be described by local features extracted from different regions of the image. In such scenarios, one set of feature vectors denotes a single instance of a particular class of interest <ref type="bibr">(an object, document, graph, etc.)</ref>. Performing machine learning tasks on such types of objects (e. g., set classification, set regression, etc.) is very challenging. While typical machine learning algorithms are designed for fixed dimensional data instances, the cardinalities of these sets are not fixed, but they are allowed to vary. Furthermore, the elements of the sets usually do not have an inherent ordering. Hence, machine learning algorithms have to be invariant to permutations of these elements.</p><p>Traditionally, the most common approach to the problem is to define a distance/similarity measure or kernel that finds a correspondence between each pair of sets, and to combine it with an instance-based machine learning algorithm such as k-nn or SVM. This approach has dominated the field, and has achieved stateof-the-art results on many datasets. However, its main disadvantage is that it is a two-step approach. Data representation and learning are independent from each other. Ideally, we would like to have an end-to-end approach. Besides the above problem, these methods usually suffer from high computational and memory complexity since they compare all sets to each other.</p><p>In recent years, neural network architectures have proven extremely successful on a wide variety of tasks, notably in computer vision, in natural language processing, and in graph mining <ref type="bibr" target="#b11">(LeCun et al., 2015)</ref>. One of the main reasons of the success of neural networks is that the representation of data is adapted to the task at hand. Specifically, recent neural network models are end-to-end trainable, and they generate features that are suitable for the task at hand. Most models that operate on sets usually update the representations of the elements of the set using some architectures, typically a stack of fully-connected layers. And then, they apply a permutation invariant function to the updated element representations to obtain a representation for the set. Although these networks have proven successful in several tasks, they usually employ very simple per-arXiv:1904.01962v2 <ref type="bibr">[cs.</ref>LG] 28 Feb 2020 mutation invariant functions such as the sum or the average of the representations of the elements. This may potentially limit the expressive power of these architectures.</p><p>In this paper, we propose a novel neural network architecture for performing machine learning tasks on sets of vectors, named RepSet. The network is capable of generating representations for unordered, variablesized feature sets. Interestingly, the proposed model produces exactly the same output for all possible permutations of a set of vectors. To achieve that, it generates a number of hidden sets and it compares the input set with these sets using a network flow algorithm such as bipartite matching. The outputs of the network flow algorithm form the penultimate layer and are passed on to a fully-connected layer which produces the output. Since the objective functions of the employed network flow algorithms are differentiable, we can update these hidden sets during training with backpropagation. Hence, the proposed neural network is end-to-end trainable, while the hidden sets are different for each problem considered. Deeper models can be obtained by stacking more fully-connected layers one after another. When the size of the sets is large, solving the flow problems can become prohibitive. Hence, we also propose a relaxed formulation, ApproxRepSet, which is also permutation invariant and which scales to very large datasets. We demonstrate the proposed architecture in two classification tasks: text categorization from sets of word embeddings, and graph classification from bags of node embeddings. The results show that the proposed model produces better or competitive results with state-of-the-art techniques. Our main contributions are summarized as follows:</p><p>• We propose RepSet a novel architecture for performing machine learning on sets which, in contrast to traditional approaches, is capable of adapting data representation to the task at hand.</p><p>• We also propose ApproxRepSet, a simplified architecture which can be interpreted as an approximation of the proposed model, and which can handle very large datasets.</p><p>• We evaluate the proposed architecture on several benchmark datasets, and achieve state-of-the-art performance.</p><p>The rest of this paper is organized as follows. Section 2 provides an overview of the related work. Section 3 provides a description of the proposed neural network for sets. Section 4 evaluates the proposed architecture in two different tasks. Finally, Section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The past years witnessed a surge of interest in the area of neural networks for sets. These networks served mainly as the answer to computer vision problems such as the automated classification of point clouds.</p><p>Although conceptually simple, the proposed architectures have achieved state-of-the-art results on many tasks. PointNet <ref type="bibr" target="#b18">(Qi et al., 2017a)</ref> and DeepSets <ref type="bibr" target="#b28">(Zaheer et al., 2017)</ref> transform the vectors of the sets (i. e., using several layers) into new representations. They then apply some permutation-invariant function to the emerging vectors to generate representations for the sets. PointNet uses max-pooling to aggregate information across vectors, while DeepSets adds up the representations of the vectors. The representation of the set is then passed on to a standard architecture (e. g., fully-connected layers, nonlinearities, etc). PointNet++ <ref type="bibr" target="#b19">(Qi et al., 2017b)</ref> and SO-Net <ref type="bibr" target="#b13">(Li et al., 2018)</ref> apply PointNet hierarchically in order to better capture local structures. Two other recent works employ neural networks to learn the parameters of the likelihood of each set <ref type="bibr" target="#b20">(Rezatofighi et al., 2017</ref><ref type="bibr" target="#b21">(Rezatofighi et al., , 2018</ref>. <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref> treat unordered sets as ordered sequences and apply RNN models to them. However, they show that the output of the network is highly dependent on the order of the elements of the set. More recently, <ref type="bibr" target="#b12">Lee et al. (2019)</ref> proposed Set Transformer, a neural network that uses self-attention to model interactions among the elements of the input set.</p><p>Besides neural network architectures, several kernels between sets of vectors have been proposed in the past to enable kernel methods (e. g., SVMs) to handle unordered sets. Most of these kernels estimate a probability distribution on each set of vectors, and then derive their similarity using distribution-based comparison measures such as Fisher kernels <ref type="bibr" target="#b6">(Jaakkola and Haussler, 1999)</ref>, probability product kernels <ref type="bibr" target="#b7">(Jebara et al., 2004;</ref><ref type="bibr" target="#b14">Lyu, 2005)</ref> and the classical Bhattacharyya similarity measure <ref type="bibr" target="#b9">(Kondor and Jebara, 2003)</ref>. Furthermore, there are also kernels that map the vectors of each set to multi-resolution histograms, and then in order to find an approximate correspondence between the two sets of vectors, they compare the histograms with a weighted histogram intersection measure <ref type="bibr">(Grauman and Darrell, 2007b,a)</ref>. Such kernels have been applied to different tasks such as to the problem of graph classification <ref type="bibr" target="#b16">(Nikolentzos et al., 2017)</ref>. Although very effective in several tasks, these kernel-based approaches suffer from high computational complexity. In most cases, the complexity of computing kernels between sets is quadratic in the number of their elements, while in classification problems, the complexity of optimizing the SVM classifier is quadratic in the number of training samples. There are also some very popular permutationinvariant metrics for comparing unordered sets of vectors such as the Earth Mover's distance which corresponds to the solution of an optimization problem that transforms one set to another. This distance was first introduced by Gaspard Monge in the context of transportation theory, and is often used in computer vision <ref type="bibr" target="#b23">(Rubner et al., 2000)</ref> and in natural language processing <ref type="bibr" target="#b10">(Kusner et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Networks for Learning Representations of Sets</head><p>Conventional machine learning algorithms are designed to operate on fixed-size feature vectors, and they are thus unable to handle sets. In our setting, each example is represented as a collection</p><formula xml:id="formula_0">X = {v 1 , v 2 , . . . , v n } of d-dimensional vectors, v i ∈ R d .</formula><p>Note that examples are allowed to vary in the number of elements. Hence, it is not necessary that all examples comprise of exactly n components. Since our input is a set X = {v 1 , v 2 , . . . , v n }, v i ∈ R d , our input domain is the power set X = 2 R d , and we would like to design an architecture whose output would be the same regardless of the ordering of the elements of X. Clearly, to achieve that, a permutation invariant function is necessary to be introduced at some layer of the architecture. In fact, the model that we propose consists simply of standard fully-connected layers along with a permutation invariant layer. We next present the proposed permutation invariant layer in detail.</p><p>Permutation invariant layer. In this paper, we propose a novel permutation invariant layer which capitalizes on well-established concepts from flows and matchings in graphs. The proposed layer contains m "hidden sets" Y 1 , Y 2 , . . . , Y m of d-dimensional vectors. These sets may have different cardinalities and their components are trainable, i. e., the elements of a hidden set Y i correspond to the columns of a trainable matrix H (i) . Therefore, each column of matrix</p><formula xml:id="formula_1">H (i) is a vector u ∈ Y i .</formula><p>A natural way to measure the similarity between the input set and each one of the hidden sets is by comparing their building blocks, i. e., their elements. To achieve that, we capitalize on network flow algorithms. Specifically, we use the bipartite matching algorithm to compute a correspondence between the elements of the input set X and the elements of each hidden set Y i . The bipartite matching problem is one of the most well-studied problems in combinatorial optimization. The input of the problem is a weighted bipartite graph G = (V, E). The set of nodes V of a bipartite graph can be decomposed into two disjoint sets V 1 and V 2 , i. e., V = V 1 ∪ V 2 . Every edge e ∈ E connects a vertex in V 1 to one in V 2 . A matching M is a subset of edges such that each node in V appears in at most one edge in M . The optimal solution to the problem can be interpreted as the similarity between the two node sets V 1 and V 2 . A bipartite graph has a natural representation as a rectangular |V 1 | × |V 2 | matrix where the ij th component is equal to the weight of the edge between the i th element of V 1 and the j th element of V 2 if that edge exists, otherwise equal to 0. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a weighted bipartite graph along with the optimal matching M . The weight of each edge (not shown in the <ref type="figure">Figure)</ref> is equal to the inner product of the representations of its endpoints. Green edges belong to the optimal matching M . In our setting, the input set X corresponds to set V 1 , the hidden set Y i corresponds to set V 2 , and the bipartite graph is complete, i. e., every element of V 1 is connected to all the elements of V 2 . The weight of each edge is the result of a differentiable function f on the representations of the edge's two endpoints. Formally, given an input set of vectors, X = {v 1 , v 2 , . . . , v |X| } and a hidden set Y = {u 1 , u 2 , . . . , u |Y | }, we can obtain the maximum matching between the elements of the two sets by solving the following linear program:</p><formula xml:id="formula_2">max |X| i=1 |Y | j=1 z ij f (v i , u j ) subject to: |X| i=1 z ij ≤ 1 ∀j ∈ {1, . . . , |Y |} |Y | j=1 z ij ≤ 1 ∀i ∈ {1, . . . , |X|} z ij ≥ 0 ∀i ∈ {1, . . . , |X|}, ∀j ∈ {1, . . . , |Y |}<label>(1)</label></formula><p>where |X|, |Y | are the cardinalities of X and Y , f (v i , u j ) is, as mentioned above, a differentiable function, and z ij = 1 if component i of X is assigned to component j of Y i and 0 otherwise. In our experiments, we have defined f (v i , u j ) as the inner product between the two vectors v i , and u j followed by</p><formula xml:id="formula_3">. . . . . . ReLU ReLU ReLU ReLU ( (<label>( ( ) ) ) )</label></formula><p>. . . . . .</p><formula xml:id="formula_4">H (1) H (2) H (3) H (m)</formula><p>Bipartite Matching <ref type="bibr">(BM)</ref> . . . the ReLU activation function.</p><formula xml:id="formula_5">Hence, f (v i , u j ) = ReLU(v i u j ).</formula><p>Given an input set X and the m hidden sets Y 1 , Y 2 , . . . , Y m , we formulate m different bipartite matching problems, and by solving all m of them, we end up with an m-dimensional vector x which corresponds to the hidden representation of set X. This m-dimensional vector can be used as features for different machine learning tasks such as set regression or set classification. For instance, in the case of a set classification problem with |C| classes, the output is computed as follows:</p><formula xml:id="formula_6">p = softmax(W x + b)</formula><p>where W ∈ R |C|×m is a matrix of trainable parameters and b ∈ R |C| is the bias term. Given a training set consisting of sets X 1 , X 2 , . . . , X N , we use the negative log likelihood of the correct labels as training loss:</p><formula xml:id="formula_7">L = − N i=1 |C| j=1 y i j log p i j</formula><p>where y i j is equal to 1 if X i belongs to the j th class, and 0 otherwise. Note that we can create a deeper architecture by adding more fully-connected layers. An overview of the proposed architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>As mentioned above, a constraint that the neural network is necessary to satisfy is to be invariant under any permutation of the elements of the input set. Our next theoretical result shows that the proposed model generates the same output for all n! permutations of the elements of an input set X.</p><p>Theorem 1. Let X be a set having elements from a countable or uncountable universe. The proposed architecture is invariant to the permutation of elements in X.</p><p>Computing the derivative with respect to the hidden sets. The objective function of the proposed architecture is differentiable, and we can find a minimum by using classical stochastic optimization techniques that have proven very successful for training deep neural networks. We next use the chain rule and show how the gradients are computed. Note that here we assume a multiclass classification setting. The gradients for different settings (e. g., regression) are computed in a similar fashion.</p><p>Let r = Wx + b. We use the chain rule in order to compute the derivative with respect to the weight matrix H (k) of hidden set Y k :</p><formula xml:id="formula_8">∂L ∂H (k) = ∂L ∂r ∂r ∂x k ∂x k ∂H (k)<label>(2)</label></formula><p>where x k is the k th component of vector x. The gradient of the loss function with respect to r is:</p><formula xml:id="formula_9">∂L ∂r = p − y<label>(3)</label></formula><p>We also have that:</p><formula xml:id="formula_10">∂r ∂x k = W k<label>(4)</label></formula><p>where W k denotes the k th column of matrix W. The k th component of vector x is equal to:</p><formula xml:id="formula_11">x k = |X| i=1 |Y k | j=1 D (k) ij f (v i , u (k) j ) = Tr(D (k) F (k) )</formula><p>where D (k) is the matrix that contains the optimal values of the variables of problem (1) for Y k , i. e., D</p><formula xml:id="formula_12">(k) ij = z (k) ij , u (k) j is the j th column of matrix H (k) , and F (k) ∈ R |X|×|Y k | is a matrix such that F (k) ij = f (v i , u j ).</formula><p>Let V be a matrix whose rows correspond to the elements of set X. Then, it holds that F (k) = ReLU(VH (k) ). This yields: <ref type="formula" target="#formula_10">(4)</ref> and <ref type="formula">(5)</ref>, we finally have:</p><formula xml:id="formula_13">∂x k ∂H (k) = ∂ ∂H (k) Tr D (k) ReLU(VH (k) ) = V D (k) (5) From Equations (2), (3),</formula><formula xml:id="formula_14">∂L ∂H (k) = ∂L ∂r ∂r ∂x k ∂x k ∂H (k) = (p − y) W k (V D (k) )</formula><p>Relaxed problem. The major weakness of the above architecture is its computational complexity. Computing a maximum cardinality matching in a weighted bipartite graph with n vertices and m edges takes time O(mn + n 2 log n), using the classical Hungarian algorithm. This prohibits the proposed model from being applied to very large datasets. To account for that, we next present ApproxRepSet, an approximation of the bipartite matching problem which involves operations that can be performed on a GPU, allowing thus efficient implementations. More specifically, given an input set of vectors, X = {v 1 , v 2 , . . . , v |X| } and a hidden set Y = {u 1 , u 2 , . . . , u |Y | }, first we identify which of the two sets has the highest cardinality. If |X| ≥ |Y |, we solve the following linear program:</p><formula xml:id="formula_15">max |X| i=1 |Y | j=1 z ij f (v i , u j ) subject to: |X| i=1 z ij ≤ 1 ∀j ∈ {1, . . . , |Y |} z ij ≥ 0 ∀i ∈ {1, . . . , |X|}, ∀j ∈ {1, . . . , |Y |} (6)</formula><p>Conversely, if |X| &lt; |Y |, then we replace the first constraint with the following:</p><formula xml:id="formula_16">|Y | j=1 z ij ≤ 1 ∀i ∈ {1, . . . , |X|}.</formula><p>This problem is clearly a relaxed formulation of the problem defined in Equation <ref type="formula" target="#formula_2">(1)</ref> where a constraint has been removed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>We next evaluate the proposed model in text categorization and graph classification, and compare it against strong baselines. Further experimental results are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Categorization</head><p>We first evaluate RepSet and ApproxRepSet in the task of text categorization. Given a document, the input to the model is the set of embeddings of its terms.</p><p>Baselines. We compare the proposed architecture against Word Mover's Distance (WMD) <ref type="bibr" target="#b10">(Kusner et al., 2015)</ref> and its supervised version (S-WMD) <ref type="bibr" target="#b5">(Huang et al., 2016)</ref>, against DeepSets <ref type="bibr" target="#b28">(Zaheer et al., 2017)</ref> and three variants of it, where we replaced the sum operator with mean (NN-mean) and max (NN-max) operators, and with an attention mechanism (NNattention), and against Set-Transformer <ref type="bibr" target="#b12">(Lee et al., 2019)</ref>.</p><p>Data and setup. We evaluate all approaches on 8 document classification datasets: (1) BBCSPORT, (2) TWITTER, (3) RECIPE, (4) OHSUMED, (5) CLAS-SIC, (6) REUTERS, (7) AMAZON, (8) 20NG. More details about the datasets are given in the supplementary material.</p><p>For the proposed models, we use a two-layer architecture: a permutation invariant layer followed by a fully-connected layer. Code is available at: https: //github.com/giannisnik/repset. We choose the number of hidden sets from {20, 30, 50, 100} and their cardinality from {10, 20, 50} based on validation experiments. For DeepSets and its three variants, we use 3 fully-connected layers of sizes {300, 100, 30} with tanh activations, followed by the aggregation operator, and then by 2 fully-connected layers of sizes {30, 10} with tanh activations. For Set-Transformer, we use a stack of set attention blocks in encoder and a pooling by multihead attention module followed by a stack of set attention blocks in decoder. The dimensionality of all hidden layers is set to 64 and the number of attention heads to 4. For WMD and S-WMD, we provide the results reported in the original papers.</p><p>Results. <ref type="table" target="#tab_0">Table 1</ref> shows the average classification error of the proposed models and those of the baselines. On all datasets except two (OHSUMED, CLASSIC), the proposed model outperforms the baselines. In some cases, the gains in accuracy over the best performing competitors are considerable. For instance, on the 20NG, TWITTER, and RECIPE datasets, RepSet achieves respective absolute improvements of 3.82%, 2.08% and 0.63% in accuracy over the best competitor, the S-WMD method. Furthermore, the proposed model outperforms DeepSets and its variants, and Set-Transformer on all datasets, and on most of them by wide margins. Overall, it is clear from <ref type="table" target="#tab_0">Table 1</ref> that RepSet is superior to the other methods in text categorization. Regarding the two variants of the proposed architecture, the model that solves exactly the bipartite matching problem (RepSet) outperforms the  We should mention at this point that besides effective, the proposed model is also highly interpretable. For instance, in the text categorization setting, the elements of each hidden set can be regarded as the terms of hidden documents which are likely to be related to the topics of the different classes. To experimentally verify that, we trained a model with 50 hidden sets on the BBCSPORT dataset. Each hidden set consisted of 20 vectors. For 5 hidden sets, we found the terms that are closer to their elements and we also computed their centroids. <ref type="table" target="#tab_1">Table 2</ref> shows the terms of the pre-trained model that were found to be most similar (using cosine similarity), with respect to the vectors and centroids. Clearly, the centroids of these 5 hidden sets are close to terms that are related to sports. Interestingly, some of these terms correspond to cricket positions, while others to names of famous soccer teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Classification</head><p>We also evaluate the proposed architecture in the graph classification task. We represent each graph as a set of vectors (i. e., the embeddings of its nodes), and pass them on to the proposed models.</p><p>Baselines. We compare the proposed models against several recent state-of-the-art approaches: (1) PSCN, a model that extracts neighborhood subgraphs of specific size, defines an ordering on the nodes of these subgraphs and feeds the emerging adjacency matrices into a convolutional neural network <ref type="bibr" target="#b15">(Niepert et al., 2016)</ref>, (2) Deep GR, an approach that improves the graphlet kernel by using the Skip-gram model (Yanardag and <ref type="bibr" target="#b26">Vishwanathan, 2015)</ref>, (3) EMD, a method that represents each graph as a set of vectors and computes the distance between each pair of graphs using the Earth Mover's Distance algorithm <ref type="bibr" target="#b16">(Nikolentzos et al., 2017)</ref>, (4) DGCNN, a model that applies a message passing architecture followed by a pooling operator based on sorting to create a fixed-sized graph representation which is then passed on to a convolutional architecture <ref type="bibr" target="#b29">(Zhang et al., 2018a)</ref>, (5) SAEN, an algorithm that decomposes graphs in a hierarchical fashion, then uses shift, aggregate and extract operations, and finally applies a standard neural network to the emerging representations <ref type="bibr" target="#b17">(Orsini et al., 2018)</ref>, (6) RetGK, a graph kernel that capitalizes on the isomorphism-invariance property of the return probabilities of random walks <ref type="bibr" target="#b30">(Zhang et al., 2018b)</ref>, and (7) DiffPool, a message passing architecture which applies at each layer a differentiable graph pooling module that clusters the nodes of the previous layer <ref type="bibr" target="#b27">(Ying et al., 2018)</ref>. We also compare the proposed models against DeepSets <ref type="bibr" target="#b28">(Zaheer et al., 2017)</ref> and the three variants of it that were presented above (NN-mean, NN-max, and NN-attention), and against Set-Transformer (Lee et al., 2019). Data and setup. We evaluate the competing methods on standard graph classification datasets derived from bioinformatics (MUTAG, PROTEINS) and social networks (IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY) <ref type="bibr" target="#b8">(Kersting et al., 2016)</ref>. Note that the social network graphs are unlabeled, while the bioinformatics graphs contain node labels. However, in our experiments, we did not take these node labels into account. More details about the datasets are given in the supplementary material. Since the datasets do not come with standard training/test splits, we performed 10-fold cross validation procedure where we randomly sampled 10% of each training fold to serve as a validation set. Furthermore, since some datasets are small, we repeated the whole experiment 10 times.</p><p>We generated embeddings for the nodes as follows: we create a single graph from each dataset by computing the disjoint union of all the graphs contained in it. In other words, for each dataset, we generate a disconnected graph whose components correspond to the graphs contained in the dataset. We then employ struc2vec, an algorithm that learns structural node representations <ref type="bibr" target="#b22">(Ribeiro et al., 2017)</ref>. Nodes with structurally similar neighborhoods are close to each other in the embeddings space. We set the dimensionality of the learned embeddings to 20. For RepSet, ApproxRepSet, DeepSets and its variants, and Set-Transformer, we use the same configuration as in the case of text categorization. Note that all the above models are trained on the same input data (i. e., sets of node embeddings). For the remaining baseline methods, we provide the results reported in the original papers.</p><p>Results. <ref type="table" target="#tab_2">Table 3</ref> shows the average classification accuracy of the proposed models and those of the baselines. We observe that the proposed models are on par with the state-of-the-art algorithms. Specifically, RepSet obtains the highest average performance among all methods on the IMDB-BINARY dataset, while it is the second best method on IMDB-MULTI, and the third best method on REDDIT-BINARY and on MUTAG. DeepSets and its variants achieve comparable accuracies to the proposed models. Specifically, the best-performing variant, NNattention, outperforms the proposed models on PRO-TEINS and REDDIT-BINARY. Set-Transformer, on the other hand, outperforms the proposed models only on IMDB-MULTI. Interestingly, even though the embeddings which the proposed model utilizes do not incorporate information about the node labels, on the MUTAG dataset (whose graphs contain node labels), it remains competitive with the baselines which take these node labels into account. On the other hand, on the second dataset that contains node labels (PRO-TEINS), our proposed models, RepSet and Approx-RepSet, are outperformed by all the baselines except DeepSets, NN-max and Set-Transformer. Last, our approximate model, ApproxRepSet, achieves comparable to state-of-the-art results in almost all datasets, while being considerably faster than RepSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Runtime Analysis</head><p>To evaluate the runtime performance and scalability of the proposed models, we created a series of synthetic datasets and measured how the average running time per epoch varies with respect to the parameters of the model. <ref type="figure">Figure 3</ref>  was performed on an NVIDIA Titan Xp GPU. As expected, we observe that RepSet is more computationally expensive than ApproxRepSet, while its running time increases significantly as the size and the number of hidden sets increase. On the other hand, the values of these parameters do not have a large impact on the running time of ApproxRepSet. We also evaluate <ref type="figure">(Figure 3 (right)</ref>) how the proposed models scale as the dimensionality of the vectors contained in the sets increases and compare them against the DeepSets model. DeepSets is the fastest model, followed by Ap-proxRepSet. The running times of both these models are much smaller than that of RepSet, while they also grow very slowly as the dimensionality of vectors increases. Conversely, the running time of RepSet increases notably especially for dimensionalities larger than 100. We also examine in <ref type="figure">Figure 4</ref> how the running time of the three models varies with respect to the number of input samples N (left) and to their cardinality |X i |, i = 1, . . . , <ref type="bibr">N (right)</ref>. Surprisingly, we find that the running time of RepSet grows slowly as the number of samples increases. On the other hand, it in-creases significantly as the cardinality of these samples increases. The running times of DeepSets and Approx-RepSet are again much lower than that of RepSet, and grow only slightly as the number of samples and their cardinality increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed RepSet, a neural network for learning set representations. RepSet computes the correspondences between the input sets and some hidden sets by solving a series of matching/network flow problems. We also introduced a relaxed version which involves fast matrix operations and scales to large datasets. Experiments in two tasks show that our architecture is competitive with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview</head><p>This document is supplementary material for the paper "Rep the Set: Neural Networks for Learning Set Representations". It is organized as follows. We will prove in Section B the Theorem 1. In Section C, we will present the proof of Proposition 1. In Section D, we will give details about the datasets we used in our experiments. Finally, in Section E, we perform a sensitivity analysis, and we present the features that the model learns on a simple synthetic dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1</head><p>For the reader's convenience we will restate Theorem 1 from the article.</p><p>Theorem 2. Let X be a set having elements from a countable or uncountable universe. The proposed architecture is invariant to the permutation of elements in X.</p><p>Proof. Let Π n be the set of all permutations of the integers from 1 to n. Let π ∈ Π |X| be an arbitrary permutation. We will apply π to the input set X. The bipartite matching problem then becomes:</p><formula xml:id="formula_17">max |X| i=1 |Y | j=1 z ij f (v π(i) , u j ) subject to: |X| i=1 z ij ≤ 1 ∀j ∈ {1, . . . , |Y |} |Y | j=1 z ij ≤ 1 ∀i ∈ {1, . . . , |X|} z ij ≥ 0 ∀i ∈ {1, . . . , |X|}, ∀j ∈ {1, . . . , |Y |} (7)</formula><p>The constraints of the optimization problem remain intact since summing the elements of a set is a permutation invariant function. Moreover, it holds that:</p><formula xml:id="formula_18">|X| i=1 |Y | j=1 z ij f (v π(i) , u j ) = |X| i=1 |Y | j=1 z π −1 (i)j f (v i , u j ) (8)</formula><p>The sets of variables that lead to the optimal solutions of problems (1) in the main paper and (7) above are identical, i. e., z * π −1 (i)j = z * ij , ∀i ∈ {1, . . . , |X|}, ∀j ∈ {1, . . . , |Y |}. Hence, the optimal value of the bipartite matching problem is the same for all n! permutations of the input, and therefore, the proposed model maps all permutations of the input into the same representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Proposition 1</head><p>For the reader's convenience we will restate Proposition 1 from the article. Proof. We assume without loss of generality that |X| ≥ |Y |. Let D * be the optimal solution to the relaxed problem, i. e., D * ij = z ij . Therefore, it holds that:</p><formula xml:id="formula_19">D * ij = 1 if i = arg max k f (v k , u j ) ∧ max k f (v k , u j ) &gt; 0 0 otherwise</formula><p>The relaxed problem is allowed to match multiple elements of Y with the same element of X. Specifically, the optimal solution of the relaxed problem matches an element of Y with an element of X if their inner product is positive and is the highest among the inner products between that element of Y and all the elements of X. Then, for every j, let i * = arg max k f (v k , u j ). For any feasible solution D of the exact problem, and for any j, we have:</p><formula xml:id="formula_20">|X| i=1 D ij f (v i , u j ) ≤ |X| i=1 D ij f (v i * , u j ) = f (v i * , u j ) |X| i=1 D ij ≤ f (v i * , u j ) = |X| i=1 D * ij f (v i , u j )<label>(9)</label></formula><p>Therefore, the objective value of the relaxed problem (obtained by D * ) gives an upper to the exact problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Text Categorization Datasets</head><p>We evaluated all approaches on 8 supervised document datasets: (1) BBCSPORT: BBC sports articles between 2004-2005, (2) TWITTER: a set of tweets labeled with sentiments positive, negative, or neutral (the set is reduced due to the unavailability of some tweets), (3) RECIPE: a set of recipe procedure descriptions labeled by their region of origin, (4) OHSUMED: a collection of medical abstracts categorized by different cardiovascular disease groups (for computational efficiency we subsample the dataset, using the first 10 classes), (5) CLASSIC: sets of sentences from academic  <ref type="formula" target="#formula_8">(2007)</ref>). We preprocess all datasets by removing all words in the SMART stop word list <ref type="bibr" target="#b24">(Salton and Buckley, 1971)</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows statistics of the 8 datasets that were used for the evaluation. We obtained a distributed representation for each word from a publicly available set of pre-trained vectors 1 . For datasets that do not come with a predefined train/test split, we report the average accuracy over five 70/30 train/test splits as well as the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Graph Classification Datasets</head><p>We evaluated the proposed architecture on the following 5 datasets: (1) MUTAG, (2) PROTEINS, (3) IMDB-BINARY, (4) IMDB-MULTI, and <ref type="formula">(5)</ref> REDDIT-BINARY.</p><p>MUTAG contains mutagenic aromatic and heteroaromatic nitro compounds. Each chemical compound is labeled according to whether or not it has mutagenic effect on the Gram-negative bacterium Salmonella typhimurium <ref type="bibr" target="#b2">(Debnath et al., 1991)</ref>. PROTEINS consists of proteins represented as graphs where vertices are secondary structure elements and there is an edge between two vertices if they are neighbors in the amino-acid sequence or in 3D space. The task is to classify proteins into enzymes and non-enzymes <ref type="bibr" target="#b0">(Borgwardt et al., 2005)</ref>. IMDB-BINARY and IMDB-MULTI contain movie collaboration graphs. The vertices of each graph represent actors/actresses and two vertices are connected by an edge if the corresponding actors/actresses appear in the same movie. Each  graph is the ego-network of an actor/actress, and the task is to predict which genre an ego-network belongs to (Yanardag and <ref type="bibr" target="#b26">Vishwanathan, 2015)</ref>. REDDIT-BINARY consists of online discussion threads represented as graphs. Each vertex corresponds to a user, and two users are connected by an edge if one of them responded to at least one of the other's comments. The task is to classify graphs into either communities (Yanardag and <ref type="bibr" target="#b26">Vishwanathan, 2015)</ref>. A summary of the datasets is given in <ref type="table" target="#tab_5">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Sensitivity Analysis</head><p>The proposed RepSet and ApproxRepSet models involve two main parameters: (1) the number of hidden sets m, and <ref type="formula" target="#formula_8">(2)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Synthetic Data</head><p>We first demonstrate the proposed RepSet architecture on a very simple dataset. The dataset consists of 4 sets of 2-dimensional vectors. The cardinality of all sets is equal to 2. The elements of the 4 sets are illustrated in <ref type="figure" target="#fig_6">Figure 7</ref>. Although seemingly simple, this dataset may prove challenging for several algorithms that apply aggregation mechanisms to the elements of the sets since all 4 sets have identical centroids while the sum of their elements is also the same for all of them. To learn to classify these sets, we used an instance of the proposed model consisting of 2 hidden sets of cardinality equal to 2. The model managed easily to discriminate between the 4 sets and to achieve perfect accuracy. A question that arises at this point is what kind of features the hidden sets of the model learn during training. Hence, besides the input sets, <ref type="figure" target="#fig_6">Figure 7</ref> also shows the vectors of the 2 hidden sets. The hidden sets learned very similar patterns, which indicates that less than 2 hidden sets may be required.</p><p>In fact, we observed that even with one hidden set, the model can achieve perfect performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of a bipartite graph generated from 2 sets of 3-dimensional vectors, and of its maximum matching M . Green color indicates an edge belongs to M . The weight of matching M is equal to 16.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the proposed model for learning representations of sets. Each element of the input set is compared with the elements of all "hidden sets", and the emerging matrices serve as the input to the bipartite matching algorithm. The objective values of the matching problems correspond to the representation of the input set and are passed on to a standard neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Proposition 1. The optimization problem defined in Equation 6 is an upper bound to the bipartite matching problem defined in Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>illustrates the running time of RepSet and ApproxRepSet as a function of the size of the hidden sets |Y i |, i = 1, . . . , m (top left) and as a function of the number of hidden sets m (bottom left). Note that for RepSet, training was performed on an Intel Xeon E5−1607 CPU (4 threads), while for ApproxRepSet, it 40 Runtimes with respect to the number of hidden sets m, the size of the hidden sets |Y i | (left) and embeddings with different dimensions (right). Runtimes with respect to the number of input sets N (left) and the size of the input sets |X i | (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Proposition 2 .</head><label>2</label><figDesc>The optimization problem defined in Equation 6 is an upper bound to the bipartite matching problem defined in Equation 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Average test error of the RepSet model with respect to the number of hidden sets m (left) and the size of the hidden sets |Y i | (right) on the TWITTER dataset. Average test error of the RepSet model with respect to the number of hidden sets m (left) and the size of the hidden sets |Y i | (right) on the RECIPE dataset.of m lead to higher test error than larger values of m. For most values of |Y i |, values of m between 20 and 50 result in the lowest test error. As regards the size of the hidden sets |Y i |, there is no consistency in the obtained results. Specifically, for small values of m (m ≤ 20), large cardinalities of the hidden sets result in better performace, while for large values of m (m ≥ 50), small cardinalities lead to smaller error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>A very simple dataset consisting of 4 examples (i. e., sets). Each set contains a pair of 2dimensional vectors (circles). The diamonds indicate the "hidden sets" that the proposed model learned during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification test error of the proposed architecture and the baselines on 8 text categorization datasets.</figDesc><table><row><cell></cell><cell>BBCSPORT</cell><cell>TWITTER</cell><cell>RECIPE</cell><cell>OHSUMED</cell><cell>CLASSIC</cell><cell>REUTERS</cell><cell>AMAZON</cell><cell>20NG</cell></row><row><cell>WMD</cell><cell>4.60 ± 0.70</cell><cell>28.70 ± 0.60</cell><cell>42.60 ± 0.30</cell><cell>44.50</cell><cell>2.88 ± 0.10</cell><cell>3.50</cell><cell>7.40 ± 0.30</cell><cell>26.80</cell></row><row><cell>S-WMD</cell><cell>2.10 ± 0.50</cell><cell>27.50 ± 0.50</cell><cell>39.20 ± 0.30</cell><cell>34.30</cell><cell>3.20 ± 0.20</cell><cell>3.20</cell><cell>5.80 ± 0.10</cell><cell>26.80</cell></row><row><cell>DeepSets</cell><cell cols="2">25.45 ± 20.1 29.66 ± 1.62</cell><cell>70.25 ± 0.00</cell><cell>71.53</cell><cell>5.95 ± 1.50</cell><cell>10.00</cell><cell>8.58 ± 0.67</cell><cell>38.88</cell></row><row><cell>NN-mean</cell><cell cols="2">10.09 ± 2.62 31.56 ± 1.53</cell><cell>64.30 ± 7.30</cell><cell>45.37</cell><cell>5.35 ± 0.75</cell><cell>11.37</cell><cell cols="2">13.66 ± 3.16 38.40</cell></row><row><cell>NN-max</cell><cell>2.18 ± 1.75</cell><cell>30.27 ± 1.26</cell><cell>43.47 ± 1.05</cell><cell>35.88</cell><cell>4.21 ± 0.11</cell><cell>4.33</cell><cell>7.55 ± 0.63</cell><cell>32.15</cell></row><row><cell>NN-attention</cell><cell>4.72 ± 0.97</cell><cell>29.09 ± 0.62</cell><cell>43.18 ± 1.22</cell><cell>31.36</cell><cell>4.42 ± 0.73</cell><cell>3.97</cell><cell>6.92 ± 0.51</cell><cell>28.73</cell></row><row><cell cols="2">Set-Transformer 4.18 ± 1.23</cell><cell>27.79 ± 0.47</cell><cell>42.54 ± 1.35</cell><cell>35.68</cell><cell>5.23 ± 0.52</cell><cell>4.52</cell><cell>7.18 ± 0.44</cell><cell>30.01</cell></row><row><cell>RepSet</cell><cell cols="3">2.00 ± 0.89 25.42 ± 1.10 38.57 ± 0.83</cell><cell>33.88</cell><cell>3.38 ± 0.50</cell><cell>3.15</cell><cell cols="2">5.29 ± 0.28 22.98</cell></row><row><cell>ApproxRepSet</cell><cell>4.27 ± 1.73</cell><cell>27.40 ± 1.95</cell><cell>40.94 ± 0.40</cell><cell>35.94</cell><cell>3.76 ± 0.45</cell><cell>2.83</cell><cell>5.69 ± 0.40</cell><cell>23.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Terms of the employed pre-trained model that are most similar to the elements and centroids of 5 hidden sets.</figDesc><table><row><cell>Hidden</cell><cell>Terms similar to</cell><cell>Terms similar to</cell></row><row><cell>set</cell><cell>elements of hidden sets</cell><cell>centroids of hidden sets</cell></row><row><cell>1</cell><cell>chelsea, football, striker, club, champions</cell><cell>footballing</cell></row><row><cell>2</cell><cell>qualify, madrid, arsenal, striker, united, france</cell><cell>ARSENAL Wenger</cell></row><row><cell>3</cell><cell>olympic, athlete, olympics, sport, pentathlon</cell><cell>Olympic Medalist</cell></row><row><cell>4</cell><cell>penalty, cup, rugby, coach, goal</cell><cell>rugby</cell></row><row><cell>5</cell><cell>match, playing, batsman, batting, striker</cell><cell>batsman</cell></row></table><note>model that approximates it (ApproxRepSet) on all datasets except from REUTERS. However, on most datasets the difference in performance is not large. Hence, although less powerful, ApproxRepSet is still capable of learning expressive representations of sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy (± standard deviation) of the proposed architecture and the baselines on the 5 graph classification datasets.</figDesc><table><row><cell></cell><cell>MUTAG</cell><cell>PROTEINS</cell><cell>IMDB BINARY</cell><cell>IMDB MULTI</cell><cell>REDDIT BINARY</cell></row><row><cell>PSCN k = 10</cell><cell cols="3">88.95 (± 4.37) 75.00 (± 2.51) 71.00 (± 2.29)</cell><cell>45.23 (± 2.84)</cell><cell>86.30 (± 1.58)</cell></row><row><cell>Deep GR</cell><cell cols="3">82.66 (± 1.45) 71.68 (± 0.50) 66.96 (± 0.56)</cell><cell>44.55 (± 0.52)</cell><cell>78.04 (± 0.39)</cell></row><row><cell>EMD</cell><cell>86.11 (± 0.84)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell cols="3">85.80 (± 1.70) 75.50 (± 0.90) 70.03 (± 0.86)</cell><cell>47.83 (± 0.85)</cell><cell>-</cell></row><row><cell>SAEN</cell><cell cols="3">84.99 (± 1.82) 75.31 (± 0.70) 71.59 (± 1.20)</cell><cell>48.53 (± 0.76)</cell><cell>87.22 (± 0.80)</cell></row><row><cell>RetGK</cell><cell cols="3">90.30 (± 1.10) 76.20 (± 0.50) 72.30 (± 0.60)</cell><cell cols="2">48.70 (± 0.60) 92.60 (± 0.30)</cell></row><row><cell>DiffPool</cell><cell>-</cell><cell>76.25</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeepSets</cell><cell cols="3">86.26 (± 1.09) 60.82 (± 0.79) 69.84 (± 0.64)</cell><cell>47.62 (± 1.18)</cell><cell>52.01 (± 1.47)</cell></row><row><cell>NN-mean</cell><cell cols="3">87.55 (± 0.98) 73.00 (± 1.21) 71.48 (± 0.48)</cell><cell>49.92 (± 0.82)</cell><cell>84.57 (± 0.84)</cell></row><row><cell>NN-max</cell><cell cols="3">85.84 (± 0.99) 71.05 (± 0.54) 69.56 (± 0.91)</cell><cell>48.28 (± 0.43)</cell><cell>80.98 (± 0.79)</cell></row><row><cell>NN-attention</cell><cell cols="4">85.92 (± 1.16) 74.48 (± 0.22) 72.40 (± 0.45) 49.56 (± 0.47)</cell><cell>88.74 (± 0.53)</cell></row><row><cell cols="6">Set-Transformer 87.71 (± 1.14) 59.62 (± 1.42) 71.21 (± 1.28) 50.25 (± 0.74) 83.79 (±) 0.83</cell></row><row><cell>RepSet</cell><cell cols="4">88.63 (± 0.86) 73.04 (± 0.42) 72.40 (± 0.73) 49.93 (± 0.60)</cell><cell>87.45 (± 0.86)</cell></row><row><cell>ApproxRepSet</cell><cell cols="3">86.33 (± 1.48) 70.74 (± 0.85) 71.46 (± 0.91)</cell><cell>48.92 (± 0.28)</cell><cell>80.30 (± 0.56)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Datasets used in text categorization experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Unique</cell><cell></cell></row><row><cell>Dataset</cell><cell>n</cell><cell>Voc</cell><cell cols="2">Words(avg) y</cell></row><row><cell>BBCSPORT</cell><cell>517</cell><cell>13243</cell><cell>117</cell><cell>5</cell></row><row><cell>TWITTER</cell><cell>2176</cell><cell>6344</cell><cell>9.9</cell><cell>3</cell></row><row><cell>RECIPE</cell><cell>3059</cell><cell>5708</cell><cell>48.5</cell><cell>15</cell></row><row><cell>OHSUMED</cell><cell cols="2">3999 31789</cell><cell>59.2</cell><cell>10</cell></row><row><cell>CLASSIC</cell><cell cols="2">4965 24277</cell><cell>38.6</cell><cell>4</cell></row><row><cell>REUTERS</cell><cell cols="2">5485 22425</cell><cell>37.1</cell><cell>8</cell></row><row><cell>AMAZON</cell><cell cols="2">5600 42063</cell><cell>45.0</cell><cell>4</cell></row><row><cell>20NG</cell><cell cols="2">11293 29671</cell><cell>72</cell><cell>20</cell></row><row><cell cols="5">papers, labeled by publisher name, (6) REUTERS: a</cell></row><row><cell cols="5">classic news dataset labeled by news topics (we use the</cell></row><row><cell cols="5">8-class version with train/test split as described in Ca-</cell></row><row><cell cols="5">chopo (2007), (7) AMAZON: a set of Amazon reviews</cell></row><row><cell cols="5">which are labeled by category product in books, dvd,</cell></row><row><cell cols="5">electronics, kitchen (as opposed to by sentiment), (8)</cell></row><row><cell cols="5">20NG: news articles classified into 20 different cate-</cell></row><row><cell cols="5">gories (we use the bydate train/test split by Cachopo</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Datasets used in graph classification.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Graphs y Nodes(avg) Edges(avg)</cell></row><row><cell>MUTAG</cell><cell>188</cell><cell>2</cell><cell>17.93</cell><cell>19.79</cell></row><row><cell>PROTEINS</cell><cell>1113</cell><cell>2</cell><cell>39.06</cell><cell>72.82</cell></row><row><cell>IMDB BINARY</cell><cell>1000</cell><cell>2</cell><cell>19.77</cell><cell>96.53</cell></row><row><cell>IMDB MULTI</cell><cell>1500</cell><cell>3</cell><cell>13.00</cell><cell>65.94</cell></row><row><cell>REDDIT BINARY</cell><cell>2000</cell><cell>2</cell><cell>429.63</cell><cell>497.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>the cardinalities of the hidden sets |Y i |, i = 1, . . . , m. We next investigate how these two parameters influence the performance of the RepSet model. Specifically, in Figures 5 and 6, we examine how the different choices of these parameters affect the performance of RepSet on the TWITTER and RECIPE datasets, respectively. We measure the test error as a function of the two parameters. Note that each hidden set Y i can have a different cardinality compared to the other sets. However, we set the cardinalities of all hidden sets to the same value. We observe that on TWITTER, the number of hidden sets m does not have a large impact on the performance, especially for small cardinalities of the hidden sets (|Y i | ≤ 50). For most cardinalities, the test error is within 1% to 3% when varying this parameter. Furthermore, in most cases, the best performance is attained when the number of hidden sets is small (m ≤ 20). Similar behavior is also observed for the second parameter on the TWITTER dataset. For most values of m, the test error changes only slightly when varying the cardinalities of the hidden sets. For m ≥ 50, the model produces best results when the cardinalities of the hidden sets |Y i | are close to 20. On the other hand, for small values of m, the model yields good performance even when the cardinalities of the hidden sets |Y i | are large. On the RECIPE dataset, both parameters have a higher impact on the performance of the RepSet model. In general, small values</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the AISTATS reviewers for their insightful comments. GN was supported by the project "ESIGMA" (ANR-17-CE40-0028).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Improving methods for single-label text categorization. Instituto Superior Técnico</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M D J C</forename><surname>Cachopo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Portugal</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximate correspondences in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Efficient learning with sets of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="725" to="760" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Supervised word mover&apos;s distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4862" to="4870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A kernel between sets of vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep learning. nature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A kernel between unordered sets of data: The gaussian mixture approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="255" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shift Aggregate Extract Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepsetnet: Predicting sets with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Conference on Computer Vision</title>
		<meeting>the 2017 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5257" to="5266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint learning of set cardinality and state distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The smart information retrieval system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
	<note>Rep the Set: Neural Networks for Learning Set Representations Yanardag</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4801" to="4811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RetGK: Graph Kernels based on Return Probabilities of Random Walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nehorai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3964" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
