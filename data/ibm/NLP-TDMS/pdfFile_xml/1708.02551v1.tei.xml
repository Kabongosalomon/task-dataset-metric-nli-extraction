<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Instance Segmentation with a Discriminative Loss Function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luc Van Gool ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Luc Van Gool ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Instance Segmentation with a Discriminative Loss Function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an offthe-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform onpar with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic instance segmentation has recently gained in popularity. As an extension of regular semantic segmentation, the task is to generate a binary segmentation mask for each individual object along with a semantic label. It is considered a fundamentally harder problem than semantic segmentation -where overlapping objects of the same class are segmented as one -and is closely related to the tasks of object counting and object detection. One could also say instance segmentation is a generalization of object detection, with the goal of producing a segmentation mask rather than a bounding box for each object. Pinheiro et al. <ref type="bibr" target="#b27">[28]</ref> obtain bounding boxes from instance segmentations by simply * Authors contributed equally <ref type="figure">Figure 1</ref>. The network maps each pixel to a point in feature space so that pixels belonging to the same instance are close to each other, and can easily be clustered with a fast post-processing step. From top to bottom, left to right: input image, output of the network, pixel embeddings in 2-dimensional feature space, clustered image.</p><p>drawing the tightest bounding box around each segmentation mask, and show that their system reaches state-of-theart performance on an object detection benchmark.</p><p>The relation between instance segmentation and semantic segmentation is less clear. Intuitively, the two tasks feel very closely related, but it turns out not to be obvious how to apply the network architectures and loss functions that are successful in semantic segmentation to this related instance task. One key factor that complicates the naive application of the popular softmax cross-entropy loss function to instance segmentation, is the fact that an image can contain an arbitrary number of instances and that the labeling is permutation-invariant: it does not matter which specific la-bel an instance gets, as long as it is different from all other instance labels. One possible solution is to set an upper limit to the number of detectable instances and to impose extra constraints on the labeling, but this may unnecessarily limit the representational power of the network and introduce unwanted biases, leading to unsatisfying results.</p><p>Most recent works on instance segmentation with deep networks go a different route. Two popular approaches introduce a multi-stage pipeline with object proposals <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b3">4]</ref>, or train a recurrent network end-toend with a custom loss function that outputs instances sequentially <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. Another line of research is to train a network to transform the image into a representation that is clustered into individual instances with a post-processing step <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22]</ref>. Our method belongs to this last category, but takes a more principled (less ad-hoc) approach than previous works and reduces the post-processing step to a minimum.</p><p>Inspired by the success of siamese networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and the triplet loss <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref> in image classification, we introduce a discriminative loss function to replace the pixel-wise softmax loss that is commonly used in semantic segmentation. Our loss function enforces the network to map each pixel in the image to an n-dimensional vector in feature space, such that feature vectors of pixels that belong to the same instance lie close together while feature vectors of pixels that belong to different instances lie far apart. The output of the network can easily be clustered with a fast and simple postprocessing operation. With this mechanism, we optimize an objective that avoids the aforementioned problems related to variable number of instances and permutation-invariance.</p><p>Our work mainly focuses on the loss function, as we aim to be able to re-use network architectures that were designed for semantic segmentation: we plug in an off-theshelf architecture and retrain the system with our discriminative loss function. In our framework, the tasks of semantic and instance segmentation can be treated in a consistent and similar manner and do not require changes on the architecture side.</p><p>The rest of this paper is structured as follows. First we give an extensive overview of the related work in section 2. In section 3 we discuss our proposed method in detail. In section 4 we set up experiments on two instance segmentation benchmarks and show that we get a performance that is competitive with the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the last few years, deep networks have achieved impressive results in semantic and instance segmentation. All top-performing methods across different benchmarks use a deep network in their pipeline. Here we discuss these prior works and situate our model between them.</p><p>Proposal-based Many instance segmentation ap-proaches build a multi-stage pipeline with a separate object proposal and classification step. Hariharan et al. <ref type="bibr" target="#b16">[17]</ref> and Chen et al. <ref type="bibr" target="#b8">[9]</ref> use MCG <ref type="bibr" target="#b2">[3]</ref> to generate categoryindependent region proposals, followed by a classification step. Pinheiro et al. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> use the same general approach, but their work focuses on generating segmentation proposals with a deep network. Dai et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> won the 2015 MS-COCO instance segmentation challenge with a cascade of networks (MNC) to merge bounding boxes, segmentation masks and category information. Many works were inspired by this approach and also combine an object detector with a semantic segmentation network to produce instances <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast to these works, our method does not rely on object proposals or bounding boxes but treats the image holistically, which we show to be beneficial for handling certain tasks with complex occlusions as discussed in section 3.3.</p><p>Recurrent methods Other recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref> employ recurrent networks to generate the individual instances sequentially. Stewart et al. <ref type="bibr" target="#b34">[35]</ref> train a network for endto-end object detection using an LSTM <ref type="bibr" target="#b18">[19]</ref>. Their loss function is permutation-invariant as it incorporates the Hungarian algorithm to match candidate hypotheses to groundtruth instances. Inspired by their work, Romera et al. <ref type="bibr" target="#b30">[31]</ref> propose an end-to-end recurrent network with convolutional LSTMs that sequentially outputs binary segmentation maps for each instance. Ren et al. <ref type="bibr" target="#b29">[30]</ref> improve upon <ref type="bibr" target="#b30">[31]</ref> by adding a box network to confine segmentations within a local window and skip connections instead of graphical models to restore the resolution at the output. Their final framework consists of four major components: an external memory and networks for box proposal, segmentation and scoring. We argue that our proposed method is conceptually simpler and easier to implement than these methods. Our method does not involve recurrent mechanisms and can work with any off-the-shelf segmentation architecture. Moreover, our loss function is permutation-invariant by design, without the need to resort to a Hungarian algorithm.</p><p>Clustering Another approach is to transform the image into a representation that is subsequently clustered into discrete instances. Silberman et al. <ref type="bibr" target="#b33">[34]</ref> produce a segmentation tree and use a coverage loss to cut it into nonoverlapping regions. Zhang et al. <ref type="bibr" target="#b45">[46]</ref> impose an ordering on the individual instances based on their depth, and use a MRF to merge overlapping predicted patches into a coherent segmentation. Two earlier works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b35">36]</ref> also use depth information to segment instances. Uhrig et al. <ref type="bibr" target="#b36">[37]</ref> train a network to predict each pixel's direction towards its instance center, along with monocular depth and semantic labels. They use template matching and proposal fusion techniques to extract the individual instances from this representation. Liang et al. <ref type="bibr" target="#b21">[22]</ref> predict pixel-wise feature vectors representing the ground truth bounding box of the instance it belongs to. With the help of a sub-network that predicts an object count, they cluster the output of the network into individual instances. Our work is similar to these works in that we have a separate clustering step, but our loss does not constrain the output of the network to a specific representation like instance center coordinates or depth ordering; it is less ad-hoc in that sense.</p><p>Other Bai et al. <ref type="bibr" target="#b6">[7]</ref> use deep networks to directly learn the energy of the watershed transform. A drawback of this bottom-up approach is that they cannot handle occlusions where instances are separated into multiple pieces. Kirillov et al. <ref type="bibr" target="#b19">[20]</ref> use a CRF, but with a novel MultiCut formulation to combine semantic segmentations with edge maps to extract instances as connected regions. A shortcoming of this method is that, although they reason globally about instances, they also cannot handle occlusions. Arnab et al. <ref type="bibr" target="#b4">[5]</ref> combine an object detector with a semantic segmentation module using a CRF model. By considering the image holistically it can handle occlusions and produce more precise segmentations.</p><p>Loss function Our loss function is inspired by earlier works on distance metric learning, discriminative loss functions and siamese networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>. Most similar to our loss function, Weinberger et al. <ref type="bibr" target="#b38">[39]</ref> propose to learn a distance metric for large margin nearest neighbor classification. Kostinger et al. <ref type="bibr" target="#b20">[21]</ref> further explore a similar LDA based objective. More recently Schroff et al. <ref type="bibr" target="#b32">[33]</ref>, building on Sun et al. <ref type="bibr" target="#b37">[38]</ref>, introduced the triplet loss for face recognition. The triplet loss enforces a margin between each pair of faces from one person, to all other faces. Xie et al. <ref type="bibr" target="#b40">[41]</ref> propose a clustering objective for unsupervised learning. Whereas these works employ a discriminative loss function to optimize distances between images in a dataset, our method operates at the pixel level, optimizing distances between individual pixels in an image. To our knowledge, we are the first to successfully use a discriminative loss based on distance metric learning principles for the task of instance segmentation with deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Discriminative loss function</head><p>Consider a differentiable function that maps each pixel in an input image to a point in n-dimensional feature space, referred to as the pixel embedding. The intuition behind our loss function is that embeddings with the same label (same instance) should end up close together, while embeddings with a different label (different instance) should end up far apart.</p><p>Weinberger et al. <ref type="bibr" target="#b38">[39]</ref> propose a loss function with two competing terms to achieve this objective: a term to penalize large distances between embeddings with the same <ref type="figure">Figure 2</ref>. The intra-cluster pulling force pulls embeddings towards the cluster center, i.e. the mean embedding of that cluster. The inter-cluster repelling force pushes cluster centers away from each other. Both forces are hinged: they are only active up to a certain distance determined by the margins δv and δ d , denoted by the dotted circles. This diagram is inspired by a similar one in <ref type="bibr" target="#b38">[39]</ref>. label, and a term to penalize small distances between embeddings with a different label.</p><p>In our loss function we keep the first term, but replace the second term with a more tractable one: instead of directly penalizing small distances between every pair of differently-labeled embeddings, we only penalize small distances between the mean embeddings of different labels. If the number of different labels is smaller than the number of inputs, this is computationally much cheaper than calculating the distances between every pair of embeddings. This is a valid assumption for instance segmentation, where there are orders of magnitude fewer instances than pixels in an image.</p><p>We now formulate our discriminative loss in terms of push (i.e. repelling) and pull forces between and within clusters. A cluster is defined as a group of pixel embeddings sharing the same label, e.g. pixels belonging to the same instance. Our loss consists of three terms:</p><p>1. variance term: an intra-cluster pull-force that draws embeddings towards the mean embedding, i.e. the cluster center.</p><p>2. distance term: an inter-cluster push-force that pushes clusters away from each other, increasing the distance between the cluster centers.</p><p>3. regularization term: a small pull-force that draws all clusters towards the origin, to keep the activations bounded.</p><p>The variance and distance terms are hinged: their forces are only active up to a certain distance. Embeddings within a distance of δ v from their cluster centers are no longer attracted to it, which means that they can exist on a local manifold in feature space rather than having to converge to a single point. Analogously, cluster centers further apart than 2δ d are no longer repulsed and can move freely in feature space. Hinging the forces relaxes the constraints on the network, giving it more representational power to achieve its goal. The interacting forces in feature space are illustrated in <ref type="figure">figure 2</ref>.</p><p>The loss function can also be written down exactly. We use the following definitions: C is the number of clusters in the ground truth, N c is the number of elements in cluster c, x i is an embedding, µ c is the mean embedding of cluster c (the cluster center), · is the L1 or L2 distance, and [x] + = max(0, x) denotes the hinge. δ v and δ d are respectively the margins for the variance and distance loss. The loss can then be written as follows:</p><formula xml:id="formula_0">L var = 1 C C c=1 1 N c Nc i=1 [ µ c − x i − δ v ] 2 + (1) L dist = 1 C(C − 1) C c A =1 C c B =1 c A =c B [2δ d − µ c A − µ c B ] 2 + (2) L reg = 1 C C c=1 µ c (3) L = α · L var + β · L dist + γ · L reg<label>(4)</label></formula><p>In our experiments we set α = β = 1 and γ = 0.001. The loss is minimized by stochastic gradient descent.</p><p>Comparison with softmax loss We discuss the relation of our loss function with the popular pixel-wise multi-class cross-entropy loss, often referred to as the softmax loss. In the case of a softmax loss with n classes, each pixel embedding is driven to a one-hot vector, i.e. the unit vector on one of the axes of an n-dimensional feature space. Because the softmax function has the normalizing property that its outputs are positive and sum to one, the embeddings are restricted to lie on the unit simplex. When the loss reaches zero, all embeddings lie on one of the unit vectors. By design, the dimensions of the output feature space (which correspond to the number of feature maps in the last layer of the network) must be equal to the number of classes. To add a class after training, the architecture needs to be updated too.</p><p>In comparison, our loss function does not drive the embeddings to a specific point in feature space. The network could for example place similar clusters (e.g. two small objects) closer together than dissimilar ones (e.g. a small and a large object). When the loss reaches zero, the system of push and pull forces has minimal energy and the clusters have organized themselves in n-dimensional space. Most importantly, the dimensionality of the feature space is independent of the number of instances that needs to be segmented. <ref type="figure" target="#fig_0">Figure 3</ref> depicts the convergence of our loss function when overfitting on a single image with 15 instances, in a 2-dimensional feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Post-processing</head><p>When the variance and distance terms of the loss are zero, the following is true:</p><p>• all embeddings are within a distance of δ v from their cluster center</p><p>• all cluster centers are at least 2δ d apart</p><p>If δ d &gt; δ v , then each embedding is closer to its own cluster center than to any other cluster center. It follows that during inference, we can threshold with a bandwidth b = δ v around a cluster center to select all embeddings belonging to that cluster. Thresholding in this case means selecting all embeddings x i that lie within a hypersphere with radius b around the cluster center x c :</p><formula xml:id="formula_1">x i ∈ C ⇔ x i − x c &lt; b<label>(5)</label></formula><p>For the tasks of classification and semantic segmentation, with a fixed set of classes, this leads to a simple strategy for post-processing the output of the network into discrete classes: after training, calculate the cluster centers of each class over the entire training set. During inference, threshold around each of the cluster centers to select all pixels belonging to the corresponding semantic class. This requires that the cluster centers of a specific class are the same in each image, which can be accomplished by coupling the cluster centers across a mini-batch. For the task of instance segmentation things are more complicated. As the labeling is permutation invariant, we cannot simply record cluster centers and threshold around them during inference. We could follow a different strategy: if we set δ d &gt; 2δ v , then each embedding is closer to all embeddings of its own cluster than to any embedding of a different cluster. It follows that we can threshold around any embedding to select all embeddings belonging to the same cluster. The procedure during inference is to select an unlabeled pixel, threshold around its embedding to find all pixels belonging to the same instance, and assign them all the same label. Then select another pixel that does not yet belong to an instance and repeat until all pixels are labeled.</p><p>Increasing robustness In a real-world problem the loss on the test set will not be zero, potentially causing our clustering algorithm for instance segmentation to make mistakes. If a cluster is not compact and we accidentally select an outlier to threshold around, it could happen that a real cluster gets predicted as two sub-clusters. To avoid this issue, we make the clustering more robust against outliers by applying a fast variant of the mean-shift algorithm <ref type="bibr" target="#b13">[14]</ref>. As before, we select a random unlabeled pixel and threshold around its embedding. Next however, we calculate the mean of the selected group of embeddings and use the mean to threshold again. We repeat this process until mean convergence. This has the effect of moving to a high-density area in feature space, likely corresponding to a true cluster center. In the experiments section, we investigate the effect of this clustering algorithm by comparing against ground truth clustering, where the thresholding targets are calculated as an average embedding over the ground truth instance labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pros and cons</head><p>Our proposed method has some distinctive advantages and disadvantages compared to other methods that we now discuss.</p><p>One big limitation of detect-and-segment approaches that is not immediately apparent from their excellent results on popular benchmarks, is that they rely on the assumption that an object's segmentation mask can be unambiguously extracted from its bounding box. This is an implicit prior that is very effective for datasets like MS COCO and Pascal VOC, which contain relatively blobby objects that do not occlude each other in complex ways. However, the assumption is problematic for tasks where an object's bounding box conveys insufficient information to recover the object's segmentation mask. Consider the synthetic scattered sticks dataset shown in <ref type="figure">figure 4</ref> as an example to illustrate the issue. When two sticks overlap like two crossed swords, their bounding boxes are highly overlapping. Given only a detection in the form of a bounding box, it is exceedingly <ref type="figure">Figure 4</ref>. Results on the synthetic scattered sticks dataset to illustrate that our approach is a good fit for problems with complex occlusions.</p><p>hard to unambigously extract a segmentation mask of the indicated object. Methods that rely on bounding boxes in their pipeline <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref> all suffer from this issue. In contrast, our method can handle such complex occlusions without problems as it treats the image holistically and learns to reason about occlusions, but does not employ a computationally expensive CRF like <ref type="bibr" target="#b4">[5]</ref>. Many real-world industrial or medical applications (conveyor belt sorting systems, overlapping cell and chromosome segmentation, etc.) exhibit this kind of occlusions. To the best of our knowledge no sufficiently large datasets for such tasks are publicly available, which unfortunately prevents us from showcasing this particular strength of our method to the full.</p><p>On the other hand, our method also has some drawbacks. Due to the holistic treatment of the image, our method performs well on datasets with a lot of similarity across the images (traffic scenes in Cityscapes or leaf configurations in CVPPP), but underperforms on datasets where objects can appear in random constellations and diverse settings, like Pascal VOC and MSCOCO. A sliding-window detectionbased approach with non-max suppression is more suited for such datasets. For example, if our method were trained on images with only one object, it would perform badly on an image that unexpectedly contained many of these objects. A detection-based approach has no trouble with this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We test our loss function on two instance segmentation datasets: The CVPPP Leaf Segmentation dataset and the Cityscapes Instance-Level Semantic Labeling Task. These datasets contain a median number of more than 15 instances per image. We also study the influence of the different components of our method and point out where there is room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>CVPPP leaf segmentation The LSC competition of the CVPPP workshop <ref type="bibr" target="#b1">[2]</ref> is a small but challenging benchmark. The task is to individually segment each leaf of a plant. The dataset <ref type="bibr" target="#b22">[23]</ref> was developed to encourage the use of computer vision methods to aid in the study of plant phenotyping <ref type="bibr" target="#b23">[24]</ref>. We use the A1 subset which consists of 128 labeled images and 33 test images. <ref type="bibr" target="#b31">[32]</ref> gives an overview of results on this dataset. We compare our performance with some of these works and two other recent approaches <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>. We report two metrics defined in <ref type="bibr" target="#b31">[32]</ref>: Symmetric Best Dice (SBD), which denotes the accuracy of the instance segmentation and Absolute Difference in Count (|DiC|) which is the absolute value of the mean of the difference between the predicted number of leaves and the ground truth over all images.</p><p>Cityscapes The large-scale Cityscapes dataset <ref type="bibr" target="#b10">[11]</ref> focuses on semantic understanding of urban street scenes. It has a benchmark for pixel-level and instance-level semantic segmentation. We test our method on the latter, using only the fine-grained annotations. The dataset is split up in 2975 training images, 500 validation images, and 1525 test images. We tune hyperparameters using the validation set and only use the train set to train our final model. We compare our results with the published works in the official leaderboard <ref type="bibr" target="#b0">[1]</ref>. We re-port accuracy using 4 metrics defined in <ref type="bibr" target="#b10">[11]</ref>: mean Average Precision (AP), mean Average Precision with overlap of 50% (AP0.5), AP50m and AP100m, where evaluation is restricted to objects within 50 m and 100 m distance, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Setup</head><p>Model architecture and general setup Since we want to stress the fact that our loss can be used with an offthe-shelf network, we use the ResNet-38 network <ref type="bibr" target="#b39">[40]</ref>, designed for semantic segmentation. We finetune from a model that was pre-trained on CityScapes semantic segmentation.</p><p>In the following experiments, all models are trained using Adam, with a learning rate of 1e-4 on a NVidia Titan X GPU.</p><p>Leaf segmentation Since this dataset only consists of 128 images, we use online data augmentation to prevent the model from overfitting and to increase the overall robustness. We apply random left-right flip, random rotation with θ ∈ [0, 2π] and random scale deformation with s ∈ [1.0, 1.5]. All images are rescaled to 512x512 and concatenated with an x-and y-coordinate map with values between -1 and 1. We train the network with margins δ v = 0.5, δ d = 1.5, and 16 output dimensions. Foreground masks are included with the test set, since this challenge only focuses on instance segmentation. Cityscapes Our final model is trained on the training images, downsampled to 768 × 384. Because of the size and variability of the dataset, there is no need for extra data augmentation. We train the network with margins δ v = 0.5, δ d = 1.5, and 8 output dimensions. In contrast to the CVPPP dataset, Cityscapes is a multi-class instance segmentation challenge. Therefore, we run our loss function independently on every semantic class, so that instances belonging to the same class are far apart in feature space, whereas instances from different classes can occupy the same space. For example, the cluster centers of a pedestrian and a car that appear in the same image are not pushed away from each other. We use a pretrained ResNet-38 network <ref type="bibr" target="#b39">[40]</ref> to generate segmentation masks for the semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis of the individual components</head><p>The final result of the semantic instance segmentation is influenced by three main components: the performance of the network architecture with our loss function, the quality of the semantic labels, and the post-processing step. To disentangle the effects of the semantic segmentation and the post-processing and to point out potential for improvement, we run two extra experiments on the Cityscapes validation set: SBD |DiC| RIS+CRF <ref type="bibr" target="#b30">[31]</ref> 66 • Semantic segmentation vs ground truth For the Cityscapes challenge, we rely on semantic segmentation masks to make a distinction between the different classes. Since our instance segmentation will discard regions not indicated in the semantic segmentation labels, the results will be influenced by the quality of the semantic segmentation masks. To measure the size of this influence, we also report performance with the ground truth semantic segmentation masks.</p><p>• Mean shift clustering vs ground truth clustering Since the output of our network needs to be clustered into discrete instances, the clustering method can potentially influence the accuracy of the overall instance segmentation. In this experiment, we measure this influence by clustering with our adapted mean shift clustering algorithm versus thresholding around the mean embeddings over the ground truth instance masks, as explained in section 3.2). <ref type="figure" target="#fig_1">Figure 5</ref> shows some results of our method on the validation set of the CVPPP dataset. The network makes very few mistakes: only the segmentation of the smallest leafs and the leaf stalks sometimes show a small error. <ref type="table">Table 1</ref> contains the numerical results. We achieve competitive results semantic segm. clustering AP AP0.5 resnet38 <ref type="bibr" target="#b39">[40]</ref> mean-shift 21.4 40.2 resnet38 <ref type="bibr" target="#b39">[40]</ref> center threshold 22.9 44.1 ground truth mean-shift 37.5 58.5 ground truth center threshold 47.8 77.8 <ref type="table">Table 3</ref>. Effect of the semantic segmentation and clustering components on the performance of our method on the Cityscapes validation set. We study this by gradually replacing each component with their ground truth counterpart. Row 1 vs row 3: the quality of the semantic segmentation has a big influence on the overall performance. Row 1 vs 2 and row 3 vs 4: the effect of the clustering method is less pronounced but also shows room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and discussion</head><p>(SBD score of 84.2) that are on-par with the state-of-the art (SBD score of 84.9). We outperform all non-deep learning methods and also the recurrent instance segmentation of <ref type="bibr" target="#b30">[31]</ref>, with a method that is arguably less complex. <ref type="figure" target="#fig_2">Figure 6</ref> shows some visual results on the Cityscapes validation set. We see that even in difficult scenarios, with street scenes containing a lot of cars or pedestrians, our method often manages to identify the individual objects. Failure cases mostly involve the splitting up of a single object into multiple instances or incorrect merging of neighboring instances. This happens in the lower left example, where the two rightmost cars are merged. Another failure mode is incorrect semantic segmentation: in the lower right example, the semantic segmentation network accidentally mistakes an empty bicycle storage for actual bikes. The instance segmentation network is left no choice but to give it a shot, and tries to split up the imaginary bikes into individual objects. Nevertheless, we achieve competitive results on the Cityscapes leaderboard <ref type="bibr" target="#b0">[1]</ref>, outperforming all but one unpublished work <ref type="bibr" target="#b4">[5]</ref>. Note that we perform on-par with the MNC-based method SAIS <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> on this dataset. See table 2 for a complete overview. A video of the results is available at https://youtu.be/FB_SZIKyX50.</p><p>As discussed in section 4.3, we are interested to know the influence of the quality of the semantic segmentations and the clustering algorithm on the overall performance. The results of these experiments can be found in table 3. As expected, the performance increases when we switch out a component with its ground truth counterpart. The effect of the semantic segmentation is the largest: comparing the first row (our method) to the third row, we see a large performance increase when replacing the ResNet-38 <ref type="bibr" target="#b39">[40]</ref> semantic segmentation masks with the ground truth masks. This can be explained by the fact that the average precision metric is an average over the semantic classes. Some classes like tram, train and bus almost never have more than one instance per image, causing the semantic segmentation to have a big influence on this metric. It is clear that the overall performance can be increased by having better semantic input ground truth ours input ground truth ours  segmentations. The last two entries of the table show the difference between ground truth clustering and mean shift clustering, both using ground truth segmentation masks. Here also, there is a performance gap. The main reason is that the loss on the validation set is not zero which means the constraints imposed by the loss function are not met. Clustering using mean-shift will therefore not lead to perfect results. The effect is more pronounced for small instances, as also noticeable in the shown examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Speed-accuracy trade-off</head><p>To investigate the trade-off between speed, accuracy and memory requirements, we train four different network mod-els on different resolutions and evaluate them on the car class of the Cityscapes validation set. This also illustrates the benefit that our method can be used with any off-theshelf network designed for semantic segmentation. <ref type="table">Table 4</ref> shows the results. We can conclude that Resnet-38 is best for accuracy, but requires some more memory. If speed is important, ENet would favor over Segnet since it is much faster with almost the same accuracy. It also shows that running on a higher resolution than 768x384 doesn't increase accuracy much for the tested networks. Note that the post-processing step can be implemented efficiently, causing only a negligible overhead. <ref type="table">Table 4</ref>. Average Precision (AP), AP using gt segmentation labels (APgt), speed of forward pass (fps), number of parameters (×10 6 ) and memory usage (GB) for different models evaluated on the car class of the Cityscapes validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we have proposed a discriminative loss function for the task of instance segmentation. After training, the output of the network can be clustered into discrete instances with a simple post-processing thresholding operation that is tailored to the loss function. Furthermore, we showed that our method can handle complex occlusions as opposed to popular detect-and-segment approaches. Our method achieves competitive performance on two benchmarks. In this paper we still used a pretrained network to produce the semantic segmentation masks. We will investigate the joint training of instance and semantic segmentation with our loss function in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Convergence of our method on a single image in a 2-dimensional feature space. Left: input and ground truth label. The middle row shows the raw output of the network (as the R-and G-channels of an RGB image), masked with the foreground mask. The upper row shows each of the pixel embeddings xi in 2-d feature space, colored corresponding to their ground truth label. The cluster center µc and margins δv and δ d are also drawn. The last row shows the result of clustering the embeddings by thresholding around their cluster center, as explained in section 3.2. We display the images after 0, 2, 4, 8, 16, 32 and 64 gradient update steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Some visual examples on the CVPPP leaf dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Some examples for different semantic classes on the Cityscapes instance segmentation validation set. Note some typical failure cases in the last row: incorrect merging of true instances and wrong semantic segmentation masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 2. Segmentation performance on the test set of the Cityscapes instance segmentation benchmark.</figDesc><table><row><cell></cell><cell></cell><cell>.6</cell><cell>1.1</cell><cell></cell></row><row><cell cols="2">MSU [32]</cell><cell>66.7</cell><cell>2.3</cell><cell></cell></row><row><cell cols="2">Nottingham [32]</cell><cell>68.3</cell><cell>3.8</cell><cell></cell></row><row><cell cols="3">Wageningen [44] 71.1</cell><cell>2.2</cell><cell></cell></row><row><cell>IPK [25]</cell><cell></cell><cell>74.4</cell><cell>2.6</cell><cell></cell></row><row><cell cols="2">PRIAn [15]</cell><cell>-</cell><cell>1.3</cell><cell></cell></row><row><cell cols="2">End-to-end [30]</cell><cell>84.9</cell><cell>0.8</cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell>84.2</cell><cell>1.0</cell><cell></cell></row><row><cell cols="5">Table 1. Segmentation and counting performance on the test set of</cell></row><row><cell cols="3">the CVPPP leaf segmentation challenge.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">AP AP0.5 AP100m AP50m</cell></row><row><cell>R-CNN+MCG</cell><cell>4.6</cell><cell>12.9</cell><cell>7.7</cell><cell>10.3</cell></row><row><cell>FCN+Depth</cell><cell>8.9</cell><cell>21.1</cell><cell>15.3</cell><cell>16.7</cell></row><row><cell>JGD</cell><cell>9.8</cell><cell>23.2</cell><cell>16.8</cell><cell>20.3</cell></row><row><cell>InstanceCut</cell><cell>13.0</cell><cell>27.9</cell><cell>22.1</cell><cell>26.1</cell></row><row><cell cols="2">Boundary-aware 17.4</cell><cell>36.7</cell><cell>29.3</cell><cell>34.0</cell></row><row><cell>DWT</cell><cell>19.4</cell><cell>35.3</cell><cell>31.4</cell><cell>36.8</cell></row><row><cell>Pixelwise DIN</cell><cell>20.0</cell><cell>38.8</cell><cell>32.6</cell><cell>37.6</cell></row><row><cell>Mask R-CNN</cell><cell>26.2</cell><cell>49.9</cell><cell>37.6</cell><cell>40.1</cell></row><row><cell>Ours</cell><cell>17.5</cell><cell>35.9</cell><cell>27.8</cell><cell>31.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe -Leuven).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://www.cityscapes-dataset.com/.Accessed:Novem-ber2016" />
		<title level="m">Cityscapes dataset</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://www.plant-phenotyping.org/datasets-home.Accessed" />
		<title level="m">Cvppp dataset</title>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up instance segmentation using deep higher-order crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08303</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to count leaves in rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03129</idno>
		<title level="m">Shape-aware instance segmentation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08272</idno>
		<title level="m">Instancecut: from edges to instances with multicut</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Finely-grained annotated datasets for image-based plant phenotyping. Pattern recognition letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image analysis: The new bottleneck in plant phenotyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="126" to="131" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>applications corner</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3-d histogram-based segmentation and leaf detection for rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning to decompose for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06449</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09410</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leaf segmentation in plant phenotyping: a collation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Polder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vukadinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine vision and applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Instance segmentation of indoor scenes using a coverage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GCPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gland instance segmentation by deep multichannel neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04889</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1731" to="1743" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-leaf tracking from fluorescence plant videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
