<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>May 13-17, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhi</forename><surname>Chen ⋆</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jinan University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Zhang</surname></persName>
							<email>zhangyuzhou3@huawei.com2chen_yingzhi@163.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhi</forename><surname>Chen ⋆</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Jinan University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhou</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<address>
									<settlement>Huawei, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">May 13-17, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3308558.3313497</idno>
					<note>ACM Reference Format: . 2019. Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction. In Proceedings of the 2019 World Wide Web Conference (WWW &apos;19),</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>Click-Through Rate Prediction</term>
					<term>Feature Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-Through Rate prediction is an important task in recommender systems, which aims to estimate the probability of a user to click on a given item. Recently, many deep models have been proposed to learn low-order and high-order feature interactions from original features. However, since useful interactions are always sparse, it is difficult for DNN to learn them effectively under a large number of parameters. In real scenarios, artificial features are able to improve the performance of deep models (such as Wide &amp; Deep Learning), but feature engineering is expensive and requires domain knowledge, making it impractical in different scenarios. Therefore, it is necessary to augment feature space automatically.In this paper, We propose a novel Feature Generation by Convolutional Neural Network (FGCNN) model with two components: Feature Generation and Deep Classifier. Feature Generation leverages the strength of CNN to generate local patterns and recombine them to generate new features. Deep Classifier adopts the structure of IPNN to learn interactions from the augmented feature space. Experimental results on three large-scale datasets show that FGCNN significantly outperforms nine state-of-the-art models. Moreover, when applying some state-of-the-art models as Deep Classifier, better performance is always achieved, showing the great compatibility of our FGCNN model. This work explores a novel direction for CTR predictions: it is quite useful to reduce the learning difficulties of DNN by automatically identifying important features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-Through Rate (CTR) is a crucial task for recommender systems, which estimates the probability of a user to click on a given item <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref>. In an online advertising application, which is a billiondollar scenario, the ranking strategy of candidate advertisements is by CTR×bid where "bid" is the profit that the system receives once the advertisement is clicked on. In such applications, the performance of CTR prediction models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40]</ref> is one of the core factors determining system's revenue.</p><p>The key challenge for CTR prediction tasks is to effectively model feature interactions. Generalized linear models, such as FTRL <ref type="bibr" target="#b20">[21]</ref>, perform well in practice, but these models lack the ability to learn feature interactions. To overcome the limitation, Factorization Machine <ref type="bibr" target="#b25">[26]</ref> and its variants <ref type="bibr" target="#b15">[16]</ref> are proposed to model pairwise feature interactions as the inner product of latent vectors and show promising results. Recently, deep neural networks (DNN) have achieved remarkable progress in computer vision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref> and natural language processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>. And some deep learning models have been proposed for CTR predictions, such as PIN <ref type="bibr" target="#b22">[23]</ref>, xDeepFM <ref type="bibr" target="#b18">[19]</ref> and etc. Such models feed raw features to a deep neural network to learn feature interactions explicitly or implicitly. Theoretically, DNN is able to learn arbitrary feature interactions from the raw features. However, due to that useful interactions are ususally sparse compared with the combination space of raw features, it is of great difficulties to learn them effectively from a large number of parameters <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Observing such difficulties, Wide &amp; Deep Learning <ref type="bibr" target="#b6">[7]</ref> leverages feature engineering in the wide component to help the learning of deep component. With the help of artificial features, the performance of deep component is improved significantly (0.6% improvement on offline AUC and 1% improvement on online CTR). However, feature engineering can be expensive and requires domain knowledge. If we can generate sophisticated feature interactions automatically by machine learning models, it will be more practical and robust.</p><p>Therefore, as shown in <ref type="figure">Figure 1</ref>, we propose a general framework for automatical feature generation. Raw features are input into a machine learning model (represented by the red box in <ref type="figure">Figure 1)</ref> to identify and generate new features <ref type="bibr" target="#b0">1</ref> . After that, the raw features and the generated new features are combined and fed into a deep neural network. The generated features are able to reduce the learning difficulties of deep models by capturing the sparse but important feature interactions in advance.</p><p>The most straightforward way for automatical feature generation is to perform Multi-Layer Perceptron (MLP 2 ) and use the hidden neurons as the generated features. However, as mentioned above, due to that useful feature interactions are usually sparse <ref type="bibr" target="#b22">[23]</ref>, it is rather difficult for MLP to learn such interactions from a huge parameter space. For example, suppose we have four user features: Name, Age, Height, Gender to predict whether a user will download an online game. Assume that the feature interaction between Age and Gender is the only signal that matters, so that an optimal model should identify this and only this feature interaction. When performing MLP with only one hidden layer, the optimal weights associated to the embeddings of Name and Height should be all 0's, which is fairly difficult to achieve.</p><p>As an advanced neural network structure, Convolutional Neural Network (CNN) has achieved great success in the area of computer vision <ref type="bibr" target="#b13">[14]</ref> and natural language processing <ref type="bibr" target="#b0">[1]</ref>. In CNN, the design of shared weights and pooling mechanism greatly reduces the number of parameters needed to find important local patterns and it will alleviate the optimization difficulties of later MLP structures. Therefore, CNN provides a potentially good solution to realize our idea (identify the sparse but important feature interactions). However, applying CNN directly could result in unsatisfactory performance. In CTR prediction, different arrange orders of original features do not have different meanings. For example, whether the arrangement order of features being (Name, Age, Height, Gender) or (Age, Name, Height, Gender) does not make any difference to describe the semantics of a sample, which is completely different from the case of images and sentences. If we only use the neighbor patterns extracted by CNN, many useful global feature interactions will be lost. This is also why CNN models do not perform well for CTR prediction task. To overcome this limitation, we perform CNN and MLP, which complement each other, to learn global-local feature interactions for feature generation.</p><p>In this paper, we propose a new model for CTR prediction task, namely Feature Generation by Convolutional Neural Network (FGCNN), which consists of two components: Feature Generation and Deep Classifier. In Feature Generation, a CNN+MLP structure is designed to identify and generate new important features from raw features. More specifically, CNN is performed to learn neighbor feature interactions, while MLP is applied to recombine them to extract global feature interactions. After Feature Generation, the feature space can be augmented by combining raw features and new features. In Deep Classifier, almost all state-of-the-art network structures (such as PIN <ref type="bibr" target="#b22">[23]</ref>, xDeepFM <ref type="bibr" target="#b18">[19]</ref>, DeepFM <ref type="bibr" target="#b8">[9]</ref>) can be adopted. Therefore, <ref type="bibr" target="#b0">1</ref> Here, new features are the feature interactions of the raw features. In the rest of this paper, we may use the term "new features" for the ease of presentation. <ref type="bibr" target="#b1">2</ref> MLP is a neural network with several fully connected layers. our model has good compatibility with the state-of-the-art models in recommender systems. For the ease of illustration, we will adopt IPNN model <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> as Deep Classifier in FGCNN, due to its good trade-off between model complexity and accuracy. Experimental results in three large-scale datasets show that FGCNN significantly outperforms nine state-of-the-art models, demonstrating the effectiveness of FGCNN. While adopting other models in Deep Classifier, better performance is always achieved, which shows the usefulness of the generated features.</p><p>Step-by-step analyses show that each component in FGCNN contributes to the final performance. Compared with traditional CNN structure, our CNN+MLP structure performs better and more stable when the order of raw features changes, which demonstrates the robustness of FGCNN.</p><p>To summarize, the main contributions of this paper can be highlighted as follows:</p><p>• An important direction is identified for CTR prediction: it is both necessary and useful to reduce the optimization difficulties of deep learning models by automatically generating important features in advance. The rest of this paper is organized as follows: Section 2 presents the proposed FGCNN model in detail. Experimental results will be shown and discussed in Section 3. Related works will be introduced in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FEATURE GENERATION BY CONVOLUTIONAL NEURAL NETWORK MODEL 2.1 Overview</head><p>In this section, we will describe the proposed Feature Generation by Convolutional Neural Network (FGCNN) model in detail. The used notations are summarized in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, FGCNN model consists of two components: Feature Generation and Deep Classifier. More specifically, Feature Generation focuses on identifying useful local and global patterns to generate new features as a complement to raw features, while Deep Classifier learns and predicts based on the augmented feature space through a deep learning model. Besides the two components, we will also formalize Feature Embedding of our model. The details of these components are presented in the following subsections.   </p><formula xml:id="formula_0">R 1 + R 2 + R 3 + R 4 …</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Embedding</head><p>In most CTR prediction tasks, data is collected in a multi-field categorical form 3 <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>, so that each data instance is normally transformed into a high-dimensional sparse (binary) vector via one-hot encoding <ref type="bibr" target="#b12">[13]</ref>. For example, (Gender=Male, Height=175, Age=18, Name=Bob) can be represented as:</p><p>(0, 1) Gender=Male (0, ..., 1, 0, 0) Height=175 (0, 1, ..., 0, 0)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Age=18</head><p>(1, 0, 0, .., 0) Name=Bob <ref type="bibr" target="#b0">(1)</ref> An embedding layer is applied upon the raw features' input to compress them to low-dimensional vectors. In our model, if a field is univalent (e.g., "Gender=Male"), its embedding is the feature embedding of the field; if a field is multivalent (e.g., "Interest=Football, Basketball"), the embedding of the field takes the sum of features' embeddings <ref type="bibr" target="#b7">[8]</ref>.</p><p>More formally, in an instance, each field i (1 ≤ i ≤ n f ) is represented as a low-dimensional vector e i ∈ R 1×k , where n f is the number of fields and k is embedding size. Therefore, each instance can be represented as an embedding matrix E = (e T 1 , e T 2 , ..., e T n f ) T , where E ∈ R n f ×k . In FGCNN model, the embedding matrix E can be utilized in both Feature Generation and Deep Classifier. To avoid the inconsistency of gradient direction when updating parameters, we will introduce another embedding matrix E ′ ∈ R n f ×k for Deep Classifier while E is used in Feature Generation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Generation</head><p>As stated in Section 1, generating new features from raw features helps improve the performance of deep learning models (as demonstrated by Wide &amp; Deep Learning <ref type="bibr" target="#b6">[7]</ref>). To achieve this goal, Feature Generation designs a proper neural network structure to identify useful feature interactions then generate new features automatically. As argued in Section 1, using MLP or CNN alone is not able to generate effective feature interactions from raw features, due to the following reasons: Firstly, useful feature interactions are always sparse in the combination space of raw features. Therefore, it is difficult for MLP to learn them from a large amount of parameters. Secondly, although CNN can alleviates optimization difficulties of (0, 0, … , 1, 0, 0) (1, 0) (0, 1, 0…, 0) (0, 0, 0,1,0…0  MLP by reducing the number of paramters, it only generates neighbor feature interactions which can lose many useful global feature interactions.</p><p>In order to overcome the weakness of applying MLP or CNN alone, as shown in the upper part of <ref type="figure" target="#fig_1">Figure 2</ref>, we perform CNN and MLP 4 as a complement to each other for feature generation. <ref type="figure" target="#fig_4">Figure 3</ref> shows an example of CNN+Recombination structure to capture global feature interactions. As can be observed, CNN learns useful neighbor feature patterns with a limited number of parameters, while recombination layer (which is a fully connected layer) generates global feature interactions based on the neighbor patterns provided by CNN. Therefore, important features can be generated effectively via this neural network structure, which has fewer parameters than directly applying MLP for feature generation.</p><p>In the following parts, we detail the CNN+Recombination structure of Feature Generation, namely, Convolutional Layer, Pooling layer and Recombination layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Convolutional Layer.</head><p>Each instance is represented as an embedding matrix E ∈ R n f ×k via feature embedding, where n f is the number of fields and k is embedding size. For convenience, reshape the embedding matrix as E 1 ∈ R n f ×k ×1 as the input matrix of the first convolutional layer. To capture the neighbor feature interactions, a convolutional layer is obtained by convolving a matrix WC 1 ∈ R h 1 ×1×1×m 1 c with non-linear activation functions (where h 1 is the height of the first convolutional weight matrix and m 1 c is the number of feature maps in the first convolutional layer). Suppose the output of the first convolutional layer is denoted as C 1 ∈ R n f ×k ×m 1 c , we can formulate the convolutional layer as follows:</p><formula xml:id="formula_1">C 1 p,q,i = tanh( 1 m=1 h 1 j=1 E 1 p+j−1,q,m WC 1 j,1,1,i ) (2) tanh(x) = exp(x) − exp(−x) exp(x) + exp(−x)<label>(3)</label></formula><p>where C 1 :,:,i denotes the i-th feature map in the first convolutional layer and p, q are the row and column index of the i-th feature map. Notice that the above equation excludes padding which is performed in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Pooling</head><p>Layer. After the first convolutional layer, a maxpooling layer is applied to seize the most important feature interactions and reduce the number of parameters. We refer h p as the height of pooling layers (width=1). The output in the first pooling layer is S 1 ∈ R (n f /h p )×k ×m 1 c :</p><formula xml:id="formula_2">S 1 p,q,i = max(C 1 p ·h p ,q,i , ... , C 1 p ·h p +h p −1,q,i ) (4)</formula><p>The pooling result of the i-th pooling layer will be the input for the (i + 1)-th convolutional layer: E i+1 = S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Recombination Layer.</head><p>After the first convolutional layer and pooling layer, S 1 ∈ R (n f /h p )×k ×m 1 c contains the patterns of neighbor features. Due to the nature of CNN, global non-neighbor feature interactions will be ignored if S 1 is regarded as the generated new features. Therefore, we introduce a fully connected layer to recombine the local neighbor feature patterns and generate important new features. We denote the weight matrix as</p><formula xml:id="formula_3">WR 1 ∈ R (n f /h p km 1 c )×(n f /h p km 1 r ) and the bias as BR 1 ∈ R (n f /h p km 1 r ) , where m 1</formula><p>c is the number of feature maps in the first convolutional layer and m 1 r is the number of new features' map in the first recombination Layer. Therefore, in the i-th recombination layer, n f /h i p m i r features are generated:</p><formula xml:id="formula_4">R 1 = tanh(S 1 · WR 1 + BR 1 )<label>(5)</label></formula><p>2.3.4 Concatenation. New features can be generated by performing CNN+Recombination multiple times. Assume there are n c convolutional layers, pooling layers and recombination layers and </p><formula xml:id="formula_5">N i = n f /h i p m i r fields of features are generated by i-th round denoted as R i . The overall new features R ∈ R N ×k (where N = n c i=1 N i ) generated by Feature Generation are formalized as: R = (R 1 , R 2 , ..., R n c )<label>(6)</label></formula><formula xml:id="formula_6">E = (E ′T , R T ) T (7)</formula><p>where E ′ is the embedding matrix of raw features for Deep Classifier (see Section 2.2). Both the raw features and new features are utilized for CTR prediction in Deep Classifier, which will be elaborated in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep Classifier</head><p>As mentioned above, raw features and new features are concatenated as an augmented embedding matrix E ∈ R (N +n f )×k , where N and n f are the number of fields of new features and raw features respectively. E is input into Deep Classifier, which aims to learn further interactions between the raw features and new generated features. In this subsection, for the ease of presentation, we adopt IPNN model <ref type="bibr" target="#b22">[23]</ref> as the network structure in Deep Classifier, due to its good trade-off between model complexity and accuracy. In fact, any advanced network structure can be adopted, which shows the compatibility of FGCNN with the existing works. The compatibility of FGCNN model will be verified empirically in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Network</head><p>Structure. IPNN model <ref type="bibr" target="#b22">[23]</ref> combines the learning ability of FM <ref type="bibr" target="#b26">[27]</ref> and MLP. It utilizes an FM layer to extract pairwise feature interactions from embedding vectors by inner product operations. After that, the embeddings of input features and the results of FM layer are concatenated and fed to MLP for learning. As evaluated in <ref type="bibr" target="#b22">[23]</ref>, the performance of IPNN is slightly worse than PIN (the best model in <ref type="bibr" target="#b22">[23]</ref>), but IPNN is much more efficient. We will illustrate the network structure of IPNN model. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, the pariwise feature interactions of augmented embedding matrix E ∈ R (N +n f )×k are modeled by an FM layer, as follows:</p><formula xml:id="formula_7">R f m = (&lt; E 1 , E 2 &gt;, ..., &lt; E N +n f −1 , E N +n f &gt;)<label>(8)</label></formula><p>where E i is the embedding of the i-th field, &lt; a, b &gt; means the inner product of a and b. The number of pairwise feature interactions in the FM layer is</p><formula xml:id="formula_8">(N +n f )(N +n f −1) 2 .</formula><p>After the FM layer, R f m is concatenated with the augmented embedding matrix E, which are fed into MLP with n h hidden layers to learn implicit feature interactions. We refer the input for the i-th hidden layer as I i and the output for the i-th hidden layer as O i . The MLP is formulated as:</p><formula xml:id="formula_9">I 1 = (R f m , f latten(E))<label>(9)</label></formula><formula xml:id="formula_10">O i = relu(I i W i + B i )<label>(10)</label></formula><formula xml:id="formula_11">I i+1 = O i<label>(11)</label></formula><p>where W i and B i are the weight matrix and bias of the i-th hidden layer in MLP. For the last hidden layer (the n h -th layer), we will make final predictions:</p><formula xml:id="formula_12">y = siдmoid(O n h W n h +1 + b n h +1 )<label>(12)</label></formula><p>2.4.2 Batch Normalization. Batch Normalization (BN) is proposed in <ref type="bibr" target="#b14">[15]</ref> to solve covariant shift and accelerate DNN training. It normalizes activations w T x using statistics within a mini-batch, as follows:</p><formula xml:id="formula_13">BN (w T x) = w T x − avд i (w T x) std i (w T x) д + b<label>(13)</label></formula><p>where д and b scale and shift the normalized values. In the FGCNN model, Batch Normalization is applied before each activation function to accelerate model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Objective Function.</head><p>The objective function in the FGCNN model is to minimize cross entropy of predicted values and the labels, which is defined as:</p><formula xml:id="formula_14">L(y,ŷ) = −yloдŷ − (1 − y)loд(1 −ŷ)<label>(14)</label></formula><p>where y ∈ {0, 1} is the label andŷ ∈ (0, 1) is the predicted probability of y = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Complexity Analysis</head><p>In this section, we analyse the space and time complexity of FGCNN model. Notations in <ref type="table" target="#tab_2">Table 1</ref> will be reused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Space</head><p>Complexity. The space complexity of FGCNN model consists of three parts, namely Feature Embedding, Feature Generation and Deep Classifier. In feature embedding, since there are two matrices (one for Feature Generation and one for Deep Classifier (as discussed in Section 2.2)), there are s 0 = 2t f k parameters, where t f is the total number of one-hot raw features. In Feature Generation, there are h i m i−1 c m i c parameters for convolutional matrix in i-th convolutional layer and n f /h i p km i c neurons in the output of i-th pooling layer. In addition, the number of parameters in i-th recombination layer is n 2 f /h 2i p k 2 m i c m i r , which will generate N i = n f /h i p m i r fields of new features (n f /h i p = N i /m i r ). Therefore, the total parameters of Feature Generation is:</p><formula xml:id="formula_15">n c i=1 h i m i−1 c m i c + n 2 f /h 2i p k 2 m i c m i r = (15) n c i=1 h i m i−1 c m i c + N 2 i k 2 m i c /m i r<label>(16)</label></formula><p>After the Feature Generation process, we can get T = n f + n c i=1 N i feature embeddings in total. Normally, m i r is usually set as 1, 2, 3, 4 and m i r = m i−1 r . Meanwhile, h p is usually set to 2 so that</p><formula xml:id="formula_16">N i = N 1 /2 i−1 . Furthermore, N 2 i k 2 &gt;&gt; h i m i−1 c and n c i=1 m i c /m i r 2 2(i−1)</formula><p>is usually a small number. The space complexity s 1 of the feature generating process can be simplified as:</p><formula xml:id="formula_17">s 1 = O( n c i=1 N 2 1 /2 2(i−1) k 2 m i c /m i r ) = O(N 2 1 k 2 )<label>(17)</label></formula><p>For the first hidden layer of Deep Classifier, the number of parameters in the weight matrix is (T (T − 1)/2 + Tk)H 1 (recall that the input is the raw and new features' embedding and their pairwised product). In the i-th hidden layer, the number of parameters is H i−1 H i . Therefore, the space complexity of Deep Classifier is:</p><formula xml:id="formula_18">s 2 = O( T (T − 1) + 2Tk 2 H 1 + n h i=2 H i H i−1 )<label>(18)</label></formula><p>where Eq. (18) can be simplified as O(</p><formula xml:id="formula_19">T 2 H 1 + n h i=2 H i H i−1 ) since Tk &lt; T (T − 1) usually.</formula><p>In summary, the overall space complexity of FGCNN is</p><formula xml:id="formula_20">s 0 + s 1 + s 2 = O(t f k + N 2 1 k 2 + T 2 H 1 + q i=1 H i H i−1 )</formula><p>, which is dominated by the number of one-hot features, the number of generated features, the number of hidden neurons and the embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Time Complexity.</head><p>We first analyze the time complexity in Feature Generation. In the i-th convolutional layer, the output dimension is n f /h i−1 p km i c = N i h p /m i r km i c (recall that N i = n f /h i p m i r ) and each point is calculated from h i m i−1 c data points where h i is the height of convolutional kernels in the i-th convolutional layer. Therefore, the time complexity of the i-th convolutional layer is</p><formula xml:id="formula_21">N i h p km i c h i m i−1 c /m i r .</formula><p>In the pooling layer, the time complexity of the i-th pooling layer is n f /h i−1 p km i c = N i h p km i c /m i r . In the i-th recombination layer, the time complexity is n/h i p km i c · N i k = N 2 i k 2 m i c /m i r (recall h p = 2 and N i = N 1 /2 i−1 ). Therefore, total </p><formula xml:id="formula_22">t 1 = n c i=1 N i h p km i c h i m i−1 c /m i r + N i h p km i c /m i r + N 2 i k 2 m i c /m i r = n c i=1 N 1 km i c /(m i r 2 i−1 )(2h i m i−1 c + 2 + N 1 /2 i−1 k) = O(N 1 k n c i=1 m i−1 c h i + N 2 1 k 2 ) (Notice: m i c /(m i r 2 i )</formula><p>is usually a small number ranging from 2 to 10) Then, we analyze the time complexity of Deep Classifier (taking IPNN as an example). In the FM layer, the time complexity to calculate pairwise inner product is</p><formula xml:id="formula_23">T (T −1)k 2 .</formula><p>The number of neurons, that are input to the first hidden layer is T (T − 1)/2 + Tk. Therefore, the time complexity of the first hidden layer is O((T (T − 1)/2 + T k)H 1 ) = O(T 2 H 1 ). For the other hidden layers, the time complexity is O(H i H i−1 ). Therefore, the total time complexity in Deep Classifier is</p><formula xml:id="formula_24">t 2 = O(T 2 H 1 ) + n h i=2 O(H i H i−1 )<label>(19)</label></formula><p>In summary, the overall time complexity for FGCNN model is</p><formula xml:id="formula_25">t 1 +t 2 = O(N 1 k n c i=1 m i−1 c h i + N 2 1 k 2 +T 2 H 1 + q i=2 H i H i−1 )</formula><p>which is dominated by the number of generated features, the number of neurons in the hidden layers, embedding size and convolutional parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to answer the following questions.</p><p>• (Q1) How does FGCNN perform, compared to the state-ofthe-art models for CTR prediction task? • (Q2) Can FGCNN improve the performance of other state-ofthe-art models by using their structures in Deep Classifier? • (Q3) How does each key structure in FGCNN boost the performance of FGCNN? • (Q4) How do the key hyper-parameters of FGCNN (i.e., size of convolutional kernels,number of convolutional layers, number of generated features) impact its performance? • (Q5) How does Feature Generation perform when the order of raw features are randomly shuffled? Note that when studying questions other than Q2, we adpot IPNN as the Deep Classifier in FGCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Datasets. Experiments are conducted in the following three datasets:</head><p>Criteo: Criteo 5 contains one month of click logs with billions of data samples. A small subset of Criteo was published in Criteo 5 http://labs.criteo.com/downloads/download-terabyte-click-logs/ Display Advertising Challenge 2013 and FFM was the winning solution <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. We select "day 6-12" as training set while select "day 13" for evaluation. Due to the enormous data volume and serious class imbalance (i.e., only 3% samples are positive), negative sampling is applied to keep the positive and negative ratio close to 1:1. We convert 13 numerical fields into one-hot features through bucketing, where the features in a certain field appearing less than 20 times are set as a dummy feature "other". Avazu: Avazu 6 was published in the contest of Avazu Click-Through Rate Prediction, 2014. The public dataset is randomly splitted into training and test sets at 4:1. Meanwhile, we remove the features appearing less than 20 times to reduce dimensionality.</p><p>Huawei App Store: In order to evaluate the performance in industrial CTR prediction problems, we conduct experiments on Huawei App Store Dataset. We collect users' click logs from Huawei App Store while logs from 20180617 to 20180623 are used for training and logs of 20180624 are used for test. Negative sampling is applied to reduce data amount and to adjust the ratio of positive class and negative class. The dataset contains app features (e.g., identification, category), user features (e.g., user's behavior history) and context features (e.g., operation time).</p><p>In addition, the statistics of the three datasets are summarized in <ref type="table" target="#tab_4">Table 2.</ref> 3.1.2 Baselines. We compare nine baseline models in our experiments, including LR <ref type="bibr" target="#b16">[17]</ref>, GBDT <ref type="bibr" target="#b5">[6]</ref>, FM <ref type="bibr" target="#b25">[26]</ref>, FFM <ref type="bibr" target="#b15">[16]</ref>, CCPM <ref type="bibr" target="#b19">[20]</ref>, DeepFM <ref type="bibr" target="#b8">[9]</ref>, xDeepFM <ref type="bibr" target="#b18">[19]</ref>, IPNN and PIN <ref type="bibr" target="#b22">[23]</ref>. Wide &amp; Deep is  <ref type="table" target="#tab_5">Table 3</ref> summerizes the hyper-parameters of each model. For Criteo and Avazu Datasets, the hyper-parameters of baseline models are set to be the same as in <ref type="bibr" target="#b22">[23]</ref>. Notice that when conducting experiments on Criteo and Avazu, we observed that FGCNN uses more parameters in Deep Classifier than other models. To make fair comparisons, we also conduct experiments to increase the parameters in MLPs of other deep models. However, all these models cannot achieve better performance than the original settings <ref type="bibr" target="#b8">9</ref> . The reason could be the overfitting problem where such models simply use embedding of raw features for training but use a complex structure. On the other hand, since our model augments the feature space and enriches the input, more parameters in Deep Classifier can boost the performance of our model. In FGCNN model, new is the number of kernels when generating new features. The number of generated features can be calculated as n c i=1 #f ields/2 i * new i . 3.1.5 Significance Test. We repeat the experiments 10 times by changing the random seed for FGCNN and the best baseline model. The two-tailed pairwise t-test is performed to detect significant differences between FGCNN and the best baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Performance (Q1)</head><p>In this subsection, we compare the performance of different models on the test set. <ref type="table" target="#tab_6">Table 4</ref> summarizes the overall performance of all the compared models on the three datasets, where the underlined numbers are the best results of the baseline models and bold 7 https://github.com/guestwalk/libffm 8 https://www.tensorflow.org/ 9 Due to the limited pages, we do not show the experimental result in the paper. <ref type="bibr" target="#b10">11</ref> According to the experimental recordings, the results of PIN in <ref type="bibr" target="#b22">[23]</ref> uses the trick of adaptive embedding size. Here, we use fixed embedding size for all the deep models. numbers are the best results of all models. We have the following observations:</p><p>Firstly, in majority of the cases, non-neural network models perform worse than neural network models. The reason is that deep neural network can learn complex feature interactions much better than the models where no feature interaction is modeled (i.e., LR), or feature interactions are modeled by simple inner product operations (i.e., FM and FFM).</p><p>Secondly, FGCNN achieves the best performance among all the models on the three evaluated datasets. It is significantly better than the best baseline models with 0.05%, 0.14% and 0.13% improvements in AUC (0.09%, 0.24% and 0.79% in log loss) on Criteo, Avazu and Huawei App Store datasets, which demonstrates the effectiveness of FGCNN. In fact, a small improvement in offline AUC is likely to lead to a significant increase in online CTR. As reported in <ref type="bibr" target="#b6">[7]</ref>, compared with LR, Wide &amp; Deep improves offline AUC by 0.275% and the improvement of online CTR is 3.9%. The daily turnover of Huawei App Store is millions of dollars. Therefore, even a few lifts in CTR brings extra millions of dollars each year.</p><p>Thirdly, with the help of the generated new features, FGCNN outperforms IPNN by 0.11%, 0.19% and 0.13% in terms of AUC (0.2%, 0.29% and 0.79% in terms of log loss) on Criteo, Avazu and Huawei App Store datasets. It demonstrates that the generated features are very useful and they can effectively reduce the optimization difficulties of traditional DNNs thus leading to better performance.</p><p>Fourthly, CCPM, which applies CNN directly, achieves the worst performance among neural network models. Moreover, CCPM performs worse than FFM on Criteo and Avazu datasets. It shows that directly using traditional CNN for CTR prediction task is inadvisable, as CNN is designed to generate neighbor patterns while the arrangement order of feature is usually no meaning in recommendation scenarios. However, in FGCNN, we leverage the strength of CNN to extract local patterns while complementing it with Recombination Layer to extract global feature interactions and generate new features. Therefore, better performance is achieved.  <ref type="table" target="#tab_7">Table 5</ref> summarizes the performance. We have the following observations: Firstly, with the help of the generated new features, the performance of all models are improved, which demonstrates the effectiveness of the generated features and shows the compatibility of FGCNN. Secondly, we observe that when only using raw features, DeepFM always outperforms DNN. But when using the augmented features, FGCNN+DNN outperforms FGCNN+DeepFM. The possible reason is that DeepFM sums up the inner product of input features to the last MLP layer which may cause contradictory gradient updates (compared with MLP) on embeddings.This could be one of the reasons why IPNN (feeding the product into MLP) outperforms DeepFM in all datasets.</p><p>In a word, the results show that our FGCNN model can be viewed as a general framework to enhance the existing neural networks by generating new features automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Effectiveness of FGCNN Variants(Q3)</head><p>We conduct experiments to study how each component in FGCNN contributes to the final performance. Each variant is generated by removing or replacing some components in FGCNN, which is described as follows:  <ref type="table" target="#tab_9">Table 6</ref>, removing any component in FGCNN leads to a drop in performance. We have the following observations:</p><formula xml:id="formula_26">•</formula><p>Firstly, FGCNN with raw features alone or with new generated features alone performs worse than the FGCNN with both of them. This result demonstrates that the generated features are good supplementaries to the original features, which are both crucial.</p><p>Secondly, the performance decrease of Appling MLP for Feature Generation, compared to FGCNN, shows the ineffectiveness of MLP to identify the sparse but important feature combinations from a large number of parameters. CNN simplifies the learning difficulties by using the shared convolutional kernels, which has much fewer parameters to get the desired combinations. Moreover, MLP recombines the neighbor feature interactions, which is extracted by CNN, to generate global feature interactions.</p><p>Thirdly, removing the Recombination Layer will limit the generated features as neighbor feature interactions. Since the arrangement order of raw features has no actual meanings in the CTR prediction task, the restriction can lead to losing important nonneighbor feature interactions thus resulting in worse performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hyper-parameter Investigation (Q4)</head><p>Our FGCNN model has several key hyper-parameters, i.e., the height of convolutional kernels, number of convolutional kernels, number of convolutional layers, and the number of kernels for generating new features. In this subsection, to study the impact of these hyper-parameters, we investigate how FGCNN model works by changing one hyper-parameter while fixing the others on Criteo and Huawei App Store datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5.1</head><p>Height of Convolutional Kernels. The height of convolutional kernels controls the perception range of convolutional layers. The larger the height is, the more features are involved in the neighbor patterns, but more parameters need to be optimized. To investigate its impact, we increase the height from 2 to the number of fields of a dataset. As shown in the top of <ref type="figure" target="#fig_6">Figure 5</ref>, the performance generally ascends first and then descends as the height of convolutional kernels increases <ref type="bibr" target="#b9">10</ref> .</p><p>The results show that as more features are involved in the convolutional kernels, higher-order feature interactions can be learned  so that the performance increases. However, due to that useful feature interactions are usually sparse, larger heights can cause more difficulties to learn them effectively, leading to a decrease in performance. This observation is consistent with the finding in Section 3.4, i.e., the performance decreases in Appling MLP for Feature Generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Number of Convolutional Layers.</head><p>As shown in the middle of <ref type="figure" target="#fig_6">Figure 5</ref>, as the number of convolutional layers increases, the performance of FGCNN is improved. Notice that more layers usually lead to higher-order feature interactions. Therefore, the result also shows the effectiveness of high-order feature interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Number of Kernels for Generating New Features.</head><p>We study how the number of generated features affects the performance of FGCNN. We use the same number of kernels for new features in different Recombination Layers. As can be observed in the bottom of <ref type="figure" target="#fig_6">Figure 5</ref>, the performance is gradually improved with more features generated. The results verify our research idea that it is useful to identify the sparse but important feature interactions first, which can effectively reduce the learning difficulties of DNNs. However, the useful feature interactions can be sparse and limited. If too many features are generated, the extra new features are noisy which will increase the learning difficulties of MLP, leading to the decrease in the performance.  <ref type="figure">Figure 6</ref>: Shuffling the order of raw features As mentioned before, CNN is designed to capture local neighbor feature patterns so that it is sensitive to the arrangement order of raw features. In our FGCNN model, the design of Recombination Layer is to learn global feature interactions based on CNN's extracted local patterns. Intuitively, our model should have more stable performance than traditional CNN's structure if the order of raw features is shuffled. Therefore, to verify it, we compare the performance of two cases: with/without Recombination Layer. The arrangement order of raw features is shuffled many times at random where the two compared cases are performed for the same shuffled arrangement order.</p><p>As shown in <ref type="figure">Figure 6</ref>, the case with Recombination Layer achieves better and more stable performance than that of without Recombination Layer. It demonstrates that with the help of Recombination Layer, FGCNN can greatly reduce the side effects of changing the arrangement order of raw features, which also demonstrates the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Click-Through Rate Prediction is normally formulated as a binary classification problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. In this section, we will introduce two important categories of models in Click-Through Rate predictions, namely shallow models and deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shallow Models for CTR Prediction</head><p>Due to the robustness and efficiency, Logistic Regression (LR) models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, such as FTRL <ref type="bibr" target="#b20">[21]</ref> are widely used in CTR prediction. To learn feature interactions, a common practice is to manually design pairwise feature interactions in its feature space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>. Poly-2 <ref type="bibr" target="#b3">[4]</ref> models all pairwise feature interactions to avoid feature engineering. Factorization Machine (FM) <ref type="bibr" target="#b25">[26]</ref> introduces low-dimensional vectors for each feature and models feature interactions through inner product of feature vectors. FM improves the ability of modelling feature interactions when data is sparse. FFM <ref type="bibr" target="#b15">[16]</ref> enables each feature to have multiple latent vectors to perform interactions with features from different fields. LR, Poly-2 and FM variants are widely used in CTR prediction in industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Learning for CTR Prediction</head><p>Deep learning has achieved great success in different areas, such as computer vision <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, natural language processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> etc. In order to leverage deep Learning for CTR prediction, several models are proposed <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref>. FNN <ref type="bibr" target="#b38">[39]</ref> is proposed in <ref type="bibr" target="#b38">[39]</ref>, which uses FM to pre-train the embedding of raw features and then feeds the embeddings to several fully connected layers. Some models adopt DNN to improve FM, such as Attentional FM <ref type="bibr" target="#b34">[35]</ref>, Neural FM <ref type="bibr" target="#b11">[12]</ref>.</p><p>Wide &amp; Deep Learning <ref type="bibr" target="#b6">[7]</ref> jointly trains a wide model and a deep model where the wide model leverages the effectiveness of feature engineering and the deep model learns implicit feature interactions. Despite the usefulness of wide component, feature engineering is expensive and requires expertise. To avoid feature engineering, DeepFM <ref type="bibr" target="#b8">[9]</ref> introduces FM layer (order-2 feature interaction) as a wide component and uses a deep component to learn implicit feature interactions. Different from DeepFM, IPNN <ref type="bibr" target="#b22">[23]</ref> (also known as PNN in <ref type="bibr" target="#b21">[22]</ref>) feeds both the result of FM layer and embeddings of raw features into MLP and results in comparable performance. Rather than using inner product to model pairwise feature interactions as DeepFM and IPNN, PIN <ref type="bibr" target="#b22">[23]</ref> uses a Micro-Network to learn complex feature interaction for each feature pair. xDeepFM <ref type="bibr" target="#b18">[19]</ref> proposes a novel Compressed Interaction Network (CIN) to explicitly generate feature interactions at the vector-wise level.</p><p>There are several models which use CNN for CTR Prediction. CCPM <ref type="bibr" target="#b19">[20]</ref> applies multiple convolutional layers to explore neighbor feature dependencies. CCPM performs convolutions on the neighbor fields in a certain alignment. Due to that the order of features has no actual meaning in CTR predictions, CCPM can only learn limited feature interactions between neighbor features. In <ref type="bibr" target="#b2">[3]</ref>, it is shown that features' arrangement order has a great impact on the final performance of CNN based models. Therefore, the authors propose to generate a set of suitable feature sequences to provide different local information for convolutional layers. However, the key weakness of CNN is not solved.</p><p>In this paper, we propose FGCNN model, which splits the CTR prediction task into two stages: Feature Generation and Deep Classifier. Feature Generation augments the original feature space by generating new features while most state-of-the-art models can be adopted in Deep Classifier to learn and prediction based on the augmented feature space. Different from traditional CNN models for CTR Prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, FGCNN can leverage the strength of CNN to extract local information and it greatly alleviates the weakness of CNN by introducing the Recombination Layer to recombine information from different local patterns learned by CNN to generating new features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a FGCNN model for CTR prediction which aims to reduce the learning difficulties of DNN models by identifying important features in advance. The model consists of two components: Feature Generation and Deep Classifier. Feature Generation leverages the strength of CNN to identify useful local patterns and it alleviates the weakness of CNN by introducing a Recombination Layer to generate global new features from the recombination of the local patterns. In Deep Classifier, most existing deep models can be applied on the augmented feature space. Extensive experiments are conducted on three large-scale datasets where the results show that FGCNN outperforms nine state-of-the-art models. Moreover, when applying other models in Deep Classifier, compared with the original model without Feature Generation, better performance is always achieved, which demonstrates the effectiveness of the generated features.</p><p>Step-by-step experiments show that each component in FGCNN contributes to the final performance. Furthermore, compared with the traditional CNN structure, our CNN+Recombination structure in Feature Generation always performs better and more stable when shuffling the arrangement order of raw features. This work explores a novel direction for CTR prediction that it is effective to automatically generate important features first rather than feeding the raw embeddings to deep learning models directly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of Feature Generation by Convolutional Neural Network Model (The hyper-parameters in the figure are the best setting of FGCNN on Avazu Dataset)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>of one-hot features n f the number of fields nc the number of convolutional layers n h the number of hidden layers h i the height of convolutional kernel in the i-th convolutional layer hp pooling height (width=1) of pooling layers ei the embedding vector for i-th field E i the input of the i-th convolutional layer m i c the number of feature maps in the i-th convolutional layer m i r the number of new features' map in the i-th recombination layer Ni the number of generated features in the i-th recombination layer (=n f /h i p m i r ) C i :,:, j the j-th output feature map of the i-th convolutional layer WC i :,:, j weights for the j-th output feature map of i-th convolutional layer S i :,:, j the j-th output feature map of the i-th pooling layer WR i weights for the i-th recombination layer BR i bias for the i-th recombination layer R i :,:, j the j-th output map of new features in the i-th recombination layer W i weights for the i-th hidden layer B i biases for the i-th hidden layer Hi number of hidden neorons in the i-th hidden layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>CNN+Recombination structure is able to capture global non-neighbor feature interactions to generate new features. CNN consists of convolutional layer and pooling layer, while Recombination consists of a fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Structure of IPNN Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Parameter study of height of convolutional kernel, number of convolutional layers and number of kernels for new features (from top to bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Notations</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Then, raw features and new features are concatenated as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="4">#instances #features #fields pos ratio</cell></row><row><cell>Criteo</cell><cell>1 × 10 8</cell><cell>1 × 10 6</cell><cell>39</cell><cell>0.5</cell></row><row><cell>Avazu</cell><cell>4 × 10 7</cell><cell>6 × 10 5</cell><cell>24</cell><cell>0.17</cell></row><row><cell cols="2">Huawei App Store 2.3 × 10 8</cell><cell cols="2">1.6 × 10 5 29</cell><cell>0.05</cell></row><row><cell cols="3">time complexity of the Feature Generation is</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Parameter Settings Note: bs=batch size, opt=optimizer, lr=learning rate, l2=l2 regularization on Embedding Layer, k=embedding size, conv=shape of convolutional kernels, kernel=number of convolutional kernels,</figDesc><table><row><cell>Param</cell><cell>Criteo</cell><cell>Avazu</cell><cell>Huawei App Store</cell></row><row><cell>General</cell><cell>bs=2000 opt=Adam lr=1e-3</cell><cell>bs=2000 opt=Adam lr=1e-3</cell><cell>bs=1000 opt=Adam lr=1e-4 l2=1e-6</cell></row><row><cell>LR</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GBDT</cell><cell>depth=25 #tree=1300</cell><cell>depth=18 #tree=1000</cell><cell>depth=10 #tree=1400</cell></row><row><cell>FM</cell><cell>k=20</cell><cell>k=40</cell><cell>k=40</cell></row><row><cell>FFM</cell><cell>k=4</cell><cell>k=4</cell><cell>k=12</cell></row><row><cell>CCPM</cell><cell>k=20 conv= 7*1 kernel=[256] net=[256*3,1]</cell><cell>k=40 conv: 7*1 kernel = [128] net= [128*3,1]</cell><cell>k=40 conv= 13*1 kernel= [8,16,32,64] net=[512,256,128,1] drop= 0.8</cell></row><row><cell></cell><cell>k=20</cell><cell>k=40</cell><cell>k=40</cell></row><row><cell>DeepFM</cell><cell>LN=T</cell><cell>LN=T</cell><cell>net=[2048,1024,512,1]</cell></row><row><cell></cell><cell>net=[700*5,1]</cell><cell>net=[500*5,1]</cell><cell>drop=0.9</cell></row><row><cell>XdeepFM</cell><cell>k=20 net=[400*3,1] CIN=[100*4]</cell><cell>k=40 net=[700*5,1] CIN:[100*2]</cell><cell>k=40 net=[2048,1024,512,1] drop=0.9 CIN:[100*4]</cell></row><row><cell>IPNN</cell><cell>k=20 LN=T net=[700*5,1]</cell><cell>k=40 LN=T net=[500*5,1]</cell><cell>k=40 net=[2048,1024, 512,256,128,1] drop=0.7</cell></row><row><cell></cell><cell>k=20</cell><cell>k=40</cell><cell>k=40</cell></row><row><cell>PIN</cell><cell>LN=T net=[700*5,1]</cell><cell>LN=T net=[500*5,1]</cell><cell>net=[2048,1024,512,1] drop=0.9</cell></row><row><cell></cell><cell>subnet=[40,5]</cell><cell>sub-net=[40,5]</cell><cell>sub-net=[80,10]</cell></row><row><cell>FGCNN</cell><cell>k=20 conv=9*1 kernel=[38,40,42,44] new=[3,3,3,3] BN=T ruenet=[4096,2048,1]</cell><cell>k=40 conv=7*1 kernel=[14,16,18,20] new=[3,3,3,3] BN=T net=[4096,2048, 1024,512,1]</cell><cell>k=40 conv=13*1 kernel=[6, 8, 10, 12] new=[2,2,2,2] BN=T net=[2048,1024, 512,256,128,1] drop=0.8</cell></row></table><note>pool=max-pooling shape, net=MLP structure, sub-net=micro metwork, drop=dropout rate, LN=layer normalization, BN= batch normalization{T=True}, new=number of kernels for new features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Overall Performance ⋆ : p &lt; 10 −2 ⋆ ⋆ : p &lt; 10 −4 (two tailed t-test) 22% ⋆ 0.5388 ⋆ 78.83% ⋆⋆ 0.3746 ⋆⋆ 94.07% ⋆⋆ 0.1134 ⋆⋆</figDesc><table><row><cell></cell><cell></cell><cell>Criteo</cell><cell></cell><cell>Avazu</cell><cell cols="2">Huawei App Store</cell></row><row><cell>Model</cell><cell>AUC</cell><cell cols="2">Log Loss AUC(%)</cell><cell cols="2">Log Loss AUC</cell><cell>Log Loss</cell></row><row><cell>LR</cell><cell>78.00%</cell><cell>0.5631</cell><cell>76.76%</cell><cell>0.3868</cell><cell>90.12%</cell><cell>0.1371</cell></row><row><cell>GBDT</cell><cell>78.62%</cell><cell>0.5560</cell><cell>77.53%</cell><cell>0.3824</cell><cell>92.68%</cell><cell>0.1227</cell></row><row><cell>FM</cell><cell>79.09%</cell><cell>0.5500</cell><cell>77.93%</cell><cell>0.3805</cell><cell>93.26%</cell><cell>0.1191</cell></row><row><cell>FFM</cell><cell>79.80%</cell><cell>0.5438</cell><cell>78.31%</cell><cell>0.3781</cell><cell>93.58%</cell><cell>0.1170</cell></row><row><cell>CCPM</cell><cell>79.55%</cell><cell>0.5469</cell><cell>78.12%</cell><cell>0.3800</cell><cell>93.71%</cell><cell>0.1159</cell></row><row><cell>DeepFM</cell><cell>79.91%</cell><cell>0.5423</cell><cell>78.36%</cell><cell>0.3777</cell><cell>93.91%</cell><cell>0.1145</cell></row><row><cell cols="2">xDeepFM 80.06%</cell><cell>0.5408</cell><cell>78.55%</cell><cell>0.3766</cell><cell>93.91%</cell><cell>0.1146</cell></row><row><cell>IPNN</cell><cell>80.13%</cell><cell>0.5399</cell><cell>78.68%</cell><cell>0.3757</cell><cell>93.95%</cell><cell>0.1143</cell></row><row><cell>PIN</cell><cell cols="2">80.18% 11 0.5393</cell><cell>78.72%</cell><cell>0.3755</cell><cell>93.91%</cell><cell>0.1146</cell></row><row><cell>FGCNN</cell><cell>80.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>not compared here because some state-of-the-art models (such as xDeepFM, DeepFM, PIN) have shown better performance in their publications. We use XGBoost [6] and libFFM 7 as the implemen- tation of GBDT and libFFM, respectively. In our experiments, the other baseline models are implemented with Tensorflow 8 .3.1.3 Evaluation Metrics. The evaluation metrics are AUC (Area Under ROC) and log loss (cross entropy).3.1.4 Parameter Settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Compatibility Study of FGCNN</figDesc><table><row><cell></cell><cell></cell><cell>Criteo</cell><cell></cell><cell>Avazu</cell><cell>Huawei App Store</cell></row><row><cell></cell><cell>AUC</cell><cell cols="2">Log Loss AUC</cell><cell>Log Loss AUC</cell><cell>Log Loss</cell></row><row><cell>FM</cell><cell cols="2">79.09% 0.5500</cell><cell cols="2">77.93% 0.3805</cell><cell>93.26% 0.1191</cell></row><row><cell>FGCNN+FM</cell><cell cols="2">79.67% 0.5455</cell><cell cols="2">78.13% 0.3794</cell><cell>93.66% 0.1165</cell></row><row><cell>DNN</cell><cell cols="2">79.87% 0.5428</cell><cell cols="2">78.30% 0.3778</cell><cell>93.85% 0.1149</cell></row><row><cell>FGCNN+DNN</cell><cell cols="2">80.09% 0.5402</cell><cell cols="2">78.55% 0.3764</cell><cell>94.00% 0.1139</cell></row><row><cell>DeepFM</cell><cell cols="2">79.91% 0.5423</cell><cell cols="2">78.36% 0.3777</cell><cell>93.91% 0.1145</cell></row><row><cell cols="3">FGCNN+DeepFM 79.94% 0.5421</cell><cell cols="2">78.44% 0.3771</cell><cell>93.93% 0.1145</cell></row><row><cell>IPNN</cell><cell cols="2">80.13% 0.5399</cell><cell cols="2">78.68% 0.3757</cell><cell>93.95% 0.1143</cell></row><row><cell>FGCNN+IPNN</cell><cell cols="2">80.22% 0.5388</cell><cell cols="2">78.83% 0.3746</cell><cell>94.07% 0.1134</cell></row><row><cell cols="3">3.3 Compatibility of FGCNN with Different</cell><cell></cell></row><row><cell>Models (Q2)</cell><cell></cell><cell></cell><cell></cell></row></table><note>As stated in Section 2.4, Feature Generation can augment the original feature space and Deep Classifier of FGCNN can adopt any advanced deep neural networks. Therefore, we select several models as Deep Classifeir to verify the utility of Feature Generation, including non- deep models (FM), deep learning models (DNN, DeepFM, IPNN).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Removing Raw Features: In this variant, raw features are not input into Deep Classifier and only the generated new features are fed to Deep Classifier. • Removing New Features: This variant removes Feature Generation. Actually, it is equivalent to IPNN. • Applying MLP for Feature Generation: Feature Generation is replaced by MLP which takes the neurons in each layer as new features. This variant uses the same hidden layers and generates the same number of features in each layer as FGCNN. • Removing Recombination Layer: This variant is to evaluate how Recombination Layer complements CNN to capture global feature interactions. The Recombination Layer is removed from Feature Generation so that the output of pooling layer serves as new features directly. The number of generated new features in each layer keeps the same as FGCNN. As shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Performance of Different FGCNN Variants</figDesc><table><row><cell>Criteo</cell><cell>Avazu</cell><cell>Huawei App Store</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Features in numerical form are usually transformed into categorical form by bucketing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">According to its function, we will call MLP with one hidden layer as recombination layer later.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">http://www.kaggle.com/c/avazu-ctr-prediction</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Due to that Feature Generation and Deep Classifier interrelate with each other, the curve has some fluctuations.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks based Click-Through Rate Prediction with Multiple Feature Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training and Testing Low-degree Polynomial Data Mappings via Linear SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1471" to="1490" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep CTR Prediction in Display Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian Sheng</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">XGBoost:A Scalable Tree Boosting System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengtze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar Deepak</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<title level="m">Wide &amp; Deep Learning for Recommender Systems. conference on recommender systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deepfm: a factorization-machine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04247</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DeepFM: An End-to-End Wide &amp; Deep Learning Framework for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Xiuqiang He, and Zhenhua Dong</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural Factorization Machines for Sparse Predictive Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical Lessons from Predicting Clicks on Ads at Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Workshop on Data Mining for Online Advertising</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Field-aware Factorization Machines for CTR Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sheng Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating conversion rate in display advertising from past erformance data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkay</forename><surname>Kuang Chih Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Orten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigkdd International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="768" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating conversion rate in display advertising from past erformance data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkay</forename><surname>Kuang Chih Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Orten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Dasdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigkdd International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="768" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongxia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05170</idno>
		<title level="m">xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A convolutional click prediction model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00311</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to Generate Reviews and Discovering Sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">User Response Learning for Directly Optimizing Campaign Performance in Display Advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Kan Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting clicks:estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Failures of Gradient-Based Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalevshwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Shammah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Crossing:Web-Scale Modeling without Manually Crafted Combinatorial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Practical Lessons for Job Recommendations in the Cold-Start Scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Recommender Systems Challenge. 4.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep &amp; cross network for ad click predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ADKDD&apos;17</title>
		<meeting>the ADKDD&apos;17</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07435</idno>
		<title level="m">Deep learning based recommender system: A survey and new perspectives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep Learning over Multifield Categorical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep Learning over Multifield Categorical Data: A Case Study on User Response Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianming</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1059" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
