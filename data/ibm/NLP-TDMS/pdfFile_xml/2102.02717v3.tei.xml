<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RoI Tanh-polar Transformer Network for Face Parsing in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RoI Tanh-polar Transformer Network for Face Parsing in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face parsing</term>
					<term>in-the-wild dataset</term>
					<term>head pose augmentation</term>
					<term>Tanh-polar representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face parsing aims to predict pixel-wise labels for facial components of a target face in an image. Existing approaches usually crop the target face from the input image with respect to a bounding box calculated during pre-processing, and thus can only parse inner facial Regions of Interest (RoIs). Peripheral regions like hair are ignored and nearby faces that are partially included in the bounding box can cause distractions. Moreover, these methods are only trained and evaluated on near-frontal portrait images and thus their performance for in-the-wild cases has been unexplored. To address these issues, this paper makes three contributions. First, we introduce iBugMask dataset for face parsing in the wild, which consists of 21, 866 training images and 1, 000 testing images. The training images are obtained by augmenting an existing dataset with large face poses. The testing images are manually annotated with 11 facial regions and there are large variations in sizes, poses, expressions and background. Second, we propose RoI Tanhpolar transform that warps the whole image to a Tanh-polar representation with a fixed ratio between the face area and the context, guided by the target bounding box. The new representation contains all information in the original image, and allows for rotation equivariance in the convolutional neural networks (CNNs). Third, we propose a hybrid residual representation learning block, coined HybridBlock, that contains convolutional layers in both the Tanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of different shapes in CNNs. Through extensive experiments, we show that the proposed method improves the state-of-the-art for face parsing in the wild and does not require facial landmarks for alignment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face parsing is a fundamental facial analysis task: it predicts per-pixel semantic labels in a target face. It provides useful features for many downstream applications, such as face recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, face beautification <ref type="bibr" target="#b2">[3]</ref>, face swapping <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, face synthesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, facial attribute recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, and facial medical analysis <ref type="bibr" target="#b13">[14]</ref>. Recently, methods based on deep Convolutional Neural Networks (CNNs), especially Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b14">[15]</ref>, have achieved impressive results on this task. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Two unique aspects distinguish face parsing from generic image parsing. The first is that face parsing does not parse the entire image but only the target face specified by a bounding box, whereas image parsing predicts a label for every pixel in the image. How to preprocess the input with bounding boxes remains an underexplored problem. Most methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> crop the face with a fixed margin and resize the cropped patch to the same dimension, which we refer to as crop-and-resize the face area. Such methods ignore hair because the margin around the hair area is hard to determine. If the selected margin is too small, the hair region would be cut off. If it is too big, too many background pixels and / or nearby faces could be included in the cropped patch, causing significant distractions to the model. Another pre-processing method is to use facial landmarks for face alignment <ref type="bibr" target="#b15">[16]</ref> such that the face is appropriately rotated. We refer to this method as align. The landmarks can be jointly obtained with the face bounding boxes <ref type="bibr" target="#b24">[25]</ref>. The main problem is that a good template has to be carefully chosen for alignment.</p><p>The other challenge is that in-the-wild images are underrepresented in existing benchmarks. Four most widely used face parsing datasets are Helen <ref type="bibr" target="#b25">[26]</ref>, LFW-PL <ref type="bibr" target="#b26">[27]</ref>, CelebAMask-HQ <ref type="bibr" target="#b27">[28]</ref> and LaPa <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_0">Figure 2</ref> shows exemple images from these datasets and Section 3 compares them in detail. Most images from Helen and LaPa are portraits, which means that only one large face in frontal view is present near the centre. CelebAMask-HQ contains images synthesised from CelebA <ref type="bibr" target="#b29">[30]</ref> using super-resolution. All faces are aliened using landmarks and resized to the same size. As such, the resulted images contain very little context information. Similarly, LFW-PL is a subset of LFW <ref type="bibr" target="#b30">[31]</ref> and the faces are aligned using landmarks. Hence, how models perform under in-the-wild conditions remains unexplored.</p><p>To tackle the first challenge, we propose RoI Tanhpolar transform (RT-Transform), that transforms the entire image into a fixed-size representation in the Tanhpolar coordinate system based on the target bounding box. <ref type="figure" target="#fig_4">Figure 6</ref> illustrates the transform process. As a fully invertible transform, it preserves all contextual information. Moreover, regardless of the face's actual size in the input image, the ratio between the face and the background remains fixed at 76% : 24% in the transformed representation. In the Tanh-polar coordinate system, planar convolutions correspond to groupconvolutions <ref type="bibr" target="#b31">[32]</ref> in rotation. Thus, Convolutional Neural Networks (CNNs) applied in the tanh-polar space would produce a representation that is equivariant to rotations in the original Cartesian space.</p><p>We further introduce Hybrid Residual Representation Learning Block (HybridBlock) that uses RT-Transform to create hybrid representations in the residual blocks. A HybridBlock consists of two 3 × 3 convolutional layers, one in the Tanh-polar coordinate system and the other in the Tanh-Cartesian coordinate system. They are operating on different-shaped receptive fields and their outputs are concatenated in the Tanh-polar system to obtain a hybrid representation. By stacking HybridBlocks, we arrive at HybridNet, a backbone network that takes as input an image transformed by RT-Transform and the target bounding box, and outputs a hybrid representation for face parsing. We then add the vanilla FCN decoder and inverse RT-Transform. The resulting framework, called RoI Tanh-polar Transformer Network (RT-Net), is shown in <ref type="figure">Figure 1</ref>.</p><p>To tackle the second challenge of lacking suitable benchmarks, we present iBugMask dataset that contains 22, 866 in-the-wild images. For the training set of 21, 866 images, we use a face profiling method <ref type="bibr" target="#b32">[33]</ref> to rotate the faces from images in Helen dataset with respect to the yaw angle, creating many large-pose and profile faces. For the 1, 000 testing images, per-pixel manual annotations for 11 regions including hair are provided. The curated images contain large variations in pose, expression, size and background clutter (see <ref type="figure" target="#fig_0">Figure 2</ref>). Extensive experiments show that iBugMask dataset is more challenging than other benchmarks and models trained on iBugMask improves performance under both intra-dataset and cross-dataset evaluation.</p><p>In summary, we offer the following contributions:</p><p>• We propose RoI Tanh-polar Transform for face parsing in the wild that transforms the target face to the Tanh-polar coordinate system based on the bounding box, preserving the context and allowing CNNs to learn representations equivariant to rotations.</p><p>• We propose Hybrid Residual Representation Learning Blocks, that extracts a hybrid representation by applying convolutions in both Tanh-polar and Tanh-Cartesian coordinates.</p><p>• We present iBugMask dataset, a novel in-the-wild face parsing benchmark that consists of more than 22 thousand images.</p><p>• We conduct extensive experiments and show that the overall framework RTNet improves the stateof-the-art on all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Increasing research effort has been devoted to face parsing due to its potential application in various face analysis tasks. In this section, we briefly review four groups of relevant works, i.e. 1) the face parsing benchmarks, 2) the face parsing methods, 3) works on scene parsing and 4) representation learning in polar space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Parsing Benchmarks</head><p>Publicly available face parsing benchmarks are comparatively scarce, mainly due to the significant amount of effort required for pixel-level annotations. Currently, two most widely-used benchmarks are LFW-PL <ref type="bibr" target="#b26">[27]</ref> and Helen <ref type="bibr" target="#b25">[26]</ref>.</p><p>The Helen dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26]</ref>  image matting algorithms <ref type="bibr" target="#b25">[26]</ref> and may not be fully accurate. Despite such disadvantages, Helen was still the only publicly-available face parsing dataset with an acceptable amount of training data for several years, and thus it has been a popular choice for evaluating face parsing methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. The LFW-PL dataset <ref type="bibr" target="#b26">[27]</ref> consists of 2, 972 facial images selected from the Labeled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b30">[31]</ref>. To obtain dense annotations, each facial image is first automatically segmented into superpixels and those superpixels are subsequently labelled as one of the following categories: facial skin, hair and background.</p><p>Recently, two large-scale face parsing datasets were released, which are CelebAMask-HQ <ref type="bibr" target="#b27">[28]</ref> and LaPa <ref type="bibr" target="#b28">[29]</ref>. Although the number of annotated samples are greatly increased, the facial images included in those two datasets are not strictly in-the-wild, since they have already been pre-processed in an unrecoverable way. In CelebAMask-HQ, the resolutions of facial images are intentionally enlarged through the superresolution technique <ref type="bibr" target="#b35">[36]</ref>, while most faces are aligned to be frontal and centralised. Besides, the background region usually comprises a small portion of the whole facial image, i.e. most environmental information has been discarded. Similar situations can be discovered in the Lapa dataset in which faces are also cropped and aligned with limited background information preserved.</p><p>Compared to those datasets, our proposed iBugMask dataset is the only face parsing benchmark consisting of fully in-the-wild images. The facial samples are neither cropped nor aligned, and we also preserve almost all the background information. It covers large variations in poses, illuminations, occlusions and scenes. A detailed comparison between the iBugMask dataset and the existing benchmarks is provided in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Parsing Methods</head><p>Face parsing is the task of pixel-wisely labelling given facial images. Earlier works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b25">26]</ref> on face parsing usually leveraged holistic priors and handcrafted features. Warrell et al. <ref type="bibr" target="#b36">[37]</ref> modelled the spatial correlations of facial parts with Conditional Random Fields (CRFs). Smith et al. <ref type="bibr" target="#b25">[26]</ref> applied SIFT features to select exemplars in facial parts and propagate the labels of these exemplars to generate complete segmentation maps. A hybrid method was proposed in <ref type="bibr" target="#b26">[27]</ref> that combined the strength of both CRFs and Restricted Boltzmann Machine in a single framework to model global and local facial structures. The idea of utilising engineering-based features can also been seen in other works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Those approaches are typically time-consuming and cannot generalise well to different scenarios, and thus they have been gradually replaced by deep-learning-based methods with encouraging performance. . CelebAMask-HQ <ref type="bibr" target="#b27">[28]</ref> and LFW-PL <ref type="bibr" target="#b26">[27]</ref> contain well-aligned faces and little context information. Helen <ref type="bibr" target="#b25">[26]</ref> contains mostly portrait images where faces are big and near the centre. LaPa <ref type="bibr" target="#b28">[29]</ref> contains face images with some variations in pose and occlusion but the faces are cropped and centred. By contrast, iBugMask contains large variations in expression, pose and background and all background information is preserved (see Section 3).</p><p>State-of-the-art performance on face parsing is mostly achieved by deep learning methods. Liu et al. <ref type="bibr" target="#b17">[18]</ref> incorporated CNNs into CRFs and proposed a multi-objective learning method to model pixel-wise likelihoods and label dependencies jointly. An interlinked CNN was present in <ref type="bibr" target="#b40">[41]</ref> to detect different facial parts, while this architecture cannot generate semantic labels for large-scale components like facial skin. Luo et al. <ref type="bibr" target="#b16">[17]</ref> applied multiple Deep Belief Networks to detect facial parts and accordingly built a hierarchical face parsing framework. Jackson et al. <ref type="bibr" target="#b20">[21]</ref> employed facial landmarks as a shape constraint to guide Fully Convolution Networks (FCNs) for face parsing. Multiple deep methods including CRFs, Recurrent Neural Networks (RNNs) and Generative Adversarial Networks (GAN) were integrated by authors of <ref type="bibr" target="#b41">[42]</ref> to formulate an end-to-end trainable face parsing model, while the facial landmarks also served as the shape constraints for segmentation predictions. The idea of leveraging shape priors to regularise segmentation masks can also be found in the Shape Constrained Network (SCN) <ref type="bibr" target="#b42">[43]</ref> for eye segmentation. In <ref type="bibr" target="#b23">[24]</ref>, a spatial Recurrent Neural Networks was used to model the spatial relations within face segmentation masks. A spatial consensus learning technique was explored in <ref type="bibr" target="#b18">[19]</ref> to model the relations between output pixels, while graph models was adopted in <ref type="bibr" target="#b34">[35]</ref> to learn the implicit relationships between facial components. To better utilise the temporal information of sequential data, authors of <ref type="bibr" target="#b43">[44]</ref> integrated ConvLSTM <ref type="bibr" target="#b44">[45]</ref> with the FCN model <ref type="bibr" target="#b14">[15]</ref> to simultaneously learn the spatial-temporal information in face videos and to obtain temporally-smoothed face masks. In <ref type="bibr" target="#b45">[46]</ref>, a Reinforcement-Learning-based key scheduler was introduced to select online key frames for video face segmentation such that the overall efficiency can be globally optimised.</p><p>Most of those methods assume the target face has already been cropped out and are well aligned. Moreover, they often ignore the hair class due to the unpredictable margins for cropping the hair region. The most related work to our paper is the RoI Tanh Warping <ref type="bibr" target="#b15">[16]</ref> which proposed to warp the entire image using the Tanh function. However, there are several limitations in this work. The warping operation requires not only the facial bounding boxes but also the facial landmarks, which can be overly redundant. Additionally, the warped image is still in Cartesian space and cannot benefit from the rotation-equivariant property in polar space. Moreover, multiple sub-networks are employed to learned the shapes of inner facial parts and hair separately, and these sub-networks need to be trained with different loss functions, making the pipeline trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Scene Parsing</head><p>Scene parsing is to segment an image into different image regions associated with semantic scene labels such as roads, pedestrians, cars, etc. Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b14">[15]</ref> is the first critical milestone of applying deep learning techniques in this field. Via replacing fully connected layers with convolutional ones, FCNs successfully adapt classical CNN classification models like VGG-16 <ref type="bibr" target="#b46">[47]</ref> or ResNet <ref type="bibr" target="#b47">[48]</ref> to solve scene parsing tasks. Following FCNs <ref type="bibr" target="#b14">[15]</ref>, a wide variety of scene parsing methods have been developed, including the application of dilated (atrous) convolutions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, the encoder and decoder structures <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>, the spatial pyramid architectures <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref> the involvement of attention mechanisms <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>, utilising Neural Architecture Search (NAS) techniques <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, etc. PSPNet <ref type="bibr" target="#b55">[56]</ref> proposed a spatial pyramid pooling module that adopts a set of spatial pooling operations of different sizes to increase the variety of receptive fields of the network. SPNet <ref type="bibr" target="#b56">[57]</ref> extended the pooling module by introducing the strip pooling module to capture long-range banded context. Deeplab family <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b53">54]</ref> devised an Atrous Spatial Pyramid Pooling (ASPP) module that consists of three parallel dilated convolutional layers to capture multi-scale context. UNet <ref type="bibr" target="#b62">[63]</ref> introduced skip connections between the encoder and the decoder sub-networks to preserve low-level details, while the details of high resolution features were maintained in HRNet <ref type="bibr" target="#b63">[64]</ref> by branching the backbone network. BiSeNet <ref type="bibr" target="#b64">[65]</ref> proposed a bilateral network consisting of a context branch and a spatial branch. Such a two-branch architecture allows BiSeNet to operate with satisfying efficiency while achieving the state-of-the-art performance. Readers are referred to <ref type="bibr" target="#b65">[66]</ref> for a more detailed review of scene parsing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Polar Representation Learning</head><p>Compared with Cartesian coordinate system, polar or log-polar space are not sensitive to certain transformations such as rotations and scaling, and therefore polar representations have been widely studied in image processing and computer vision. Early applications of polar transformations included face detection <ref type="bibr" target="#b66">[67]</ref>, face tracking <ref type="bibr" target="#b67">[68]</ref>, face recognition <ref type="bibr" target="#b68">[69]</ref>, the aggregation of hand-crafted descriptors <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>, etc. Recently, how to integrate polar representations with deep CNN models have been increasingly explored. The traditional CNN architectures can be insensitive to translations, i.e. translation equivalence, yet it is not the case for other transformations such as rotations and scaling. Representation learning in polar space, on the other hand, can effectively overcome such limitations through its equivariances to rotations and scales. Polar Transformer Networks (PTN) <ref type="bibr" target="#b71">[72]</ref> is one of the first attempts to construct a CNN model that maps Cartesianbased images into polar coordinates for better tolerances to transformations like rotations and dilation. In PTN, a shallow network consisting of several 1 × 1 convolutional layers first scans the whole image to predict a polar origin. This predicted origin together with the input image are then fed into a differentiable polar trans-former module to generate image representations in logpolar systems. The obtained polar representation is invariant with respect to the original object locations while rotations and dilations are now shifts, which are handled equivariantly by a conventional classifier CNN. Ebel et al. <ref type="bibr" target="#b72">[73]</ref> utilises PTN to extract polar-based local descriptors for key-point matching, leading to more robust performance. Different from those works, our RoI Tanh-polar Transformer network warps the whole image into a Tanh-polar representation that can emphasise the Region of Interests (RoI) through oversampling in RoI areas and undersampling in the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>In this section, we introduce a new in-the-wild face parsing benchmark, iBugMask, that consists of a poseaugmented training set and a manully-annotated testing set. We compare their characteristics with existing face parsing datasets. The main motivation for the new benchmark is that existing benchmarks only contain faces with limited variations in expression, pose and context information, which makes them less suitable for capturing characteristics of real-world face images. Moreover, large scale training data is key to the success of CNN-based models, but existing face parsing datasets do not provide sufficient training data for such challenging cases. <ref type="figure" target="#fig_0">Figure 2</ref> shows exemplar images from different face parsing benchmarks with their colour-coded labels overlaid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of Existing Benchmarks</head><p>CelebAMask-HQ [28] contains 30, 000 synthesised faces from the CelebA dataset <ref type="bibr" target="#b29">[30]</ref>. All images are scaled to 512 × 512 and faces are well-aligned at the centre using facial landmarks. Background information is either removed or blurred. 19 facial classes are labelled: background, skin, left/right brow, left/right eye, upper/lower lip, left/right ear, nose, inner mouth, hair, hat, eyeglass, earring, necklace, neck, and cloth. The dataset is divided into 24, 183 images for the training, 2, 993 images for the validation, and 2, 824 images for the testing.</p><p>LFW-PL [27] contains 2, 927 images of resolution 250 × 250 with faces aligned in the centre. Face and hair regions are annotated using superpixel-based methods, thus resulting in inaccurate labels. The dataset is divided into 1, 500 images for the training, 500 images for the validation, and 927 images for the testing.</p><p>Helen <ref type="bibr" target="#b25">[26]</ref> is the most popular face parsing benchmark and it contains 2, 330 real-world images with rich context information. 11 semantic labels are annotated: background, skin, left/right brow, left/right eye, upper/lower lip, inner mouth, nose and hair. There are significant annotation errors for facial skin and hair classes in the training set, as discussed in <ref type="bibr" target="#b15">[16]</ref>, because these labels were automatically generated using image matting. The authors of Helen only cleaned the testing set to guarantee fair comparison in the test set. Helen dataset is divided into 2, 000 images for the training, 230 images for the validation and 100 images for the testing.</p><p>LaPa <ref type="bibr" target="#b28">[29]</ref> is a face parsing dataset containing more than 22, 176 facial images with relatively more variations in expression, pose and occlusion. The same 11 semantic classes are annotated as in Helen and the annotation process was guided by 106-point facial landmarks. The dataset is divided into 18, 176 images for the training, 2, 000 images for the validation, and 2, 000 images for the testing. The faces contain some variations in pose and occlusion but the background and hair region are largely removed since the faces are cropped with a hand-picked margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">iBugMask: An In-the-wild Face Parsing Benchmark</head><p>The proposed iBugMask consists of two parts: a training set obtained by pose augmentation and a manually curated testing set. The training set contains 21, 866 images while the testing set contains 1, 000 images. We describe these two parts in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">A Large-Pose Augmented Training Set</head><p>For machine learning models to learn to parse faces with large variations in head poses, the training set needs to contain a balanced distribution over poses. However, existing datasets contain faces mostly with absolute yaw angles less than 45 degrees. This means that models trained on these datasets cannot handle faces with extreme poses.</p><p>We propose to solve this problem by synthesising training faces with large poses. First, we examined the training set Helen, and manually corrected the labelling errors as in <ref type="bibr" target="#b15">[16]</ref>. Next, we augmented the data with a face profiling method <ref type="bibr" target="#b32">[33]</ref> that has been applied to augment face alignment datasets. One major advantage of this method is that it creates 3D meshes for both internal face and external face regions, which preserve the unpredictable hair regions as well as important context information for face parsing. Through face profiling, we augment the training set of Helen to a large scale one with many faces having large variation in head poses. With the fitted 3D model, we gradually enlarge the yaw angle of image at the step of 5 • until 90 • . Considering that the fidelity of a synthesised face is negatively related with the ∆yaw, we resample the augmented images of each face with probabilities 0.8 ∆yaw/5 • . In Table 1, we compare our training set with other training sets and show that ours contains much more variations in pose, facial expression, and background. We conduct  extensive experiments in Sec. 5. The results show that all models trained on our augmented training set improve over their counterparts trained on other datasets for in-the-wild face parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">A Manually Curated Testing Set</head><p>In in-the-wild images, faces can appear at any location in an image, with various distracting contextual information around it. In existing benchmarks, the target faces are cropped and centred by the data providers, largely removing background, part of hair and other faces. This introduces bias and evaluating methods on pre-processed images does not honestly reveal their robustness to the distracting context noise.</p><p>To fairly evaluate face parsing models under in-thewild conditions, we present iBugMask dataset that contains 1, 000 challenging face images and manuallyannotated labels for 11 semantic classes: background,  <ref type="figure">Figure 4</ref>: Distributions for facial parts in iBugMask. Left: pixel distribution for facial parts. We compare different facial parts in the first subplot. We merge the inner parts to compare with skin and hair in the second subplot. Right: region distribution for facial parts. Inner mouth the least seen region in the dataset.</p><p>facial skin, left/right brow, left/right eye, nose, upper/lower lip, inner mouth and hair. The images are curated from challenging in-the-wild face alignment datasets, including 300W <ref type="bibr" target="#b73">[74]</ref> and Menpo <ref type="bibr" target="#b74">[75]</ref>. Compared to the existing face parsing datasets, iBugMask contains in-the-wild scenarios such as "party" and "conference", which include more challenging appearance variations or multiple faces. There is a larger number of profile faces. More expressions other than "neutral" and "smile" are also included (e.g. "surprise" and "scream"). Examples can be found in the rightmost column of <ref type="figure" target="#fig_0">Figure 2</ref>. <ref type="table" target="#tab_4">Table 2</ref> compares characteristics of different benchmarks. We use 3DDFA <ref type="bibr" target="#b32">[33]</ref> to estimate the yaw angles with facial landmarks obtained by FAN <ref type="bibr" target="#b75">[76]</ref>. We use the facial expression classifier proposed by Want et al. <ref type="bibr" target="#b76">[77]</ref> to estimate the facial expressions. <ref type="figure" target="#fig_3">Figure 5</ref> shows the absolute yaw angle distributions of benchmarks. Finally, <ref type="figure">Figure 4</ref> shows the pixel and region distributions in the testing set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>We introduce the RoI Tanh-polar Transformer Network for face parsing in the wild. <ref type="figure">Figure 1</ref> shows the overall framework: given an in-the-wild image in Cartesian coordinates and a bounding box of the target face,  In in-the-wild face parsing, the target face is specified by a bounding box and is often not centralised. A common pre-processing step is to extend the facial bounding box with a certain margin and then to crop out the facial images, which are further resized into a certain resolution depending on the employed deep models. We refer to this pre-processing technique as crop-and-resize. In this pre-processing approach, however, the cropping margin needs to be carefully selected. An overly loose margin may introduce irrelevant and distracting information, e.g. other faces, while a margin that are too narrow can lead to the ignorance of useful image regions like hairs, both of which are undesirable in the face parsing task. Another pre-processing method is to use facial landmarks for face alignment <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref> such that the face is appropriately rotated. We refer to this method as align. The landmarks can be jointly obtained with the face bounding boxes <ref type="bibr" target="#b24">[25]</ref>.</p><p>To overcome the limitations in the crop-and-resize method and eliminate the need for facial landmarks, we propose the RoI Tanh-polar transform that warps the whole image to a canonical representation in the Tanhpolar space. Compared with the classical crop-andresize and align, the only prerequisite of our method is the detected bounding box. Besides, our mapping can also introduce rotation equivariance to CNN models because of the polar-based representations. The RT-Transform is illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>. Let v = (x, y) represent the Cartesian coordinate of a point in the original image, and let w and h represent the width and height of the bounding box, respectively. We select the centre of the bounding box as the polar origin. We first fit an ellipse e to the target bounding box to the bounding rectangle, described by</p><formula xml:id="formula_0">x 2 a 2 + y 2 b 2 = 1,<label>(1)</label></formula><p>where a = 0.5 w √ π , b = 0.5 h √ π , and w and h are the width and height of the bounding box. We then define the Tanh-polar coordinate system by an injective map f :</p><formula xml:id="formula_1">f ( v) := (tanh( y x ), tanh( || v|| || v e || ))<label>(2)</label></formula><p>where v e = (x e , y e ) is the vector on the broader of the target face ellipse e and v e and v are parallel. A new representation is constructed by resampling the input image over a rectangular grid in the Tanh-polar coordinate system. Following typical transformer networks <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b78">79]</ref>, we use bilinear interpolation for points that do not coincide with the pixel locations in the input image. We name this as RoI Tanh-polar transform (RT-transform). It can be observed that: 1) compared to representations obtained by crop-and-resize, all information in the input image is preserved in the new representation; 2) the normalisation with v e ensures that the target face always occupies around 76% (since tanh(1) = 0.76). It is worth noting that the proposed RT-Transform is invertible and differentiable. Therefore, not only can the input RGB images be transformed but also the intermediate feature maps in CNNs. Rotation Equivariance. To handle rotation of the target face, previous face parsing works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b15">16]</ref> rely on transforming facial landmarks to canonical locations correct the rotation of the target face. We show that using the Tanh-polar representation can eliminate such pre-processing step. The Tanh-polar coordinate system by definition is a canonical coordinate system <ref type="bibr" target="#b79">[80]</ref> for the rotation group SO(2) with angle θ ∈ [−π, π]. This is because for the rotation transformation T θ v = (x cos θ − y sin θ, x sin θ + y cos θ), the Tanh-polar coordinate system satisfies <ref type="bibr" target="#b78">[79]</ref> </p><formula xml:id="formula_2">f (T θ v) = f ( v) + e θ ,<label>(3)</label></formula><p>where e θ = (θ, 0). Thus, a rotation transformation T θ appears as a translation by (θ, 0) under the Tanhpolar coordinate system f . As a result, the planar convolution that is self-consistent with respect to translation <ref type="bibr" target="#b78">[79]</ref> in f is now equivalent to SO(2) groupconvolution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b71">72]</ref> in the Cartesian space.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hybrid Residual Representation Learning Block</head><p>Using the Tanh-polar representation as input to CNNs, rotation equivariance is achieved but translation equivariance may be lost. To overcome this, we propose Hybrid Residual Representation Learning Block, dubbed as HybridBlock, a CNN building block similar to the Residual Block <ref type="bibr" target="#b47">[48]</ref>.</p><p>The incentive of designing HybridBlock is to have two branches of convolutions learn representations that are complementary. One branch (Tanh-polar branch) learns the rotation equivariant representations while the other branch (Tanh-Cartesian branch) learns translation equivariant representations. The detailed components of a HybridBlock is depicted on the right in <ref type="figure">Figure 1</ref>.</p><p>We define Tanh-Cartesian coordinate system by</p><formula xml:id="formula_3">f TC ( v) := (tanh( x || v e || ), tanh( y || v e || )).<label>(4)</label></formula><p>The input to HybridBlock is a Tanh-polar representation X T P of shape (h, w, c). The residual path uses a stack of 1 × 1, 3 × 3 and 1 × 1 convolutions following the bottleneck design <ref type="bibr" target="#b47">[48]</ref>. The first 1 × 1 conv layer is used to reduce the channel dimension and its output feature maps are transformed to the Tanh-Cartesian space. In each coordinate system a 3 × 3 conv layer is used to compute feature maps, which are then concatenated in the Tanh-polar space. The last 1 × 1 conv layer restores the channel dimension so the residual representation can be added to the input X T P .</p><p>Direct Transformation from Tanh-polar to Tanh-Cartesian. To obtain Tanh-Cartesian representations, a naive approach is to inverse-transform from f to Cartesian and then resample with Equ. 4. However, iterated resampling will degrade image quality and amplify the influence of interpolation artefacts. To circumvent  this issue, we find the correspondence between the sampling grids in both coordinates and directly resample the Tanh-polar representation.</p><p>Hybrid Receptive Field. The receptive field (RF) is the region in the input space that a particular neuron is looking at. The two 3 × 3 convolution layers in different coordinate systems have RFs of different shapes. The Tanh-polar one has the arc-shaped RF while the other has the rectangle-shaped RF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50-layer HybridNet.</head><p>We follow the design of ResNets <ref type="bibr" target="#b47">[48]</ref> and stack HybridBlocks to create a new backbone network HybridNet50. Thanks to the grouped conv1x1 and the conv3x3 with halved channels, the overall number of parameters are less than the ResNet50 backbone (23.5 M versus 17.8M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">RTNet: the Overall Framework</head><p>With the previously introduced components, we now describe the overall framework of RoI Tanh-polar transformer network (RTNet) for face parsing in the wild. As in <ref type="figure">Figure 1</ref>, RTNet is based on the FCN framework <ref type="bibr" target="#b14">[15]</ref>. An input image I of arbitrary resolution with the target bounding box is transformed to I tp in the Tanh-polar space. By default, the size of I tp is set to be 512 × 512. Next, HybridNet-50 is used to extract features from I tp , followed by a naive FCN decoder head to predict the segmentation mask in the Tanh-polar space. Finally, the segmentation mask is inverse-transformed to Cartesian as the final output that has the same resolution with the input image I. FCN Decoder. We use the FCN-8s <ref type="bibr" target="#b14">[15]</ref> decoder to predict the masks. More advanced decoders like ASPP <ref type="bibr" target="#b53">[54]</ref> require dedicated hyperparameter-tuning that may largely affect the performance. The adopted decoder consists of two conv3x3 layers and a bilinear upsampling layer to map the feature maps to pixel-wise prediction logits. Loss function. We use Cross-Entropy loss loss CE and Dice loss loss dice <ref type="bibr" target="#b80">[81]</ref>. Two losses are jointly optimised with a factor of λ and the overall loss l is</p><formula xml:id="formula_4">l = λloss CE + (1 − λ)loss dice .<label>(5)</label></formula><p>The losses are computed on the Tanh-polar coordinates since the outputs are of the same size and the computation can be batched and accelerated. Mixed padding. Zero-padding is used in most CNNs to keep feature map size.This is not for the Tanh-polar representation, as it is periodic about the angular axis. We use wrap-around padding on the vertical dimension and replication padding on the horizontal dimension. Bounding box augmentation. To improve robustness of RTNet, we augment the input bounding box during training time by adding a random shift and a random scaling. The augmentation can also be conducted during test time to the input image multiple times, and the inverse-transformed prediction masks can be averaged for smoother results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Baseline Methods</head><p>We adopt the following criteria to select baseline methods. First, the model should be able to parse inner facial components as well as hair. Second, it is opensourced and we are able to re-produce the reported performance by re-training the model from scratch. Third, the number of hyper-parameters has to be relatively small so that the performance does not rely on advanced training techniques. This allows that the same training setup can be applied and training can finish with reasonable computing resources and time. The selected models include the classic models like FCN <ref type="bibr" target="#b14">[15]</ref>, as well as the advanced ones, such as Deeplabv3+ <ref type="bibr" target="#b53">[54]</ref> and SP-Net <ref type="bibr" target="#b56">[57]</ref>. We collected their open-sourced codes and built an unified benchmarking codebase such that the same training and evaluation procedures are ensured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Training and Evaluation</head><p>Data. Each model is trained with four datasets, i.e. Helen <ref type="bibr" target="#b25">[26]</ref>, CelebAMask-HQ <ref type="bibr" target="#b27">[28]</ref>, LaPa <ref type="bibr" target="#b28">[29]</ref> and ours, among which Helen, LaPa and ours have the same set of labelling classes so the models trained on them can be evaluated directly. CelebAMask-HQ has more labelling classes and we assigned those additional classes to the background during training and evaluation. The target bounding box in each image is generated from the groundtruth mask to eliminate the bias from face detection. Evaluation. We adopt two popular metrics, intersection over union (IoU), and F1 score (F1) for evaluation. We report the metrics for all foreground classes and their mean. The predicted masks are evaluated on the original image scale. For methods with crop-andresize pre-processing, we resize the predicted masks to the size of the cropped image and then zero-pad it to match the original image resolution. For our RTNet, we apply inverse RT-Transform to the predicted masks. We did not employ other common evaluation techniques such as multi-scale, flipping or multi-cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Implementation Details</head><p>We use PyTorch <ref type="bibr" target="#b81">[82]</ref> to implement all baselines and our methods. The backbone networks are pre-trained on ImageNet <ref type="bibr" target="#b82">[83]</ref>. We use Stochastic Gradient Descent (SGD) to optimise the losses. The initial learning rates are set to 0.01 and the poly learning rate annealing schedule is adopted with power = 0.9. All methods are trained for 50 epochs and early stopping is adopted if the mean IoU on the validation set stops growing for 15 epochs. For all methods, we apply random scaling in the range of [0.5, 2.0], random horizontal flip and random brightness as data augmentation methods during training. For our methods, we transform the entire image to 512 × 512 with our RT-Transform. Batch size is set to 4 in all experiments. All training and evaluation are conducted on two RTX 2080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results on iBugMask</head><p>We compare our model with different baselines that use align as the input pre-processing method. The alignment templates are adopted from open-sourced Arc-Face [84] library 1 . These templates have been shown successful in face recognition tasks. We report results for all facial parts. Eyebrows, eyes, lips and inner mouth are merged to Inner Parts. <ref type="table" target="#tab_6">Table 3</ref> shows the benchmarking results. Our first observation is that iBugMask is challenging and cannot be readily solved. Compared with existing benchmarks, the models' performance on iBugMask is not saturated. For example, the mean F1 score on Helen has reached over 90% but our best results on iBugMask are around 86%. We believe iBugMask can serve as a challenging benchmark for face parsing in the wild.</p><p>Our second observation is that when using face alignment for pre-processing, the baseline models perform comparably on inner parts. However, the performance on hair is largely degraded because the templates cannot handle different hairstyles. In contrast, RT-transform allows our model to capture complete hair and face regions without being cut out.</p><p>Lastly, without landmarks and alignment, our RTNet perform better than other methods in eyes, eyebrows, skin and hair regions, and comparably in nose, lips and mouth. When compared to the baseline FCN, we observe a large improvement in eyebrows and eyes. This could be attributed to the fact that the hybrid representation can better capture elongated regions. <ref type="figure">Figure 9</ref> visualises the prediction results of different methods, and our RTNet can better capture the varying hair styles, profile poses, occlusions, etc., which again verifies the superior performance of our method under in-the-wild scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>We conduct extensive ablation studies to better understand the working mechanisms in RTNet. All variants in this section were trained on pose-augmented images and evaluated on iBugMask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Effectiveness of RT-Transform</head><p>We compare the performance of 4 pre-processing techniques: 1) Resize: resizing all the input images to 512×512 with zero-padding to preserve the aspect ratios; 2) Crop-and-resize: cropping the face out with 40% margin and then resizing the cropped face to 512 × 512; 3) Align: we use 5 landmarks returned by Reti-naFace <ref type="bibr" target="#b24">[25]</ref> to align the target faces using the opensource library and warp to 512 × 512; 4) RT-Transform: warping the whole image to a representation of size 512 × 512 in the Tanh-polar space with the proposed RT-Transform. <ref type="table" target="#tab_7">Table 4</ref> shows the F1 scores of different preprocessing methods on iBugMask. It can be seen that resizing the input images to the same size gives the lowest accuracy. This is in line with our expectation, as faces vary largely in size and uniformly resizing them will cause confusions. As for the crop-and-resize approach, only a small amount of improvement is observed, especially for the Hair class. This is potentially because the pre-defined cropping margin cannot guarantee a full coverage of the hair region, which will cause accuracy loss on such regions. The alignment method gives competitive results on inner parts and facial skin. However, the performance on hair has degraded by a large amount because the warping template cannot account for different hair styles. In contrast, our RT-Transform achieves the best performance on all three categories, and this can be attributed to the proposed Tanh-Polar transform that can emphasise the facial region while preserving all the contextual background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Design of HybridBlock</head><p>We also conduct ablation studies to verify the design of HybridBlocks. We started with the Resnet50 backbone and gradually replace the residual blocks with Hy-bridBlocks at different places of the network. In particular, the backbone comprises a stem layer and 4 stages of residual blocks, and we followed common practice <ref type="bibr" target="#b84">[85]</ref> to replace blocks in the last three stages with the proposed HybridBlock which has fewer parameters. Results of different replacing stages are reported in <ref type="table" target="#tab_8">Table 5</ref>, and we can observe that HybridBlocks can always introduce performance improvement with fewer parameters. Besides, the highest accuracy is achieved when using HybridBlocks in all the three stages, which demonstrate the effectiveness and the generality of the proposed Hy-birdBlocks. <ref type="table" target="#tab_10">Table 6</ref> quantifies the performance gains of the bounding box augmentation and mix-padding described in Sec. 4.3. The box augmentation can make the model more robust to the bounding box noise. And mixpadding is necessary as the Tanh-polar representation is periodic about the angular axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Bounding box augmentation and mix-padding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effectiveness of Pose-augmented Training Set</head><p>To show the effectiveness of the pose augmentation, we train 6 on 4 different training sets. For simplicity and faster training, we use crop-and-resize with 40% margin to pre-process the input image to obtain a 512 × 512 facial image for the baseline models. We make the following observations:</p><p>Training on pose-augmented images improves all methods. We can also observe that training on poseaugmented images improved all methods, especially on the inner facial parts. This can be reasonably be attributed to that pose-augmented images is constructed in a way that the numbers of faces are balanced across different poses and that in-the-wild information is also preserved. In contrast, CelebAMask-HQ is a synthesised dataset with limited variations in pose and background. Although CelebAMask-HQ contains a larger number of facial images, models trained on this dataset achieve less competitive performance than trained on others.    RTNet consistently outperforms other methods. The results on iBugMask show that our approach outperforms all other methods. Moreover, when trained with our proposed pose-augmented images, RTNet significantly outperform all baselines, especially on the hair class, which indicates that: 1) Compared with other benchmarking datasets, pose-augmented images can better benefit the in-the-wild learning of segmentation models, despite that most of its facial images were generated through the pose augmentation technique, and 2) Different from the baselines, our RTNet can learn from the in-the-wild data more effectively and thus can demonstrate more robust performance on the unconstrained iBugMask dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparison with the State-of-the-arts</head><p>In additional to the self-collected iBugMask dataset, we also train and evaluate our method on various face parsing benchmarks.</p><p>Results on Helen. <ref type="table" target="#tab_2">Table 10</ref> compares our RTNet with other state-of-the-art methods on Helen <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>. Our model achieves slightly better performance on Facial Skin while significantly outperforms others on the Inner Parts and Hair classes.</p><p>Results on LFW-PL.  ily spot that our RTNet consistently exhibits the highest performance on all three categories, which further demonstrates the generality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Model Efficiency</head><p>We also examine the running efficiency of different models by evaluating 1) the number of model parameters, 2) Floating Point Operations per Second (FLOPS) and 3) the actual inference time per sample. All models are measured on the same machine with a GTX1080Ti GPU with an (512, 512, 3) input size. To ensure a fair comparison, we repeat the evaluation process 100 runs for each method and report the average. As shown in <ref type="table" target="#tab_14">Table 9</ref>, our model has the smallest model size and also operates with the fewest FLOPS when compared with three representative face parsing approaches. Although the inference time of our models are slightly longer than that of FCN and SPNet due to the direct sampling between two coordinates, we believe the time difference (16 ms) is tolerable as our method has shown improved performance over others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have approached in-the-wild face parsing from three aspects: data, representation and model. We have proposed a novel benchmark, iBug-Mask, for training and evaluating face parsing methods in unconstrained environment. We have created a largescale training set using pose augmentation and shown its effectiveness. We have solved the dilemma of face cropping and eliminated the need for facial landmarks by proposing a new Tanh-polar representation obtained by the proposed RoI Tanh-polar transform. Equivareriance with respect to rotations has also been achieved with the new representation. HybridBlock is introduced to extract features in both Tanh-polar and Tanh-Cartesian coordinates. We have achieved the state-of-the-art performance on iBugMask as well as other existing face parsing benchmarks. We expect our RT-Transform to be      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples from benchmarks with colour-coded labels (best viewed in colour)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples of face data augmentation using 3DDFA<ref type="bibr" target="#b32">[33]</ref>. The first column shows the original images and the other three columns show synthesised images with different ∆yaw until yaw = 90 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Absolute yaw angle distributions of different testing sets. Yaw is estimated with 3DDFA<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FitFigure 6 :</head><label>6</label><figDesc>Input image and bounding box ( ! , ! , , ℎ) RoI Tanh-polar transform (RT-Transform). (1) Input is an image and the target bounding box. (2) An eclipse e is fitted to the box.(3)The grey patterns depict the Tanh-polar sampling grid. v is an arbitrary vector and v e is on the eclipse e in the same direction as v. (4) Transformed image. Due to the normalisation by v e , the boundary of the face is located at ρ = tanh(1) = 0.76 regardless of the face size. All information is preserved and the proportion between face and background is fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Scale Invariance. Equation 2 shows that the warped image would always have a fixed ratio between the face area and the background area regardless of the face's original size in the input images as long as the provided bounding box has the correct size. This means a model trained in the ROI Tanh-polar space would per-form equally well on small faces as well as on large faces in the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Rotation equivariance. Rotation is reduced to translation in the Tanh-polar coordinate system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Direct</head><label></label><figDesc>Transform to Tanh-Cartesian Direct Transform to Tanh-Polar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Direct transform between Tanh-polar and Tanh-Cartesian coordinates. We do not sample on the original image but directly transform between two coordinates. Translation equivariance is recovered in Tanh-Cartesian coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>includes 2, 330 facial images with 194 landmark annotations that are obtained through Amazon Mechanical Truck. In this dataset, the inner facial components including eyes, eyebrows, nose, inner mouth and upper/lower lips are manually annotated by human, while the ground-truths for the rest facial parts, i.e. facial skin and hairs, are generated via</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HybridBlock</cell></row><row><cell></cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prediction</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell><cell></cell><cell>(h, w, c)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv1x1, group2</cell><cell>(h, w, c)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Split</cell><cell></cell><cell>(h, w, c/2)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Direct Transform to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(h, w, c/2)</cell><cell cols="2">Tanh-Cartesian</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv3x3</cell><cell></cell><cell>Conv3x3</cell></row><row><cell></cell><cell>RT-Transform</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RT-Transform Inverse</cell><cell>(h, w, c/2)</cell><cell cols="2">Tanh-polar Direct Transform to (h, w, c/2)</cell></row><row><cell>512</cell><cell>512</cell><cell>Stem</cell><cell>3x Residual Block</cell><cell>4x HybridBlock</cell><cell>6x HybridBlock</cell><cell>3x HybridBlock</cell><cell>32</cell><cell>32</cell><cell>Conv Layer</cell><cell>Upsampling</cell><cell>Add Concatenate Conv1x1</cell><cell></cell><cell>(h, w, c) (h, w, c) (h, w, c)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Encoder</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Decoder</cell><cell></cell><cell></cell></row></table><note>Figure 1: Left: RoI Tanh-polar Transformer Network (RTNet): a facial image is transformed to Tanh-polar coordinates with RT-Transform. The encoder consists of a Stem layer, one stage of Residual Blocks and three stages of HybridBlocks. Bounding box is used in RT-transform and HybridBlocks to warp tensors between Tanh-polar and Tanh-Cartesian coordinates. The decoder consists of a Conv layer and a Bilinear Upsampling layer. The output mask is transformed back to Cartesian coordinates using inverse RT-Transform. Right: HybridBlock. Yellow rectangles are layers in Tanh-polar space while blue ones are layers in Tanh-Cartesian space. Tuples (h, w, c) are the shape of the output tensor for each operation. "Split" and "Concatenate" operations are performed along the channel dimension (see Section 4.2).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of training sets. Ours has large variations in pose, expression and background.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of existing benchmark datasets. iBugMask has large variations in pose, expression and background.</figDesc><table /><note>the whole image is first projected into the Tanh-polar space through the proposed RoI Tanh-Polar Transform in Section 4.1. We further introduce a deep CNN en- coder named HybridNet to extract semantic features of the Tanh-Polar-warped image. Consisting of several Hybrid Residual Representation Learning blocks (Sec- tion 4.2), the proposed HybridNet takes advantages of both Tanh-Cartesian and Tanh-Polar coordinate sys- tems and thus can generate more robust spatial features. Those features are fed into a FCN decoder [15] to obtain Tanh-polar-based segmentation masks which are then mapped back into the Cartesian coordinate as the final output.4.1. RoI Tanh-Polar Transform 4.1.1. To Crop or Not To Crop?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on iBugMask. F1 scores are reported in percentage. Eyebrows, eyes, lips and inner mouth are merged to Inner Parts. Qualitative results on iBugMask of four methods: FCN<ref type="bibr" target="#b14">[15]</ref>, Deeplabv3+<ref type="bibr" target="#b53">[54]</ref>, SPNet<ref type="bibr" target="#b56">[57]</ref> and ours. Our method can handle large variations in head pose, hair styles, expressions and occlusions.</figDesc><table><row><cell>FCN</cell></row><row><cell>Deeplabv3+</cell></row><row><cell>SPNet</cell></row><row><cell>Ours</cell></row><row><cell>Figure 9:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of our model with different pre-processing methods. F1 scores are reported in percentage.</figDesc><table><row><cell>Hybrid Stages</cell><cell>Inner Parts</cell><cell>Facial Skin</cell><cell cols="2">Hair # Params (in millions)</cell></row><row><cell>-</cell><cell>85.6</cell><cell>90.7</cell><cell>81.2</cell><cell>31.4</cell></row><row><cell>Stage 2</cell><cell>85.8</cell><cell>90.9</cell><cell>81.4</cell><cell>31.2</cell></row><row><cell>Stage 3</cell><cell>85.6</cell><cell>91.6</cell><cell>81.2</cell><cell>29.9</cell></row><row><cell>Stage 4</cell><cell>85.8</cell><cell>91.4</cell><cell>81.4</cell><cell>29.0</cell></row><row><cell cols="2">Stage All 85.8</cell><cell>91.8</cell><cell>81.8</cell><cell>27.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The effectiveness of HybridBlock in different stages of the backbone. F1 scores in percentage are reported.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8</head><label>8</label><figDesc>compares our RTNet with other state-of-the-art methods on LFW-PL<ref type="bibr" target="#b26">[27]</ref>. Our model achieves comparable results in Inner Parts, and outperforms other methods in Facial Skin and Hair.Results on LaPa.Table 11compares results from different methods on LaPa<ref type="bibr" target="#b28">[29]</ref> dataset and we can eas-</figDesc><table><row><cell>BBox augment</cell><cell>Mix-padding</cell><cell>Inner Parts</cell><cell>Facial Skin</cell><cell>Hair</cell></row><row><cell>N</cell><cell>Y</cell><cell>83.8</cell><cell>91.4</cell><cell>80.7</cell></row><row><cell>Y</cell><cell>N</cell><cell>85.0</cell><cell>91.4</cell><cell>80.8</cell></row><row><cell>Y</cell><cell>Y</cell><cell>85.8</cell><cell>91.8</cell><cell>81.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study. Random bounding box augmentation during training time, and mix-padding all contribute to improve the F1 scores (in percent).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of the pose-augmented training set iBugMask-train. Baseline models use crop-and-resize for pre-processing. The mean F1 scores are reported (in percentage).</figDesc><table><row><cell>Methods</cell><cell>Skin</cell><cell>Hair</cell><cell cols="2">Background accuracy</cell></row><row><cell>Liu et al.[18]</cell><cell cols="2">93.93% 80.70%</cell><cell>97.10%</cell><cell>95.12%</cell></row><row><cell cols="3">Long et al.[15] 92.91% 82.69%</cell><cell>96.32%</cell><cell>94.13%</cell></row><row><cell cols="3">Chen et al.[51] 92.54% 80.14%</cell><cell>95.65%</cell><cell>93.44%</cell></row><row><cell cols="3">Chen et al.[86] 91.17% 78.85%</cell><cell>94.95%</cell><cell>92.49%</cell></row><row><cell cols="3">Zhou et al.[87] 94.10% 85.16%</cell><cell>96.46%</cell><cell>95.28%</cell></row><row><cell>Liu et al.[24]</cell><cell cols="2">97.55% 83.43%</cell><cell>94.37%</cell><cell>95.46%</cell></row><row><cell>Lin et al.[16]</cell><cell cols="2">95.77% 88.31%</cell><cell>98.26%</cell><cell>96.71%</cell></row><row><cell>RTNet</cell><cell cols="2">95.85% 90.08%</cell><cell>98.55%</cell><cell>97.11%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-art methods on LFW-PL. F1 scores for each region and the overall pixel accuracy are reported.</figDesc><table><row><cell>Measurement</cell><cell cols="4">FCN Deeplabv3+ SPNet Ours</cell></row><row><cell>Params (M)</cell><cell>32.95</cell><cell>39.64</cell><cell cols="2">39.13 27.29</cell></row><row><cell>FLOPS (GMac)</cell><cell>26.55</cell><cell>31.39</cell><cell cols="2">29.60 21.99</cell></row><row><cell>Inference Time (ms)</cell><cell>54</cell><cell>74</cell><cell>63</cell><cell>70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Efficiency comparison between four methods: FCN, Deeplabv3+, SPNet and ours. Input images are of size (512, 512, 3). Models are profiled on the same machine and values are the mean of 100 runs. Lower values indicate better efficiency. Our model is more efficient in the number of parameters and FLOPS. M stands for Million, GMac for Giga Multiply-accumulate operations, ms for milliseconds.applicable to other face analysis tasks, where the heuristic pre-processing steps, such as cropping with bounding boxes and rotation correction with landmarks, are unavoidable.</figDesc><table><row><cell>Methods</cell><cell cols="7">Eyes Brows Nose I-mouth U-lip L-lip Mouth</cell><cell>Inner parts</cell><cell>Facial skin</cell><cell>Hair</cell><cell>Foreground mean</cell></row><row><cell cols="2">Smith et al.[26] 78.5</cell><cell>72.2</cell><cell>92.2</cell><cell>71.3</cell><cell>65.1</cell><cell>70.0</cell><cell>85.7</cell><cell>80.4</cell><cell>88.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhou et al.[88]</cell><cell>87.4</cell><cell>81.3</cell><cell>95.0</cell><cell>83.6</cell><cell>75.4</cell><cell>80.9</cell><cell>92.6</cell><cell>87.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al.[18]</cell><cell>76.8</cell><cell>71.3</cell><cell>90.9</cell><cell>80.8</cell><cell>62.3</cell><cell>69.4</cell><cell>84.1</cell><cell>84.7</cell><cell>91.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Liu et al.[24]</cell><cell>86.8</cell><cell>77.0</cell><cell>93.0</cell><cell>79.2</cell><cell>74.3</cell><cell>81.7</cell><cell>89.1</cell><cell>88.6</cell><cell>92.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Wei et al.[23]</cell><cell>84.7</cell><cell>78.6</cell><cell>93.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.5</cell><cell>90.2</cell><cell>91.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Wei et al.[20]</cell><cell>89.0</cell><cell>82.6</cell><cell>95.2</cell><cell>86.7</cell><cell>80.0</cell><cell>86.4</cell><cell>93.6</cell><cell>91.5</cell><cell>91.5</cell><cell>-</cell><cell></cell></row><row><cell>Lin et al.[16]</cell><cell>89.6</cell><cell>83.1</cell><cell>95.6</cell><cell>86.7</cell><cell>79.6</cell><cell>89.8</cell><cell>95.0</cell><cell>92.4</cell><cell>94.5</cell><cell>83.5</cell><cell>88.6</cell></row><row><cell>RTNet</cell><cell>89.3</cell><cell>84.9</cell><cell>94.9</cell><cell>89.9</cell><cell>94.1</cell><cell>90.9</cell><cell>95.6</cell><cell>92.7</cell><cell>96.2</cell><cell>90.6</cell><cell>91.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Comparison with state-of-the-art methods on Helen. F1 scores are reported in percentage. Bold values are for the best results.</figDesc><table><row><cell>Methods</cell><cell cols="8">L-Eye R-Eye U-lip I-mouth L-lip Nose L-Brow R-Brow Skin Hair Mean</cell></row><row><cell>Zhao et al. [56]</cell><cell>86.3</cell><cell>86.0</cell><cell>83.6</cell><cell>86.9</cell><cell>84.7 94.8</cell><cell>86.8</cell><cell>86.9</cell><cell>93.5 94.1 88.4</cell></row><row><cell>Liu et al. [29]</cell><cell>88.1</cell><cell>88.0</cell><cell>84.4</cell><cell>87.6</cell><cell>85.7 95.5</cell><cell>87.7</cell><cell>87.6</cell><cell>97.2 96.3 89.8</cell></row><row><cell>Te et al. [35]</cell><cell>89.5</cell><cell>90.0</cell><cell>88.1</cell><cell>90.0</cell><cell>89.0 97.1</cell><cell>86.5</cell><cell>87.0</cell><cell>97.3 96.2 91.1</cell></row><row><cell>Ours</cell><cell>91.5</cell><cell>90.9</cell><cell>88.7</cell><cell>90.5</cell><cell>90.5 96.9</cell><cell>90.1</cell><cell>89.1</cell><cell>97.8 96.5 92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Comparison with state-of-the-art methods on the LaPa benchmark. F1 scores are reported in percentage.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://git.io/JOrvm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>All datasets used in the experiments were obtained by, and all training, testing, and ablation studies have been conducted at, Department of Computing, Imperial College London, UK.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching thermal to visible face images using a semantic-guided generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face segmentor-enhanced deep feature learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biometrics, Behavior, and Identity Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="223" to="237" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beauty emakeup: A deep makeup transfer system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="701" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Tran</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fsgan: Subject agnostic face swapping and reenactment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On hallucinating context and background pixels from a face mask using multi-scale gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesis of high-quality visible faces from polarimetric thermal faces using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Riggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="845" to="862" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Puppeteergan: Arbitrary portrait animation with semantic-aware appearance transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving facial attribute prediction using semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6942" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On symbiosis of attribute prediction and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-task framework for facial attributes classification through end-to-end face parsing and deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">328</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tcminet: Face parsing for traditional chinese medicine inspection via a hybrid neural network with context aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93069" to="93082" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Face parsing with roi tanh-warping</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2480" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-objective convolutional learning for face labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards Learning Structure via Consensus for Face Segmentation and Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mathai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate facial image parsing at real-time speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="4659" to="4670" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A cnn cascade for landmark guided semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Residual encoder decoder network and adaptive prior for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning adaptive receptive fields for deep image parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3947" to="3955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face parsing via recurrent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Sifei Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00525</idno>
		<idno>doi:10.1109/ CVPR42600.2020.00525</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5202" to="5211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exemplarbased face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Augmenting crfs with boltzmann machine shape priors for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new dataset and boundary-attention semantic segmentation for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11637" to="11644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learned-Miller, Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 07-49</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
	<note>Group equivariant convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge-aware graph representation learning and reasoning for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="258" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation, International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Labelfaces: Parsing facial features by multiclass labeling with an epitome prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J D</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2481" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint adaptive colour modelling and skin, hair and clothing segmentation using coherent probabilistic index maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection and analysis of hair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1164" to="1169" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Markov random field models for hair and face segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sumengen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Gokturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interlinked convolutional neural networks for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">End-to-end semantic face segmentation with conditional random fields as convolutional, recurrent and adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Güçlü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Güçlütürk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Lier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Van Gerven</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03305</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shape constrained network for eye segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1952" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Face mask extraction in video sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic face video segmentation via reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition, International Conference on Machine Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03708</idno>
		<title level="m">Dilated convolutions with lateral inhibitions for semantic image segmentation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Strip Pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3141" to="3149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Asymmetric nonlocal neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Searching for efficient multiscale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>N. Navab, J. Hornegger, W. M. Wells, A. F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Convolutional networks for biomedical image segmentation</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05566</idno>
		<title level="m">Image segmentation using deep learning: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scale invariant face detection method using higher-order local autocorrelation features extracted from log-polar image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>Third IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A new log-polar mapping for space variant imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="865" to="875" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Application to face detection and tracking</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Facial recognition using partial Log-Polar transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/SICE International Symposium on System Integration (SII)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A fast local descriptor for dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Z</forename><surname>Carlos Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<title level="m">Polar transformer networks, International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Beyond cartesian representations for local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Suppressing uncertainties for large-scale facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Equivariant Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The canonical coordinates method for pattern deformation: theoretical and computational considerations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Segman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1171" to="1183" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alché-Buc, E. Fox, R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03736</idno>
		<title level="m">Face parsing via a fullyconvolutional continuous crf neural network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<editor>V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
